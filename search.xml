<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Google從Android 15 (Android-V)開始將淘汰NNAPI改推LiteRT (TensorFlow Lite)</title>
      <link href="/posts/666bbf67/"/>
      <url>/posts/666bbf67/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在幾年前的文章中曾經介紹過關於如何使用NNAPI來在設備上運行AI模型</p><ul><li>延伸閱讀: <a href="https://meetonfriday.com/posts/668f0de1/">Neural Networks API Introduction: Android APP背後是如何執行一個神經網路模型的?</a></li></ul><p>在Google在最新的<a href="https://developer.android.com/ndk/guides/neuralnetworks" target="_blank" rel="noopener">NNAPI文檔</a>中宣布隨著Android 15(V)的推出即將淘汰NNAPI，未來將主推LiteRT (TensorFlow Lite)。本文摘要整理相關資料並描述Google這麼做的原因為何。</p><h2 id="NNAPI-will-be-Deprecated-with-the-release-of-Android-15-Android-V"><a href="#NNAPI-will-be-Deprecated-with-the-release-of-Android-15-Android-V" class="headerlink" title="NNAPI will be Deprecated with the release of Android 15 (Android-V)"></a>NNAPI will be Deprecated with the release of Android 15 (Android-V)</h2><p>根據NNAPI的google最新document描述到:</p><blockquote><p>Warning: With the release of Android 15, NNAPI will be deprecated. While you can continue to use NNAPI, we expect the majority of devices in the future to use the CPU backend, and therefore for performance critical workloads, we recommend migrating to alternative solutions, for example the TF Lite GPU runtime.</p><p>For more information, see the <a href="https://developer.android.com/ndk/guides/neuralnetworks/migration-guide" target="_blank" rel="noopener">NNAPI Migration Guide</a>.</p></blockquote><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1741587911/blog_posts_2025/nnapi_is_decrepted_iifmh6.png" alt=""></p><p>而在NNAPI migration guide中則說明了原因:</p><ol><li>自從NNAPI release後, 由於ODML (On Device Machine Learning)的快革新, 如transformer, diffusion等model的快速進版, 開發者需要頻繁的更新基礎工具和開發工具</li><li>為了滿足這些需求, Google提供了TensorFlow Lite (LiteRT) in Play Services, 為這些AI模型提供了可更新的TensorFlow runtime服務</li></ol><p>並且提及<strong>在Android 15 (Android-V)後, NNAPI將被標記為棄用(deprecated)並建議開發者改採用TensorFlow Lite (LiteRT) in Play Services</strong></p><h2 id="LiteRT-TensorFlow-Lite-Runtime"><a href="#LiteRT-TensorFlow-Lite-Runtime" class="headerlink" title="LiteRT = TensorFlow Lite Runtime"></a>LiteRT = TensorFlow Lite Runtime</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1741590305/blog_posts_2025/litert_rtrlrj.png" alt=""><br>LiteRT (TensorFlow Lite RunTime) 其實就是大家熟悉的TensorFlow Lite, 只是改名了</p><a id="more"></a><p>LiteRT，或TensorFlow Lite，是Google專為設備上的AI設計的執行環境。支援將常用的AI訓練框架 (TensorFlow, Pytorch, JAX…)轉換成tflite格式並執行。</p><p>主要的一些key feature有:</p><ol><li><strong>Optimized for on-device machine learning</strong>: 針對ODML的一些限制進行了優化<ol><li>Latency: 資料不須來回server的時間</li><li>Privacy: 資料不離開終端設備</li><li>Connectivity: 不須連網</li><li>Size: 模型會被縮小 &amp; 用二進位儲存</li><li>Power consumption: 更有效率的inference</li></ol></li><li><strong>Multi-platform support</strong>: 相容Android, iOS, embedded Linux &amp; MCU等平台</li><li><strong>Multi-framework model options</strong>: 支援將多種模型格式轉換成FlatBuffers格式(.tflite)</li><li><strong>Diverse language support</strong>: 支援多種語言的開發SDK(Java/Kotlin, Swift, Objective-C, C++, Python)</li><li><strong>High performance</strong>: 支援HW delegates, 能夠將AI模型跑在GPU or NPU上</li></ol><h2 id="Hardware-Acceleration-of-LiteRT"><a href="#Hardware-Acceleration-of-LiteRT" class="headerlink" title="Hardware Acceleration of LiteRT"></a>Hardware Acceleration of LiteRT</h2><p>LiteRT的主要用途是用在終端設備，那對於搭載特殊HW可以用來加速AI運算的平台是如何做到加速的?</p><p>在document中有一個section就在針對hardware acceleration這件事提供了幾個topic，下面簡單整理一下。</p><h3 id="LiteRT-Delegates"><a href="#LiteRT-Delegates" class="headerlink" title="LiteRT Delegates"></a>LiteRT Delegates</h3><p>Delegates，中文譯成代理，在LiteRT上的意思是讓平台上的AI acceleration hardware可以被用來做AI運算的加速，如GPU, TPU, NPU…然而，考慮到以下諸多面向，工程師在終端平台上要有效率的應用hardware進行AI運算加速變得棘手:</p><ol><li>儘管預設LiteRT是使用ARM Neon的指令集來加速CPU，但CPU本質上仍然不是適合做AI運算的hardware。</li><li>對於GPU / NPU這種hardware來說，雖然因為硬體特性適合進行AI加速運算，但你可能需要根據其硬體的特性來撰寫相對應的kernel (e.g. OpenCL / OpenGL for GPU)</li><li>再來不同hardware的特性不同，不是所有OP都適合放在同一個hardware上運行，此時要如何進行相關的資源分派就變成一個很困難的問題</li></ol><p><strong>TensorFlow Lite’s Delegate API可以幫助工程師簡化這些問題，他的角色就是做為TFlite runtime &lt;-&gt; low-level APIs之間的橋樑</strong></p><ol><li>如果你是AI developer: 你不用管底層hardware的特性，你只需要透過LiteRT去撰寫你的AI應用，如果vendor有支援對應的hardware delegate，你就可以使用到該硬體的加速性能</li><li>如果你是hardware vendor: 你有自己的NPU，你可以透過相關API根據你hardware的特性實作出OP kernel，然後接上LiteRT給developer使用你的硬體</li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1741598941/blog_posts_2025/tflite_delegate_yutnwf.png" alt=""></p><p>(目前LiteRT官網上支援比較全面的就只有GPU delegate，但針對<a href="./#NPU-delegates">Android平台Qualcomm® AI Engine Direct Delegate也已經Ready</a>，後面會稍微帶到。)</p><h3 id="How-to-Implement-Your-Hardware-Delegrate"><a href="#How-to-Implement-Your-Hardware-Delegrate" class="headerlink" title="How to Implement Your Hardware Delegrate?"></a>How to Implement Your Hardware Delegrate?</h3><p>Document有一頁講到可以如何實作自己的delegate，但什麼時候會需要建立一個custom delegate?</p><ol><li>你想要整合一個新的 ML 推論引擎，而該引擎不受任何現有委派的支持</li><li>你擁有一個自訂硬體加速器 (例如你是hardware vendor)，可以改善已知情況下的執行時間</li><li>你正在開發 CPU 優化（例如OP fusion），可以加速某些模型</li></ol><p>以下圖為例，你可以透過delegate來將AI computing graph的部分sub-graph dispatch到特定的accelerator上進行加速<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1741599377/blog_posts_2025/tflite_delegate_diagram_xhpdjr.png" alt=""></p><h4 id="How-do-delegates-work"><a href="#How-do-delegates-work" class="headerlink" title="How do delegates work?"></a>How do delegates work?</h4><p>假設你原本有一張computing graph如下:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1741600772/blog_posts_2025/delegrate_example_1_xca2qe.png" alt=""></p><p>然後你有一個hardware accelerator (或是你想要做CPU OP fusion)，可以將這兩個OP同時處理，你可以把這兩個OP變成一個delegate node一起處理，於是computing graph就變成了</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1741600772/blog_posts_2025/delegrate_example_2_z1p2gi.png" alt=""></p><p>更多細節可以參考document中的<a href="https://ai.google.dev/edge/litert/performance/implementing_delegate#example" target="_blank" rel="noopener">example</a>告訴你可以怎麼做。</p><p>到此為止，文件中提及了可以怎麼銜接LiteRT跟hardware的low-level implement，至於hardware要如何針對不同的情境, OP做實作就是hardware vendor的工作了。</p><h2 id="Hardware-Acceleration-on-Android"><a href="#Hardware-Acceleration-on-Android" class="headerlink" title="Hardware Acceleration on Android"></a>Hardware Acceleration on Android</h2><h3 id="Acceleration-service"><a href="#Acceleration-service" class="headerlink" title="Acceleration service"></a>Acceleration service</h3><p>對於Android，LiteRT提供了一個名為<strong>Acceleration Service for Android API</strong>，他會在一開始先用你的LiteRT model跑過internal inference benchmarks (x-seconds level)</p><p>只要準備好model, data sample &amp; golden, acceleration service就會幫你跑完並提供你建議使用的hardware，然後你就可以使用該hardware來進行加速</p><ul><li>根據document，目前僅支援GPU / CPU，其他的hardware還在未來的規劃中<ol><li><code>GpuAccelerationConfig</code>: converted to GPU delegate during the execution time</li><li><code>CpuAccelerationConfig</code></li></ol></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1741597893/blog_posts_2025/acceleration_service_blovcq.png" alt=""></p><h3 id="GPU-delegates"><a href="#GPU-delegates" class="headerlink" title="GPU delegates"></a>GPU delegates</h3><p>目前有兩種方式可以使用GPU delegates</p><ol><li><a href="https://ai.google.dev/edge/litert/android/gpu" target="_blank" rel="noopener">Interpreter API</a></li><li><a href="https://ai.google.dev/edge/litert/android/gpu_native" target="_blank" rel="noopener">Native (C/C++) API</a></li></ol><p>有興趣的可以再去進一步了解。</p><h3 id="NPU-delegates"><a href="#NPU-delegates" class="headerlink" title="NPU delegates"></a>NPU delegates</h3><p>根據文件描述，目前只有Qualcomm的AI Engine Direct Delegate有對接上LiteRT。</p><p>Qualcomm的<a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/introduction.html?product=1601111740009302" target="_blank" rel="noopener">AI Engine Direct SDK</a>是一個新提出的SDK，用於針對高通的hardware提供一個unified  API, modular and extensible per-accelerator libraries, which form a reusable basis for full stack AI solutions.</p><ul><li>特地提一下他和Qualcomm的 QPNS (<a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/introduction.html?product=1601111740010412" target="_blank" rel="noopener">Qualcomm Neural Processing SDK</a>)是不同的東西，QNPS是產生一個可移植的.dlc binary file用於在高通平台上運行AI運算。AI Engine Direct SDK則是封裝了QNPS和其他的開發介面提供了一個更完整的AI solutions，下圖可以看到整個AI Engine Direct SDK的software stack</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1741601770/blog_posts_2025/Qcom_ai_engine_direct_SDK_xmeosw.png" alt=""></p><p>其他家廠商預計在未來也會support (coming soon)。</p><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://meetonfriday.com/posts/668f0de1/">Neural Networks API Introduction: Android APP背後是如何執行一個神經網路模型的?</a></li><li><a href="https://ai.google.dev/edge/litert#next-steps" target="_blank" rel="noopener">LiteRT overview</a></li><li><a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/introduction.html?product=1601111740009302" target="_blank" rel="noopener">Qualcomm AI Engine Direct SDK</a></li><li><a href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/introduction.html?product=1601111740010412" target="_blank" rel="noopener">Qualcomm Neural Processing SDK for AI</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[課程筆記]Linux Driver正點原子課程筆記5 - 新字元設備驅動實驗</title>
      <link href="/posts/edcd30e6/"/>
      <url>/posts/edcd30e6/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多課程筆記，至<a href="https://meetonfriday.com/posts/85f1c2a/">[課程筆記]課程筆記系列總覽</a>可以看到目前已發布的所有文章！〗</p><h2 id="Course-5-新字元設備驅動實驗"><a href="#Course-5-新字元設備驅動實驗" class="headerlink" title="Course 5 - 新字元設備驅動實驗"></a>Course 5 - 新字元設備驅動實驗</h2><h3 id="新字元設備驅動原理"><a href="#新字元設備驅動原理" class="headerlink" title="新字元設備驅動原理"></a>新字元設備驅動原理</h3><p>在之前的課程中了解到了怎麼註冊一個chrdev，但之前使用的api是舊版的存在著一些問題，例如<code>register_chrdev()</code>這個function</p><ol><li><strong>會佔用主設備號下的所有次設備號</strong>，浪費了很多次設備號</li><li>必須手動指定主設備號，也就是說你得先手動查詢哪個主設備號還沒被佔用，然後再去指定</li></ol><p>所以這節課程會使用新的api來撰寫字元驅動，新版的api有兩種方式:</p><ul><li><code>alloc_chrdev_region()</code>: 向linux kernel申請設備號<ul><li>給定次設備號和名字，告訴linux要申請幾個設備號，讓linux直接幫你尋找主設備號</li></ul></li><li><code>register_chrdev_region()</code>: 給定主設備號去向linux申請<ul><li>給定次設備號為0的devid，讓linux自己去找該主設備下可使用的次設備號</li></ul></li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* fs/char_dev.c */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">alloc_chrdev_region</span><span class="params">(<span class="keyword">dev_t</span> *dev, <span class="keyword">unsigned</span> baseminor, <span class="keyword">unsigned</span> count,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">const</span> <span class="keyword">char</span> *name)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">register_chrdev_region</span><span class="params">(<span class="keyword">dev_t</span> from, <span class="keyword">unsigned</span> count, <span class="keyword">const</span> <span class="keyword">char</span> *name)</span></span>;</span><br></pre></td></tr></table></figure><p>不論是哪種申請方式，釋放時都是call <code>unregister_chrdev_region()</code></p><a id="more"></a><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">unregister_chrdev_region</span><span class="params">(<span class="keyword">dev_t</span> from, <span class="keyword">unsigned</span> count)</span></span>;</span><br></pre></td></tr></table></figure><p>一般來說在設計時兩種情況都會考慮到，一個範例code如下:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> major, minor;</span><br><span class="line"><span class="keyword">dev_t</span> devid;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (major) &#123;</span><br><span class="line">    devid = MKDEV(major, <span class="number">0</span>);</span><br><span class="line">    register_chrdev_region(devid, <span class="number">1</span>, <span class="string">"test"</span>);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    alloc_chrdev_region(&amp;devid, <span class="number">0</span>, <span class="number">1</span>, <span class="string">"test"</span>);</span><br><span class="line">    major = MAJOR(devid);</span><br><span class="line">    minor = MINOR(devid);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="新字元設備註冊方式"><a href="#新字元設備註冊方式" class="headerlink" title="新字元設備註冊方式"></a>新字元設備註冊方式</h3><p>介紹一個新的char structure: cdev</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* include/linux/cdev.h */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">cdev</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">kobject</span> <span class="title">kobj</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">module</span> *<span class="title">owner</span>;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">file_operations</span> *<span class="title">ops</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">list</span>;</span></span><br><span class="line">    <span class="keyword">dev_t</span> dev;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> count;</span><br><span class="line">&#125; __randomize_layout;</span><br></pre></td></tr></table></figure><p>使用cdev表示char device，然後使用<code>cdev_init()</code>初始化結構體，然後透過<code>cdev_add()</code>進行添加到linux kernel中。最後，註銷時使用<code>cdev_del()</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* fs/char_dev.c */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cdev_init</span><span class="params">(struct cdev *cdev, <span class="keyword">const</span> struct file_operations *fops)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">memset</span>(cdev, <span class="number">0</span>, <span class="keyword">sizeof</span> *cdev);</span><br><span class="line">    INIT_LIST_HEAD(&amp;cdev-&gt;<span class="built_in">list</span>);</span><br><span class="line">    kobject_init(&amp;cdev-&gt;kobj, &amp;ktype_cdev_default);</span><br><span class="line">    cdev-&gt;ops = fops;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cdev_add</span><span class="params">(struct cdev *p, <span class="keyword">dev_t</span> dev, <span class="keyword">unsigned</span> count)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> error;</span><br><span class="line"></span><br><span class="line">    p-&gt;dev = dev;</span><br><span class="line">    p-&gt;count = count;</span><br><span class="line"></span><br><span class="line">    error = kobj_map(cdev_map, dev, count, <span class="literal">NULL</span>,</span><br><span class="line">             exact_match, exact_lock, p);</span><br><span class="line">    <span class="keyword">if</span> (error)</span><br><span class="line">        <span class="keyword">return</span> error;</span><br><span class="line"></span><br><span class="line">    kobject_get(p-&gt;kobj.parent);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cdev_del</span><span class="params">(struct cdev *p)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cdev_unmap(p-&gt;dev, p-&gt;count);</span><br><span class="line">    kobject_put(&amp;p-&gt;kobj);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果回頭去trace <code>register_chrdev()</code>的code，其實他背後的本質就是上述這些flow，既然這樣為何不用<code>register_chrdev()</code>就好？</p><ul><li>還記得一開始提到<code>register_chrdev()</code>的缺點嗎？他會佔用主設備號下的所有次設備號</li></ul><h3 id="自動創建設備節點"><a href="#自動創建設備節點" class="headerlink" title="自動創建設備節點"></a>自動創建設備節點</h3><p>到這裡還有另一個需要探討的議題: </p><p>driver儘管寫完了，但目前為止每次都仍要透過<code>mknod</code>來創建設備節點，例如<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">0</span><br></pre></td><td class="code"><pre><span class="line">mknod /dev/newchrled c 249 0</span><br></pre></td></tr></table></figure></p><p>接下來將介紹如何自動創建設備節點</p><h4 id="udev"><a href="#udev" class="headerlink" title="udev"></a>udev</h4><p>udev是Linux kernel 2.6系列引入的設備管理器，主要功能是管理/dev目錄下的設備節點，在熱插拔機制下，添加/刪除硬體時需要負責處理/dev目錄以及所有user space的行為。</p><p>傳統的建立設備節點需要手動添加，透過使用<code>mknod</code>方法，但這樣並不易管理，比方說可能有設備根本沒掛載，但他仍然再/dev下存在著設備節點。</p><p>採用udev的方法的話，只有備kernel檢測到的設備才會去添加設備節點。因為這些設備節點在每次系統啟動時被創建，他們會備存在ramfs(一個內存中的文件系統，不占用任何空間)，設備節點不會占用大量的磁碟空間，因此使用的memory可以忽略。</p><h4 id="mdev"><a href="#mdev" class="headerlink" title="mdev"></a>mdev</h4><p>在使用busybox建構rootfs的時候，busybox會創建一個udev的簡化版本-mdev的簡化版本 - mdev，在嵌入式系統一般都是用mdev來管理設備節點的自動插入和刪除。</p><p>在/etc/init.d/rcS中可以看到如下的設置</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &#x2F;sbin&#x2F;mdev &gt; &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;hotplug</span><br></pre></td></tr></table></figure><p>上述命令設置熱插拔事件由mdev來管理，所以接下來來看一下如何用mdev來實現設備節點的新增和刪除。</p><p>自動創建設備節點是在驅動的entry function完成的，一般會在<code>cdev_add</code>後面加入相關代碼。</p><p>首先需要<strong>創建一個class</strong>，class被定義在include/linux/device.h內</p><h4 id="創建-註銷class"><a href="#創建-註銷class" class="headerlink" title="創建/註銷class"></a>創建/註銷class</h4><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* include/linux/device.h */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">class</span> &#123;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span>*name;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">module</span>*<span class="title">owner</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">attribute_group</span>**<span class="title">class_groups</span>;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">attribute_group</span>**<span class="title">dev_groups</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">kobject</span>*<span class="title">dev_kobj</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> (*dev_uevent)(struct device *dev, struct kobj_uevent_env *env);</span><br><span class="line">    <span class="keyword">char</span> *(*devnode)(struct device *dev, <span class="keyword">umode_t</span> *mode);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span> (*class_release)(struct class *class);</span><br><span class="line">    <span class="keyword">void</span> (*dev_release)(struct device *dev);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> (*shutdown_pre)(struct device *dev);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">kobj_ns_type_operations</span> *<span class="title">ns_type</span>;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">void</span> *(*<span class="keyword">namespace</span>)(struct device *dev);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span> (*get_ownership)(struct device *dev, <span class="keyword">kuid_t</span> *uid, <span class="keyword">kgid_t</span> *gid);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">dev_pm_ops</span> *<span class="title">pm</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">subsys_private</span> *<span class="title">p</span>;</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>使用時需要<code>#include &lt;linux/device.h&gt;</code>透過<code>class_create</code>來自動創建class</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* include/linux/device.h */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* This is a #define to keep the compiler from merging different</span></span><br><span class="line"><span class="comment"> * instances of the __key variable */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> class_create(owner, name)\</span></span><br><span class="line">(&#123;\</span><br><span class="line">    <span class="keyword">static</span> struct lock_class_key __key;\</span><br><span class="line">    __class_create(owner, name, &amp;__key);\</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><ul><li>第一個參數是owner, 通常都帶THIS_MODULE</li><li>第二個參數帶入driver name</li></ul><p>而註銷class的時候則是透過<code>class_destroy</code></p><ul><li>帶入class參數</li></ul><h4 id="創建-註銷設備"><a href="#創建-註銷設備" class="headerlink" title="創建/註銷設備"></a>創建/註銷設備</h4><p>有了創建/註銷class後，需要在該class下創建一個設備，使用<code>device_create</code>來創立</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* include/linux/device.h */</span></span><br><span class="line"><span class="function">struct device *<span class="title">device_create</span><span class="params">(struct class *cls, struct device *parent,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">dev_t</span> devt, <span class="keyword">void</span> *drvdata,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">const</span> <span class="keyword">char</span> *fmt, ...)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><code>device_create</code>是一個不定參數的function</li><li>class代表設備要建立在哪個class下面</li><li>parent代表父設備, 一般為NULL</li><li>devt是設備號</li><li>drvdata是設備可能會使用到的一些數據, 一般為NULL</li><li>fmt是設備名稱, 如果fmt=xxx的話就會生成/dev/xxx文件</li></ul><p>同樣的，卸載驅動的時候也要註銷設備，透過<code>device_destroy</code>來註銷</p><h3 id="文件私有數據和goto對錯誤的處理"><a href="#文件私有數據和goto對錯誤的處理" class="headerlink" title="文件私有數據和goto對錯誤的處理"></a>文件私有數據和goto對錯誤的處理</h3><p>driver常常會用到一些私有的數據，好的設計方式是透過structure把他們都包起來</p><p>在<code>struct file</code>中其實有個private_data的變數可以用來將需要的data struct存起來，如此就不用在code中另外宣告需要的變數</p><ul><li>通常會在<code>open()</code>內設置filp-&gt;private_data，然後在其他function來存取私有數據</li><li><code>release()</code>的時候會做對應的釋放</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* include/linux/fs.h */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">file</span> &#123;</span></span><br><span class="line">    <span class="keyword">union</span> &#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">llist_node</span><span class="title">fu_llist</span>;</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">rcu_head</span> <span class="title">fu_rcuhead</span>;</span></span><br><span class="line">    &#125; f_u;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">path</span><span class="title">f_path</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">inode</span>*<span class="title">f_inode</span>;</span><span class="comment">/* cached value */</span></span><br><span class="line">    <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">file_operations</span>*<span class="title">f_op</span>;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Protects f_ep_links, f_flags.</span></span><br><span class="line"><span class="comment"> * Must not be taken from IRQ context.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="keyword">spinlock_t</span>f_lock;</span><br><span class="line">    <span class="keyword">enum</span> rw_hintf_write_hint;</span><br><span class="line">    <span class="keyword">atomic_long_t</span>f_count;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> f_flags;</span><br><span class="line">    <span class="keyword">fmode_t</span>f_mode;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">mutex</span><span class="title">f_pos_lock</span>;</span></span><br><span class="line">    <span class="keyword">loff_t</span>f_pos;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">fown_struct</span><span class="title">f_owner</span>;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">cred</span>*<span class="title">f_cred</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">file_ra_state</span><span class="title">f_ra</span>;</span></span><br><span class="line"></span><br><span class="line">    u64f_version;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> CONFIG_SECURITY</span></span><br><span class="line">    <span class="keyword">void</span>*f_security;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    <span class="comment">/* needed for tty driver, and maybe others */</span></span><br><span class="line">    <span class="keyword">void</span>*private_data;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> CONFIG_EPOLL</span></span><br><span class="line">    <span class="comment">/* Used by fs/eventpoll.c to link all the hooks to this file */</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span><span class="title">f_ep_links</span>;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span><span class="title">f_tfile_llink</span>;</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span> <span class="comment">/* #ifdef CONFIG_EPOLL */</span></span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">address_space</span>*<span class="title">f_mapping</span>;</span></span><br><span class="line">    <span class="keyword">errseq_t</span>f_wb_err;</span><br><span class="line">&#125; __randomize_layout</span><br><span class="line">  __attribute__((aligned(<span class="number">4</span>)));<span class="comment">/* lest something weird decides that 2 is OK */</span></span><br></pre></td></tr></table></figure><p>最後來講講linux driver在init過程中如果發生錯誤時，我們需要將我們init的東西都釋放掉，這時我們常常會透過<code>goto</code>來做流程的控制，透過好的goto流程設計可以讓我們的driver code有更好的可讀性以及程式架構。</p><p>了解了上述的所有內容後，最後來重寫之前寫過的led driver code</p><ul><li>下方的code還不包含init錯誤時goto的處理</li></ul><h4 id="driver"><a href="#driver" class="headerlink" title="driver"></a>driver</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/module.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/kernel.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/init.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/fs.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/slab.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/uaccess.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/io.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/cdev.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/device.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LED_MAJOR 200</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LED_NAME <span class="meta-string">"led"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* register PA */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CCM_CCGR1_BASE         (0x020C406C)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SW_MUX_GPIO1_IO03_BASE (0x020E0068)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SW_PAD_GPIO1_IO03_BASE (0x020E02F4)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> GPIO1_DR_BASE          (0x0209C000)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> GPIO1_GDIR_BASE        (0x0209C004)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* pointer of VA */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __iomem *IMX6U_CCM_CCGR1;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __iomem *SW_MUX_GPIO1_IO03;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __iomem *SW_PAD_GPIO1_IO03;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __iomem *GPIO1_DR;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __iomem *GPIO1_GDIR;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LEDOFF 0</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LEDON 1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> NEWCHRLED_NAME <span class="meta-string">"newchrled"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> NEWCHRLED_CNT 1</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">newchrled_dev</span>&#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">cdev</span> <span class="title">cdev</span>;</span></span><br><span class="line">    <span class="keyword">dev_t</span> devid; <span class="comment">// device num</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">class</span> *<span class="title">class</span>;</span> <span class="comment">// class </span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">device</span> *<span class="title">device</span>;</span> <span class="comment">// device</span></span><br><span class="line">    <span class="keyword">int</span> major;</span><br><span class="line">    <span class="keyword">int</span> minor;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">led_switch</span><span class="params">(u8 sta)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    u32 val = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (databuf[<span class="number">0</span>] == LEDOFF) &#123;</span><br><span class="line">        val = readl(GPIO1_DR);</span><br><span class="line">        val |= ~(<span class="number">1</span> &lt;&lt; <span class="number">3</span>);</span><br><span class="line">        writel(val, GPIO1_DR); <span class="comment">// close led</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        val = readl(GPIO1_DR);</span><br><span class="line">        val &amp;= ~(<span class="number">1</span> &lt;&lt; <span class="number">3</span>);</span><br><span class="line">        writel(val, GPIO1_DR); <span class="comment">// open led</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">newchrled_open</span><span class="params">(struct inode *inode, struct file *filp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    filp-&gt;private_data = &amp;newchrled;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">newchrled_release</span><span class="params">(struct inode *inode, struct file *filp)</span></span></span><br><span class="line"><span class="function"></span>&#123;   </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">newchrled_dev</span> *<span class="title">dev</span> = (<span class="title">struct</span> <span class="title">newchrled_dev</span>)<span class="title">filp</span>-&gt;<span class="title">private_data</span>;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">ssize_t</span> <span class="title">newchrled_write</span><span class="params">(struct file *filp, <span class="keyword">const</span> <span class="keyword">char</span> __user *buf, <span class="keyword">size_t</span> count, <span class="keyword">loff_t</span> *ppos)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret;</span><br><span class="line">    <span class="keyword">int</span> val = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> databuf[<span class="number">1</span>];</span><br><span class="line">    ret = copy_from_user(databuf, buf, count);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>)&#123;</span><br><span class="line">        printk(<span class="string">"kernel write fail\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> -EFAULT;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    led_switch(databuf[<span class="number">0</span>]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">newchrled_dev</span> <span class="title">newchrled</span>;</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">file_operations</span> <span class="title">newchrled_fops</span> = &#123;</span></span><br><span class="line">    .owner = THIS_MODULE,</span><br><span class="line">    .<span class="built_in">write</span> = newchrled_write,</span><br><span class="line">    .<span class="built_in">open</span>  = newchrled_open,</span><br><span class="line">    .<span class="built_in">release</span> = newchrled_release,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> __init <span class="title">newchrled_init</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> val = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 1. init led */</span></span><br><span class="line">    IMX6U_CCM_CCGR1 = ioremap(CCM_CCGR1_BASE, <span class="number">4</span>);</span><br><span class="line">    SW_MUX_GPIO1_IO03 = ioremap(SW_MUX_GPIO1_IO03_BASE, <span class="number">4</span>);</span><br><span class="line">    SW_PAD_GPIO1_IO03 = ioremap(SW_PAD_GPIO1_IO03_BASE, <span class="number">4</span>);</span><br><span class="line">    GPIO1_DR = ioremap(GPIO1_DR_BASE, <span class="number">4</span>);</span><br><span class="line">    GPIO1_GDIR = ioremap(GPIO1_GDIR_BASE, <span class="number">4</span>);</span><br><span class="line">    </span><br><span class="line">    val = readl(IMX6U_CCM_CCGR1);</span><br><span class="line">    val &amp;= ~(<span class="number">3</span> &lt;&lt; <span class="number">26</span>); <span class="comment">// clr IMX6U_CCM_CCGR1[26:27]</span></span><br><span class="line">    val |= <span class="number">3</span> &lt;&lt; <span class="number">26</span>; <span class="comment">// set IMX6U_CCM_CCGR1[26:27] to 1</span></span><br><span class="line">    writel(val, IMX6U_CCM_CCGR1);</span><br><span class="line">    </span><br><span class="line">    writel(<span class="number">0x5</span>, SW_MUX_GPIO1_IO03);</span><br><span class="line">    writel(<span class="number">0x10B0</span>, SW_PAD_GPIO1_IO03);</span><br><span class="line">  </span><br><span class="line">    val = readl(GPIO1_GDIR);</span><br><span class="line">    val |= <span class="number">1</span> &lt;&lt; <span class="number">3</span>;</span><br><span class="line">    writel(val, GPIO1_GDIR);</span><br><span class="line">  </span><br><span class="line">    val = readl(GPIO1_DR);</span><br><span class="line">    val &amp;= ~(<span class="number">1</span> &lt;&lt; <span class="number">3</span>);</span><br><span class="line">    writel(val, GPIO1_DR); <span class="comment">// open led    </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 2. register dev number */</span></span><br><span class="line">    <span class="keyword">if</span> (newchrled.major) &#123;</span><br><span class="line">        newchrled.devid = MKDEV(newchrled.major, <span class="number">0</span>);</span><br><span class="line">        register_chrdev_region(newchrled.devid, NEWCHRLED_CNT, NEWCHRLED_NAME);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        ret = alloc_chrdev_region(&amp;newchrled.devid, <span class="number">0</span>, NEWCHRLED_CNT, NEWCHRLED_NAME);</span><br><span class="line">        newchrled.major = MAJOR(newchrled.devid);</span><br><span class="line">        newchrled.minor = MINOR(newchrled.devid);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        printk(<span class="string">"newchrled chr_dev region err\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    printk(<span class="string">"major:%d, minor:%d\n"</span>, newchrled.majorm newchrled.minor);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 3. register chardev */</span></span><br><span class="line">    newchrled.cdev.owner = THIS_MODULE;</span><br><span class="line">    cdev_init(&amp;newchrled.cdev, &amp;newchrled_fops);</span><br><span class="line">    ret = cdev_add(&amp;newchrled.cdev, newchrled.devid, NEWCHRLED_CNT);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 4. auto create device node */</span></span><br><span class="line">    newchrled<span class="class">.<span class="keyword">class</span> = <span class="title">class_create</span>(<span class="title">THIS_MODULE</span>, <span class="title">NEWCHRLED_NAME</span>);</span></span><br><span class="line">    <span class="keyword">if</span> (IS_ERR(newchrled.class))</span><br><span class="line">        <span class="function">reutrn <span class="title">PTR_ERR</span><span class="params">(newchrled.class)</span></span>;</span><br><span class="line">        </span><br><span class="line">    newchrled.device = device_create(newchrled.class, <span class="literal">NULL</span>, newchrled.devid, <span class="literal">NULL</span>, NEWCHRLED_NAME);</span><br><span class="line">    <span class="keyword">if</span> (IS_ERR(newchrled.device))</span><br><span class="line">        <span class="keyword">return</span> PTR_ERR(newchrled.device);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> __exit <span class="title">newchrled_exit</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> val = <span class="number">0</span>;</span><br><span class="line">    val = readl(GPIO1_DR);</span><br><span class="line">    val |= ~(<span class="number">1</span> &lt;&lt; <span class="number">3</span>);</span><br><span class="line">    writel(val, GPIO1_DR); <span class="comment">// close led</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 1. unmap va */</span></span><br><span class="line">    iounmap(IMX6U_CCM_CCGR1);</span><br><span class="line">    iounmap(SW_MUX_GPIO1_IO03);</span><br><span class="line">    iounmap(SW_PAD_GPIO1_IO03);</span><br><span class="line">    iounmap(GPIO1_DR);</span><br><span class="line">    iounmap(GPIO1_GDIR);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 2. unregister */</span></span><br><span class="line">    cdev_del(&amp;newchrled.cdev);</span><br><span class="line">    printk(<span class="string">"chrdev unregister\n"</span>);</span><br><span class="line">    </span><br><span class="line">    unregister_chrdev_region(newchrled.devid, NEWCHRLED_CNT);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 3. destroy device */</span></span><br><span class="line">    device_destroy(newchrled.class, newchrled.devid);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 4. dectroy class */</span></span><br><span class="line">    class_destroy(newchrled.class);</span><br><span class="line">    </span><br><span class="line">    reutrn <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">module_init(newchrled_init);</span><br><span class="line">module_exit(newchrled_exit);</span><br><span class="line">MODULE_LICENSE(<span class="string">"GPL"</span>);</span><br><span class="line">MODULE_AUTHOR(<span class="string">"john"</span>);</span><br></pre></td></tr></table></figure><h4 id="app"><a href="#app" class="headerlink" title="app"></a>app</h4><p>app的部分沒有什麼變動，所以就不重寫了，要看的可以看前幾次的課程筆記</p>]]></content>
      
      
      <categories>
          
          <category> 【課程筆記】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux kernel </tag>
            
            <tag> study </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[讀書心得]跟著柴鼠學FQ 做自己的提款機</title>
      <link href="/posts/638f6f64/"/>
      <url>/posts/638f6f64/</url>
      
        <content type="html"><![CDATA[<h2 id="書籍資訊"><a href="#書籍資訊" class="headerlink" title="書籍資訊"></a>書籍資訊</h2><ul><li>書名: 跟著柴鼠學FQ 做自己的提款機</li><li>作者: 柴鼠兄弟  </li><li>出版日期: Mar 26, 2020</li><li>博客來網址: <a href="https://www.books.com.tw/exep/assp.php/john85051232/products/0010852671?utm_source=john85051232&amp;utm_medium=ap-books&amp;utm_content=recommend&amp;utm_campaign=ap-202103" target="_blank" rel="noopener">跟著柴鼠學FQ 做自己的提款機</a></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1616850636/blog_posts_2021/2021_03/E605f2eb43b5ef415191559-0_quus4c.jpg" alt=""></p><p>(本文是【跟著柴鼠學FQ 做自己的提款機】的讀書心得和書籍導讀，如果你看完覺得這本書對你是有幫助的，歡迎<a href="https://www.books.com.tw/exep/assp.php/john85051232/products/0010852671?utm_source=john85051232&amp;utm_medium=ap-books&amp;utm_content=recommend&amp;utm_campaign=ap-202103" target="_blank" rel="noopener">購買原作</a>支持作者！)</p><blockquote><p>你可以把這本書當成是你重獲自由的「投資工具使用說明書」，FQ就像提款機的零件，只有親手組裝自己的提款機，才能成為你收入階層的跳板，讓資產向上提升──<br>．低薪族，靠理財加薪<br>．月光族，存到第一桶金<br>．小資族，變優渥</p><p>本書你將學到：<br>．從0到1最難，喚醒投資理財為什麼重要（讓理財變積極）<br>．把財經術語化繁為簡，讓你成為理財國國民（看懂財經新聞、財報，不再被騙）<br>．投資前，先理財（記帳不再半途而廢，成為好習慣）<br>．破解理財迷思，不再讓錢縮水（抓出理財工具的缺陷BUG，防止隱性虧錢）<br>．建構被動收入的機器（懂得各種被動收入的工具、概念）<br>．比賺錢更困難的事（風險控管，不讓投資成果一夕烏有）</p></blockquote><a id="more"></a><hr><h2 id="讓理財事半功倍的致富觀"><a href="#讓理財事半功倍的致富觀" class="headerlink" title="讓理財事半功倍的致富觀"></a>讓理財事半功倍的致富觀</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_500,h_300/v1616848882/blog_posts_2021/2021_03/scott-graham-5fNmWej4tAA-unsplash_jscjrq.jpg" alt=""></p><h3 id="注意力，是你最值錢的貨幣"><a href="#注意力，是你最值錢的貨幣" class="headerlink" title="注意力，是你最值錢的貨幣"></a>注意力，是你最值錢的貨幣</h3><p>「在2020年代，要學好投資做好理財，當務之急可能不是怎麼管理錢，而是得先學會管理你的注意力！」</p><p>因為注意力正試每個人身上最有商業價值的天然資源，可是卻很少人發現。</p><p>一個人一天到底有多少注意力可以用？假設醒著時我們就有注意力好了，那一天扣掉睡覺的6小時我們還有18小時的注意力。而在網路影音社群普及的現在，頻道有千萬個隨時等著你點擊，媒體對注意力的需求遠遠大過人們的供給，這些訊息只要賺不到注意力就會失去數位價值，很快就會被商業演算法剔除，所以注意力價值飆高，各種媒體為了搶奪你的注意力無所不用其極。</p><p>為什麼大家都想要奪得我們的注意力？ 因為<strong>注意力就是錢！</strong> 當我們點了一則新聞、看了一支影片或者已讀了一條訊息，這些訊息藉由注意力進入你的腦中，默默影響著你的各種行為，例如消費行為、投資行為、投票行為。</p><p>舉個例子，我們花錢購買東西，其實只是注意力被提領的一種形式。生活中的各種行為背後往往都是無數個訊息所左右著，你在潛移默化中吸收的微小資訊可能就是造成你做出某種決策的主因。</p><p>既然我們說注意力是一種貨幣，當然也可以拿來做投資，把注意力用來累積對未來有幫助的東西，它將會回報給將來的自己。也因此<strong>培養將注意力花在知識、技術、健康的習慣變得額外重要</strong>。</p><h3 id="除了IQ和EQ，你還需要FQ"><a href="#除了IQ和EQ，你還需要FQ" class="headerlink" title="除了IQ和EQ，你還需要FQ"></a>除了IQ和EQ，你還需要FQ</h3><p>IQ(Intelligence Quotient)智商，用來衡量一個人的智力程度；</p><p>EQ(Emotional Quotient)情商，用來量化一個人對自我情緒管理的能力；</p><p>而<strong>FQ，Financial Quotient</strong>，廣義來說則是代表一個人對金錢的整體價值觀和創造財富的能力，狹義的可以說是對金融世界的各種商品、工具和運作邏輯的根本認識。</p><p>比起IQ和EQ，FQ更容易藉由後天練習而累積，也是一輩子受用無窮的能力。因為當我們對知識了解的越深入，能看到的層次就更豐富，面對困境和變局，才能做出更有品質的決定。</p><h2 id="真正的懶人投資，最基礎的FQ-定存、外幣"><a href="#真正的懶人投資，最基礎的FQ-定存、外幣" class="headerlink" title="真正的懶人投資，最基礎的FQ - 定存、外幣"></a>真正的懶人投資，最基礎的FQ - 定存、外幣</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_500,h_300/v1616849186/blog_posts_2021/2021_03/john-mcarthur-ROQzKIAdY78-unsplash_1_nqslg1.jpg" alt=""></p><h3 id="錢住的地方，從認識存款帳戶開始"><a href="#錢住的地方，從認識存款帳戶開始" class="headerlink" title="錢住的地方，從認識存款帳戶開始"></a>錢住的地方，從認識存款帳戶開始</h3><p>在儲蓄的世界，存款簿只有一種形式，因應不同目的，金融機構提供了多種存款服務，最常見的分別為：</p><ol><li>活存、活儲</li><li>定存、定儲</li></ol><h4 id="活存"><a href="#活存" class="headerlink" title="活存"></a>活存</h4><p>活存又可分為四大類: </p><ol><li>活期存款: 主要提供給自然人(一般個人)、法人(公司機構)開立的帳戶型態</li><li>活期儲蓄存款: 專門提供給自然人、非營利法人(學會、協會、基金會)而設計的存款帳戶，利息會相較活存較高</li><li>支票存款: 透過開立支票付款的一種活存帳戶</li><li>證券活期存款: 用來買賣股票、期貨、ETF…等證券商品的交割帳戶，利息最低</li></ol><h4 id="定存"><a href="#定存" class="headerlink" title="定存"></a>定存</h4><p>常見的定存主要分為兩大類，這邊對於儲蓄的定義和活存相同，但還有些差異:</p><ol><li>定期存款<ol><li>但定存只能單利計算</li><li>約定天數較為靈活，小則1個月大至3年都有</li></ol></li><li>定期儲蓄存款<ol><li>定儲為月配息，可以達到月複利的效果，也就是每個月計算利息後滾入本金成為下個月計算利息的基礎</li><li>約定天數較死板，大部分都1年起跳</li><li>根據儲蓄需求不同又細分3種<ol><li>整存整付: 直接存入一筆錢，月複利計算</li><li>零存整付: 分期每個月存入一筆錢，月複利計算</li><li>存本取息: 存入一筆錢，將每個月的利息都取出來，也因此無法達到複利的效果</li></ol></li></ol></li></ol><p>講了那麼多，哪一種存款帳戶能夠領比較多的利息呢?  書中有做一個簡單易懂的表格，可以幫助讀者快速了解，有興趣的讀者歡迎<a href="https://www.books.com.tw/exep/assp.php/john85051232/products/0010852671?utm_source=john85051232&amp;utm_medium=ap-books&amp;utm_content=recommend&amp;utm_campaign=ap-202103" target="_blank" rel="noopener">購買原作</a>觀看。</p><h3 id="定存利率，不只是報酬率而已"><a href="#定存利率，不只是報酬率而已" class="headerlink" title="定存利率，不只是報酬率而已"></a>定存利率，不只是報酬率而已</h3><p>銀行的定存利率可以在零風險的前提下提供約1%的投資報酬，換句話說，<strong>如果一個投資工具的風險無法像定存一樣低，報酬率也沒贏多少，那還不如閉著眼睛放定存就好。</strong>因此在衡量各種投資商品的效益時，定存便成為一個最容易衡量的標準。</p><p>而央行也會透過升息或降息來調整市場機制:</p><ul><li>比方說景氣好時，市場上流動的金錢就或越來越多，因此就容易造成通貨膨脹，此時央行就可以透過升息來鼓勵人民把金錢存入銀行</li><li>而當景氣差時，大家都不想花錢而想把錢存起來，此時央行就可以透過降息來鼓勵大家把錢拿出去投資</li></ul><p><strong>了解了一個政府如何透過調整利率來影響市場機制後，我們就可以透過利率的調動來觀察一個國家的經濟情況，甚至與全球市場的相互關係。</strong></p><h3 id="投資外幣，最重要的三件事"><a href="#投資外幣，最重要的三件事" class="headerlink" title="投資外幣，最重要的三件事"></a>投資外幣，最重要的三件事</h3><p>首先，要能夠看得懂匯率的四個欄位：現金/即期、買入/賣出</p><ul><li>現金/即期: 就是指你在兌換的時候是現鈔或非現金交易(例如只把帳戶內的錢進行兌換)，通常即期的匯率比現金好，因為現金還要考慮到手續費、保管費等過程</li><li>買入/賣出: 這是讓我最容易搞混的，不過書中提到一個很簡單的記憶法: <strong>買入和賣出都是以對方的角度來看</strong><ul><li>比方說美金的買入價是29元，代表對方要用29元新台幣和你購買1美元</li><li>而當美金的賣出價是30.385元，則代表對方要用30.385新台幣賣出1美元給你</li></ul></li></ul><p>從上面的例子來看，就可以發現賣出價比買入價還高對吧？</p><p>也就是說，如果你馬上用30.385買入一美金，然後馬上賣出，你就會瞬間虧損30.385 - 29 的價差。所以這一來以往之間的交易是會折損貨幣的，除非你兌換的匯率價差大於這中間的虧損你才能夠賺到錢。</p><h3 id="搞懂美元的匯率走勢，降低投資風險"><a href="#搞懂美元的匯率走勢，降低投資風險" class="headerlink" title="搞懂美元的匯率走勢，降低投資風險"></a>搞懂美元的匯率走勢，降低投資風險</h3><p>國際上常用「需要多少貨幣兌換1美元」來代表該幣匯率的升貶</p><ul><li>該幣貶值: 當數字越大，代表要用越多的錢才能換到1美元</li><li>該幣升值: 當數字越小，代表可以用更少的錢兌換到1美元</li></ul><p>為何要了解美元的匯率走勢？</p><p>因為美元不只是國際貨幣的兌換基準，許多原物料如黃金、石油都是用美元計價，所以了解美元的走勢能夠幫助我們更快了解國際情勢的表現。</p><h2 id="幫財富開外掛，打造被動收入-基金、股票"><a href="#幫財富開外掛，打造被動收入-基金、股票" class="headerlink" title="幫財富開外掛，打造被動收入 - 基金、股票"></a>幫財富開外掛，打造被動收入 - 基金、股票</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_500,h_300/v1616848878/blog_posts_2021/2021_03/jason-briscoe-amLfrL8LGls-unsplash_t2kkpc.jpg" alt=""></p><h3 id="新手從基金開始練習，鍛鍊三種本領"><a href="#新手從基金開始練習，鍛鍊三種本領" class="headerlink" title="新手從基金開始練習，鍛鍊三種本領"></a>新手從基金開始練習，鍛鍊三種本領</h3><p>在書中，作者推薦投資新手可以從基金開始研究，因為基金比起股市相對穩定，但比起定存又能夠得到更好的投報率。</p><p>此外，和其他工具相比，更能從中獲得三種本領：</p><ol><li>長期投資: 基金的持有都屬於大範圍的標的，反應特地區域或商品的趨勢，也因此性質偏向長期持有，培養新手的耐心</li><li>投資眼光: 基金種類百百種，每一種的性質都不同，還需要兼顧趨勢跟考量風險，也因此很適合新手學習</li><li>國際視野: 和上述相同，基金的性質和組成往往都是牽一髮動全身，尤其是一些國外的基金更能夠幫助新手培養國際視野</li></ol><h3 id="股票小學，教你搞懂遊戲規則和基本術語"><a href="#股票小學，教你搞懂遊戲規則和基本術語" class="headerlink" title="股票小學，教你搞懂遊戲規則和基本術語"></a>股票小學，教你搞懂遊戲規則和基本術語</h3><p>在這節中，作者詳盡了介紹了許多股市的基本觀念，包含：</p><ol><li>如何開戶: 首先你需要先和券商開立一個證券戶，才能夠進入股市的世界進行投資，在這之中不同券商的交易手續費和優惠又不盡相同</li><li>交易流程: 介紹委託、搓合、交割等名詞，搞懂這些讓你不會因為違約交割(購買了股票但時間到卻付不出錢)而被罰款，甚至無法再投資股市</li><li>股市的交易時間: 盤前、盤中、盤後</li><li>交易單位: 股市可以分成一張，一張又等於一千股，了解不同的單位才不會買錯，想買100股台積電買成100張xD</li><li>交易價格: 開盤價、收盤價、平盤價是怎麼來的？下單時市價單和限價單傻傻分不清楚？</li><li>手續費</li></ol><p>了解這些概念後你才會有基本的能力來進入股票市場，不然你可能連下單App要怎麼用都看不懂～</p><p>前面一些比較基本的單元就不在這裡著墨了，歡迎<a href="https://www.books.com.tw/exep/assp.php/john85051232/products/0010852671?utm_source=john85051232&amp;utm_medium=ap-books&amp;utm_content=recommend&amp;utm_campaign=ap-202103" target="_blank" rel="noopener">閱讀原作</a>學習！</p><h4 id="交易價格"><a href="#交易價格" class="headerlink" title="交易價格"></a>交易價格</h4><p>在交易價格的部分，台股從2020年3月實施逐筆交易制度，股票買賣的委託類別分成了兩種: 限價單 和 市價單</p><ul><li>限價單: 限定在某個特定的價格才交易。不過券商app為了方便，在限價中又提供了四種取價方式: 現價、漲停、跌停、平盤</li><li>市價單: 不限定要在那個價格交易，由目前市場價位是多少就是多少，優勢是用最大的價格彈性來換取優先成交的機會，但如果當時漲停了你也可能因此買在了相對高點</li></ul><p>除了限價和市價的設定，選項條件又分成三種</p><ul><li><strong>ROD(當日有效)</strong>: 委託的有效性直到當日收盤為止</li><li><strong>IOC(立即成交或取消)</strong>: 不能立即成交的部分就取消。例如委託賣出10張，但目前只成交了3張，剩下的7張就會全部取消，需要重新委託下單</li><li><strong>FOK(全部成交或取消)</strong>: 和IOC不同的方式，需要全部都成交才會成功。例如一筆委託賣出10張，但只成交了3張時，這筆委託就會全部取消，一張都不會幫你賣出</li></ul><h4 id="股款計算"><a href="#股款計算" class="headerlink" title="股款計算"></a>股款計算</h4><p>在股市交易時有兩個費用需要留意:</p><ul><li>券商手續費: 法定是0.1425%，低於20元以20元計算</li><li>證券交易稅: 成交金額的千分之3(ETF是千分之1)，這筆只在賣股票時會跟你扣</li></ul><p>接下來介紹在股市語言裡最常使用的四個文法：基本面、技術面、消息和籌碼面，幫助你了解、判斷一個股票的好壞</p><h3 id="基本面-一家公司存在的基礎條件"><a href="#基本面-一家公司存在的基礎條件" class="headerlink" title="基本面: 一家公司存在的基礎條件"></a>基本面: 一家公司存在的基礎條件</h3><p>基本面的九大指標，可以分別用「做什麼」、「賺什麼」、「算什麼」來幫助理解</p><h4 id="做什麼-股本、市值、產業"><a href="#做什麼-股本、市值、產業" class="headerlink" title="做什麼: 股本、市值、產業"></a>做什麼: 股本、市值、產業</h4><p>股本又稱為資本額，也就是發行股數乘上票卷面額(一般來說票卷面額都是10元)</p><ul><li>當股本越大，代表發行股票數量多，也因此流動性高。換句話，要影響公司的股價就得具備足夠大的資金才能辦到，也比較不會暴漲或暴跌</li><li>而股本小的公司就相對容易被作為炒作的目標</li></ul><p>但股票的成交價會隨著市長波動，所以我們在股市看到的實際上是市價，把市價乘上發行股數就會得到該公司的總市值</p><h4 id="賺什麼-EPS、ROA、ROE"><a href="#賺什麼-EPS、ROA、ROE" class="headerlink" title="賺什麼: EPS、ROA、ROE"></a>賺什麼: EPS、ROA、ROE</h4><p>EPS(Earnings Per Share)，每股稅後盈餘，也有人稱每股純益：平均每一股賺多少錢？</p><p>為什麼需要EPS？每家公司資本有大有小，單位不同如何比較哪家公司比較好呢？</p><p><strong>EPS就是把公司的「稅後淨利」除以這間公司的普通股股票數量，就可以知道這家公司平均每一股繳完賺多少錢，目的是用來計算一家公司的獲利</strong></p><ul><li>根據規定，上市公司每個月都要公佈營收，每季要公布財報，所以EPS通常是以季為單位，四季EPS加總就會是全年的EPS</li><li>而大家常在說的股利又是怎麼來的？公司會在每年的董事會決定要從EPS李提撥多少比例來發放股利，也就是「盈餘分配率」</li></ul><p><strong>ROA(Return on Assets)，資產報酬率，也就是把淨利除以資產，也就是公司投入了多少資源去賺到這些錢</strong>。</p><ul><li>可以從ROA看到一家公司對整體資產的運用能力，ROA越高，代表公司對資產的運用能力越好</li></ul><p>但一家公司的總資產常常有一部分是對外發債或跟銀行貸款來的，扣除這些負債之後才是公司真正的淨值。</p><p><strong>ROE(Return on Equity)，股東權益報酬率，則是用來評估一家公司透過股東資金獲利的能力</strong>。把所賺的淨利除以公司淨值(即股東權益)</p><ul><li>ROE越高，代表一家公司對淨資產的運用能力越好</li><li>但解讀ROE時會有個盲點: 一家公司如果都是用貸款的方式來運作(融資槓桿)，這樣算出來的ROE很高時，代表公司很會用借來的錢去運作，但背後也承擔著較大的風險</li></ul><h4 id="算什麼-本益比、股價淨值比、殖利率"><a href="#算什麼-本益比、股價淨值比、殖利率" class="headerlink" title="算什麼: 本益比、股價淨值比、殖利率"></a>算什麼: 本益比、股價淨值比、殖利率</h4><p>前面有提到一家公司平均每股能賺多少錢可以看EPS。而股價和EPS之間的關係就是<strong>本益比(PER, Price Earnings Ratio)：股價成本和每股純益之間的比例，也就是把股價除以年度的EPS</strong></p><p>透過本益比可以依定程度的預估次場的趨勢。舉例來說A、B、C 3家性質相同的公司股價都是100元</p><ul><li>A的年度EPS為4元，可以算出A的本益比是25倍</li><li>B的年度EPS為5元，可以算出B的本益比是20倍</li><li>C的年度EPS為2元，可以算出C的本益比是50倍</li></ul><p>如果這個性質產業過去平均的本益比是25倍好了，那我們就可以回推B公司的股價少說要有5*25=125元才對，此時B只有100元，代表未來就比較有機會上漲</p><p>而對於C公司，現在的本益比是30倍就高於產業的平均本益比了，如果用平均本益比25倍乘上C的EPS 2元=50元，代表C的合理股價可能會在50元，但現在卻在100元，所以C未來可能有機會跌</p><p>阿如果一家公司的EPS是負的，那就沒辦法算本益比了。此時可以用<strong>股價淨值比(Price Book Ratio, PBR): 把股價除以每股淨值，用來評估現在的股價偏離公司淨值多少</strong></p><ul><li>股價淨值比以1為基準，高於1代表股價高於淨值；反過來低於1代表股價低於淨值</li><li>不過淨值來自資產減負債，所以忽略了公司的未來。比方說公司的負債來自於對未來設備的投資研發，但這些卻不會呈現在股價淨值比上</li></ul><p>最後，<strong>殖利率(yield)，是存股族最看重的指標，也就是股利除以目前的股價</strong></p><ul><li>目前一般來說，大家很喜歡拿5%的殖利率作為判斷一個投資標的的好壞</li></ul><h3 id="技術面-為了記錄數字，簡化閱讀而設計"><a href="#技術面-為了記錄數字，簡化閱讀而設計" class="headerlink" title="技術面: 為了記錄數字，簡化閱讀而設計"></a>技術面: 為了記錄數字，簡化閱讀而設計</h3><p>在下單App中往往有許多技術指標的功能，但卻不知道是什麼意思以及該如何使用？</p><p>在這章節中本書介紹了六項最常見的技術指標，透過淺顯易懂的介紹幫助大家了解技術面的基本概念：</p><ul><li>K線: 得知當天漲跌狀況和震盪幅度</li><li>均線: 判斷目前股價強弱或未來走勢</li><li>成交量: 影響股價漲跌的重要原因</li><li>法人買賣: 最有影響力的大咖</li><li>資券餘額: 了解未來行情的走勢</li><li>KD: 黃金交叉、80/20法則和鈍化</li></ul><h2 id="閱讀更多本書好文！"><a href="#閱讀更多本書好文！" class="headerlink" title="閱讀更多本書好文！"></a>閱讀更多本書好文！</h2><p>在後續的內容，柴鼠兄弟還介紹了許多值得一讀的概念，包含：</p><ul><li>股票的籌碼面概念</li><li>以及這幾年很夯的ETF，包含0050 0056是什麼？</li><li>在股市市場裡，到底該存股還是賺價差？</li><li>存股族常常靠領股利來獲利，除權息是什麼意思？有哪幾種常見的手法？</li><li>定期定額可以風險攤平，但背後的盲點是什麼？</li><li>在投資中賺錢很難，但比賺錢更難的事情 - 保持賺錢，為此你應該需要用什麼心態去面對投資這件事情？</li></ul><p>撰寫這篇閱讀心得的我是個剛出社會的職場新鮮人，在工作後有了一些小錢可以投資後也開始踏入投資理財的世界裡，從一開始的矇矇懂懂到現在稍微有些概念。</p><p>一路上柴鼠兄弟的Youtube頻道幫助了我很多，在閱讀這本書之後也對投資理財的許多工具有了更深的理解。</p><p>如果你是和我一樣，剛要踏入投資理財世界的初心者，非常推薦你去<a href="https://www.books.com.tw/exep/assp.php/john85051232/products/0010852671?utm_source=john85051232&amp;utm_medium=ap-books&amp;utm_content=recommend&amp;utm_campaign=ap-202103" target="_blank" rel="noopener">購買一本原作</a>來閱讀！</p>]]></content>
      
      
      <categories>
          
          <category> reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>令人搖頭的羅技產品品質? G413鍵盤出保紀錄(2021年)</title>
      <link href="/posts/5cb74faa/"/>
      <url>/posts/5cb74faa/</url>
      
        <content type="html"><![CDATA[<p>這篇文是講述自己對於羅技產品的親身體驗以及產品故障出保過程。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1615392305/blog_posts_2021/2021_03/logitech-logo_nou3yj.png" alt=""></p><h2 id="G413機械鍵盤使用體驗"><a href="#G413機械鍵盤使用體驗" class="headerlink" title="G413機械鍵盤使用體驗"></a>G413機械鍵盤使用體驗</h2><p>羅技產品在市場上一直深受電競玩家的喜愛，想當初第一次接觸羅技產品是在研究所的時候。</p><p>當初在參加清交大合辦的”梅竹黑客松”的時候，羅技作為出題者也有參與，並且給予參加羅技組的參賽者”競賽時間內可免費體驗羅技系列產品”的福利，也因此當時在參加競賽的時候體驗到了羅技的鍵盤、滑鼠、攝影機，當初使用的鍵盤印象中就是<a href="https://www.logitechg.com/zh-tw/products/gaming-keyboards/g413-mechanical-gaming-keyboard.920-008315.html" target="_blank" rel="noopener">G413</a></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_500/v1615393006/blog_posts_2021/2021_03/g413_avbail.png" alt=""></p><p>G413是羅技的機械式鍵盤，採用羅技的Romer-G鍵軸</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_700/v1615391774/blog_posts_2021/2021_03/%E6%93%B7%E5%8F%96_fwz6p8.png" alt=""></p><blockquote><p>Romer-G™ 觸感機械軸具有 1.5 公釐的短距觸發行程，這表示速度—11.5 公釐相對於標準機械軸的 2.0 公釐觸發行程。比競爭對手的標準機械軸速度快 25%。完美結合了高速、精確與安靜效能，Romer-G 觸感軸在保持清晰觸發的同時也提供可信任的觸發時機。</p></blockquote><p>(以上介紹擷取自<a href="https://www.logitechg.com/zh-tw/products/gaming-keyboards/g413-mechanical-gaming-keyboard.920-008315.html" target="_blank" rel="noopener">羅技官網</a>)</p><a id="more"></a><p>不得不說羅技的機械鍵盤使用起來體驗真的不錯，觸感很好。競賽期間使用起來的體驗很爽，所以也決定在競賽結束後入手了一台G413來陪我寫論文。</p><p>儘管用起來很爽，</p><p>但是，</p><p>但是，</p><p>但是，</p><p>他的<strong>品質實在是有 夠 爛</strong></p><p>G413我用了<strong>不到一年</strong>就開始發生敲擊按鍵時會有時會觸擊兩次的情況發生，或是有時候明明按了卻沒有反應。隨著使用時間變長現象也越來越頻繁。</p><p>如果說產品久了都會這樣，可是我使用還不到一年耶????? 這段期間也無不正常的使用</p><p>這情況實在讓我很崩潰</p><p>而且同時間我其實也買了一顆羅技滑鼠 - G102，也有發生連點的情況……</p><p>當下實在是對羅技的產品失望透了。</p><p>後來在偶然的情況下，發現當初於順發購買時其實留有發票，G413還在兩年的保固期內，就想死馬當活馬醫，試試看能不能出保。</p><h2 id="G413出保過程紀錄"><a href="#G413出保過程紀錄" class="headerlink" title="G413出保過程紀錄"></a>G413出保過程紀錄</h2><p>既然是在順發買的，我就先打電話給順發詢問我是否能出保，以及出保的話程序會是如何，然後我得到的回覆是”由於產品已經購買超過一年，帶著產品和發票去順發，然後他們會幫我把產品寄回給羅技，並酌收部分費用”</p><p>我是個很懶的人，而且把鍵盤讓他們寄回去的話這段期間是要我如何工作，原本想放棄改日再去買一個新的，並打算從此再也不買羅技產品時…</p><p>我發現網路上有些文章有提到羅技的出保過程其實相對簡單，由<strong>官方出保的話甚至不用把產品寄回去，他們會直接寄一個全新的給你</strong><br>(是說，從這也能看出來有多少人對羅技產品不滿，大家都一直在出保QQ)</p><p>好哇!</p><p>既然這樣那我也來試試看，於是我就去研究了一下如何透過官網出保，下面記錄我的出保過程</p><h3 id="到Logit-Support網站填單"><a href="#到Logit-Support網站填單" class="headerlink" title="到Logit Support網站填單"></a>到Logit Support網站填單</h3><p>至<a href="https://support.logi.com/hc/zh-tw/requests/new?ticket_form_id=360000621393" target="_blank" rel="noopener">Logit Support</a>輸入產品序號查詢產品，產品序號就是產品的S/N碼</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1615393947/blog_posts_2021/2021_03/logit01_bncoxe.png" alt=""></p><p>查到你的產品後，然後接下來就是在【敘述】的小框框內瘋狂抱怨他們家的產品有多不好!</p><p>然後根據網友們的經驗，我也一併<strong>附上了P/N碼跟S/N碼還有發票的照片</strong>然後送出表單</p><p>然後你就會收到一封信告訴你說他們收到你的表單了，會盡快處理，並且附上一個ticket number做為日後查詢用</p><h3 id="直接打電話給客服"><a href="#直接打電話給客服" class="headerlink" title="直接打電話給客服"></a>直接打電話給客服</h3><p>然後因為我很懶，不想等他慢慢處理，所以我就在隔天直接打電話給客服，並附上了我的ticket number詢問官方是否能出保</p><p>客服查了一下確認產品仍在保固內後就說可以出保，他們的出保方式是會先<strong>寄一個mail請你填地址等資訊，然後他們就會寄一個全新的產品給你，舊的自行處理銷毀即可</strong></p><p>然後你就會收到一封mail:</p><blockquote><p>親愛的客戶， 孫先生：</p><p>您好！</p><p>感謝您聯繫我們，您的票號為______。</p><p>我是羅技專員______，很榮幸能為您服務。</p><p>您的問題我們會盡全力協助您</p><p>請您提供以下資料，以利作業：</p><p>請確保以下資料完全正確無誤，下單系統無法進行任何更改，屆時，任何變動將會產生費用，並由客戶端自行承擔。<br>1) 中文全名：<br>2) 中文收件地址：<br>3) 六碼郵遞區號：<br>4) 手機號碼：</p><p>和</p><p>1) 英文全名 （護照或其它證件使用的名字）：<br>2) 英文收件地址：</p><p>重要提示:</p><p>羅技公司並不會收回您的舊產品。但請您還未收到我們寄送過去的替換產品之前，先不要將故障產品丟掉，替換品收到後，確定沒問題，舊產品自行處理即可。</p><p>如果您需要進一步協助，可撥打羅技電子線上支援專線:0800-012-300，並報上票號<5832151>，以便我們能更快速為您查詢。</p><p>敬祝 安康 !</p></blockquote><p>填完後他們就會開始準備寄送產品給你了!</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1615395061/blog_posts_2021/2021_03/158023858_4450482498301300_1038617494037274960_o_jjl0og.jpg" alt=""></p><p>我是在3/9申請出保的，由於G413他們還有庫存所以三天後，也就是3/11就收到了</p><p>這個速度我非常滿意!!</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>總結來說，我個人喜歡羅技簡潔的Logo(個人喜好簡潔風的設計)、產品本身的設計以及使用上的體驗，但無奈的是羅技的產品的品質讓我覺得很糟…..個人用過的產品都是用不久就壞掉了，以往使用其他家產品都沒遇過這個問題。</p><p>不過羅技<strong>在售後服務的部分我覺得做的是相當不錯的</strong>，對於保固期間內的產品他們會很乾脆的換一個新的給你，並且過程不會做太多刁難，這次的出保體驗整個下來是相當滿意的。所以也打算再給羅技一次機會，再使用他們家產品一段時間觀察看看</p><p>也希望他們未來能好好控管產品品質阿~</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[讀書心得]35歲，為什麼還沒被重用？</title>
      <link href="/posts/dedf856f/"/>
      <url>/posts/dedf856f/</url>
      
        <content type="html"><![CDATA[<h2 id="書籍資訊"><a href="#書籍資訊" class="headerlink" title="書籍資訊"></a>書籍資訊</h2><ul><li>書名: 35歲，為什麼還沒被重用</li><li>作者:  薛明玲口述、賴燕芳 採訪撰文</li><li>出版日期: Jan 12, 2014</li><li>金石堂網址: <a href="https://www.kingstone.com.tw/basic/2014941292156/" target="_blank" rel="noopener">35歲，為什麼還沒被重用</a></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1615226210/blog_posts_2021/2021_03/E6046647e3ce0f716626826-0_fiwdki.png" alt=""></p><p>(本文是【35歲，為什麼還沒被重用】的讀書心得)</p><blockquote><p>是什麼關鍵，讓鴻海、台達電、統一、旺旺成為簽證客戶？<br>是什麼原因，讓近百位合夥人願意提早將退休金全部結算？<br>一本值得反覆咀嚼的好書，告訴你如何創造自己價值、如何找出成功方法，如何走出數字迷思。</p></blockquote><p>薛明玲，曾任全球四大會計師事務所之一的「資誠聯合會計師事務所」所長，也是台灣第一個和企業老闆並坐股東會的會計師。事實上，自稱「老薛」的他，是一個沒有顯赫家世、求學不順利、在台中鄉下長大的孩子。</p><p>「把每一件經過手邊的事情做到最好」，是薛明玲馳騁職場的成功心法。他曾主動將整套稅務法規整理出兩頁的「實用密笈」送給客戶，迅速建立起專業知名 度。在股東會現場，他會準備多張試算表，提供企業主隨機應變……他發現，成功企業家最在意的，不僅是財報數字正不正確，更強調邏輯思維！</p><p>他說，35歲是人生的分水嶺，心態也轉趨成熟，這時若有成功者指引方向、提供經驗，就能少走冤枉路。因此，薛明玲提供自己30多年的「職場金三角」工 作學，也分享他與鄭崇華、郭台銘、杜英宗、戴勝益等企業家領袖，深度接觸的心得，給你再出發的勇氣，縱使35歲之後，機會依舊等著你。<br><a id="more"></a></p><h2 id="內容導讀"><a href="#內容導讀" class="headerlink" title="內容導讀"></a>內容導讀</h2><p>日本趨勢大師大前研一曾將三十五~五十歲稱為「魔力的十五年」。也就是說，在三十五歲前後人將來到體力與工作經驗的最高峰，是上班族的生涯黃金關鍵點。</p><p>若以二十五歲從學校畢業算起，三十五歲正式工作十年的重要分水嶺，此時遇到的挑戰與壓力也特別多。薛明玲認為，排出壓力最好的方法，就是把事情做到沒有後顧之憂，因此，我向來秉持「把每一件經過手邊的事情做到最好」的原則。</p><p>由於長期處於一個熟悉的工作環境時，容易讓自己陷入思考模式的僵化。薛明玲對此提出了一個「職場金三角」的概念：<strong>投資自己、經營自己、淬煉自己</strong>。這是一套有步驟性的工作學，讓大家可以根據如下建議提升自己在職場中的各方面能力。</p><h3 id="職場金三角"><a href="#職場金三角" class="headerlink" title="職場金三角"></a>職場金三角</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_450/v1615226494/blog_posts_2021/2021_03/_2_okjdxr.jpg" alt=""></p><h3 id="一、投資自己"><a href="#一、投資自己" class="headerlink" title="一、投資自己"></a>一、投資自己</h3><p><strong>別再被學經歷限制，四種能力幫你破局而出！</strong></p><p>投資自己，就是強化本身職能、儲備未來戰力。想要出人頭地，就必須比別人更努力！薛明玲四個核心優勢：執行力、耐挫力、堅持力、人際力，不僅培養自己的軟實力，也讓你職場籌碼倍增，跨出關鍵一步改變人生。</p><ul><li>執行力: 把每一件手邊的事情做到最好</li><li>耐挫力: 在逆境之中，找出不放棄的理由</li><li>堅持力: 生命隨時會轉彎，只怕準備不夠</li><li>人際力: 當一個可以「被充分信任的人」</li></ul><p><br/></p><h4 id="【執行力】不遲到盲從-以自律贏得企業的信任"><a href="#【執行力】不遲到盲從-以自律贏得企業的信任" class="headerlink" title="【執行力】不遲到盲從 - 以自律贏得企業的信任"></a>【執行力】不遲到盲從 - 以自律贏得企業的信任</h4><p>在作者的工作中觀察發現，如果老闆開會經常遲到，那這家公司的經營績效一定不好，因為一個人岩自己的時間都管理不好，更何況是公司治理。</p><p>他建議職場工作者，<strong>「把腦袋的時間調早二十分鐘」就可以做時間的主人</strong>。因為遲到的代價太高，往往就是這幾分鐘的時間，足以改變一個人的終生前途。</p><p><br/></p><h4 id="【執行力】不說沒辦法-碰到難題就靠除法解決"><a href="#【執行力】不說沒辦法-碰到難題就靠除法解決" class="headerlink" title="【執行力】不說沒辦法 - 碰到難題就靠除法解決"></a>【執行力】不說沒辦法 - 碰到難題就靠除法解決</h4><p>年輕人遇到主管交付的任務絕不要先說「No」或「沒辦法」，要相信主管有識人之明，信賴你才會把重要的事情交給你去完成。</p><p>日本會計師三田真哉的出版作品《就算客人白吃白喝，也別請工讀生 — 培養數字敏感度的金錢知識》中曾提到數字的暴力性，也就是數字帶給人的恐懼。例如上班族每年收到主管給予的龐大業績目標時往往會因為害怕無法達成的恐懼而面臨龐大的壓力。</p><p>當真的在工作上遇到一時難以解決的問題時，不要想著這龐大的困難度，<strong>試著把問題均分成許多小任務來解決，耐著性子一件件做起</strong>，最後一定能化零為整，成功完成。</p><p><br/></p><h4 id="【耐挫力】勇於承擔！主動回報相關資訊，平息質疑風波"><a href="#【耐挫力】勇於承擔！主動回報相關資訊，平息質疑風波" class="headerlink" title="【耐挫力】勇於承擔！主動回報相關資訊，平息質疑風波"></a>【耐挫力】勇於承擔！主動回報相關資訊，平息質疑風波</h4><p>再薛明玲身為國票會計師時，面臨了一起很大的弊案，當時在各界的質疑壓力下，他四處奔波準備資料回答來自各界的質詢，由於他有問必答，有時候不僅回答問題本身，還主動說明問題的背景及近一步的解決方案，快速且專業的態度使得在當時很快就扭轉風向，重拾大家的信任……</p><p><strong>「沒有一個人是故意要找你麻煩」</strong>，在遇到困難時可以換位思考，以提問者的角度和立場出發，用對方較容易理解的語言、文字作充分說明。這種設身處地為對方著想、即時回應對方需求的態度是面對問題時很重要的一項特質。</p><p><br/></p><h4 id="【耐挫力】記取教訓-揮別莽撞，從錯誤中成長"><a href="#【耐挫力】記取教訓-揮別莽撞，從錯誤中成長" class="headerlink" title="【耐挫力】記取教訓!揮別莽撞，從錯誤中成長"></a>【耐挫力】記取教訓!揮別莽撞，從錯誤中成長</h4><p>「 人不要怕摔跤，尤其年輕時摔跤付出的學習成本低，傷痛也會很快過去。」每個人難免會犯錯，如果是戰術上的錯，反而是人生的經驗；但如果是想法上的偏差而犯了戰術略上的錯，就有可能全盤皆輸。工作中的一些不如意事，有時反而是很好的省思機會。</p><p><br/></p><h4 id="【耐挫力】抗壓有法-冷靜應變、尋求幫助，就能突圍前進"><a href="#【耐挫力】抗壓有法-冷靜應變、尋求幫助，就能突圍前進" class="headerlink" title="【耐挫力】抗壓有法!冷靜應變、尋求幫助，就能突圍前進"></a>【耐挫力】抗壓有法!冷靜應變、尋求幫助，就能突圍前進</h4><p>「熱情、正向思考和面對挫折的勇氣，不僅是成功的三把鑰匙，也是年輕人能走出挫敗陰影的解藥。」首先，年輕人必須具備熱情，不論背後的動力來源是金錢或是興趣，有了熱情才會執著於工作，比別人創造更多的機會。而樂觀和正向思考是面對事情的態度，因為天下沒有完美的解法，只有更好的辦法。<br><br/></p><h4 id="【堅持力】保持樂觀，所以不抱怨-隨時自我增值，有實力不怕被埋沒"><a href="#【堅持力】保持樂觀，所以不抱怨-隨時自我增值，有實力不怕被埋沒" class="headerlink" title="【堅持力】保持樂觀，所以不抱怨 - 隨時自我增值，有實力不怕被埋沒"></a>【堅持力】保持樂觀，所以不抱怨 - 隨時自我增值，有實力不怕被埋沒</h4><blockquote><p>可貴的不是在景氣大好時的成功，而是沒有在景氣谷底時被擊垮。</p></blockquote><p>薛明玲建議年輕人，當壓力來臨時，要先認清事情的本質: 如果遇到的是每個人都會碰到的壓力，例如學生時代的考試，大家的壓力都一樣，這時只要全力以赴即可。</p><p><br/></p><h4 id="【人際力】要能被充分信任，必須做到-養成早到習慣、凡事做好準備、展現與眾不同"><a href="#【人際力】要能被充分信任，必須做到-養成早到習慣、凡事做好準備、展現與眾不同" class="headerlink" title="【人際力】要能被充分信任，必須做到: 養成早到習慣、凡事做好準備、展現與眾不同"></a>【人際力】要能被充分信任，必須做到: 養成早到習慣、凡事做好準備、展現與眾不同</h4><p>貴人可遇不可求，如果身邊真的沒有貴人，沒關係，你也可以成為自己的貴人！</p><p>要做自己的貴人，最重要的關鍵在於「被充分信任」 - 養成早到習慣、凡事做好準備、展現與眾不同，就是被充分信任的三大元素。</p><h3 id="二、經營自己"><a href="#二、經營自己" class="headerlink" title="二、經營自己"></a>二、經營自己</h3><p><strong>學習像老闆一樣思考，找回工作的自主權！</strong></p><p>經營自己，首先要捨棄以自己的角度想事情。薛明玲以領導者的格局，提供六大核心能力，教你如何深得老闆器重！此外，對於職場所需的數字邏輯及商業經營概念，他則運用實際的企業案例作說明，不懂財會原理也能迅速上手。</p><p>這邊就節錄這個章節的一個小主題，對於剛轉職成社會新鮮人的我來說，覺得這個主題是非常適合自己現在所需要的。</p><h4 id="職場的六個潛規則"><a href="#職場的六個潛規則" class="headerlink" title="職場的六個潛規則"></a>職場的六個潛規則</h4><p>「上班常遲到」、「忙碌到忘記回復重要郵件」、「穿著過於隨興，與職場形象格格不入」……這些看來與能力和操守無關的事情，有時其實就是背後影響升遷、甚至是丟掉工作的大事。這些人盡皆知，卻不會說明的職場潛規則，對年輕人而言就像是引導汽車前進方向的GPS，確保努力的方向和未來的目標一致。</p><ul><li>挑人的標準? 從電子郵件的回復速度、品質判斷優劣</li><li>對於初入職場的年輕人，建議以「聆聽」和「主動發問」展現積極態度</li><li>如何安排私人時間? 隨時待機，讓客戶隨時都找的到</li><li>如何訓練員工? 每人平均超過一百小時的課程訓練</li><li>用什麼方式行銷資誠? 成立老薛智識網部落格</li><li>如何精進專業能力? 像海綿一樣無止盡的學習</li></ul><h3 id="三、淬鍊自己"><a href="#三、淬鍊自己" class="headerlink" title="三、淬鍊自己"></a>三、淬鍊自己</h3><p><strong>向成功者看齊，培養創意領袖的前瞻思考！</strong></p><p>淬鍊自己，最快的方式就是「見賢思齊」。薛明玲分享了國內外成功企業與領導人的故事，包括奇異公司、巴菲特、杜英宗、鄭崇華等，並歸納出這些成功案例的致勝關鍵，以及如何運用創新與科技，保證讓你大呼過癮！</p><p>在這個章節中，主要採訪了薛明玲的領導哲學，以及如何從其他企業的經營和行銷模式中觀察他們成功的要素: 包含一個成功的領導者應該具有什麼樣的領導風範，和洞察能力。</p><p>在這一章節中，可以看到許多企業的經營模式以及它們是如何成功的，這邊就不做太多的著墨，有興趣的可以去看看原書~</p><h2 id="讀後心得"><a href="#讀後心得" class="headerlink" title="讀後心得"></a>讀後心得</h2><p>這本書其實是在大學時期買的，當初買的時候被大學同學看到還被嘲笑，為什麼還沒35歲的自己當初會想買這本書？</p><p>人生過程中總會遇到許多彎路，希望能透過許多前輩的經驗中學習如何少走一些歪路，讓自己在未來能夠更加茁壯。抱持著這樣的想法買下了這本。</p><p>儘管時至今日，出了社會後才真正把這本書看完，但薛明玲所長的這本書籍從自身經歷出發，告訴我們該用什麼心態去面對工作，以及該用什麼方式去學習並面對人生中的不同挑戰，是個非常直得一讀的經驗談。</p>]]></content>
      
      
      <categories>
          
          <category> reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[課程筆記]Linux Driver正點原子課程筆記4 - Led燈驅動實驗</title>
      <link href="/posts/9aca0070/"/>
      <url>/posts/9aca0070/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多課程筆記，至<a href="https://meetonfriday.com/posts/85f1c2a/">[課程筆記]課程筆記系列總覽</a>可以看到目前已發布的所有文章！〗</p><h2 id="Course-4-Led燈驅動實驗"><a href="#Course-4-Led燈驅動實驗" class="headerlink" title="Course 4 - Led燈驅動實驗"></a>Course 4 - Led燈驅動實驗</h2><p>嘗試透過driver去操作led register來達到led開關的實驗。</p><h3 id="地址映射"><a href="#地址映射" class="headerlink" title="地址映射"></a>地址映射</h3><p>Linux下<strong>無法直接對physical address做讀寫操作</strong>，因為linux會enable <strong>MMU(Memory Manage Unit)</strong>，MMU的功能為</p><ol><li>virtual address (VA) &lt;-&gt; physical address (PA)的mapping</li><li>內存保護、register的訪問權限</li></ol><p>對於32 bit的processor來說，virtual address的range是2^32=4GB。假設開發版有512MB的physical address，則MMU可以將512MB映射到4GB的虛擬空間。</p><ul><li>但PA有512MB，VA有4GB，要如何mapping? 這邊先不討論，有興趣的可以去看MMU的詳細介紹</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/v1614705021/blog_posts_2021/2021_03/BTMfx9z_zdahbi.png" alt=""></p><a id="more"></a><p>因此，如果要對physical address做讀寫，必須要該PA對應的VA，這邊會用到兩個function: </p><ul><li><code>ioremap()</code>: 獲得PA對應的VA<ul><li>第一個參數: 物理地址起始位置，第二個參數: 轉換的char數量</li></ul></li><li><code>iounmap()</code>: 釋放VA<ul><li>參數:要釋放的VA</li></ul></li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* arch/arm/include/asm/io.h */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> __iomem *<span class="title">ioremap</span><span class="params">(<span class="keyword">resource_size_t</span> res_cookie, <span class="keyword">size_t</span> <span class="built_in">size</span>)</span></span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ioremap ioremap</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ioremap_nocache ioremap</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">iounmap</span><span class="params">(<span class="keyword">volatile</span> <span class="keyword">void</span> __iomem *iomem_cookie)</span></span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> iounmap iounmap</span></span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* arch/arm/mm/ioremap.c */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> __iomem *<span class="title">ioremap</span><span class="params">(<span class="keyword">resource_size_t</span> res_cookie, <span class="keyword">size_t</span> <span class="built_in">size</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> arch_ioremap_caller(res_cookie, <span class="built_in">size</span>, MT_DEVICE,</span><br><span class="line">                   __builtin_return_address(<span class="number">0</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下來按照影片教學，試著寫一套驅動&amp;app來透過他們家的開發版操控led燈，但我沒有他們的開發版，所以就照著寫了一遍code練習</p><p>裡面register的addr是參照影片來寫的，所以如果不能動不要問這值是哪裡來的<br>Ｑ＿Ｑ</p><h4 id="driver"><a href="#driver" class="headerlink" title="driver"></a>driver</h4><ol><li>init相關設置(e.g. init led燈需要init gpio, clk..)可以放在*fops的open或module init，端看情境<ul><li>初始化如需讀寫register，記得要取得PA對應的VA</li></ul></li><li><strong>linux不推薦直接對取得的va addr做讀寫</strong>，而是用提供的api來操作(<code>readb()</code>, <code>readw()</code>, <code>readl()</code>, <code>writeb()</code>, <code>writew()</code>, <code>writel()</code>)</li><li>關燈的控制不能寫在<code>led_release()</code>，why?<ul><li>因為如果寫在<code>led_release()</code>，每次app <code>close()</code>時都會關閉燈，情境上不太適合</li></ul></li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/module.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/kernel.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/init.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/fs.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/slab.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/uaccess.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/io.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LED_MAJOR 200</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LED_NAME <span class="meta-string">"led"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* register PA */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CCM_CCGR1_BASE         (0x020C406C)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SW_MUX_GPIO1_IO03_BASE (0x020E0068)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SW_PAD_GPIO1_IO03_BASE (0x020E02F4)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> GPIO1_DR_BASE          (0x0209C000)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> GPIO1_GDIR_BASE        (0x0209C004)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* pointer of VA */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __iomem *IMX6U_CCM_CCGR1;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __iomem *SW_MUX_GPIO1_IO03;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __iomem *SW_PAD_GPIO1_IO03;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __iomem *GPIO1_DR;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> __iomem *GPIO1_GDIR;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LEDOFF 0</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LEDON 1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">led_switch</span><span class="params">(u8 sta)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    u32 val = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (databuf[<span class="number">0</span>] == LEDOFF) &#123;</span><br><span class="line">        val = readl(GPIO1_DR);</span><br><span class="line">        val |= ~(<span class="number">1</span> &lt;&lt; <span class="number">3</span>);</span><br><span class="line">        writel(val, GPIO1_DR); <span class="comment">// close led</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        val = readl(GPIO1_DR);</span><br><span class="line">        val &amp;= ~(<span class="number">1</span> &lt;&lt; <span class="number">3</span>);</span><br><span class="line">        writel(val, GPIO1_DR); <span class="comment">// open led</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">led_open</span><span class="params">(struct inode *inode, struct file *filp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">led_release</span><span class="params">(struct inode *inode, struct file *filp)</span></span></span><br><span class="line"><span class="function"></span>&#123;   </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">ssize_t</span> <span class="title">led_write</span><span class="params">(struct file *filp, <span class="keyword">const</span> <span class="keyword">char</span> __user *buf, <span class="keyword">size_t</span> count, <span class="keyword">loff_t</span> *ppos)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret;</span><br><span class="line">    <span class="keyword">int</span> val = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> databuf[<span class="number">1</span>];</span><br><span class="line">    ret = copy_from_user(databuf, buf, count);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>)&#123;</span><br><span class="line">        printk(<span class="string">"kernel write fail\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> -EFAULT;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    led_switch(databuf[<span class="number">0</span>]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">file_operations</span> *<span class="title">fops</span> </span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    .owner   = THIS_MODULE,</span><br><span class="line">    .<span class="built_in">write</span>   = led_write,</span><br><span class="line">    .<span class="built_in">open</span>    = led_open,</span><br><span class="line">    .<span class="built_in">release</span> = led_close,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> __init <span class="title">led_init</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> val = <span class="number">0</span>;</span><br><span class="line">    printk(<span class="string">"led init\n"</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 1. get va of register */</span></span><br><span class="line">    IMX6U_CCM_CCGR1 = ioremap(CCM_CCGR1_BASE, <span class="number">4</span>);</span><br><span class="line">    SW_MUX_GPIO1_IO03 = ioremap(SW_MUX_GPIO1_IO03_BASE, <span class="number">4</span>);</span><br><span class="line">    SW_PAD_GPIO1_IO03 = ioremap(SW_PAD_GPIO1_IO03_BASE, <span class="number">4</span>);</span><br><span class="line">    GPIO1_DR = ioremap(GPIO1_DR_BASE, <span class="number">4</span>);</span><br><span class="line">    GPIO1_GDIR = ioremap(GPIO1_GDIR_BASE, <span class="number">4</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 2. init */</span></span><br><span class="line">    val = readl(IMX6U_CCM_CCGR1);</span><br><span class="line">    val &amp;= ~(<span class="number">3</span> &lt;&lt; <span class="number">26</span>); <span class="comment">// clr IMX6U_CCM_CCGR1[26:27]</span></span><br><span class="line">    val |= <span class="number">3</span> &lt;&lt; <span class="number">26</span>; <span class="comment">// set IMX6U_CCM_CCGR1[26:27] to 1</span></span><br><span class="line">    writel(val, IMX6U_CCM_CCGR1);</span><br><span class="line">    </span><br><span class="line">    writel(<span class="number">0x5</span>, SW_MUX_GPIO1_IO03);</span><br><span class="line">    writel(<span class="number">0x10B0</span>, SW_PAD_GPIO1_IO03);</span><br><span class="line">  </span><br><span class="line">    val = readl(GPIO1_GDIR);</span><br><span class="line">    val |= <span class="number">1</span> &lt;&lt; <span class="number">3</span>;</span><br><span class="line">    writel(val, GPIO1_GDIR);</span><br><span class="line">  </span><br><span class="line">    val = readl(GPIO1_DR);</span><br><span class="line">    val &amp;= ~(<span class="number">1</span> &lt;&lt; <span class="number">3</span>);</span><br><span class="line">    writel(val, GPIO1_DR); <span class="comment">// open led</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">/* 2. register chrdev */</span></span><br><span class="line">    ret = register_chrdev(LED_MAJOR, LED_NAME, &amp;fops);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        printk(<span class="string">"chrdev register fail\n"</span>);</span><br><span class="line">        <span class="keyword">return</span> -EIO;</span><br><span class="line">    &#125;</span><br><span class="line">        </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> __exit <span class="title">led_exit</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> val = <span class="number">0</span>;</span><br><span class="line">    val = readl(GPIO1_DR);</span><br><span class="line">    val |= ~(<span class="number">1</span> &lt;&lt; <span class="number">3</span>);</span><br><span class="line">    writel(val, GPIO1_DR); <span class="comment">// close led</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 1. unmap va */</span></span><br><span class="line">    iounmap(IMX6U_CCM_CCGR1);</span><br><span class="line">    iounmap(SW_MUX_GPIO1_IO03);</span><br><span class="line">    iounmap(SW_PAD_GPIO1_IO03);</span><br><span class="line">    iounmap(GPIO1_DR);</span><br><span class="line">    iounmap(GPIO1_GDIR);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* 2. unregister */</span></span><br><span class="line">    unregister_chrdev(LED_MAJOR, LED_NAME);</span><br><span class="line">    printk(<span class="string">"chrdev unregister\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">module_init(led_init);</span><br><span class="line">module_exit(led_exit);</span><br><span class="line"></span><br><span class="line">MODULE_LICENSE(<span class="string">"GPL"</span>);</span><br><span class="line">MODULE_AUTHOR(<span class="string">"john"</span>);</span><br></pre></td></tr></table></figure><h4 id="app"><a href="#app" class="headerlink" title="app"></a>app</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ./ledAPP &lt;filename&gt; &lt;num&gt;</span></span><br><span class="line"><span class="comment"> * @num: 0 代表關燈, 1代表開燈</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LEDOFF 0</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LEDON 1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> fd = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">char</span> *filename;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> databuf[<span class="number">1</span>];</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (argc != <span class="number">3</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    </span><br><span class="line">    filename = argv[<span class="number">1</span>];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* open() return a file descriptor */</span></span><br><span class="line">    fd = <span class="built_in">open</span>(filename, O_RDWR);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (fd &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Can not open file %s\n"</span>, filename);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    </span><br><span class="line">    databuf[<span class="number">0</span>] = atoi(argv[<span class="number">2</span>]);</span><br><span class="line">    ret = <span class="built_in">write</span>(fd, databuf, <span class="keyword">sizeof</span>(databuf));</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        printk(<span class="string">"led control fail\n"</span>);</span><br><span class="line">        <span class="built_in">close</span>(fd);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">close</span>(fd);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 【課程筆記】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux kernel </tag>
            
            <tag> study </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[課程筆記]Linux Driver正點原子課程筆記3 - 我的第一個Linux驅動</title>
      <link href="/posts/62f55520/"/>
      <url>/posts/62f55520/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多課程筆記，至<a href="https://meetonfriday.com/posts/85f1c2a/">[課程筆記]課程筆記系列總覽</a>可以看到目前已發布的所有文章！〗</p><h2 id="Course-3-我的第一個Linux驅動"><a href="#Course-3-我的第一個Linux驅動" class="headerlink" title="Course 3 - 我的第一個Linux驅動"></a>Course 3 - 我的第一個Linux驅動</h2><h3 id="字符設備驅動框架"><a href="#字符設備驅動框架" class="headerlink" title="字符設備驅動框架"></a>字符設備驅動框架</h3><p>上次有說到字符驅動提供給外部的api都被定義在file_operations structs內，這個結構被定義在/include/linux/fs.h下</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* /include/linux/fs.h */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">file_operations</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">module</span> *<span class="title">owner</span>;</span></span><br><span class="line">    <span class="keyword">loff_t</span> (*llseek) (struct file *, <span class="keyword">loff_t</span>, <span class="keyword">int</span>);</span><br><span class="line">    <span class="keyword">ssize_t</span> (*<span class="built_in">read</span>) (struct file *, <span class="keyword">char</span> __user *, <span class="keyword">size_t</span>, <span class="keyword">loff_t</span> *);</span><br><span class="line">    <span class="keyword">ssize_t</span> (*<span class="built_in">write</span>) (struct file *, <span class="keyword">const</span> <span class="keyword">char</span> __user *, <span class="keyword">size_t</span>, <span class="keyword">loff_t</span> *);</span><br><span class="line">    <span class="keyword">ssize_t</span> (*read_iter) (struct kiocb *, struct iov_iter *);</span><br><span class="line">    <span class="keyword">ssize_t</span> (*write_iter) (struct kiocb *, struct iov_iter *);</span><br><span class="line">    <span class="keyword">int</span> (*iterate) (struct file *, struct dir_context *);</span><br><span class="line">    <span class="keyword">int</span> (*iterate_shared) (struct file *, struct dir_context *);</span><br><span class="line">    <span class="keyword">__poll_t</span> (*poll) (struct file *, struct poll_table_struct *);</span><br><span class="line">    <span class="keyword">long</span> (*unlocked_ioctl) (struct file *, <span class="keyword">unsigned</span> <span class="keyword">int</span>, <span class="keyword">unsigned</span> <span class="keyword">long</span>);</span><br><span class="line">    <span class="keyword">long</span> (*compat_ioctl) (struct file *, <span class="keyword">unsigned</span> <span class="keyword">int</span>, <span class="keyword">unsigned</span> <span class="keyword">long</span>);</span><br><span class="line">    <span class="keyword">int</span> (*mmap) (struct file *, struct vm_area_struct *);</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> mmap_supported_flags;</span><br><span class="line">    <span class="keyword">int</span> (*<span class="built_in">open</span>) (struct inode *, struct file *);</span><br><span class="line">    <span class="keyword">int</span> (*<span class="built_in">flush</span>) (struct file *, <span class="keyword">fl_owner_t</span> id);</span><br><span class="line">    <span class="keyword">int</span> (*<span class="built_in">release</span>) (struct inode *, struct file *);</span><br><span class="line">    <span class="keyword">int</span> (*fsync) (struct file *, <span class="keyword">loff_t</span>, <span class="keyword">loff_t</span>, <span class="keyword">int</span> datasync);</span><br><span class="line">    <span class="keyword">int</span> (*fasync) (<span class="keyword">int</span>, struct file *, <span class="keyword">int</span>);</span><br><span class="line">    <span class="keyword">int</span> (*lock) (struct file *, <span class="keyword">int</span>, struct file_lock *);</span><br><span class="line">    <span class="keyword">ssize_t</span> (*sendpage) (struct file *, struct page *, <span class="keyword">int</span>, <span class="keyword">size_t</span>, <span class="keyword">loff_t</span> *, <span class="keyword">int</span>);</span><br><span class="line">    <span class="function"><span class="keyword">unsigned</span> <span class="title">long</span> <span class="params">(*get_unmapped_area)</span><span class="params">(struct file *, <span class="keyword">unsigned</span> <span class="keyword">long</span>, <span class="keyword">unsigned</span> <span class="keyword">long</span>, <span class="keyword">unsigned</span> <span class="keyword">long</span>, <span class="keyword">unsigned</span> <span class="keyword">long</span>)</span></span>;</span><br><span class="line">    <span class="keyword">int</span> (*check_flags)(<span class="keyword">int</span>);</span><br><span class="line">    <span class="keyword">int</span> (*flock) (struct file *, <span class="keyword">int</span>, struct file_lock *);</span><br><span class="line">    <span class="keyword">ssize_t</span> (*splice_write)(struct pipe_inode_info *, struct file *, <span class="keyword">loff_t</span> *, <span class="keyword">size_t</span>, <span class="keyword">unsigned</span> <span class="keyword">int</span>);</span><br><span class="line">    <span class="keyword">ssize_t</span> (*splice_read)(struct file *, <span class="keyword">loff_t</span> *, struct pipe_inode_info *, <span class="keyword">size_t</span>, <span class="keyword">unsigned</span> <span class="keyword">int</span>);</span><br><span class="line">    <span class="keyword">int</span> (*setlease)(struct file *, <span class="keyword">long</span>, struct file_lock **, <span class="keyword">void</span> **);</span><br><span class="line">    <span class="keyword">long</span> (*fallocate)(struct file *file, <span class="keyword">int</span> mode, <span class="keyword">loff_t</span> offset,</span><br><span class="line">              <span class="keyword">loff_t</span> len);</span><br><span class="line">    <span class="keyword">void</span> (*show_fdinfo)(struct seq_file *m, struct file *f);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CONFIG_MMU</span></span><br><span class="line">    <span class="keyword">unsigned</span> (*mmap_capabilities)(struct file *);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    <span class="keyword">ssize_t</span> (*copy_file_range)(struct file *, <span class="keyword">loff_t</span>, struct file *,</span><br><span class="line">            <span class="keyword">loff_t</span>, <span class="keyword">size_t</span>, <span class="keyword">unsigned</span> <span class="keyword">int</span>);</span><br><span class="line">    <span class="keyword">int</span> (*clone_file_range)(struct file *, <span class="keyword">loff_t</span>, struct file *, <span class="keyword">loff_t</span>,</span><br><span class="line">            u64);</span><br><span class="line">    <span class="keyword">int</span> (*dedupe_file_range)(struct file *, <span class="keyword">loff_t</span>, struct file *, <span class="keyword">loff_t</span>,</span><br><span class="line">            u64);</span><br><span class="line">    <span class="keyword">int</span> (*fadvise)(struct file *, <span class="keyword">loff_t</span>, <span class="keyword">loff_t</span>, <span class="keyword">int</span>);</span><br><span class="line">&#125; __randomize_layout;</span><br></pre></td></tr></table></figure><p>簡單介紹一下這個struct內的member: </p><ul><li>owner: 擁有該結構的module pointer，一般設置為THIS_MODULE</li><li><code>llseek()</code>: 用於修改當前文件的讀寫位置</li><li><code>open()</code>: 打開文件</li><li><code>read()</code> &amp; <code>write()</code>:用來存取文件</li><li><code>poll()</code>: 用來輪詢device是否可以進行non-blocking的讀寫</li><li><code>unlocked_ioctl()</code> &amp; <code>compat_ioctl()</code>: 提供對設備控制的街口<ul><li>關於這部分的詳細介紹可以參閱<a href="https://meetonfriday.com/posts/736969d7/">[Linux Kernel慢慢學]Different betweeen ioctl, unlocked_ioctl and compat_ioctl</a></li></ul></li><li><code>mmap()</code>: 將device memory mapping到process memory，如此就不用做繁瑣的memory copy</li></ul><p>function pointer不用全部實現，會用到哪個就實作哪個就好。</p><a id="more"></a><h3 id="驅動模塊的加載與卸載"><a href="#驅動模塊的加載與卸載" class="headerlink" title="驅動模塊的加載與卸載"></a>驅動模塊的加載與卸載</h3><p>Linux驅動可以編譯到kernel裡面(zImage)。也可以編譯成module(.ko)，需要的時候在載入就好，既然可以加載那就必須提供加載和卸載的func。</p><p>一個基礎的模板如下:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/module.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/kernel.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* LICENSE */</span></span><br><span class="line">MODULE_LICENSE(<span class="string">"GPL"</span>);</span><br><span class="line">MODULE_AUTHOR(<span class="string">"john"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> __init <span class="title">chrdevbase_init</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    printk(KERN_INFO <span class="string">"module init\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> __exit <span class="title">chrdevbase_exit</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    printk(KERN_INFO <span class="string">"module exit\n"</span>);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">module_init(chrdevbase_init); <span class="comment">/* probe entry */</span></span><br><span class="line">module_exit(chrdevbase_exit); <span class="comment">/* exit entry */</span></span><br></pre></td></tr></table></figure></p><p>kernel內沒有<code>printf()</code>，取而代之的是<code>printk()</code>，可以顯示不同level的log，log level的定義位於/include/linux/kern_levels.h</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* /include/linux/kern_levels.h */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> KERN_EMERGKERN_SOH <span class="meta-string">"0"</span><span class="comment">/* system is unusable */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> KERN_ALERTKERN_SOH <span class="meta-string">"1"</span><span class="comment">/* action must be taken immediately */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> KERN_CRITKERN_SOH <span class="meta-string">"2"</span><span class="comment">/* critical conditions */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> KERN_ERRKERN_SOH <span class="meta-string">"3"</span><span class="comment">/* error conditions */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> KERN_WARNINGKERN_SOH <span class="meta-string">"4"</span><span class="comment">/* warning conditions */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> KERN_NOTICEKERN_SOH <span class="meta-string">"5"</span><span class="comment">/* normal but significant condition */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> KERN_INFOKERN_SOH <span class="meta-string">"6"</span><span class="comment">/* informational */</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> KERN_DEBUGKERN_SOH <span class="meta-string">"7"</span><span class="comment">/* debug-level messages */</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> KERN_DEFAULTKERN_SOH <span class="meta-string">"d"</span><span class="comment">/* the default kernel loglevel */</span></span></span><br></pre></td></tr></table></figure><p>而在/include/linux/printk.h中定義了CONSOLE_LOGLEVEL_DEFAULT的預設值是7，也就是說<strong>優先級高於7的log才會被印出在console上</strong><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* /include/linux/printk.h */</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Default used to be hard-coded at 7, quiet used to be hardcoded at 4,</span></span><br><span class="line"><span class="comment"> * we're now allowing both to be set from kernel config.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CONSOLE_LOGLEVEL_DEFAULT CONFIG_CONSOLE_LOGLEVEL_DEFAULT</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CONSOLE_LOGLEVEL_QUIET CONFIG_CONSOLE_LOGLEVEL_QUIET</span></span><br></pre></td></tr></table></figure></p><p><code>module_init()</code>是driver被載入時會執行的function，對於不同driver之間有時候是有著載入順序的相依性的，此時就可以透過呼叫不同的init function來控制，更深入的探討可以參考<a href="https://meetonfriday.com/posts/c4426b79/">[Linux Kernel慢慢學]Linux modules載入及載入順序</a></p><p>而關於build code使用到了kbuild，一個Makefile的模板如下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KERNELDIR :&#x3D; &#x2F;lib&#x2F;modules&#x2F;$(shell uname -r)&#x2F;build</span><br><span class="line">CURRENT_PATH :&#x3D; $(shell pwd)</span><br><span class="line"></span><br><span class="line">obj-m :&#x3D; chrdevbase.o</span><br><span class="line"></span><br><span class="line">build: kernel_modules</span><br><span class="line"></span><br><span class="line">kernel_modules:</span><br><span class="line">    make -C $(KERNELDIR) M&#x3D;$(CURRENT_PATH) modules</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">    make -C $(KERNELDIR) M&#x3D;$(CURRENT_PATH) clean</span><br></pre></td></tr></table></figure><ul><li>C表示切換到指定的目錄中</li><li>M表示模塊源碼目錄</li></ul><p>編好後，可以透過<code>ismod</code>, <code>modprobe</code>, <code>rmmod</code>來加載和卸載，透過<code>lsmod</code>, <code>modinfo</code>來進行查看</p><ul><li><code>ismod</code>並不能解決module的依賴關係</li><li><code>modprobe</code>會去/lib/modules/<kernel_version>查看modules進行查找<ul><li>如果沒有該目錄，就自己創一個，然後將driver(.ko)放進去</li><li><code>modprobe &lt;driver_name&gt;.ko</code></li><li>如果出現”can’t open ‘modules.dep’: no such file or directory”，可以透過<code>depmod</code>自動生成</li></ul></li></ul><h3 id="字符設備的註冊與註銷"><a href="#字符設備的註冊與註銷" class="headerlink" title="字符設備的註冊與註銷"></a>字符設備的註冊與註銷</h3><p>基於模塊加載&amp;卸載的code繼續修改，註冊&amp;註銷char device或使用到下列兩個function，被定義在/include/linux/fs.h中</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* /include/linux/fs.h */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">register_chrdev</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> major, <span class="keyword">const</span> <span class="keyword">char</span> *name,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">const</span> struct file_operations *fops)</span></span></span><br><span class="line"><span class="function">                  </span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">unregister_chrdev</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> major, <span class="keyword">const</span> <span class="keyword">char</span> *name)</span></span></span><br></pre></td></tr></table></figure><ul><li>marjor是主設備號，可以透過<code>cat /proc/devices</code>查看當前已註冊的的主設備號</li><li>name是device name</li><li>fops就是之前提到過的file_operations struct，也就是這個driver能提供哪些操作</li></ul><h3 id="設備號"><a href="#設備號" class="headerlink" title="設備號"></a>設備號</h3><p>為了方便管理，Linux下每個設備都有一個設備號，<strong>設備號由主設備號和次設備號構成</strong></p><ul><li>前12bit為主設備號，表示某個具體的驅動</li><li>後20bit為次設備號，表示使用該驅動的各個設備</li></ul><p>Linux使用dev_t的type來表示設備號，可以看到它是一個unsigned int<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* /include/linux/types.h */</span></span><br><span class="line"><span class="keyword">typedef</span> u32 <span class="keyword">__kernel_dev_t</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">__kernel_dev_t</span><span class="keyword">dev_t</span>;</span><br></pre></td></tr></table></figure></p><p>關於主設備號、次設備號的定義位於include/linux/kdev_t.h</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* /include/linux/kdev_t.h */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MINORBITS20</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MINORMASK((1U &lt;&lt; MINORBITS) - 1)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAJOR(dev)((unsigned int) ((dev) &gt;&gt; MINORBITS))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MINOR(dev)((unsigned int) ((dev) &amp; MINORMASK))</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MKDEV(ma,mi)(((ma) &lt;&lt; MINORBITS) | (mi))</span></span><br></pre></td></tr></table></figure><p>透過這幾個macro來產生設備號</p><p>但<code>register_chrdev()</code>並沒有要填寫次設備號？他會<strong>佔用當前主設備號下的所有次設備號</strong>，比較不方便，所以有其他比較好用的register func，後面會提到 </p><h3 id="file-operations的具體實現"><a href="#file-operations的具體實現" class="headerlink" title="file_operations的具體實現"></a>file_operations的具體實現</h3><p>在字符設備驅動框架已經提過，我們必須要實作fops對應的function，然後將fops帶入註冊函數的參數中。</p><p>先寫個簡單的殼出來:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/module.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/kernel.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/init.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/fs.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHRDEVBASE_MAJOR 200</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHRDEVBASE_NAME <span class="meta-string">"chrdevbase"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">chrdevbase_open</span><span class="params">(struct inode *inode, struct file *filp)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    printk(KERN_INFO <span class="string">"chrdevbase_open\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">struct <span class="keyword">int</span> <span class="title">chrdevbase_close</span><span class="params">(struct inode *inode, struct file *filp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    printk(KERN_INFO <span class="string">"chrdevbase_close\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">struct <span class="keyword">int</span> <span class="title">chrdevbase_read</span><span class="params">(struct file *filp, <span class="keyword">char</span> *<span class="built_in">buffer</span>, <span class="keyword">size_t</span> length, <span class="keyword">loff_t</span> offset)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    printk(KERN_INFO <span class="string">"chrdevbase_read\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">struct <span class="keyword">int</span> <span class="title">chrdevbase_write</span><span class="params">(struct file *filp, <span class="keyword">char</span> *buf, <span class="keyword">size_t</span> len, <span class="keyword">loff_t</span> offset)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    printk(KERN_INFO <span class="string">"chrdevbase_write\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">file_operations</span> <span class="title">chrdevbase_fops</span> = &#123;</span></span><br><span class="line">    .owner = THIS_MODULE,</span><br><span class="line">    .<span class="built_in">open</span> = chrdevbase_open,</span><br><span class="line">    .<span class="built_in">close</span> = chrdevbase_close,</span><br><span class="line">    .<span class="built_in">read</span> = chrdevbase_read,</span><br><span class="line">    .<span class="built_in">write</span> = chrdevbase_write,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> __init <span class="title">chrdevbase_init</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    printk(KERN_INFO <span class="string">"module init\n"</span>);</span><br><span class="line">    ret = register_chrdev(CHRDEVBASE_MAJOR, CHRDEBBASE_NAME, &amp;chrdevbase_fops);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        printk(KERN_INFO <span class="string">"chrdevbase init failed!\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> __exit <span class="title">chrdevbase_exit</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    printk(KERN_INFO <span class="string">"module init\n"</span>);</span><br><span class="line">    unregister_chrdev(CHRDEVBASE_MAJOR, CHRDEBBASE_NAME);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">module_init(chrdevbase_init);</span><br><span class="line">module_exit(chrdevbase_exit);</span><br></pre></td></tr></table></figure><p>關於struct中採用了”.(dot)”的成員變數宣告方式，可以參考<a href="https://meetonfriday.com/posts/39485259/">[Linux Kernel慢慢學]探討Designated Initializers</a></p><h3 id="應用程序編寫"><a href="#應用程序編寫" class="headerlink" title="應用程序編寫"></a>應用程序編寫</h3><p><strong>Linux下一切皆文件，所以要操作driver也需要將該driver的字元文件打開</strong>，下面編寫一個簡易的應用程序來介紹如何開啟字元驅動文件</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ./chrdevbaseApp &lt;filename&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> fd = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">char</span> *filename;</span><br><span class="line">    <span class="keyword">char</span> readbuf[<span class="number">10</span>], writebuf[<span class="number">100</span>];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* open() return a file descriptor */</span></span><br><span class="line">    fd = <span class="built_in">open</span>(filename, O_RDWR);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (fd == <span class="number">-1</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Can not open file %s\n"</span>, filename);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* read */</span></span><br><span class="line">    ret = <span class="built_in">read</span>(fd, readbuf, <span class="number">50</span>);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Read file fail\n"</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* write */</span></span><br><span class="line">    ret = <span class="built_in">write</span>(fd, writebuf, <span class="number">50</span>);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Write file fail\n"</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* close */</span></span><br><span class="line">    ret = <span class="built_in">close</span>(fd);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Close file failed\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>測試：</p><ol><li>加載驅動: <code>modbrpobe &lt;driver_name&gt;.ko</code><ul><li>加載完可透過<code>cat /proc/devices</code>確認</li></ul></li><li>創建device node: <code>mknod /dev/&lt;driver_name&gt; c 200 0</code><ul><li>c代表字元驅動、200是主設備號、0是次設備號</li><li>創建完後會在/dev下面看到驅動文件</li></ul></li><li>透過應用開啟字元驅動文件: <code>./chrdevbaseAPP /dev/chrdevbase</code></li></ol><h3 id="chrdevbase虛擬設備驅動的完善"><a href="#chrdevbase虛擬設備驅動的完善" class="headerlink" title="chrdevbase虛擬設備驅動的完善"></a>chrdevbase虛擬設備驅動的完善</h3><p>在<a href="###file_operations的具體實現">file_operations的具體實現</a>和<a href="###應用程序編寫">應用程序編寫</a>中只完成了驅動以及app的殼，這邊會將驅動和app剩下的code繼續完成。</p><h4 id="驅動"><a href="#驅動" class="headerlink" title="驅動"></a>驅動</h4><ol><li>驅動和數據的傳遞: data在kernel space和 user space不能直接調用，要透過<code>copy_to_user()</code>, <code>copy_from_user()</code>來進行轉換<ul><li><code>#include &lt;linux/uaccess.h&gt;</code></li></ul></li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/module.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/kernel.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/init.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/fs.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/uaccess.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHRDEVBASE_MAJOR 200</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHRDEVBASE_NAME <span class="meta-string">"chrdevbase"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> readbuf[<span class="number">100</span>];</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> writebuf[<span class="number">100</span>];</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> kerneldata[] = &#123;<span class="string">"kernel data"</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">chrdevbase_open</span><span class="params">(struct inode *inode, struct file *filp)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    printk(KERN_INFO <span class="string">"chrdevbase_open\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">struct <span class="keyword">int</span> <span class="title">chrdevbase_close</span><span class="params">(struct inode *inode, struct file *filp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    printk(KERN_INFO <span class="string">"chrdevbase_close\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">struct <span class="keyword">int</span> <span class="title">chrdevbase_read</span><span class="params">(struct file *filp, <span class="keyword">char</span> *buf, <span class="keyword">size_t</span> length, <span class="keyword">loff_t</span> offset)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    printk(KERN_INFO <span class="string">"chrdevbase_read\n"</span>);</span><br><span class="line">    <span class="built_in">memcpy</span>(readbuf, kerneldata, <span class="keyword">sizeof</span>(kerneldata));</span><br><span class="line">    ret = copy_to_user(<span class="built_in">buffer</span>, readbuf, count);</span><br><span class="line">    <span class="keyword">if</span> (ret == <span class="number">0</span>) &#123;</span><br><span class="line">        printk(<span class="string">"kernel send data: %s\n"</span>, readbuf);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">/* error handle */</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">struct <span class="keyword">int</span> <span class="title">chrdevbase_write</span><span class="params">(struct file *filp, <span class="keyword">char</span> *buf, <span class="keyword">size_t</span> len, <span class="keyword">loff_t</span> offset)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    printk(KERN_INFO <span class="string">"chrdevbase_write\n"</span>);</span><br><span class="line">    ret = copy_from_user(writebuf, buf, count);</span><br><span class="line">    <span class="keyword">if</span> (ret == <span class="number">0</span>) &#123;</span><br><span class="line">        printk(<span class="string">"kernel receive data: %s\n"</span>, writebuf);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">/* error handle */</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">file_operations</span> <span class="title">chrdevbase_fops</span> = &#123;</span></span><br><span class="line">    .owner = THIS_MODULE,</span><br><span class="line">    .<span class="built_in">open</span> = chrdevbase_open,</span><br><span class="line">    .<span class="built_in">close</span> = chrdevbase_close,</span><br><span class="line">    .<span class="built_in">read</span> = chrdevbase_read,</span><br><span class="line">    .<span class="built_in">write</span> = chrdevbase_write,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> __init <span class="title">chrdevbase_init</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    printk(KERN_INFO <span class="string">"module init\n"</span>);</span><br><span class="line">    ret = register_chrdev(CHRDEVBASE_MAJOR, CHRDEBBASE_NAME, &amp;chrdevbase_fops);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        printk(KERN_INFO <span class="string">"chrdevbase init failed!\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> __exit <span class="title">chrdevbase_exit</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    printk(KERN_INFO <span class="string">"module init\n"</span>);</span><br><span class="line">    unregister_chrdev(CHRDEVBASE_MAJOR, CHRDEBBASE_NAME);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">module_init(chrdevbase_init);</span><br><span class="line">module_exit(chrdevbase_exit);</span><br></pre></td></tr></table></figure><h4 id="app"><a href="#app" class="headerlink" title="app"></a>app</h4><p>App要可以透過驅動對device做讀寫，open char device file後要可以做read/write</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ./chrdevbaseApp &lt;filename&gt; &lt;num&gt;</span></span><br><span class="line"><span class="comment"> * @num: 1 代表read, 2代表write</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> fd = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">char</span> *filename;</span><br><span class="line">    <span class="keyword">char</span> readbuf[<span class="number">10</span>], writebuf[<span class="number">100</span>];</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">char</span> usrdata[] = &#123;<span class="string">"usr data"</span>&#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (argc != <span class="number">3</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* open() return a file descriptor */</span></span><br><span class="line">    fd = <span class="built_in">open</span>(filename, O_RDWR);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (fd == <span class="number">-1</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Can not open file %s\n"</span>, filename);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (atoi(argv[<span class="number">2</span>]) == <span class="number">1</span> ) &#123;</span><br><span class="line">         <span class="comment">/* read */</span></span><br><span class="line">        ret = <span class="built_in">read</span>(fd, readbuf, <span class="number">50</span>);</span><br><span class="line">        <span class="keyword">if</span> (ret &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"Read file fail\n"</span>);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"App read data: %s\n"</span>, readbuf);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (atoi(argv[<span class="number">2</span>]) == <span class="number">2</span> ) &#123;</span><br><span class="line">        <span class="built_in">memcpy</span>(writebuf, usrdata, <span class="keyword">sizeof</span>(usrdata));</span><br><span class="line">        <span class="comment">/* write */</span></span><br><span class="line">        ret = <span class="built_in">write</span>(fd, writebuf, <span class="number">50</span>);</span><br><span class="line">        <span class="keyword">if</span> (ret &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"Write file fail\n"</span>);   </span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"Write data: %s \n"</span>, writebuf);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/* close */</span></span><br><span class="line">    ret = <span class="built_in">close</span>(fd);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Close file failed\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>能夠透過app &amp; driver之間做讀寫，代表未來就可以透過 app 去操控 driver 對 devices 做不同的操作(例如開關燈)</p>]]></content>
      
      
      <categories>
          
          <category> 【課程筆記】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux kernel </tag>
            
            <tag> study </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>全台灣最大數據分析競賽上線啦!蝦皮2020數據分析競賽心得</title>
      <link href="/posts/10ddd910/"/>
      <url>/posts/10ddd910/</url>
      
        <content type="html"><![CDATA[<p>這篇文章紀錄2020/11/21的<a href="https://careers.shopee.tw/bestcoder/" target="_blank" rel="noopener">蝦皮數據分析競賽</a>心得~</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/v1614182087/blog_posts_2021/2021_02/%E5%9C%96%E7%89%871_mfa3qb.png" alt=""></p><h2 id="競賽介紹"><a href="#競賽介紹" class="headerlink" title="競賽介紹"></a>競賽介紹</h2><p>儘管競賽名稱是<strong>全台灣最大數據分析競賽</strong>，但實際上的競賽內容我覺得還不太夠格冠上這個名稱xD</p><p>為何這樣說呢?</p><p>首先，組別總共分成學生組和職業組，根據組別其實競賽題型差異極大:</p><ul><li>學生組: 考類似leetcode的演算法題型</li><li>職業組: 考kaggle數據分析競賽</li></ul><p>並且使用的平台都是在kaggle上(不太知道學生組這樣的題型要如何在kaggle上去做排名?)</p><p>接下來是競賽時間，他是線下賽，一開始想說一個kaggle的線上賽好說歹說都是星期起跳的，線下賽好歹你也要給個一天的時間吧</p><p>結果競賽時間只有<strong>2個小時</strong>，你沒看錯就是120分鐘而已呢。</p><a id="more"></a><p>如果對一個數據分析的流程比較有概念的話，通常我們知道資料處理的過程大致分成幾個階段</p><ol><li>Data Understaning: 了解資料、問題資料前處理</li><li>Analysis: 資料分析(統計分析、視覺化)，透過探索式資料分析(Exploratory Data Analysis, EDA)去幫助我們快速理解數據</li><li>Modeling: 進行前處理後嘗試去建模，包含baseline model</li><li>Tuning: 開始根據結果進行調參</li><li>Evaluation: 如何知道我們的模型夠不夠好? 根據training data / validation data來進行判斷</li></ol><p>1 - 5 通常會是一個循環，可能走到 step4, step5後又回去step1繼續觀察不同的特徵…</p><p>上述看起來五項並不多，不過光是上面這幾項就夠折騰了，更何況還要在2個小時內搞定真的是很大的挑戰。</p><p>不過還是很感謝三名隊友願意接受我的邀請，跟我一起去台北南港展覽館只為了比這兩個小時(競賽時間也是報名完後才知道的，如果事前知道只有兩個小時就可能不會報名了…還有個隊友特地從高雄搭高鐵上來)</p><p>接下來就分享一下當天現場的狀況搂~</p><h2 id="歷程紀錄"><a href="#歷程紀錄" class="headerlink" title="歷程紀錄"></a>歷程紀錄</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1614182675/blog_posts_2021/2021_02/153040464_4404556852893865_1861831945638981743_n_luxiuh.jpg" alt=""></p><div class="img-desc">  很久沒來南港展覽館了，上一次來也是參加某個年會的時候，儘管活動時間只有半天，但場佈其實仍然壯大</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1614182676/blog_posts_2021/2021_02/153944082_4404556932893857_2770265123389897453_n_gscijw.jpg" alt=""></p><div class="img-desc">  很壯觀的競賽會場，蝦皮的袋子內其實就是廣告紙xD 好歹給個紀念品嘛(其實賽後有送競賽T-shirt)</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1614182675/blog_posts_2021/2021_02/152798623_4404556789560538_7181707760012121518_n_dxl3ov.jpg" alt=""></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1614182675/blog_posts_2021/2021_02/153271830_4404556636227220_603931400285809603_n_ktfqvn.jpg" alt=""></p><div class="img-desc">  廣告紙裡面有一張"臨時抱佛腳"...? 雖然我沒有去登記所以不知道到底是什麼</div><p>下午正式開始活動後，主辦方會花點時間介紹Telegram的使用，以及Kaggle平台的操作方式，所以沒有經驗的參賽者也不會遇到太大問題(吧)。</p><p>介紹完後就是短得可憐的兩小時競賽，職業組的題目是: <strong>給予一堆id在某期間的交易紀錄，預測他在未來某段時間的交易次數</strong>(應該是這樣，有點久了不太記得題目了QQ)</p><p>給予的資料量不算大，但其實也不小，並且有很多個不同的csv file要去分析和了解資料集的欄位，所以處理上還是需要一點時間。</p><p>我們team 4個人的分工大致如下:</p><ol><li>一開始大家都先讀題，確保了解題目</li><li>2-3個人相互分工進行前處理，1個人去處理後續的modeling<ul><li>前處理分工的方式很重要(e.g.如何直接使用對方處理好的data來做自己的工作)，不然大家會做到相同的事情</li></ul></li><li>確保能夠產生第一版可以上傳的output.csv之後，開始進行調參跟其他嘗試</li></ol><p>儘管事前有先討論過分工模式，但實際上兩個小時真的太趕了，我們team來說光是在了解資料和簡單的前處理其實就花了大約一個半小時(因為中途誤解題目又多花了很多時間)，接下來才是簡單的建模和測試，然後弄一弄沒什麼時間調參時間就結束了</p><p>在最後一次的結果我們晚了幾秒來不及上傳，不過賽後上傳後發現我們的分數是可以擠進銅牌的，真是太可惜了QQ</p><p>最後雖然沒有得名，但還是一次蠻新鮮的體驗，很少參加到線下的數據分析競賽，而且我也因為工作的緣故疏於練習python，導致在競賽中拖累隊友QQ</p><p>資料處理真的需要熟練度的，平常就該好好練習~</p><p>最後附上冠軍隊伍的名稱，很有趣，主持人唸到一半後就直接放棄了xD</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/v1614182676/blog_posts_2021/2021_02/152738764_4404556709560546_2537025077757566192_n_blp8ot.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> competitive programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[讀書心得]富爸爸商學院</title>
      <link href="/posts/15ad89ab/"/>
      <url>/posts/15ad89ab/</url>
      
        <content type="html"><![CDATA[<h2 id="書籍資訊"><a href="#書籍資訊" class="headerlink" title="書籍資訊"></a>書籍資訊</h2><ul><li>書名: 富爸爸商學院</li><li>作者: Robert T. Kiyosaki, Sharon L. Lechter</li><li>出版日期: Jan 25, 2005</li><li>博客來網址(如果覺得這是一本好書，請支持原作唷！): <a href="https://www.books.com.tw/exep/assp.php/john85051232/products/0010865217?utm_source=john85051232&amp;utm_medium=ap-books&amp;utm_content=recommend&amp;utm_campaign=ap-202103" target="_blank" rel="noopener">富爸爸商學院：銷售致富的財商教育</a></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1614172662/blog_posts_2021/2021_02/E603651d7bd3d4949915534-0_c3asav.png" alt=""></p><p>(本文是【富爸爸商學院】的讀書心得和書籍導讀，如果你看完覺得這本書對你是有幫助的，歡迎購買原做支持作者！)<br>聽到「直銷」或「業務」，你的第一反應是先拒絕再說？<br>你知道銷售力可以帶來超越想像的成長，培養出過人的商業技巧與成功心態嗎？</p><p>跟大多數人一樣，當羅勃特．T．清崎第一次聽到直銷時，他也抱著懷疑的態度。</p><p>不過，經過深入了解，他發現直銷產業其實是一所真正的商學院，不只沒有高不可攀的入學門檻，還蘊藏著足以改變一生的核心價值。</p><a id="more"></a><h2 id="內容摘要"><a href="#內容摘要" class="headerlink" title="內容摘要"></a>內容摘要</h2><h3 id="富人為什麼更加富有"><a href="#富人為什麼更加富有" class="headerlink" title="富人為什麼更加富有"></a>富人為什麼更加富有</h3><blockquote><p>「如果你想致富，就要與富人或者能幫助妳致富的人建立網絡。」</p></blockquote><p>非常不幸的是，「很多人終其一生，都是與財務上扯自己後腿的人建立網絡」</p><p>因此，本書的觀點之一，就是想告訴大家，直銷企業擁有能夠幫助你更加富有的人。</p><h3 id="致富的其它途徑"><a href="#致富的其它途徑" class="headerlink" title="致富的其它途徑"></a>致富的其它途徑</h3><p>「如果你想成為富人，就要尋找最適合自己的致富途徑。」下面列出了一些人們可以選擇的致富途徑。</p><ol><li>你可以因為某人擁有財富而與其結婚，成為一個富人</li><li>你可以透過詐騙而成為一個富人</li><li>你可以因為貪婪而致富</li><li>你可以透過降低個人生活水準而致富</li><li>你可以透過辛勤工作而致富</li><li>你可以憑藉非凡的聰明、才智、魅力或者天賦致富</li><li>你可以憑藉好運氣致富</li><li>你可以透過繼承一大筆遺產致富</li><li>你可以透過投資致富</li><li>你可以透過建立自己的企業致富</li><li>你可以透過建立一個直銷企業而致富</li></ol><p>而這本書主要提出的是第十一種方法，透過建立一個直銷企業，儘管作者並沒有真正的參與過一個直銷企業的創立，但他仍以他的觀點來敘述直銷企業的各種優勢及劣勢，以及為何他覺得這是一個不錯的致富途徑。</p><h3 id="直銷企業的核心價值之一-真正平等的機會"><a href="#直銷企業的核心價值之一-真正平等的機會" class="headerlink" title="直銷企業的核心價值之一 - 真正平等的機會"></a>直銷企業的核心價值之一 - 真正平等的機會</h3><blockquote><p>「直銷企業奉行門戶開放原則」</p></blockquote><p>相對於傳統教育體制和公司存在的問題，也就是「適者生存」的價值系統，這個系統秉持著如果一個人不適合一件事情，這個環境會直接淘汰他。</p><p>例如在學校老師覺得學業表現不好的學生在其他領域也不會有什麼好的表現；公司老闆會因為一件事情做不好就抹滅了你所有的努力。而對於直銷企業基本上不存在什麼進入門檻，在絕大數情況下，直銷公司都是非常富有同情心的企業，如果願意進一步了解並按自己的步驟去學習研究，直銷企業就會對你大有幫助。</p><h3 id="直銷企業的核心價值之二-改變人生的財商教育"><a href="#直銷企業的核心價值之二-改變人生的財商教育" class="headerlink" title="直銷企業的核心價值之二 - 改變人生的財商教育"></a>直銷企業的核心價值之二 - 改變人生的財商教育</h3><p>作者推薦直銷企業的首要原因在於，直銷企業提供了一種全新的教育體系。</p><p>相較於傳統的教育體系往往建立在畏懼的基礎上，使得人們畏懼失敗，而不是積極應對挑戰，從自身錯誤中汲取教訓。直銷企業鼓勵大家一同學習、奮鬥，從實戰經驗中去攝取經驗，並坦然面對失敗，這是一種比較切合實際的教育方式。</p><p>這點蠻認同的，畢竟在學校老師往往告訴我們，認真讀書未來才能怎樣怎樣，這給了我們一種既定的印象: 如果學業不好以後做什麼都會失敗。這使得我們不敢冒險，沒有足夠的勇氣去面對自己想嘗試的事情。但實際上任何成功的背後都是屬不清的失敗和冒險累積而來的。</p><h3 id="直銷企業的核心價值之三-志同道合的朋友圈"><a href="#直銷企業的核心價值之三-志同道合的朋友圈" class="headerlink" title="直銷企業的核心價值之三 - 志同道合的朋友圈"></a>直銷企業的核心價值之三 - 志同道合的朋友圈</h3><p>又或者可以這麼說，同溫層。</p><p>當需要做出一些有挑戰、風險的決定時，可能大多時候從身邊都會聽到「會不會太冒險」這種意見或擔心。對作者來說，跳脫舒適圈離職去創業時，最艱難的工作莫過於如何應對朋友、家人和同事的說法和想法。</p><blockquote><p>「這是現金流象限的改變，而非工作崗位的改變」</p></blockquote><p>作者在富爸爸系列書籍所提出的現金流象限，根據自己的現金流來源將人分成了四個不同的象限</p><ul><li>E 雇員</li><li>S 自由企業者 / 小企業所有人</li><li>B 企業所有人</li><li>I 投資者</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1614174594/blog_posts_2021/2021_02/03_%E5%AF%8C%E7%88%B8%E7%88%B8%E7%8F%BE%E9%87%91%E6%B5%81_ESBI%E5%9B%9B%E8%B1%A1%E9%99%90_w2orzw.png" alt=""></p><p>(圖片引用自<a href="https://imirene.co/%E5%AF%8C%E7%88%B8%E7%88%B8%E7%8F%BE%E9%87%91%E6%B5%81/" target="_blank" rel="noopener">ESBI現金流四象限，你在哪一個象限？</a>)</p><p>根據所屬象限的不同，價值觀也就不同。比方說，對於E象限的人來說，他們尋求的往往是「安穩」；S象限的人追求「獨立、自主」；B象限追求「團隊合作」；而I象限追求「財富自由」。</p><p>所屬象限的改變，意味著價值觀和朋友圈的改變。因此，認清自己現在處於哪個象限，以及未來想要成為什麼象限的人，又應該如何踏出第一步是很重要的事情。</p><p>最後，如果你是以B象限作為目標，那麼直銷或許蠻適合你，因為那邊聚集了許多同樣具有B象限特質的夥伴。</p><h3 id="直銷企業的核心價值之四-等比級數增加的人脈網絡"><a href="#直銷企業的核心價值之四-等比級數增加的人脈網絡" class="headerlink" title="直銷企業的核心價值之四 - 等比級數增加的人脈網絡"></a>直銷企業的核心價值之四 - 等比級數增加的人脈網絡</h3><p>梅特卡夫法則提到</p><p><strong>一個網絡的經濟價值 = 用戶數量的平方</strong></p><p>以電話這個產品來舉例，如果只有一台電話，那麼無論這個產品做的在好也沒有任何經濟價值。但當有兩個人、三個人….越多人使用時，每增加一部電話所附帶的經濟價值實際上等比級數增加，而不是等差級數。</p><p>而直銷企業，就是運用了梅特卡夫法則的力量，大家的任務就是去模仿或者重複像自己一樣的人，當人越來越多時，背後的網絡經濟價值也會巨量成長。並且直銷企業並不像傳統企業，門檻相對低很多，只要有個好創意，有好的人脈網絡，那麼你也可以試著著手成立一個直銷企業。</p><h3 id="直銷企業的核心價值之五-培養個人的推銷技巧"><a href="#直銷企業的核心價值之五-培養個人的推銷技巧" class="headerlink" title="直銷企業的核心價值之五 - 培養個人的推銷技巧"></a>直銷企業的核心價值之五 - 培養個人的推銷技巧</h3><blockquote><p>「在商業領域，推銷技巧是最重要的」</p></blockquote><p>在書中作者向富爸爸請教如何成為一個B象限的成功者時，富爸爸叫他去從一名推銷員做起。</p><p>其實，推銷並不只是一份工作技巧，還融入了我們的各種生活中。例如，當人們上台演講時，就是向台下的觀眾推銷自己的理念；嬰兒肚子餓而哭鬧時，就是在向周圍的人溝通、推銷自己的感受；當你在和一名女性約會時，就是在和她推銷自己；我們在尋找工作面試時，就是在向面試官推銷自己的能力。</p><p>由此可見，這個名為推銷的技術並不只是推銷員應該具備的特質，而是所有人都應該好好學習的一項能力。</p><blockquote><p>「人們應該學習克服自己對於推銷的畏懼，而不是讓這種畏懼主宰自己的生活。」</p></blockquote><p>出於本性，人們都害怕被拒絕，害怕向他人推銷自己想要推銷的東西後而遭到拒絕的滋味。直銷業的好處在於，她讓人們有機會直是內心的畏懼、進而克服內心畏懼，而在一次次的克服中，將使人更加歷練成長。</p><p>「世上最成功的人，其實也就是被拒絕最多的人」，因為當你被拒絕的越多，被人接受、支持的機會也就越多。</p><h3 id="直銷企業的核心價值之六-培養個人領導技巧"><a href="#直銷企業的核心價值之六-培養個人領導技巧" class="headerlink" title="直銷企業的核心價值之六 - 培養個人領導技巧"></a>直銷企業的核心價值之六 - 培養個人領導技巧</h3><p>直銷企業最重要的價值之一 - 領導技巧訓練。領導技巧同時也是在B象限取得成功的重要關鍵之一。</p><blockquote><p>一個好的領導技巧需要設法去激發人們的精神</p></blockquote><p>這句話是什麼意思呢？一種錯誤的交流方式是，講者透過各種情感方法去激勵聽眾，讓他們去做出講著想要他們所做的事情。例如，下面這些話就是利用情感作為手段的例子：</p><ol><li>「如果你在學校都學得不好，那以後就找不到好工作。」</li><li>「如果你不準時上班，就會被解僱。」</li><li>「謹慎行事，不必冒不必要的風險。」</li><li>「你不會辭職，因為沒人能像我們公司給你這麼好的待遇。」</li></ol><p>這些交流都是利用大家的恐懼或貪慾的情感，鼓勵人們去從事某項工作。當恐懼或貪慾成為激勵人們的主導因素時，就有可能泯滅我們的精神。</p><p>而好的領導者他們具備<strong>能夠針對聽眾的精神層面去進行交流</strong>，例如下列這些歷史偉人們曾說過的話：</p><ol><li>「決定美國人自由或者被奴役的時刻已經來臨。」 - 喬治 華盛頓</li><li>「不要讓自己不能做的事情干擾了自己能做的事情。」 - 約翰 伍登</li><li>「不要想去做一位成功的人，而是要努力去做一位有價值的人。」 - 愛因斯坦</li></ol><h3 id="直銷企業的核心價值之七-不為金錢工作"><a href="#直銷企業的核心價值之七-不為金錢工作" class="headerlink" title="直銷企業的核心價值之七 - 不為金錢工作"></a>直銷企業的核心價值之七 - 不為金錢工作</h3><p>作者認為，在談論涉及金錢的時候，存在著三種感受，與之對應的是三種不同的生活方式:</p><ol><li>恐懼的感受: 當一個人身無分文，無家可歸的時候，不知道明天會在哪裡的情況</li><li>憤怒與挫折的感受: 當你有了工作，但生活卻完全被工作給壟罩，沒有屬於自己的時間去做其他事情。這種人明白如果自己停止工作，就無法維持正常生活。</li><li>快樂、安寧和愜意的生活感受: 生活在一種平和的心態中，無論自己是否繼續工作，都會持續不斷得到一大筆收入，也就是大家所說的財富自由。</li></ol><p>這三種不同的感受，每個人都希望成為第三種，在此之前須要認清的是自己的核心價值觀: <strong>財富是什麼?</strong></p><blockquote><p>財富是停止工作後，還能維持生活的”時間”</p></blockquote><p>作者用了時間做為衡量財富的單位，如果一個人有一千元，每天的基本開銷是一百元，那麼他的財富就是十天。透過這個概念可以來衡量想讓自己不再受金錢所苦大約需要多少的財富。</p><p>而在現金流象限中，E/S象限的人和B/I象限的人所擁有的金錢價值觀又不同: 假設把財務報表簡化成四個項目: <strong>收入、開支、資產、負債</strong></p><ul><li>E/S象限的人關注的是如何提高自己的收入</li><li>B/I象限的人則是傾向關注的是資產的部分</li></ul><blockquote><p>情商是財富的基礎</p></blockquote><p>擁有高情商是擁有高財富的基礎，巴菲特曾說到「一個不能管理好自己情感的人，肯定無法管理好自己的資金」。</p><p>想了解自己的情商對生活造成了什麼影響嗎? 試著自己回答下列問題: 我的情商讓自己成為了一個……的人?</p><ol><li>過於害羞</li><li>缺乏安全感</li><li>害怕被人拒絕</li><li>過於武斷</li><li>過於暴躁</li><li>太衝動</li><li>一發怒</li><li>懶惰</li><li>安於變化</li></ol><h3 id="直銷企業的核心價值之八-追逐夢想"><a href="#直銷企業的核心價值之八-追逐夢想" class="headerlink" title="直銷企業的核心價值之八 - 追逐夢想"></a>直銷企業的核心價值之八 - 追逐夢想</h3><blockquote><p>沒有偉大夢想的人，永遠過著平民百姓的生活</p></blockquote><p>世界上有五類夢想家:</p><ol><li>沉迷於輝煌過去的夢想家: 沉浸於過去的成就，但沒有為自己找尋新目標的人，他將不會有動力繼續成長</li><li>只有小夢想的夢想家: 這些人所謂的夢想都是很容易實現的，但他們卻從來沒有認真想過如何實現自己的夢想，只會找各種理由藉口推託，例如「我太忙了」、「我沒有時間去完成」……</li><li>實現了自己過去的夢想，卻沒有確立新夢想的夢想家: 這種類型的人往往會對於自己目前的生活產生厭倦感</li><li>有著大夢想，卻沒有具體計劃實現，最終一無所有的夢想家</li><li>擁有大夢想，逐步實現這些夢想，並又有了更大夢想的夢想家</li></ol><p>大部分人都希望自己是第五種夢想家，但實際上所做出的行動又是哪一種類型的夢想家呢?</p><h2 id="讀後心得"><a href="#讀後心得" class="headerlink" title="讀後心得"></a>讀後心得</h2><p>儘管這是一本介紹直銷企業的書籍，但書本內提到了幾個致富所需的特質和觀念，這些觀念並不只能用在直銷企業上，而是各行各業都適用。</p>]]></content>
      
      
      <categories>
          
          <category> reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[課程筆記]Linux Driver正點原子課程筆記1 &amp; 2 - Linux驅動開發 &amp; 字符設備驅動開發基礎實驗</title>
      <link href="/posts/81763900/"/>
      <url>/posts/81763900/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多課程筆記，至<a href="https://meetonfriday.com/posts/85f1c2a/">[課程筆記]課程筆記系列總覽</a>可以看到目前已發布的所有文章！〗</p><h2 id="Course-1-Linux驅動開發"><a href="#Course-1-Linux驅動開發" class="headerlink" title="Course 1 - Linux驅動開發"></a>Course 1 - Linux驅動開發</h2><h3 id="驅動開發思維"><a href="#驅動開發思維" class="headerlink" title="驅動開發思維"></a>驅動開發思維</h3><p>為何需要Linux driver?</p><ul><li>直接在Linux下操作register不現實，太複雜也不安全</li><li>Linux具有各種驅動框架(driver framework)，使得各種設置能夠依照一訂的規範進行開發</li></ul><p>Linux下一切皆文件，而所有的driver都放置在/dev/xxx下，可以透過fops(file operation)來操作(打開、關閉、讀寫)</p><p>新版的Linux下supoort device tree，是一個.dts，記錄devices information，kernel會去分析.dts文件了解有哪些設備</p><h3 id="驅動開發分類"><a href="#驅動開發分類" class="headerlink" title="驅動開發分類"></a>驅動開發分類</h3><p>linux 驅動分成三大類</p><ol><li>char device driver: 字符設備驅動，按照字符依序存取，大部分的驅動都是這種，e.g. i2c</li><li>block device driver: 能以固定大小長度傳送和轉移，可以不需要按照順序操作，e.g. SD card</li><li>network device driver: 網路設備驅動，e.g. wifi</li></ol><a id="more"></a><p>一個設備可以同時是不同類型的驅動，例如USB WIFI, SDIO WIFI是network device driver但同時也是char device driver</p><h2 id="Course-2-字符設備驅動開發基礎實驗"><a href="#Course-2-字符設備驅動開發基礎實驗" class="headerlink" title="Course 2 - 字符設備驅動開發基礎實驗"></a>Course 2 - 字符設備驅動開發基礎實驗</h2><h3 id="應用程序和驅動的交互原理"><a href="#應用程序和驅動的交互原理" class="headerlink" title="應用程序和驅動的交互原理"></a>應用程序和驅動的交互原理</h3><p>驅動就是用來和device溝通的程式。Linux驅動編譯除了要編寫一個driver外，還要編寫一個測試應用程式(application)</p><ul><li>單晶片機下驅動和應用是放在一個文件內</li><li>Linux下驅動和應用是完全分開的，所以此時就牽扯到user space &amp; kernel space</li></ul><p>以下部分內容引用自<a href="https://blog.csdn.net/liujianfei526/article/details/77824639" target="_blank" rel="noopener">linux内存管理—用户空间和内核空间</a></p><blockquote><p>关于虚拟内存有三点需要注意：</p><ul><li>4G的进程地址空间被人为的分为两个部分—用户空间与内核空间。用户空间从0到3G（0xc0000000）,内核空间占据3G到4G。用户进程通常情况下只能访问用户空间的虚拟地址，不能访问内核空间的虚拟地址。例外情况只有用户进程进行系统调用（代表用户进程在内核态执行）等时刻可以访问到内核空间。</li><li>用户空间对应进程，所以每当进程切换，用户空间就会跟着变化；而内核空间是由内核负责映射，它并不会跟着进程变化，是固定的。内核空间地址有自己对应的页表，用户进程各自有不同的页表。</li><li>每个进程的用户空间都是完全独立、互不相干的。<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_500/v1611393582/blog_posts_2021/2021_01/%E6%88%AA%E5%9C%96_2021-01-23_%E4%B8%8B%E5%8D%885.18.36_ylx316.png" alt=""></li></ul></blockquote><p>user space &amp; kernel space: 為了安全性而做的區隔</p><ul><li>linux kernel &amp; driver在kernel space執行</li><li>application跑在user space</li></ul><p>因此當app要access kernel resource時有三種方式</p><ul><li>system call: 可以透過API來間接調用system call，例如POSIX &amp; C Lib</li><li>interrupt </li><li>trap: software interrupt</li></ul><p>每個system call會有一個id，例如在<a href="https://elixir.bootlin.com/linux/v4.19/source/arch/arm64/include/asm/unistd32.h" target="_blank" rel="noopener">/arch/arm64/include/asm/unistd32.h</a>中可以看到system call和對應的id</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* /arch/arm64/include/asm/unistd32.h */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> __SYSCALL</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __SYSCALL(x, y)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __NR_restart_syscall 0</span></span><br><span class="line">__SYSCALL(__NR_restart_syscall, sys_restart_syscall)</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __NR_exit 1</span></span><br><span class="line">__SYSCALL(__NR_exit, sys_exit)</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __NR_fork 2</span></span><br><span class="line">__SYSCALL(__NR_fork, sys_fork)</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __NR_read 3</span></span><br><span class="line">__SYSCALL(__NR_read, sys_read)</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __NR_write 4</span></span><br><span class="line">__SYSCALL(__NR_write, sys_write)</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __NR_open 5</span></span><br><span class="line">__SYSCALL(__NR_open, compat_sys_open)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>要使用system call，必須要先透過trap觸發軟體中斷進入kernel mode，然後還需要指定system call id，然後整個流程大致如下圖:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_500/v1611393636/blog_posts_2021/2021_01/zEXqLKH_baes0s.png" alt=""></p><h3 id="字符設備驅動開發流程"><a href="#字符設備驅動開發流程" class="headerlink" title="字符設備驅動開發流程"></a>字符設備驅動開發流程</h3><p>Linux下一切皆文件(/dev/xxx)，application可以透過<code>open()</code>來打開設備，並透過<code>read()</code>, <code>write()</code>來存取設備, 並透過<code>close()</code>來關閉</p><p>因此在設計驅動的時候，需要撰寫對應的<code>open()</code>, <code>close()</code>, <code>write()</code>… function，這些被記錄在file_operations的struct內向外提供</p>]]></content>
      
      
      <categories>
          
          <category> 【課程筆記】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux kernel </tag>
            
            <tag> study </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[課程筆記]課程筆記系列總覽</title>
      <link href="/posts/85f1c2a/"/>
      <url>/posts/85f1c2a/</url>
      
        <content type="html"><![CDATA[<p>本文記錄了自己在上課時所記錄的一些課程筆記，可以透過這邊文章連結到所有以往發過的課程筆記文章。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1605107656/headerimage/001_yqduwu_c28mgu.png" alt=""></p><a id="more"></a><h2 id="正點原子【第四期】手把手教你學-Linux之驅動開發篇"><a href="#正點原子【第四期】手把手教你學-Linux之驅動開發篇" class="headerlink" title="正點原子【第四期】手把手教你學 Linux之驅動開發篇"></a>正點原子【第四期】手把手教你學 Linux之驅動開發篇</h2><p>開始學習時間: 2021/01/24<br>講師: 原子哥<br>課程連結: <a href="https://www.bilibili.com/video/BV1fJ411i7PB" target="_blank" rel="noopener">bilbil - 正点原子【第四期】手把手教你学 Linux之驱动开发篇</a></p><p>由於沒有code、板子和教材講義，所以課程筆記的部分是自己打出來的，裡面難免會有typo。並加上了一些額外的資料補充，所以裡面並不完全是課程內容。</p><p>內文的code會盡量跟著trace，如果有trace kernel code的部分，trace的版本為<a href="https://elixir.bootlin.com/linux/v4.19/source" target="_blank" rel="noopener">kernel 4.19</a></p><div class="table-container"><table><thead><tr><th>課程列表</th><th style="text-align:center">上課時間</th></tr></thead><tbody><tr><td><a href="https://meetonfriday.com/posts/81763900/">Course 1 - Linux驅動開發</a></td><td style="text-align:center">2021/01/24</td></tr><tr><td><a href="https://meetonfriday.com/posts/81763900/">Course 2 - 字符設備驅動開發基礎實驗</a></td><td style="text-align:center">2021/01/24</td></tr><tr><td><a href="https://meetonfriday.com/posts/62f55520/">Course 3 - 我的第一個Linux驅動</a></td><td style="text-align:center">2021/03/02</td></tr><tr><td><a href="https://meetonfriday.com/posts/9aca0070/">Course 4 - Led燈驅動實驗</a></td><td style="text-align:center">2021/03/03</td></tr><tr><td><a href="https://meetonfriday.com/posts/edcd30e6/">Course 5 - 新字元設備驅動實驗</a></td><td style="text-align:center">2021/03/29</td></tr></tbody></table></div><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><div class="table-container"><table><thead><tr><th>課程列表</th><th style="text-align:center">上課時間</th></tr></thead><tbody><tr><td><a href="https://meetonfriday.com/posts/8e7bb892/">[巨匠Live講堂]深度學習優化器新霸主</a></td><td style="text-align:center">2020/11/25</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 【課程筆記】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> study </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Learning Deep Features for Discriminative Localization</title>
      <link href="/posts/2b446abf/"/>
      <url>/posts/2b446abf/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>paper: <a href="https://arxiv.org/pdf/1512.04150.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1512.04150.pdf</a></p><p>會寫這篇主要是因為，CNN雖然厲害，對大多數人來說他還是一個人黑盒子，有時候我們根本不知道模型學到了什麼，如果模型沒學好就拿出來用就可能會發生如下悲劇: <a href="https://www.ptt.cc/bbs/Gossiping/M.1604224525.A.FFD.html?fbclid=IwAR0RRvGcY3No7mblHpIz5zgeFwKtFT05GPJ9y8OIttOiQ-7KWIlBpjQ-pkI" target="_blank" rel="noopener">[新聞] 蘇格蘭AI誤把裁判的光頭當成足球跟拍轉播</a></p><blockquote><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_300/v1610184422/blog_posts_2021/2021_01/%E6%88%AA%E5%9C%96_2021-01-09_%E4%B8%8B%E5%8D%885.26.50_letcfh.png" alt=""><br>將科技應用在運動比賽上是一件很棒的事情，但有的時候，即使是最偉大的發明也有出錯的可能。上週末在蘇格蘭的一場足球比賽中就發生了這樣的事情，一個由 AI 控制的攝影機，不小心把邊審的光頭誤認是足球。<br>由程式控制的攝影機會自動追蹤球在哪、並且嘗試跟拍，不再需要真人攝像師去控制。除了這會搶走一個人的工作之外，理論上聽起來還蠻有道理的。問題是──攝影機無法分辨足球和光頭的區別，它不斷的聚焦在邊審這個沒太大動作的人身上，而不是場上的球員。<br>…</p></blockquote><p>所以解釋性AI就很重要，我們想試著去了解模型學到或看到了什麼。</p><p>本文介紹CNN可視化的一種技術，<strong>Classification Activaion Mapping (CAM)，透過heatmap的方式顯示模型分類時認為圖片重要的依據為何</strong>。概念不難但卻非常好用，而且很酷。</p><p>在跟老闆介紹時，如果只跟他說模型有多猛架構多炫…老闆可能還是無法理解，有了這個技術就可以拿去跟老闆說”看看我的CNN模型都學到了什麼玩意兒～”</p><p>相信老闆看到圖片的效果一定會眼前為之一亮!</p><a id="more"></a><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks</p></blockquote><p>這篇論文的貢獻是重新審視Network in Network中提出的<strong>Global Average Pooling (GAP)</strong>，並提出用它來做物件定位(localization)的想法。</p><ul><li>再說一遍，走在CNN的浪潮上，你不可不知道的一篇論文: <a href="https://meetonfriday.com/posts/a151bfa2/">[論文速速讀]Network In Network</a></li></ul><p>當初GAP只被用來作為regularization training的一項技術，但實際上發現GAP具有能建構出一種好的localizable deep representation的能力，這能夠幫助模型去很好的定位出不同類別的物體。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>CNN各層的Conv Layer已經被證明是一個物體檢測器(Object detector)，透過卷積能夠很好的定位出物體在圖片中的位置。但這項能力在接上全連階層Fully connected Layer後就會喪失，因為<strong>FC將每個neuron連接的做法抹消了位置的information</strong>。</p><ul><li>因此有些Model已經在嘗試不要在最後一層接上FC，例如NIN和GoogleNet </li></ul><blockquote><p>In our experiments, we found that the advantages of this global average pooling layer extend beyond simply acting as a regularizer - In fact, with a little<br>tweaking, the network can retain its remarkable localization ability until the final layer</p></blockquote><p>GAP當初在NIN被提出來的時候最早是作為一個正則化技術的應用，用來防止overfitting。但文章的作者發現，透過一些小設計就能夠讓使用GAP的網路模型在不用特別訓練定位任務下，同樣具備良好的定位能力。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1610178896/blog_posts_2021/2021_01/%E6%88%AA%E5%9C%96_2021-01-09_%E4%B8%8B%E5%8D%883.53.36_sny13o.png" alt=""></p><p>作者把他提出的方法稱作Class activation mapping (CAM)，透過CAM + GAP，就能夠讓模型再分類分的好的同時也學到良好的定位能力。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>作者原本把Related Work歸類在Introduction怪怪的…摁…不管他xD</p><p>大致回顧了兩種技術: </p><ul><li>弱監督物體定位</li><li>可視化CNN</li></ul><h3 id="Weakly-supervised-object-localization"><a href="#Weakly-supervised-object-localization" class="headerlink" title="Weakly-supervised object localization"></a>Weakly-supervised object localization</h3><p>弱監督學習是什麼？</p><p>監督式學習(Supervised Learning)是給予data和label的情況下去對模型建模，而現實情況下label往往不夠大量，不論是礙於成本考量或是取得難度。</p><p>因此弱監督學習的概念就是: 在不增加label的情況下，使用不同的技術來對模型訓練，他有很多不同的類別，例如大家耳熟能詳的<strong>主動學習(Active Learning)</strong>與<strong>半監督學習(Semi-Supervisied Learning)</strong>就是其中的分支。</p><ul><li>有興趣的可以參考<a href="https://zhuanlan.zhihu.com/p/81404885" target="_blank" rel="noopener">浅谈弱监督学习（Weakly Supervised Learning)</a></li><li>在弱監督對物體定位這塊已經有不少方法，例如可以將圖片的隨機區域Mask掉，然後去觀察分類的準確度，那影響準確度最大的部分是不是就是最接近該物體的定位區域了呢？</li></ul><p>而這些方法都不夠泛化及實用(因為不能做到End-to-End，或者有不少需要前處理的部份)，但作者提出的方法卻沒有這問題。</p><blockquote><p>We believe that while the max and average functions are rather similar, the use of average pooling encourages the network to identify the complete extent of the object. The basic intuition behind this is that the loss for average pooling benefits when the network identifies all discriminative regions of an object as compared to max pooling</p></blockquote><p>此外，也探討了為何是GAP和不是Global Maximun Pooling? 作者提出的想法相較於GMP，GAP在辨別物體區域上的loss更小，詳細的內容可以看論文的Section 3.2或者本文後面的實驗結果</p><h3 id="Visualizing-CNNs"><a href="#Visualizing-CNNs" class="headerlink" title="Visualizing CNNs"></a>Visualizing CNNs</h3><p>在CNN可視化的研究方便也有不少，但大多都是提出某一層卷積來分析他的activation，並沒有對最後的FC層做研究，所以還不夠全面。</p><ul><li>題外話，CNN可視化的開山始祖ZFNet，有興趣的可以去看看 <a href="https://meetonfriday.com/posts/3013fdb9/">[論文速速讀]Visualizing and Understanding Convolutional Networks</a></li></ul><p><del>沒關係，把FC層移除掉就不用做研究了！</del>，在使用GAP提升regularization的同時也能夠更好的去解析模型不是更棒嗎~</p><h2 id="Class-Activation-Mapping"><a href="#Class-Activation-Mapping" class="headerlink" title="Class Activation Mapping"></a>Class Activation Mapping</h2><p>CAM的意思代表模型區別某一個類別時所看重的最重要區域在哪裏，生成CAM的過程如下，這張圖懂了就全懂了:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1610181053/blog_posts_2021/2021_01/v2-02cffea9152847e7aea65a3d60e72986_hd_gnq9sb.jpg" alt=""></p><p>在CNN模型最後一層，也就是Output Lyaer的前一層使用GAP，並且使用FC將GAP Layer跟Output Layer做映射。如此一來，我們可以很直觀的去解釋這個模型的架構:</p><ol><li>首先，對於神經網路通常越後面的層代表越高級的深層特徵</li><li>對於最後一層Conv的Feature Map<strong>做GAP得到的每個point可以作為最後一層Feature Map的重要性分數</strong></li><li>這些代表點透過全連接和對應的Label關聯後，連接邊的權重就可以想成<strong>該特徵對於模型分類到這個物體所佔的重要性</strong></li></ol><p>基於此架構，我們把Conv最後一層每個map都取出來，也取出他們GAP後與label的權重($w_{1}, w_{2}…w_{n}$)，然後把他們加權平均起來，以heatmap的形式呈現就可以代表<strong>模型分類這個類別時所看重的區域了</strong></p><p>這邊我沒有照著論文介紹，而是採用了我認為比較直觀的理解方式，有興趣看原文的人可以再去看一下原文的介紹。</p><p>總而言之，在這架構下，只要模型訓練的好，我們就可以取出Output Layer的每個類別點，反推回去看說模型看這個類別時都在看那裡了，如下圖所示</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1610181929/blog_posts_2021/2021_01/%E6%88%AA%E5%9C%96_2021-01-09_%E4%B8%8B%E5%8D%884.45.10_qwwfst.png" alt=""></p><h2 id="Weakly-supervised-Object-Localization"><a href="#Weakly-supervised-Object-Localization" class="headerlink" title="Weakly-supervised Object Localization"></a>Weakly-supervised Object Localization</h2><p>這邊針對以前的網路，拔呀拔把最後的FC都拔掉換成GAP然後看看是否會造成不良的影響，並探討他們能否有一樣甚至更好的定位能力。</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1610182140/blog_posts_2021/2021_01/%E6%88%AA%E5%9C%96_2021-01-09_%E4%B8%8B%E5%8D%884.48.36_rsrsbu.png" alt=""></p><ul><li>在這邊發現使用作者方法的模型再分類上效果會差一點點，但並沒有下降太多，都還在可接受的範圍</li></ul><h3 id="Localization"><a href="#Localization" class="headerlink" title="Localization"></a>Localization</h3><p>這邊只是想介紹他們是怎麼用CAM來做定位的，畢竟CAM他並不是一個框框。</p><p>作者他們的方法是: <strong>最大CAM值的20%作為一個threshold來畫框</strong>，並用此來做localization和其他方法比較，結果如下</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1610182141/blog_posts_2021/2021_01/%E6%88%AA%E5%9C%96_2021-01-09_%E4%B8%8B%E5%8D%884.48.41_fg6ogn.png" alt=""></p><ul><li>可以發現GoogLeNet-GAP的方法比以往沒有用GAP的模型定位準確度都來得好</li><li>這裡也有比較GAP和GMP，發現GAP的效果是比GMP好的</li><li>值得一提的是<strong>這並沒有特別做object detection的training</strong>，單純是train classification後把結果拿來用而已，也就是說做你也可以用這方法在沒有bounding box的情況下做到object detection</li></ul><p>接下來看圖比較有感覺，下圖的上面兩張是使用GoogLeNet-GAP搭配threshold所框出的框框(紅色部分)，下半部分的兩張是AlexNet的框框(紅色部分)，而綠色框框是Ground truth。可以看出用GAP的效果有著比較好的定位能力</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1610182565/blog_posts_2021/2021_01/%E6%88%AA%E5%9C%96_2021-01-09_%E4%B8%8B%E5%8D%884.55.48_oya9zz.png" alt=""></p><p>礙於現在工作因素，之後的論文沒辦法每篇都看得那麼仔細。不過原文的後續仍然對不同領域的task做了許多實驗，有興趣的讀者歡迎前往觀看！</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文就介紹到這邊，主要介紹了CAM的想法以及一些比較重要的實驗結果。</p><p>有了這種概念後，我們就知道可以透過這種技術去</p><ul><li>跟老闆解釋模型到底學到了什麼</li><li>在沒有提供bounding box information下，一樣可以做到object detection的task(透過CAM with threshold)</li></ul><p>當然，這一切的前提是你的<strong>模型必須訓練得夠好</strong>，分類準確的前提下才有意義。</p><p>最後，如果有耐心看到這邊的讀者或許會有一個疑問: </p><p><strong>“CAM要求CNN模型最後一層必須是GAP，那以前那些最後一層不是GAP的模型不都要通通重新改架構才能用這套方法了嗎?”</strong></p><p>這個問題後來被其他研究學家解決了，新的方法可以適用任何模型，他就叫做<strong>Grad-CAM</strong></p><ul><li>概念和CAM很像，但並不是使用GAP來取得每一個feautre map的權重</li><li>該篇論文就不另外介紹了，有興趣的讀者可以參考<a href="https://meetonfriday.com/posts/df7592be/">[ML]Explainable ML介紹(CAM、Grad-CAM)</a></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/81404885" target="_blank" rel="noopener">浅谈弱监督学习（Weakly Supervised Learning</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>驅動程式安裝失敗: 文件的哈希值不在指定的目錄中，此文件可能已損壞或被竄改</title>
      <link href="/posts/640a781c/"/>
      <url>/posts/640a781c/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前陣子工作的時候需要手動更新驅動程式，但卻無法順利更新，請教同仁他們以前也沒有遇過這個問題，結果為此花了不少時間在找解決方法…</p><p>後來終於搞好了…特此紀錄一下如何手動更新驅動，以及更新時遇到的問題和解決方法。</p><h2 id="如何手動更新驅動"><a href="#如何手動更新驅動" class="headerlink" title="如何手動更新驅動?"></a>如何手動更新驅動?</h2><p>以Win10為例，滑鼠右鍵點擊左下角的開始圖示，選【裝置管理員】</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/v1609951582/blog_posts_2021/2021_01/%E6%93%B7%E5%8F%96_vsnmqx.png" alt=""></p><p>如果有驅動沒有順利更新的話，則應該會看到有問號的圖示。</p><p>此時對它點擊右鍵 -&gt; 【更新驅動程式】 -&gt; 瀏覽電腦上的驅動程式選擇驅動程式的路徑進行更新即可。</p><a id="more"></a><h2 id="手動更新驅動失敗-文件的哈希值不在指定的目錄中"><a href="#手動更新驅動失敗-文件的哈希值不在指定的目錄中" class="headerlink" title="手動更新驅動失敗: 文件的哈希值不在指定的目錄中"></a>手動更新驅動失敗: 文件的哈希值不在指定的目錄中</h2><p>然後我遇到的問題是更新時出現失敗訊息，並且出現如下訊息</p><p><strong>“文件的哈希值不在指定的目錄中，此文件可能已損壞或被竄改”</strong></p><p>一開始還在懷疑是不是公司給的驅動是損壞的，或是版本不對，試著重新下載無數次後仍然沒辦法。</p><p>所以往外部找資源看看有沒有人遇到相同的問題，然後終於找到了，下列內容參考<a href="https://jingyan.baidu.com/article/0f5fb09930de116d8334ea12.html" target="_blank" rel="noopener">安装驱动提示文件的哈希值不在指定的目录文件中</a>這篇文章:</p><blockquote><p>今天小编在安装一个硬件设备的驱动时提示消息身份验证代码 (MAC) 哈希函数通常与数字签名一起用于对数据进行签名，而消息检测代码 (MDC) 哈希函数则用于数据完整性。所以哈希值不在制定目录就说明驱动程序版本过低或者不适合此系统。一般这种错误会出现在win8或win10系统中，下面小编分享一下解决方案，供大家参考！</p></blockquote><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/v1609952201/blog_posts_2021/2021_01/cfa9ae04541bd10f25aa152eba0e1799e82aa720_trirat.jpg" alt=""></p><p>解決方法: <strong>關閉驅動程式簽名功能</strong></p><ol><li>對左下角開始點右鍵 -&gt; 【設定】 -&gt; 【更新與安全性】</li><li>-&gt; 【復原】 -&gt; 【進階啟動】重新啟動電腦</li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/v1609952194/blog_posts_2021/2021_01/%E6%93%B7%E5%8F%96_vrztav.png" alt=""></p><p>電腦重開機後會進入一個選單介面，因為我懶不想重開電腦，後續的圖就引用該篇文章了。</p><p>選擇【難疑解答】</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/v1609953274/blog_posts_2021/2021_01/a749bb0f94fc508c9a43df4c01775ddd894cfd20_e6wuwj.jpg" alt=""></p><p>選擇【高級選項】</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/v1609953273/blog_posts_2021/2021_01/e40b3127e7ef2806618c8a69b840b6f39087f220_p596v1.jpg" alt=""></p><p>選擇【啟動設置】</p><ul><li>這邊我當初用的時候介面樣子和圖片不太一樣，如果找不到啟動設置的可以觀察一下介面中應該有其他地方可以點選到<strong>第二頁</strong>(對…這是有第二頁的!)</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/v1609953272/blog_posts_2021/2021_01/b7b28f87031c99c0be71f932af2fa872951fed20_z71uvg.jpg" alt=""></p><p>選擇【重啟】重新啟動電腦</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/c_scale,w_600/v1609953274/blog_posts_2021/2021_01/955ea0e434daf05e53ae61c5751d96d81919e520_bzmgc5.jpg" alt=""></p><p>重啟後，就有選項可以選，選擇<strong>【禁用驅動程式強制簽名】</strong></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/v1609953630/blog_posts_2021/2021_01/a68c126efbf202b3565a81cf30f4da5873dadb20_dyuukh.jpg" alt=""></p><p>之後就可以順利安裝驅動程式了！</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://jingyan.baidu.com/article/0f5fb09930de116d8334ea12.html" target="_blank" rel="noopener">安装驱动提示文件的哈希值不在指定的目录文件中</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux Kernel慢慢學]了解Linux CCF (Common Clock Framework)中的那些基本元件</title>
      <link href="/posts/f43c1090/"/>
      <url>/posts/f43c1090/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Linux CCF (Common Clock Framework)是一個用來管理clock的子系統，CCF向使用clk的driver提供操作clock的interface，同時也向clk driver的開發者提供一個管理clock的通用框架。</p><p>這篇文章作為了解Linux Clock框架的事前學習，介紹了一個系統的時間相關元件通常由哪幾項元件組成，而他們各自又是什麼東西。</p><p>(下面是自己的粗淺學習筆記，如有錯誤還請留言更正)</p><h2 id="Introduction-of-Clock-Tree"><a href="#Introduction-of-Clock-Tree" class="headerlink" title="Introduction of Clock Tree"></a>Introduction of Clock Tree</h2><p>要了解CCF，就必須先了解一個時間系統由那些元建構成，這裡引用wowotech大大的文章做介紹(wowotech的文章對我在學習Linux之路扮演了很大的啟蒙導師的角色，大推!)</p><blockquote><p>如今，可运行Linux的主流处理器平台，都有非常复杂的clock tree，我们随便拿一个处理器的spec，查看clock相关的章节，一定会有一个非常庞大和复杂的树状图，这个图由clock相关的器件，以及这些器件输出的clock组成。下图是一个示例：<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1609622156/blog_posts/8321e8ac90bb43c1f00543082b74fbd820141020150656_gthycs.gif" alt=""></p></blockquote><p>一個系統中clock相關的元件大致有:</p><ol><li><strong>晶振、震盪器 (Oscillator, Crystal)</strong>: 用於產生固定頻率的電子訊號</li><li><strong>鎖相迴路 (Phase Locked Loop, PLL)</strong>: 用於產生穩定的倍頻</li><li><strong>除頻器 (Divider)</strong>: 用於頻率的除頻</li><li><strong>多工器 (Multiplexer)</strong>: 用於選擇頻率來源</li><li><strong>門控 (Clock Gate, CG)</strong>: 用於Clock Enable / Disable</li><li><strong>設備 (Consumer)</strong>: 使用該時鐘的設備</li></ol><p>下面依序介紹各個元件扮演的角色，以及簡介他們背後的原理是怎麼運作的。</p><a id="more"></a><h2 id="Oscillator"><a href="#Oscillator" class="headerlink" title="Oscillator"></a>Oscillator</h2><p>系統需要的Clock Source由晶振所產生，但晶振<strong>只能產生固定的頻率</strong>，並且基於工藝和成本的因素沒辦法產生很高的頻率</p><p>如果此時系統需要使用不同的頻率該怎麼辦? 這時候就可以透過PLL和Divider就出場了</p><h2 id="PLL"><a href="#PLL" class="headerlink" title="PLL"></a>PLL</h2><p>PLL用來產生<strong>倍頻</strong>的頻率，使得單一頻率源的晶振可以有不同倍率的頻率供給不同的consumer</p><p>但他是怎麼做到的呢? PLL的組成大致可分成三個部分:</p><ul><li>Phase Detector (PD): 相位檢測器</li><li>Low-pass Filter (LPF): 低通濾波器</li><li>Voltage-Controlled Oscillator (VCO): 壓控振蕩器，可用電壓調整輸出頻率的一個元件</li></ul><p>運作流程大致可以解析如下:</p><ol><li>PD會比對參考訊號和輸出訊號的相位差異，將相位差異轉成電壓訊號輸出<ul><li>參考訊號$可以想成外部clock source的頻率輸入</li><li>輸出訊號則是VCO產生的頻率</li></ul></li><li>電壓訊號經過低通濾波器(LRF)轉換成VCO的控制電壓</li><li><strong>VCO根據控制電壓來對調整輸出的頻率，最終達到參考訊號=輸出訊號</strong><ul><li>當輸入的參考訊號和VCO產生的頻率相同時，則我們稱PLL被鎖定</li></ul></li></ol><h3 id="PLL怎麼做到倍頻"><a href="#PLL怎麼做到倍頻" class="headerlink" title="PLL怎麼做到倍頻?"></a>PLL怎麼做到倍頻?</h3><p>而倍頻這件事要如何達成? 就是動動手腳騙一下PD</p><p>在VCO產生的頻率後面接一個分頻器(Divider)，使得PD接收到的輸出訊號的值是實際上的1/N倍，如此一來就可以產生N倍的頻率了！</p><p>了解上述的簡介後，看一下架構圖會更加清楚(引用自<a href="https://www.analog.com/cn/design-center/landing-pages/002/tech-articles-taiwan/phase-locked-loop-pll-fundamentals.html#" target="_blank" rel="noopener">analog devices</a>的圖片)</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1609628465/blog_posts/184330_Fig_02_wqkald.png" alt=""></p><p>這麼做有什麼好處?</p><ul><li>使用PLL搭配低頻的晶振就可以產生高頻的訊號源，降低成本</li><li>使用PLL可以在不更改硬體的情況下產生不同倍數的高頻，因此可以產生不同倍數的高頻</li></ul><p>此外，透過對PLL的了解，我們也知道一個系統能產生的最高頻率並不是晶振的上限，而是<strong>取決於VCO的上限</strong></p><h2 id="Divider"><a href="#Divider" class="headerlink" title="Divider"></a>Divider</h2><p>光有PLL做倍頻還不夠，試想一個情境:</p><p>我們的clock source是10MHz，想要產生各式各樣不同的頻率該怎麼辦?</p><p>10的倍數，如20Mhz、30Mhz還可以用PLL做出來，但如果我想要26Mhz、3Mhz這種頻率的話呢?</p><p>這時候<strong>PLL搭配Divider就可以做出各式各樣的頻率</strong>，比方說我可以先用/10的Divider得到1Mhz，然後再用PLL去取得我想要的倍頻</p><h3 id="Divider是怎麼做的"><a href="#Divider是怎麼做的" class="headerlink" title="Divider是怎麼做的?"></a>Divider是怎麼做的?</h3><p>而Divider背後是怎麼做的呢? 這就扯到大學學的數位電路學中的<strong>正反器(Flip-flop, FF)</strong>了</p><p>大學學到的電路學大部分都還給教授了QQ 所以這邊又花了點時間複習一下</p><p>學東西都要有個脈絡，所以還是從一開始講起，在數位電路中有組合邏輯電路和序向邏輯電路:</p><ul><li>組合邏輯電路的輸出狀態僅和當下的輸入有關</li><li>序向邏輯電路因為具有記憶功能，上一個時間的輸出也會影響到下一個時間的輸出<ul><li>序向邏輯電路的基本元件有門閥(Latch)和正反器(Flip-flop)</li></ul></li></ul><p>門閥(Latch)可由兩個NAND或兩個NOR組成，不過當訊號一發生改變的時候輸出也會發生相對應的改變，很難界定出下一個狀態是哪個時間點，所以有了正反器(Flip-flop)</p><p>正反器即是，在門閥的電路上，加上<strong>時脈訊號(clock)</strong>與邏輯閘的設計，使得可以以時脈作為一個個狀態的時間點，然後就有了讓你大學各種記不起來的正反器(RS正反器、D型正反器、JK正反器…)</p><p>而基本上會希望正反器在每次時脈為高態的時候只改變一次(即clock=1時只會做一次改變)，否則就無法很明確的區分出高態時每次的變化，因此有了<strong>邊緣觸發(edge triggering)</strong>的正反器，使正反器在時序信號發生變化時才會出發，進而改變輸出訊號，根據改變的時機又分成</p><ul><li>正緣觸發(positive edge triggering)</li><li>負緣觸發(negitive edge triggering)</li></ul><p>而所有的正反器都可以搭配邊緣觸發變成邊緣觸發正反器。</p><p>接下來…沒有接下來了…</p><p>沒有要再繼續介紹下去了，我愧對我大學電路學教授，有興趣的可以去看<a href="http://w3.khvs.tc.edu.tw/ischool/public/resource_view/openfid.php?id=2275" target="_blank" rel="noopener">CPLD數位邏輯設計</a>複習</p><p>好了，該回頭講除頻器這個玩意兒，它其實就是一個<strong>用正反器做出來的計數器</strong>，舉一個JK正反器為例子，下面是他的電路圖和真值表(圖片引用自<a href="http://w3.khvs.tc.edu.tw/ischool/public/resource_view/openfid.php?id=2275" target="_blank" rel="noopener">CPLD數位邏輯設計</a>)    </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1609628480/blog_posts/s828T13_ciu0oe.png" alt=""></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1609628479/blog_posts/uEGkQwR_yct2ir.png" alt=""></p><p>當J=1, K=1時該正反器具備toggle功能(下個時間的狀態會與當前相反)，如果在這狀態下搭配負緣觸發會發生什麼事情呢?</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_600/v1609628481/blog_posts/z7Kf4K1_dlt5vi.png" alt=""></p><p>有沒有發現，$Q_{n}$的頻率變成CLK的1/2了! 這就達到了分頻的效果</p><ul><li>而如果要得到1/4頻率，要怎麼辦呢? 2個正反器即可，其餘依此類推</li></ul><h2 id="Multiplexer"><a href="#Multiplexer" class="headerlink" title="Multiplexer"></a>Multiplexer</h2><p>現在多虧了PLL、Divider，我們可以有許多不同頻率的clk source了，那device要使用哪一個來源的頻率就可以透過多工器來進行選擇</p><p>多工器也是數位電路學的東西，反正就你給一個控制訊號它可以給你對應的input，細節回去自己K書</p><h2 id="Clock-Gate"><a href="#Clock-Gate" class="headerlink" title="Clock Gate"></a>Clock Gate</h2><p>Clock Gate就是一個門閥，用來決定要不要讓訊號進入你的device，如果我的理解沒錯的話</p><ul><li>clear CG: 代表打開CG讓訊號進入</li><li>set CG: 關閉CG門閘，阻止訊號進入</li></ul><p>對於register的操作，我永遠搞不懂clean/set到底是要設成1還是清成0…</p><h2 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h2><p>前面的元件都順利設好之後，你的device就能開開心心使用該頻率的clock來運作啦!</p><p>到此我們對CCF中相關元件有了初步的認識，然後就可以開開心心的繼續看wowotech的文章了:）</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="http://www.wowotech.net/linux_kenrel/clk_overview.html" target="_blank" rel="noopener">Linux common clock framework(1)_概述</a></li><li><a href="https://www.analog.com/cn/design-center/landing-pages/002/tech-articles-taiwan/phase-locked-loop-pll-fundamentals.html#" target="_blank" rel="noopener">鎖相迴路(PLL)基本原理</a></li><li><a href="http://w3.khvs.tc.edu.tw/ischool/public/resource_view/openfid.php?id=2275" target="_blank" rel="noopener">CPLD數位邏輯設計</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【Linux Kernel慢慢學】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> linux kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux Kernel慢慢學]學習Linux Kernel的Coding Style</title>
      <link href="/posts/f43c1090/"/>
      <url>/posts/f43c1090/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 【Linux Kernel慢慢學】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> linux kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Hexo在Github Page上架設部落格 - 2020年度回顧</title>
      <link href="/posts/55a05954/"/>
      <url>/posts/55a05954/</url>
      
        <content type="html"><![CDATA[<p>約在2020四月開始，我停筆了在WordPress上長達四年的部落格經營，將平台轉向架在Github Page上搭配Hexo來客製化自己的部落格。</p><p>至今已經過了將近8個月，是時候來寫一下架設這個部落格背後的過程，以及當初設計時考慮的各種面向，順便來回顧一下這幾個月部落格的成長。</p><h2 id="從WordPress到Github-Page-Hexo"><a href="#從WordPress到Github-Page-Hexo" class="headerlink" title="從WordPress到Github Page + Hexo"></a>從WordPress到Github Page + Hexo</h2><p>當初會痛定思痛轉移平台的原因主要是因為WordPress的編輯器對於我實在很不夠用，我需要一個能夠讓我很方便的來撰寫程式碼以及數學公式的解決方案，想了許久還是Markdown最方便了，最後決定選擇對Markdown支援度很高的Hexo。</p><p>其實從國中開始寫部落格開始，到處流浪搬家換平台已經許多次了QQ，關於這段辛酸血淚可以至<a href="https://meetonfriday.com/about/">About</a>這篇觀看。</p><h2 id="使用Hexo架設部落格的過程"><a href="#使用Hexo架設部落格的過程" class="headerlink" title="使用Hexo架設部落格的過程"></a>使用Hexo架設部落格的過程</h2><p>總之第一次使用Hexo然後又是自己架網站，網站功能、RWD、介面設計、SEO優化、流量分析…等都要自己來，我一直對前後端很不熟，所以這邊其實花了蠻多時間反反覆覆的修改。</p><p>下面簡單敘述了一下當初設計時考量到的幾個面向:</p><ol><li><strong>網站主題</strong></li><li><strong>深色主題的設計</strong></li><li><strong>便捷的的文章編輯</strong></li><li><strong>一鍵式的文章發布</strong></li><li><strong>SEO優化、流量分析</strong></li></ol><a id="more"></a><h3 id="網站主題"><a href="#網站主題" class="headerlink" title="網站主題"></a>網站主題</h3><p>Hexo其實有許多現成Template可以用，所以只需要找到喜歡的模板後稍加修改就好了。但由於我想要找一個簡潔又是深色的主題，最後我挑的主題是<a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">hexo-theme-snail</a>。</p><p>不像<a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">hexo-theme-next</a>有廣大的維護以及開發者，我挑選的模板許多功能都不夠完善，介面什麼的也都需要自己去改，這部分為了熟悉Hexo也花了不少時間。</p><p>為什麼不用hexo-theme-next就好？ 因為不想跟別人撞版型呀xD</p><p>舉個例子，由於Hexo產生的只是一個靜態網站(static website)，他不像wordpress可以有後端平台或是資料庫來做留言板，所以留言功能就必須藉助第三方的外掛，像是現在看到的留言板就是<a href="https://disqus.com/" target="_blank" rel="noopener">DISQUS</a>弄出來的，印象中Disqus的網站介面還不能選繁體中文，如果要使用繁體中文就必須透過一些小方式來操作</p><ul><li>關於如何使用繁體中文見面可以參考<a href="https://www.wpchen.net/zh/posts/disqus#_Disqus-4" target="_blank" rel="noopener">【Day 8】如何安裝 Disqus 留言板</a></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1607783689/blog_posts/%E6%88%AA%E5%9C%96_2020-12-12_%E4%B8%8B%E5%8D%8810.33.53_w3hf2b.png" alt=""></p><div class="img-desc">  Disqus的留言板</div><p>如果你留言的話，你的預設頭像還是一隻很可愛的貓咪在敲電腦喔，這麼可愛的頭像還不快留言嗎？</p><h3 id="深色背景的配色-Dark-Mode-UI"><a href="#深色背景的配色-Dark-Mode-UI" class="headerlink" title="深色背景的配色 - Dark Mode UI"></a>深色背景的配色 - Dark Mode UI</h3><p>使用深色主題是我個人的偏好，但深色主題如果設計的不好又不容易長期閱讀，偏偏我的部落格定位又是技術導向居多，許多篇文章的閱讀時間都篇長，所以當初在設計部落格的深色模式花了一番苦心。</p><p>比方說其實現在文字的顏色並不是純白(因為對比太強眼睛會不舒服)，而是根據不同區塊的重要程度來調整白色的透明度。</p><p>當初為了深色主題的UI還去研究了很多文章，網路上有許多針對Google 2019年後支持的Dark Mode以及Apple的Dark Mode做的分析介紹，以這些這些文章為雛形去調整了整體的配色方案。下面這幾篇都是當初看到覺得很不錯的介紹文：</p><ul><li><a href="https://kknews.cc/zh-tw/news/y52yama.html" target="_blank" rel="noopener">一篇吃透 Dark Mode，搞定「暗黑/深色」適配</a></li><li><a href="https://kknews.cc/zh-tw/tech/y5pn38n.html" target="_blank" rel="noopener">如何適配深色模式？用一個 APP 改版案例讓你學會</a></li><li><a href="https://kknews.cc/zh-tw/tech/k4yg9pv.html" target="_blank" rel="noopener">深色主題設計的8個小技巧！深色模式成為2020年的新趨勢</a></li></ul><h3 id="便捷的的文章編輯-Markdown-VS-Code"><a href="#便捷的的文章編輯-Markdown-VS-Code" class="headerlink" title="便捷的的文章編輯 - Markdown + VS Code"></a>便捷的的文章編輯 - Markdown + VS Code</h3><p>會看上Hexo的原因是它支援使用Markdown來撰寫我的文章，Markdown簡單好上手的特性使得我可以專注在自己的文章內容上，並且很方便的支援程式碼、數學公式(Latex)的編輯。而關於文章標題、內容的配色或字體等設計再透過css去設計就可以了。</p><p>有了Markdown，那接下來就只需要一個好的文章編輯器就可以開心撰寫部落格了！ 所以就用了輕量好用的VS Code，透過內建terminal可以直接下<code>hexo server</code>開啟local server，一邊寫文章一邊觀看寫完後的呈現，撰寫完還可以直接git push到github repo上部署，簡單好用。<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1607788286/blog_posts/%E6%88%AA%E5%9C%96_2020-12-12_%E4%B8%8B%E5%8D%8811.50.59_kravfv.png" alt=""></p><div class="img-desc">  使用VS code搭配Markdown語法撰寫部落格文章</div><h3 id="一鍵式的文章發布-Travis-CI"><a href="#一鍵式的文章發布-Travis-CI" class="headerlink" title="一鍵式的文章發布 - Travis CI"></a>一鍵式的文章發布 - Travis CI</h3><p>Hexo搭配Github Page雖然方便，但必須有一個source repo來維護原始的檔案，然後透過<code>hexo deploy</code>產生出靜態網站的檔案資料夾，實際上github page連結的repo是靜態網站資料夾。如果上面文字敘述有點不懂的話，實際上的架構圖是這樣的:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">------------------     deploy    ------------------- </span><br><span class="line">| hexo file repo | ------------&gt; | Static Web repo |</span><br><span class="line">------------------               ------------------- </span><br><span class="line">                                          ↕ Github Page</span><br><span class="line">                                 ------------------- </span><br><span class="line">                                 |     Website     |</span><br><span class="line">                                 -------------------</span><br></pre></td></tr></table></figure></p><p>如此一來就每次寫完文章就必須</p><ol><li>將文章更新到hexo file repo </li><li>然後將deploy的檔案更新到static web repo</li></ol><p>同時維護兩份repo實在是有點麻煩。所以我就讓Travis CI來幫我完成中間這段。Travis CI是一個分佈式的整合服務，可以用來建構和測試github上的repo，常被用來做CI/CD。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1607843985/blog_posts/unnamed_uxfmlq.jpg" alt=""></p><div class="img-desc">  Travis CI</div><p>透過Travis CI簡化原本兩步驟發布流程，以後寫完文章將檔案更新到hexo file repo，Travic CI就會在他們的環境下把我的這個repo clone下來做deploy，然後再幫我更新到Static web repo上。</p><p>如此以後我只要將文章push到hexo file repo，其他就通通不用管了。</p><h3 id="SEO優化、流量分析-Google-Analystics"><a href="#SEO優化、流量分析-Google-Analystics" class="headerlink" title="SEO優化、流量分析 - Google Analystics"></a>SEO優化、流量分析 - Google Analystics</h3><p>其實我對SEO很不熟，以前WordPress都幫我弄好好的，現在突然通通都要自己來還真的花了不少的心力。</p><p>像是要產生可以讓搜尋引擎可以爬到的sitemap.xml，網站的結構該如何設計(標題以H2來標註、章節以H3…)，如何處理Paging所產生的重複內容…這些也花了蠻多時間在研究的，雖然現在已經暫且告一個段落，但或許還有許多不足的地方要修改。</p><p>而SEO的成效則是透過GCS(Google Search Console）來觀察網站在搜尋引擎上的排名和曝光度。</p><p>最讓我困擾的是Hexo沒有一個好的後台介面(由於是靜態網站)，所以無法觀察部落格的流量增減，於是就使用了Google Analystics，自從v4出來後見面更得更加好看也更方便使用了。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_700/v1607845057/blog_posts/%E6%88%AA%E5%9C%96_2020-12-13_%E4%B8%8B%E5%8D%883.36.44_tzsafs.png" alt=""></p><div class="img-desc">  Google Analystics v4介面</div><h2 id="網站流量的成長"><a href="#網站流量的成長" class="headerlink" title="網站流量的成長"></a>網站流量的成長</h2><h3 id="WordPress時期"><a href="#WordPress時期" class="headerlink" title="WordPress時期"></a>WordPress時期</h3><p>當初WordPress的部落格在斷斷續續經營了四年多後，已經達到了76000多的瀏覽次數，在2020的四月平均每天已經有150多次的瀏覽數。儘管這數字並不高，但對部落客來說還是一種不小的肯定以及成就感。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_1000/v1607720938/blog_posts/%E6%88%AA%E5%9C%96_2020-12-12_%E4%B8%8A%E5%8D%885.07.02_pk0roj.png" alt=""></p><div class="img-desc">  WordPress部落格在後台的流量統計</div><h3 id="Hexo時期"><a href="#Hexo時期" class="headerlink" title="Hexo時期"></a>Hexo時期</h3><p>下圖是大約從2020/4月開始，使用Hexo在Github Page上架設網站的流量成長</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1607845390/blog_posts/%E6%88%AA%E5%9C%96_2020-12-13_%E4%B8%8B%E5%8D%883.42.56_wvg6zi.png" alt=""></p><p>這是從頭開始的成果，由於我沒有把Wordpress的流量導過來…當初用的是wordpress.com的方案，所以連302轉址外掛都不能用…就乾脆從頭來了，而且中途SEO還沒做好前成效也糟糕的一塌糊塗。</p><p>儘管現在流量還是很少，但看得出有慢慢在成長也算是一種小小成就。希望未來能繼續加油。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>關於研究所的那些事 - 推甄、找教授、生活日常</title>
      <link href="/posts/f955f6de/"/>
      <url>/posts/f955f6de/</url>
      
        <content type="html"><![CDATA[<p>每年的秋冬都會有一些大學的學弟妹來詢問一些研究所的相關事情(阿不過今年應該是最後一屆了，畢竟接下來的學弟妹我也都不認識了)，在與他們聊天的過程中也使自己重新又回顧了一遍研究所的時光。</p><p>關於研究所的文章其實當初零零散散的有撰寫了一些文章記錄，不過腦殘如我部落格搬家之後SEO沒做好，所以現在可能也沒有那麼方便搜尋到那些文章，想說不如就趁這篇把文章都搜集起來順便補充一些內容吧。</p><p>所以這篇文章算是個系列文統整然後加上一些補充，主題涵蓋了<strong>研究所推甄、面試、找教授，以及研究生生活的一些日記</strong>，如果好奇研究生生活都是怎麼過的讀者可以去看看(雖然當初日記系列都是用幹話的形式來敘述的xD)</p><h2 id="研究所推甄準備、面試、找教授"><a href="#研究所推甄準備、面試、找教授" class="headerlink" title="研究所推甄準備、面試、找教授"></a>研究所推甄準備、面試、找教授</h2><p>首先，這邊的研究所主要是以資工所來撰寫的，申請其它所得請斟酌觀看。</p><p>由於我當初是透過推甄申請上研究所的，不得不說推甄真的是一條需要持之以恆的路，你必須大學三年都將課業維持得很好，此外最好還要有一些課外的活動、競賽、作品等經歷來證明自己不單只會唸書。</p><p>如果在看這篇的人是正在準備推甄的人，那麼首先先說聲恭喜你，這是你大學努力的成果，希望大家都能推到自己理想中的學校。</p><a id="more"></a><p>相關的文章我覺得當初打的蠻清楚的，所以這邊就直接附上連結，如果有不清楚的部分都歡迎留言詢問：）</p><ul><li><a href="https://meetonfriday.com/posts/44bbe391/">[研究所推甄]資工所推甄心得1-備審準備</a></li><li><a href="https://meetonfriday.com/posts/60ab2ab9/">[研究所推甄]資工所推甄心得2-口試過程</a></li><li><a href="https://meetonfriday.com/posts/b6f10ec8/">[研究所推甄]資工所推甄心得3-找尋指導教授</a></li></ul><p>如果還有什麼需要補充的話，我想應該是找指導教授的部分吧(苦笑)，只能說自己閱曆尚淺，建議在找指導教授時應該要更深入的<strong>了解教授的帶人方式、以及未來自己可能會接觸到的領域教授是否真的熟悉</strong>，如果可以，請問教授能否讓你一同參加個一兩次Lab group meeting會更能了解教授帶人的方式。</p><h2 id="研究生生活"><a href="#研究生生活" class="headerlink" title="研究生生活"></a>研究生生活</h2><p>這兩篇是當初碩士的時候寫的年度回顧，可以看一下過得多麼<del>悲慘</del></p><ul><li><a href="https://meetonfriday.com/posts/79132bb1/">碩一生活總結</a></li><li><a href="https://meetonfriday.com/posts/2c756e7/">碩二生活總結</a></li></ul><p>認真來講，如果有人問我研究生都在做什麼或應該要做什麼，我會這樣回答：</p><ol><li>完成教授交辦的事務，例如計畫</li><li>修課，儘早修完畢業門檻</li><li>撰寫自己的碩士論文</li><li>提升自己的專業能力(實習 or 自主學習)</li><li>完成任何想做出社會前的事情</li></ol><p>兩年的時間主要做前三件事，雖然每件事都不簡單但久了其實也挺單調乏味的，所以可以適當的給自己安排一下課外的活動，出去走走旅行、認識新朋友都是不錯的選擇。</p><p>而對於第四點，出社會之後發現在學校所學跟職場還是有一段不小的Gap，尤其當你的工作內容跟本身所學不同時，幾乎就重頭學起。所以如果能有機會透過實習提早知道自己適合或不適合什麼那會是一件很棒的事情。<br>如果沒有也沒關係，在確定工作之後到報到之前，如果能好好利用這段期間多少學習一下，剛進去公司時應該會比較輕鬆一點。</p><p>第五點也是很重要的，想想<strong>往後可能三四十年都在工作，如果情況允許或許沒有必要馬上急著入職場</strong>（我是說或許，請好好與自己對話確認自己的想法）</p><p>畢竟，成為上班族之後，不再是學生的你不能說不去上班就不去，所以趁還有許多自由的時候好好享受吧！ </p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>[C]在不同檔案間共用變數?你可以用extern阿</title>
      <link href="/posts/9514e1a0/"/>
      <url>/posts/9514e1a0/</url>
      
        <content type="html"><![CDATA[<p>這篇不是教學文喔，嚴格來說是工作上發生的蠢事，因為太蠢了所以紀錄一下，順便做個筆記(寫這篇廢文的時候真的是羞愧到想挖洞把自己埋起來，我對不起大學教授我程式沒學好…咦好像也不用對不起，他也沒教什麼QQ)</p><p>好了，故事Start抖</p><h2 id="事發總有原因，蠢事總要有人聽"><a href="#事發總有原因，蠢事總要有人聽" class="headerlink" title="事發總有原因，蠢事總要有人聽"></a>事發總有原因，蠢事總要有人聽</h2><p>總之呢，這個糗事發生在今天的上班過程中，當時的狀況是如下的，我有三份檔案:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- main.c</span><br><span class="line">- api.c</span><br><span class="line">- api.h</span><br></pre></td></tr></table></figure><p>透過main.c來撰寫主要的程式碼，然後會用到的api則是放在api.c和api.h。</p><p>然後我需要透過main.c裡面的一些變數來控制api.c裡面的function，原本是想說透過call api的時候傳參數進去，大概長這樣子:</p><a id="more"></a><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* main.c */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> control_var = <span class="number">0</span>;</span><br><span class="line">    foo(control_var);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然後<code>foo(int control_var)</code>的定義跟實作在api.h / api.c中</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* api.h */</span></span><br><span class="line"><span class="comment">/* function declaration */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">(<span class="keyword">int</span> control_var)</span></span>;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* api.c */</span></span><br><span class="line"><span class="comment">/* function definition */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">(<span class="keyword">int</span> control_var)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (control_var) &#123;</span><br><span class="line">        <span class="comment">/* Do something */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="動動你的小腦想想看"><a href="#動動你的小腦想想看" class="headerlink" title="動動你的小腦想想看"></a>動動你的小腦想想看</h2><p>這邊涉及了幾個C語言的基本觀念，由於在寫這篇廢文的時候已經十二點半啦，明天還要上班QQ </p><p>所以先列出關鍵字讓大家搜尋，以後有機會再補上</p><ul><li>函數定義(definition) &amp; 宣告(declaration)的差異?</li><li>重複定義的思考: 為什麼通常宣告都寫在.h，而定義都寫在.cpp?</li><li>Variable scope?</li><li>File compilation &amp; linking </li></ul><h2 id="「你幹嘛那麼搞工-」"><a href="#「你幹嘛那麼搞工-」" class="headerlink" title="「你幹嘛那麼搞工?」"></a>「你幹嘛那麼搞工?」</h2><p>總之我為了原本想好的這種寫法，大改程式碼的架構(看似簡單但其實要改的部分很多，所以要花很多時間)，直到前輩跑過來看我怎麼還沒下班…然後…</p><p>「你幹嘛那麼搞工」</p><p>「因為…我想用變數去控制api的邏輯…」</p><p>「你用extern就好阿」</p><p>「…」</p><p>「…」</p><p>這個幾百年沒用過的東西才這樣慢慢浮出我腦海中，總之呢，如果用了extern，那就可以讓變數<strong>在不同檔案之間共用</strong>，只要在.h檔案中宣告</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* api.h */</span></span><br><span class="line"><span class="comment">/* function declaration */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span>;</span><br><span class="line"><span class="keyword">extern</span> <span class="keyword">int</span> control_var;</span><br></pre></td></tr></table></figure><p>然後就可以在main.c 跟api.c中使用該變數:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* main.c */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    control_var = <span class="number">0</span>;</span><br><span class="line">    foo();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* api.c */</span></span><br><span class="line"><span class="comment">/* function definition */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (control_var) &#123;</span><br><span class="line">        <span class="comment">/* Do something */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>這樣就不用透過傳遞參數的方式來做了! </p><p>不過要注意的是要維護好該變數的值，不然可能一不注意就悲劇了，<strong>就像你的人生一樣</strong>。</p><ul><li>p.s. 其實這不是一個好的寫法，因為對於架構跟可讀性、維護性來說都有一些問題，不過就是紀錄一下其實還有extern可以用這樣</li></ul><h2 id="Extern-v-s-Static"><a href="#Extern-v-s-Static" class="headerlink" title="Extern v.s. Static"></a>Extern v.s. Static</h2><p>相對於Extern使得變數在不同檔案間可以被共用，Static除了改變生命週期外也有改變Scope的用途，即使得變數只在各自檔案可見</p><p>細節我就不講了，這篇文章寫得蠻好的，有興趣可以參考:</p><ul><li><a href="https://medium.com/@alan81920/c-c-%E4%B8%AD%E7%9A%84-static-extern-%E7%9A%84%E8%AE%8A%E6%95%B8-9b42d000688f" target="_blank" rel="noopener">C/C++ 中的 static, extern 的變數</a></li></ul><p>好，來睏!</p>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[課堂筆記]深度學習優化器新霸主</title>
      <link href="/posts/8e7bb892/"/>
      <url>/posts/8e7bb892/</url>
      
        <content type="html"><![CDATA[<p>這篇文章是巨匠LIVE講堂(1124)上課的課堂筆記，主要在介紹近年的深度學習優化器技術。</p><ul><li>由於是上課時的筆記內容，所以不會是介紹文的形式</li></ul><p>講者: 尹相志</p><p>課程網址: <a href="https://www.pcschool.com.tw/activity/109/10911-live11m-Adam/index.aspx?fromto=99268008&amp;utm_source=LINE_PicSee&amp;fbclid=IwAR2y2TZIe9BZ-0i_TKOvEz5CiXvyVZzoa9kheDqi_wComjkNK9YHSHO6Zzghttps://www.pcschool.com.tw/activity/109/10911-live11m-Adam/index.aspx?fromto=99268008&amp;utm_source=LINE_PicSee&amp;fbclid=IwAR2y2TZIe9BZ-0i_TKOvEz5CiXvyVZzoa9kheDqi_wComjkNK9YHSHO6Zzg" target="_blank" rel="noopener">11月Live講堂_深度學習優化器新霸主</a></p><p>(下方圖片皆引用自尹相志老師的slide)</p><h2 id="隨機梯度下降-Stochastic-Gradient-Descent-SGD"><a href="#隨機梯度下降-Stochastic-Gradient-Descent-SGD" class="headerlink" title="隨機梯度下降(Stochastic Gradient Descent, SGD)"></a>隨機梯度下降(Stochastic Gradient Descent, SGD)</h2><p>不同的優化器走到低點所需的時間都不同</p><p>Local Minima: 過去常遇到的問題，一但走到很難跳出來，如何避免?</p><p>鞍點: 除了該點外，其他的梯度都比該點還低(鞍點梯度=0)，導致很優化方向很容易走偏</p><p>深度學習很重要的問題: learning rate對於loss的影響</p><ul><li>家用電腦: 10^-3往下調整</li></ul><h2 id="優化器的兩大類型"><a href="#優化器的兩大類型" class="headerlink" title="優化器的兩大類型"></a>優化器的兩大類型</h2><ul><li>基於學習速率調整</li><li>基於動量(Mommentum)調整: 加入過去移動的方向 &amp; 速度(慣性的概念) </li></ul><a id="more"></a><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>使用了一階、二階動量的資訊</p><ul><li>為了不要被過去的資訊主導太多，所以使用了averaging的方式，時間越久的資訊越不重要</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/SpqGIct.png" alt=""></p><p>Adam有什麼問題?</p><ul><li>自適應: 根據過去的資訊來做統計的估算，在學習的初期由於沒有足夠的數據，variance很大，此時動量的估計很容易造成錯誤 </li><li>解決方式: 預熱，將前面幾個epoch設成很低的學習速率(e.g. 10^-6)<ul><li>因為很難收斂，也不用擔心會跑掉</li><li>在這個variance很小的情況下也可以慢慢蒐集相關的資訊</li></ul></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/5fGcATb.png" alt=""></p><ul><li>上圖中，上半部是沒有預熱的情況，此時不同的epoch更新的重點(分布)非常不同<ul><li>代表這10個epoch都在更新神經網路不同的地方</li></ul></li><li>使用預熱後，發現分布比較一致，每次更新會更新相同區域，能夠提供更好的優化穩定性 <ul><li>但預熱太慢，怎麼辦?</li><li>RAdam</li></ul></li></ul><h2 id="Lookahead-Optimizer-k-steps-forward-1-step-back"><a href="#Lookahead-Optimizer-k-steps-forward-1-step-back" class="headerlink" title="Lookahead Optimizer: k steps forward, 1 step back"></a>Lookahead Optimizer: k steps forward, 1 step back</h2><p>同時維持兩種不同的更新策略: </p><ul><li>Fast weights 快版:原本的更新策略 </li><li>Slow weights 慢版:快版N次後，將整個路徑只取頭尾，作為慢版更新的方向</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/G0aUm78.png" alt=""></p><p>和目前既有的優化器是完全正交獨立的，所以這個機制可以加入現有的所有優化器</p><h2 id="Ranger"><a href="#Ranger" class="headerlink" title="Ranger"></a>Ranger</h2><p>解決初期方差過大的問題 RAdam + 解決後期優化路徑區率過大問題(LookAhead) = Ranger</p><ul><li>不太適用的模型: RNN-based (LSTM), (講者推測) Lookahead有兩種步調，但在RNN-based內就只有一個neuron在繞，在這種過程Fast weights的方向變動太大了，導致Slow weights更新的方向可能不是好的</li></ul><p>Ranger還可以加什麼?</p><h2 id="LARS"><a href="#LARS" class="headerlink" title="LARS"></a>LARS</h2><p>在過去，發現batch數量大到一個程度後，效果越來越差</p><ul><li>大batch的採樣梯度應該要更具代表性? 但可能蓋掉了一些小資訊</li><li>當batch夠大的時候，學習速率也應該要跟著變大才能夠相襯<ul><li>但學習速率到底該多大多小?</li><li>傳統優化器使用單一學習速率值，有可能造成某層的權重變化超過原始權重(有些需要大學習速率，有些需要較小的學習速率)，導致學習震盪</li></ul></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/dLCz2o5.png" alt=""></p><p>上圖可以看到不同層需要的權值/學習率變化幅度不同</p><ul><li>第二欄: 權值</li><li>第三欄: 更新量</li><li>第四欄: 比值</li></ul><p>甚至可以看到有很多層根本不用更新(比值很大的部分)</p><p>LARS(Layer-wise Adaptive Rate Scaling)</p><ul><li>計算每層的內部統計量來逐層調節學習率</li><li>適用需要短時間訓練完模型的場景</li><li>適用需要大batch的應用(Ex: SimCLR)</li></ul><p>而LARS又跟前述優化器設計無關，所以可以再合併起來: LARS + Ranger = Ranger-LARS</p><ul><li>但如果批次數量小的時候，其實不太會用到</li></ul><h2 id="梯度中心化-Gradient-Centralization"><a href="#梯度中心化-Gradient-Centralization" class="headerlink" title="梯度中心化: Gradient Centralization"></a>梯度中心化: Gradient Centralization</h2><p>Gradient Centralization: 對全部梯度減掉平均值(沒有除以標準差)</p><p>對梯度做某種層級的Normalization，好處:</p><ol><li>減掉梯度均值 -&gt; 縮小梯度範圍，降低梯度爆炸問題</li><li>Normalization -&gt; 有助於模型泛化</li></ol><p>已被證實有助於提升效果</p><p>目前只對FC, Conv做，所以不會影響BN layer</p><h2 id="自動混和精度訓練-Automatic-Mixed-Precision-Training"><a href="#自動混和精度訓練-Automatic-Mixed-Precision-Training" class="headerlink" title="自動混和精度訓練: Automatic Mixed Precision Training"></a>自動混和精度訓練: Automatic Mixed Precision Training</h2><p>Pytorch 1.6 support</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/UnOFmK2.png" alt=""></p><p>受到模型量化啟發，再訓練階段時在兩種精度下切換縮放，達到家夾提高模型泛化的效果</p><ul><li>再fp16 &amp; fp32間切換</li><li>計算loss的時候用fp32確保精度</li><li>inference的時候用fp16節省效率</li><li>更新權重: 將模型縮放回原本fp32來進行更新</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/q3NIpnl.png" alt=""></p><p>有些部分再轉成fp16的時候會不夠用(fp16的分布沒有涵蓋到模型權重的分布) -&gt; 造成直接變成0</p><ul><li>這些部分需要使用fp32來確保精度</li><li>可以用fp16表示的部分則繼續使用fp16</li><li>自動搜索切點來切出那個點</li></ul><p>達到節約空間 &amp; 訓練變快 &amp; 更robust</p>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Networks API Introduction: Android APP背後是如何執行一個神經網路模型的?</title>
      <link href="/posts/668f0de1/"/>
      <url>/posts/668f0de1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇是對於官方文檔<a href="https://developer.android.com/ndk/guides/neuralnetworks" target="_blank" rel="noopener">Neural Networks API</a>的中文閱讀筆記，內容是閱讀後透過自己的理解重新撰寫的，所以不全然是文檔的翻譯內容。</p><h2 id="Android-Neural-Networks-API"><a href="#Android-Neural-Networks-API" class="headerlink" title="Android Neural Networks API"></a>Android Neural Networks API</h2><p>一個深度學習的應用分成兩個階段</p><ul><li><strong>training phase 訓練階段</strong>: 大部分的人都會在server上訓練好自家的深度學習模型(不會在手機上訓練，是想訓練多久…)</li><li><strong>inference phase 推論階段</strong>: 將訓練好的模型拿來進行推論的應用，此時模型根據scenario就有可能放在手機、IOT device這些小型裝置，而不是將資料送到server上做再拿回來(因為這樣會有很高的latency)</li></ul><p>為什麼要在device端做推論? 當然也可以把資料傳到server上推論完在下載回來，但這樣會衍生出一些問題:</p><ul><li><strong>Latency</strong>: 儘管server端的運算速度快很多，但資料傳送之間的I/O會造成很大的延遲，尤其對於一些real time的應用至關重要</li><li><strong>Availability</strong>: 在沒有網路的地方就無法將資料上傳到server計算</li><li><strong>Speed</strong>: 現在的行動設備上有越來越多支持深度學習運算的特化晶片，使得行動端的推論計算效率也大大提升，也逐漸能cover目前的edge ai application</li><li><strong>Privacy</strong>: 對於一些有隱私議題的應用來說，將資料上傳到server並不是一個好選擇，此時在local端自己處理資料匯市比較好的解決方案</li><li><strong>Cost</strong>: 使用行動端裝置自己處理推論，省掉了許多額外的成本(例如server的費用)</li></ul><a id="more"></a><p>也因此，現在的趨勢都是在裝置端自己處理推論，靠著深度學習特化晶片來加速深度學習的運算。而為了在這些裝置上能夠更加有效的執行推論，許多相對應的議題也被提出，例如: </p><ul><li>模型壓縮及量化(Quantization)</li><li>根據不同需求客製化的晶片:例如 APU、VPU、GPU、DSP…</li></ul><p>對於手機端，為了使手機上的應用端可以更加方便的使用底層的不同裝置，Google在<strong>Android 8.1 (API level 27)後提供了ANN API(Android Neural Networks API)</strong>。</p><p>ANN API使用C語言所撰寫，對於上層的深度學習框架(Tensorflow Lite, Caffe2…)提供了底層不同硬體資源的支持。</p><ul><li>例如你的手機晶片如果具備GPU、APU、或DSP等不同的硬體加速器，那就可以透過ANN API來選擇不同的底層執行你的模型</li></ul><h2 id="Understand-the-Neural-Networks-API-runtime"><a href="#Understand-the-Neural-Networks-API-runtime" class="headerlink" title="Understand the Neural Networks API runtime"></a>Understand the Neural Networks API runtime</h2><p>NN Runtime是什麼? 他是一個Android service，用來與底層的設備溝通，並將上層應用端的操作透過NN API傳遞給底層的硬體去執行。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1605107660/blog_posts/nnapi_architecture_px59gs.png" alt=""></p><p>先來看一下NN runtime的架構圖，從圖中可以發現:</p><ul><li>NN API基本上是給上層的深度學習框架來使用的，所以一般的APP並不能直接使用它</li><li>透過NN API來控制ANN Runtime這個Service，Android背後透過ANN Runtime來控制底層要用那些硬體加速器執行操作</li><li>而硬體必須要透過<strong>Android NN HAL(Hardware abstraction layer)</strong>向上提供自己的資訊，例如自己是誰、可以支援那些操作…</li></ul><p>如此，ANN Runtime就可以知道上層模型有哪些指令，底層有哪些硬體，而這些指令又可以跑在那些硬體上，最後再去根據這些硬體適合哪些操作來分配他們。</p><ul><li>對於沒有特殊硬體加速器的設備，則會在CPU上執行</li></ul><h2 id="Use-NN-API-to-create-a-neural-network-model"><a href="#Use-NN-API-to-create-a-neural-network-model" class="headerlink" title="Use NN API to create a neural network model"></a>Use NN API to create a neural network model</h2><p>這章節簡單的介紹如何用NN API寫出一個神經網路模型。</p><p>神經網路模型背後其實是一個計算圖(computational graph)，所以我們必須先用NN API創造出一個對應的有向圖(directed graph)，然後將對應的資料(weight, bias)指派給圖上的點。</p><p>NN API主要有4個概念:</p><ul><li><strong>Model</strong>: 一個神經網路的模型，包含了模型的相關操作(例如Conv, activation function…)<ul><li>創建Model的過程是一個synchronous operation，一旦創建成功就可以在不同thread和compilation之間使用</li><li>NN API中是一個ANeuralNetworksModel的instance</li></ul></li><li><strong>Compilation</strong>: 編譯器，負責將Model轉換成machine code<ul><li>創建Compilation的過程是一個synchronous operation，一旦創建成功就可以在不同thread和executions之間使用</li><li>在NN API中是使用ANeuralNetworksCompilation instance</li></ul></li><li><strong>Memory</strong>: 是一個shared memory (memory mapped files，NN API可以使用該memory buffer與底層的driver有效率的進行數據存取。通常在創建一個model時，我們也必須為每個graph上的node創建對應的memory buffer，包含weight, input和output的tensor<ul><li>在NN API中是一個ANeuralNetworksMemory instance</li></ul></li><li><strong>Execution</strong>: 指定input、output，然後執行Model的Interface<ul><li>這是一個asynchronous operation，不同thread都可以執行相同的execution。一旦execution執行完畢，則所有thread會被release</li><li>在NN API中是一個ANeuralNetworksExecution instance</li></ul></li></ul><p>了解這四個概念後，就不難理解他的Programming Flow了</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1605434414/blog_posts/nnapi_flow_y769wr.png" alt=""></p><ol><li>透過<code>ANeuralNetworksModel_create()</code>創建模型，包含以下步驟:<ol><li>指定graph中的operands，並可以將預訓練好的權重等data載入ANeuralNetworksMemory，然後在此時從ANeuralNetworksMemory讀進來</li><li>指定graph中的operations</li><li>指定input &amp; output node</li></ol></li><li>調用<code>ANeuralNetworksCompilation_create()</code>創建編譯器</li><li>調用<code>ANeuralNetworksExecution_create()</code>創建執行單元</li><li>調用<code>ANeuralNetworksExecution_startCompute()</code>呼叫底層開始執行，並等待執行結束(注意這裡是asynchronous operation，所以可以平行計算)</li></ol><p>此外，一個編譯好的Compilation也可以給不同的Execution使用，只要給定不同的input以及create一個新的ANeuralNetworksExecution instance即可。</p><p>關於詳細的example code可以參考官方文檔學習。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://developer.android.com/ndk/guides/neuralnetworks" target="_blank" rel="noopener">Neural Networks API</a></li><li><a href="https://blog.csdn.net/qkhhyga2016/article/details/78800912" target="_blank" rel="noopener">Android 之AI硬件和NNAPI介绍</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux Kernel慢慢學]快速上手Makefile和Kbuild Makefile</title>
      <link href="/posts/5523c739/"/>
      <url>/posts/5523c739/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Linux Kernel中是透過<strong>Kbuild Makefile</strong>(Kernel build, 基於makefile的compile tool)來對kernel code編譯，然後最終產生vmlinux file。所以了解Kbuild是很重要的，在看一個kernel code的時候可以透過Makefile file來幫助了解編譯的架構構成。</p><p>要了解Kbuild Makefile就要知道什麼是Makefile，但是Makefile細節太多了，講不完齁</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_500/v1604723743/blog_posts/2010720986sCasEOMm_klxxwz.png" alt=""></p><p>詳細完整的教學網路上很多了，所以這篇文章不講瑣碎的原理跟細節，而是試著從實際看code會看到的內容來介紹:</p><ol><li>快速介紹Makefile的概念，我們只講重點</li><li>快速介紹Kbuild規則，我們只講重點和常會用到的部分</li><li>搭配kernel code來實際看一下例子</li></ol><a id="more"></a><h2 id="從0開始的make快速上手教學"><a href="#從0開始的make快速上手教學" class="headerlink" title="從0開始的make快速上手教學"></a>從0開始的make快速上手教學</h2><p>先科普一下一個程式到產生執行檔的過程: 由c/c++等語言邊寫的程式碼檔案，會先經過編譯器(compiler)產生object file，然後再透過連結器(linker)將一個或多個object file組成執行檔executable。</p><p>今天如果需要編譯一個由c寫的程式碼我們可以直接下gcc指令來編譯，但下列一些狀況就很適合使用Makefile:</p><ol><li>例如包含多個.c(object file)檔案，彼此檔案之間的相關性很複雜的時候，人工來維護這些相關性比較麻煩</li><li>有時候要編譯的檔案很多(例如linux kernel)，但我們只改了一部分的code，那跟這部分無關的其實可以都不用重新編譯以節省時間</li></ol><p>此時我們可以透過一些工具來幫我們做檢查並自動編譯，Makefile就是一個好工具。夠過指令<code>make</code>去找資料夾下的Makefile（還有其他名稱會被查找，這裡有個查找的順序）文件，然後會做這些檢查:</p><ol><li>如果<strong>目標(target)</strong>的<strong>相依性項目(dependencies)</strong>中需要的的source code file沒有被編譯過，則編譯相關檔案然後生成我們需要的檔案</li><li>如果目標的相依檔案有一個source code file被更動過，則他將會被重新編譯</li><li>如果有目標的相依項目有header file被修改過，則所有使用到該header file的都會被重新編譯</li></ol><p>目標跟相依檔案是什麼意思？考慮一個gcc command作為例子:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">0</span><br></pre></td><td class="code"><pre><span class="line">gcc -o hello_make main.o</span><br></pre></td></tr></table></figure><br>這段指令是說透過連結main.o產生一個hello_make的執行檔，但我們沒有main.o呀？</p><p>所以我們需要先編譯main.c來產生main.o，透過先下這個指令:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">0</span><br></pre></td><td class="code"><pre><span class="line">gcc -c main.c</span><br></pre></td></tr></table></figure></p><p>上述的兩個步驟隱含了hello_make、main.o和main.c的關係: </p><ol><li>所以為了產生hello_make(target)，我們需要main.o(dependency)</li><li>而因為需要main.o(target)，我們需要main.c(dependency)</li></ol><p>這種目標跟相依性的概念就是Makefile的核心！</p><p>上面這個指令在等價於Makefile的這種寫法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">all: main.o</span><br><span class="line">    gcc -o hello_make main.o</span><br><span class="line"></span><br><span class="line">main.o: main.c</span><br><span class="line">    gcc -c main.c</span><br></pre></td></tr></table></figure></p><p>這樣有沒有比較有概念了呢？</p><p>比較了解Makefile是什麼後，來正式看一下他的格式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># comment in Makefile</span><br><span class="line">&lt;target&gt;: &lt;dependiency1&gt; &lt;dependiency2&gt; &lt;dependiency3&gt;...</span><br><span class="line"># Note that makefile use tab!!! not space!!!</span><br><span class="line">    &lt;rule1&gt;</span><br><span class="line">    &lt;rule2&gt;</span><br></pre></td></tr></table></figure></p><ol><li>一個Makefile可以有很多個target，在下<code>make</code>時<strong>預設會找第一個target作為最終目標</strong><ul><li>如果要執行特定的target可以下<code>make &lt;target&gt;</code>command</li></ul></li><li>如果要執行的target的相依性項目不存在，則繼續往下看其他的target來產生所需的檔案</li></ol><p>最後，一些Makefile常用小知識:</p><ul><li>Makefile是使用<strong>Tab</strong>而不是空格，所以如果編譯器有將Tab自動轉成4個空格的，打美蝶斯！</li><li>$是變數的意思，所以可以透過變數設定來控制我們的rule<ul><li>由於$是變數，所以如果要執行shell command會使用兩個錢錢符號 </li></ul></li><li>%是萬用匹配符號，例如<code>%.o: %.c</code>就是將所有.o檔對應的.c file作為dependency</li><li>以及其他許多噁心的符號:<ul><li>$@: rule當前的target name</li><li>$&lt;: rule當前的(第一個)dependency name</li><li>$*: rule當前的dependency name，但不含副檔名</li><li>$?: rule當前比targets還新的dependencies</li></ul></li></ul><p>瞭解了這些就可以開始去找網路上的Makefile練功打怪了，詳細的教學不是本篇的重點，主要是讓大家對Makefile有個基本概念，有興趣的也可以去看References的相關文章繼續學習唷。</p><h2 id="番外篇-要如何遞迴make所有子目錄？"><a href="#番外篇-要如何遞迴make所有子目錄？" class="headerlink" title="番外篇: 要如何遞迴make所有子目錄？"></a>番外篇: 要如何遞迴make所有子目錄？</h2><p>Makefile其實是可以進入某個目錄然後去執行該目錄下的make的，使用“-C”參數即可，例如:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">0</span><br></pre></td><td class="code"><pre><span class="line">make -C subdirectory</span><br></pre></td></tr></table></figure><br>他就會先進到subdirectory這個資料夾中去執行<code>make</code>，然後再回來繼續執行這一層的<code>make</code>。</p><p>可是如果這些資料夾名稱我都要一個一個手動填也是很麻煩，要如何自動取得底下所有資料夾，然後讓他一個一個自己進去make呢？搭配shell command來達成: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SUBDIRS &#x3D; $(shell ls -d *&#x2F;)</span><br><span class="line">all:</span><br><span class="line">    for dir in $(SUBDIRS) ; do \</span><br><span class="line">        make -C  $$dir ; \</span><br><span class="line">    done</span><br></pre></td></tr></table></figure><p>搞定!</p><h2 id="Linux-Kernel的Makefile-Kbuild-Makefile"><a href="#Linux-Kernel的Makefile-Kbuild-Makefile" class="headerlink" title="Linux Kernel的Makefile: Kbuild Makefile"></a>Linux Kernel的Makefile: Kbuild Makefile</h2><p>Kbuild是Kernel build的縮寫，為了方便對kernel進行編譯而提供了一些特別的功能。</p><p>Linux Kbuild主要有四個部分:</p><ul><li>.config: kernel相關設置</li><li>arch/$(ARCH)/Makefile: 與架構相關的Makefile</li><li>scripts/Makefile.*: Kbuild的一些規則</li><li>根目錄下的Makefile以及其他資料夾下的Makefiles </li></ul><p>Kbuild的用法相當簡單，對於一個driver的撰寫者最常會碰到的就2種:</p><ul><li><code>obj-y</code>: 將module編進kernel中<ul><li>注意對於<code>obj-y</code>直接將module編進kernel時，編譯的順序很重要，因為這會影響開機時module init的順序，如果順序沒搞好可能就會遇到問題！可以參考<a href="https://meetonfriday.com/posts/c4426b79/#platform-device%E6%9C%89%E5%A4%9A%E5%80%8Bmodule%E6%99%82%E5%8A%A0%E8%BC%89%E7%9A%84%E9%A0%86%E5%BA%8F">[Linux Kernel慢慢學]Linux modules載入及載入順序</a></li></ul></li><li><code>obj-m</code>: 將module編成.ko檔，以便之後可以進行載入</li></ul><p>其他還有產生library的<code>lib-y</code>、產生executable的<code>hostprogs-y</code>…有興趣的再自己去看</p><p>那Kbuild要怎麼進入一個資料夾呢？ 透過<code>obj-$(CONFIG_) += directory/</code>的方式</p><ul><li>在kernel code裡面較少使用上面番外篇的遞迴形式來跑所有的子目錄，因為通常kernel的Makefile還會搭配一堆config，這些config可以讓你選擇這次編譯時要將哪些module編進來哪些不要，所以會需要一個一個資料夾的方式條列在Makefile內</li></ul><p>實際來看一了例子吧！kernel 4.14的<a href="https://elixir.bootlin.com/linux/v4.14/source/drivers/clk/Makefile" target="_blank" rel="noopener">/drivers/clk/Makefile</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># &#x2F;drivers&#x2F;clk&#x2F;Makefile</span><br><span class="line"></span><br><span class="line"># SPDX-License-Identifier: GPL-2.0</span><br><span class="line"># common clock types</span><br><span class="line">obj-$(CONFIG_HAVE_CLK)+&#x3D; clk-devres.o clk-bulk.o</span><br><span class="line">obj-$(CONFIG_CLKDEV_LOOKUP)+&#x3D; clkdev.o</span><br><span class="line">obj-$(CONFIG_COMMON_CLK)+&#x3D; clk.o</span><br><span class="line">obj-$(CONFIG_COMMON_CLK)+&#x3D; clk-divider.o</span><br><span class="line">obj-$(CONFIG_COMMON_CLK)+&#x3D; clk-fixed-factor.o</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># hardware specific clock types</span><br><span class="line"># please keep this section sorted lexicographically by file path name</span><br><span class="line">obj-$(CONFIG_MACH_ASM9260)+&#x3D; clk-asm9260.o</span><br><span class="line">obj-$(CONFIG_COMMON_CLK_AXI_CLKGEN)+&#x3D; clk-axi-clkgen.o</span><br><span class="line">obj-$(CONFIG_ARCH_AXXIA)+&#x3D; clk-axm5516.o</span><br><span class="line">obj-$(CONFIG_COMMON_CLK_CDCE706)+&#x3D; clk-cdce706.o</span><br><span class="line">obj-$(CONFIG_COMMON_CLK_CDCE925)+&#x3D; clk-cdce925.o</span><br><span class="line">obj-$(CONFIG_ARCH_CLPS711X)+&#x3D; clk-clps711x.o</span><br><span class="line">obj-$(CONFIG_COMMON_CLK_CS2000_CP)+&#x3D; clk-cs2000-cp.o</span><br><span class="line">obj-$(CONFIG_ARCH_EFM32)+&#x3D; clk-efm32gg.o</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># please keep this section sorted lexicographically by directory path name</span><br><span class="line">obj-$(CONFIG_COMMON_CLK_AT91)+&#x3D; at91&#x2F;</span><br><span class="line">obj-$(CONFIG_ARCH_ARTPEC)+&#x3D; axis&#x2F;</span><br><span class="line">obj-$(CONFIG_ARC_PLAT_AXS10X)+&#x3D; axs10x&#x2F;</span><br><span class="line">obj-y+&#x3D; bcm&#x2F;</span><br><span class="line">obj-$(CONFIG_ARCH_BERLIN)+&#x3D; berlin&#x2F;</span><br><span class="line">obj-$(CONFIG_H8300)+&#x3D; h8300&#x2F;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>透過<code>CONFIG_</code>是m或是y來決定kernel要怎麼去編譯這些modules，而這些<code>CONFIG_</code>是透過Kconfig來進行設置，最後記錄在.config內。</p><p>Kernel各個driver下的Makefile幾乎都長的差不多這樣，甚至比外面琳瑯滿目的Makefile還相對單純，所以看一兩個就熟悉了。</p><h3 id="Linux-Makefile-Code-Trace"><a href="#Linux-Makefile-Code-Trace" class="headerlink" title="Linux Makefile Code Trace"></a>Linux Makefile Code Trace</h3><p>最後，既然是Makefile，總該有個進入點吧？ </p><p>想像上應該是對根目錄的Ｍakefile下<code>make</code>相關指令，然後應該有一個地方會定義需要被一同編譯進去的子資料夾。</p><p>稍微來trace一下根目錄下的<a href="https://elixir.bootlin.com/linux/v4.14/source/Makefile" target="_blank" rel="noopener">Makefile</a>: </p><ol><li>第一個target在_all<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"># That&#39;s our default target when none is given on the command line</span><br><span class="line">PHONY :&#x3D; _all</span><br><span class="line">_all:</span><br></pre></td></tr></table></figure></li><li>line 194: _all依賴all，這段就不放code了</li><li>all依賴vmlinux<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br></pre></td><td class="code"><pre><span class="line"># The all: target is the default when no target is given on the</span><br><span class="line"># command line.</span><br><span class="line"># This allow a user to issue only &#39;make&#39; to build a kernel including modules</span><br><span class="line"># Defaults to vmlinux, but the arch makefile usually adds further targets</span><br><span class="line">all: vmlinux</span><br></pre></td></tr></table></figure></li><li>vmlinux依賴四個項目: scripts/link-vmlinux.sh, vmlinux_prereq, $(vmlinux-deps), FORCE<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1004</span><br><span class="line">1005</span><br></pre></td><td class="code"><pre><span class="line">vmlinux: scripts&#x2F;link-vmlinux.sh vmlinux_prereq $(vmlinux-deps) FORCE</span><br><span class="line">    +$(call if_changed,link-vmlinux)</span><br></pre></td></tr></table></figure></li><li>其中，$(vmlinux-deps)依賴$(vmlinux-dirs)，$(vmlinux-dirs)定義了所有應該被編譯進kernel的folder。後續的細節就不追了，有興趣可以參考<a href="https://blog.csdn.net/dwdwdw2/article/details/78587782" target="_blank" rel="noopener">linux kernel Makefile编译流程分析</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br></pre></td><td class="code"><pre><span class="line">vmlinux-dirs:&#x3D; $(patsubst %&#x2F;,%,$(filter %&#x2F;, $(init-y) $(init-m) \</span><br><span class="line">             $(core-y) $(core-m) $(drivers-y) $(drivers-m) \</span><br><span class="line">             $(net-y) $(net-m) $(libs-y) $(libs-m) $(virt-y)))</span><br></pre></td></tr></table></figure></li></ol><p>最後，介紹一下關於FORCE的用法：</p><p>實際上他就是一個沒有dependency和rule的target，在Makefile中如果<strong>依賴和指令都為空的target每次都需要強制生成</strong>，所以對於每次都需要強制重新產生的target中可以在最後加上FORCE的項目。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1714</span><br><span class="line">1715</span><br></pre></td><td class="code"><pre><span class="line">PHONY +&#x3D; FORCE</span><br><span class="line">FORCE:</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://sites.google.com/site/mymakefile/makefile-yu-fa-jian-jie" target="_blank" rel="noopener">Makefile 語法簡介</a></li><li><a href="https://mropengate.blogspot.com/2018/01/makefile.html" target="_blank" rel="noopener">簡單學 makefile：makefile 介紹與範例程式</a></li><li><a href="https://stackoverflow.com/questions/9249757/recursive-make-in-subdirectories" target="_blank" rel="noopener">Recursive make in subdirectories</a></li><li><a href="https://www.twblogs.net/a/5c177e18bd9eee5e40bbc7d9" target="_blank" rel="noopener">【Linux + Makefile】十分鐘教你學會Makefile的FORCE</a></li><li><a href="https://www.kernel.org/doc/html/latest/kbuild/makefiles.html#who-does-what" target="_blank" rel="noopener">Linux Kernel Makefiles</a></li><li><a href="https://gohalo.me/reference/linux/kernel/KBUILD_system.pdf" target="_blank" rel="noopener">KBUILD 系统原理分析</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【Linux Kernel慢慢學】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> linux kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux Kernel慢慢學]define macro with hashtag(&quot;#&quot; and &quot;##&quot;)</title>
      <link href="/posts/b7efb858/"/>
      <url>/posts/b7efb858/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>這篇的標題定的很…白話(?) 主要是在看Linux kernel code的時候有時候常常會看到一堆define，有些define還會搭配井字號的寫法還會讓你很看不懂，所以重新寫一個小短篇順便幫自己複習一下。</p><p>然後井字號這種特殊符號又很難去search，所以標題就決定也這樣下，不知道未來遇到同樣問題的人搜尋起來會不會比較容易xD</p><p>來個小測驗，如果你覺得你夠懂#define了，下面這段Linux kernel code你看得懂嗎?</p><ul><li>來自<a href="https://meetonfriday.com/posts/c4426b79/">[Linux Kernel慢慢學]Linux modules載入及載入順序</a></li></ul><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* include/linux/init.h. line=172 */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __define_initcall(fn, id) \</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">initcall_t</span> __initcall_#<span class="meta">#fn##id __used \</span></span><br><span class="line">    __attribute__((__section__(<span class="string">".initcall"</span> #id <span class="string">".init"</span>))) = fn;</span><br></pre></td></tr></table></figure><p>比較陌生的應該是跟井字號的用法(# 和 ##)，他們其實被稱作<strong>Preprocessor operators</strong></p><p>所以這篇一開始會快速介紹define macro，接下來開始介紹和Preprocessor operators的搭配，對這部分有興趣的可以直接看最後一Part。</p><a id="more"></a><h2 id="1分鐘學會-define-macro"><a href="#1分鐘學會-define-macro" class="headerlink" title="1分鐘學會#define macro"></a>1分鐘學會#define macro</h2><p>define macro的用法很簡單，快速舉個例子:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_LEVEL 100</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, MAX_LEVEL);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Compiler在<strong>編譯時期</strong>就會把這個MAX_LEVEL直接<strong>替換</strong>成後面的100，就4這麼簡單。</p><p>define常被用來定義一些常量，或是一些config。例如你的Code有個部分會引用到版本號，那每次版本號一變動所有相關的code都要跟著改很麻煩，此時你可以<code>#define VERSION 1.1</code>，然後把code都用VERSION來替代，這樣未來如果版本變了只需要改一次define的值就好。</p><ul><li>此外，也可以透過define macro來節省一些function call，具體可以在後面的介紹觀察到</li></ul><p>最後，為了和function, variable區分，<strong>define macro通常都會使用大寫</strong>。</p><h2 id="define不只能用在常數"><a href="#define不只能用在常數" class="headerlink" title="define不只能用在常數"></a>define不只能用在常數</h2><p>define也可以用在function，例如:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ADD(a, b) a + b <span class="comment">// wrong way of define</span></span></span><br></pre></td></tr></table></figure></p><p>但要注意，上面這兩種寫法是錯誤的。因為define只是將後面的東西<strong>替換</strong>你的<code>add(a, b)</code>，所以如果你在呼叫的時候這樣寫:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ADD(a, b) a + b; <span class="comment">// wrong way of define</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> c = ADD(<span class="number">1</span>, <span class="number">2</span>); <span class="comment">// exec correct, c = 3</span></span><br><span class="line">    <span class="keyword">int</span> d = ADD(<span class="number">1</span>, <span class="number">2</span>) * <span class="number">0</span>; <span class="comment">// wrong output, d = 1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>可以發現d的值跟預想的不同，因為<code>ADD(1, 2)</code>只是被替換成<code>1+2</code>，並不是實際去執行了，所以d的assign右半邊實際上是<code>1+2*0</code>，也就得到了1的output。</p><p>解決方法是在define前後<strong>加上括號</strong>，確保define被替換的內容也能正確地執行<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ADD(a, b) (a + b)</span></span><br></pre></td></tr></table></figure></p><p>所以define macro使用在function或condition(當然也可以拿來寫if else)的時候要很小心。</p><p>再看另一個例子:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SWAP(a, b) &#123;\ </span></span><br><span class="line">                        a ^= b; \</span><br><span class="line">                        b ^= a; \</span><br><span class="line">                        a ^= b; \</span><br><span class="line">                    &#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123;</span><br><span class="line">    a ^= b;</span><br><span class="line">    b ^= a;</span><br><span class="line">    a ^= b;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> x = <span class="number">1</span>, y = <span class="number">2</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; x &lt;&lt; y &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">// 12</span></span><br><span class="line">    SWAP(x, y);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; x &lt;&lt; y &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">// 21</span></span><br><span class="line">    swap(x, y);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; x &lt;&lt; y &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>; <span class="comment">// 21</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>會發現<code>SWAP</code>成功的將x y調換了，但<code>swap()</code>卻沒有，為什麼呢?</p><ul><li>因為swap()是call by value</li><li>而SWAP是define macro，實際上compiler是把那三行<strong>替換</strong>到<code>main()</code>中而不是呼叫一個function<ul><li>有注意到我一值強調<strong>替換</strong>嗎? 這是define macro很重要的行為要分辨清楚以後才不會出錯</li></ul></li></ul><p>然後有發現這個例子中define變成了多行的敘述了嗎? 沒錯，define當然可以分成多行來撰寫。</p><h2 id="define也可以寫多行"><a href="#define也可以寫多行" class="headerlink" title="define也可以寫多行"></a>define也可以寫多行</h2><p>如果define的敘述太長怎麼辦，一般建議程式碼一行不要超過<strong>79</strong>個字元，所以有時候有需要分行的需求，此時就可以這樣寫:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SWAP_XOR(a, b) &#123;\ </span></span><br><span class="line">                        a ^= b; \</span><br><span class="line">                        b ^= a; \</span><br><span class="line">                        a ^= b; \</span><br><span class="line">                    &#125;</span><br></pre></td></tr></table></figure></p><p>透過用斜線的方式來寫多行的define macro。</p><h2 id="當-define碰上井字號"><a href="#當-define碰上井字號" class="headerlink" title="當#define碰上井字號"></a>當#define碰上井字號</h2><p>這些符號稱之Preprocessor operators，共有三種。下面快速總結，淺顯易懂:</p><ul><li><strong>Stringizing operator (#)</strong>: 用來將引數變成一個字串</li><li><strong>Charizing operator (#@)</strong>: 用來將引數變成一個字元</li><li><strong>Token-pasting operator (##)</strong>: 用來拼接引數</li></ul><p>然後再搭配個簡單的例子就知道了:</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAKESTR(s) #s</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAKECHAR(x)  #@x</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CONS(a,b) int(a##e##b)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    a = MAKECHAR(b);        <span class="comment">// a = 'b';</span></span><br><span class="line">    b = MAKESTR(hello)      <span class="comment">// b = "hello";</span></span><br><span class="line">    c = CONS(<span class="number">2</span>,<span class="number">3</span>);          <span class="comment">// c = 2e3;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>了解了之後，最後回頭看我們在前言提到的那段kernel code:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* include/linux/init.h. line=172 */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __define_initcall(fn, id) \</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">initcall_t</span> __initcall_#<span class="meta">#fn##id __used \</span></span><br><span class="line">    __attribute__((__section__(<span class="string">".initcall"</span> #id <span class="string">".init"</span>))) = fn;</span><br></pre></td></tr></table></figure><br>如果fn=module, id=3的話，那<code>__define_initcall(fn, id)</code>就會變成</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">initcall_t</span> __initcall_module3 __used \</span><br><span class="line">    __attribute__((__section__(<span class="string">".initcall"</span> <span class="string">"6"</span> <span class="string">".init"</span>))) = fn;</span><br></pre></td></tr></table></figure><p>你答對了嗎?</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://docs.microsoft.com/en-us/cpp/preprocessor/preprocessor-operators?view=vs-2019" target="_blank" rel="noopener">Preprocessor operators</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【Linux Kernel慢慢學】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> linux kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]ILSVRC系列文回顧 - 歷屆CNN模型介紹</title>
      <link href="/posts/1e087b70/"/>
      <url>/posts/1e087b70/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><h2 id="前言-ILSVRC系列文的里程碑"><a href="#前言-ILSVRC系列文的里程碑" class="headerlink" title="前言 - ILSVRC系列文的里程碑"></a>前言 - ILSVRC系列文的里程碑</h2><p>在10月底的論文速速讀中終於完成了SENet的中文論文導讀，該篇也是ILSVRC系列的最後一篇了(因為最後一屆舉辦就是2017年)。</p><p>這些知名的模型其實在碩士修課的時候就都學過了(<a href="https://meetonfriday.com/posts/18a141c2/">[DL]淺談CNN在Object Classification上的各種架構</a>)，但一直都沒有完整的看過每一篇論文。畢業之後想通過規劃一系列的文章來逼自己看論文，也透過這一系列的論文筆記幫助對深度學習有興趣的朋友。</p><p>「為什麼要浪費時間把每篇都看一次? 知道怎麼用不就好了嗎?」</p><p>可能有人會這麼想，不過我認為每個模型的發展都有它背後要解決的問題，模型比別人好也有其原因，看完這一系列的文章後希望讀者可以<strong>對於要用什麼模型來處理自己的任務比較有概念。</strong></p><p>而未來如果需要進一步去設計屬於自己的模型的時候，也期望這系列的文章也能夠讓你<strong>更加了解應該要如何設計</strong>，例如隨插隨用的Inception block, ResNet block, SE block是什麼…又應該怎麼把這些模塊融入自己的設計模型中，預期能為自己的模型達到什麼樣的效果。</p><a id="more"></a><h2 id="ILSVRC是什麼"><a href="#ILSVRC是什麼" class="headerlink" title="ILSVRC是什麼"></a>ILSVRC是什麼</h2><p>簡單來說，ILSVRC就是基於大神李飛飛創立的ImageNet dataset來舉辦的圖像競賽，是機器視覺領域最受追捧也是最具權威的學術競賽之一，代表了影像領域的最高水平。</p><p>如果不知道什麼是ILSVRC或ImageNet的朋友，維基給你，先去看個吧: <a href="https://zh.wikipedia.org/wiki/ImageNet" target="_blank" rel="noopener">ImageNet</a></p><h2 id="ILSVRC各種模型介紹"><a href="#ILSVRC各種模型介紹" class="headerlink" title="ILSVRC各種模型介紹"></a>ILSVRC各種模型介紹</h2><p>下圖引用自另一篇介紹<a href="https://chtseng.wordpress.com/2017/11/20/ilsvrc-%E6%AD%B7%E5%B1%86%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B/" target="_blank" rel="noopener">ILSVRC 歷屆的深度學習模型</a>，讓大家對ILSVRC歷屆的模型有個概念:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1603547272/blog_posts/4749_udebe9stqa_lep59y.png" alt=""></p><p>下面我們將會介紹2012年後的所有模型，包含:</p><ol><li>AlexNet</li><li>ZFNet</li><li>GoogLeNet</li><li>VGG</li><li>ResNet</li><li>SENet</li></ol><p>這篇文章是基於看過這些論文後重新整理一次的內容，但仍會有部分和一年前寫的(<a href="https://meetonfriday.com/posts/18a141c2/">[DL]淺談CNN在Object Classification上的各種架構</a>)有部分相同。</p><p>此外，這篇文章我也會<strong>附上Pytorch和Tensorflow的model source code</strong>(盡量以官方為主，但如果真的找不到我就會附其他github上的資源)，大家可以在了解模型的架構也對照一下code是怎麼寫出來的，會使你對模型更加熟悉。</p><hr><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><ul><li><a href="https://meetonfriday.com/posts/e54c12ea/">[論文速速讀]ImageNet Classification with Deep Convolutional Neural Networks</a></li><li>Pytorch: <a href="https://pytorch.org/docs/stable/_modules/torchvision/models/alexnet.html#alexnet" target="_blank" rel="noopener">Pytorch source code</a></li><li>Tensorflow: <a href="https://github.com/hjptriplebee/AlexNet_with_tensorflow" target="_blank" rel="noopener">hjptriplebee/AlexNet_with_tensorflow</a></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594579999/blog_posts/%E6%93%B7%E5%8F%96_woxr2t.png" alt=""></p><p>作為ILSVRC第一次嶄露頭角的CNN模型(在此之前的方法還是feature extraction + machine leaning based)，AlexNet的特點如下:</p><ul><li>使用了兩張GPU來training，所以架構上有特別做設計</li><li>使用<strong>ReLU</strong>作為activation function，提升了訓練的收斂速度</li><li>加入<strong>dropout</strong>技巧防止overfitting</li><li>使用了Local Response Normalization(LRN)來對ReLU做Normalization</li><li>5層Conv layer + 3層FC layer架構，Conv的部分使用到了11x11, 7x7, 5x5, 3x3都有</li></ul><hr><h3 id="ZFNet"><a href="#ZFNet" class="headerlink" title="ZFNet"></a>ZFNet</h3><ul><li><a href="https://meetonfriday.com/posts/3013fdb9/">[論文速速讀]Visualizing and Understanding Convolutional Networks</a></li><li>Pytorch: <a href="https://github.com/arvention/ZFNet-PyTorch" target="_blank" rel="noopener">arvention/ZFNet-PyTorch</a></li><li>Tensorflow: <a href="https://github.com/amir-saniyan/ZFNet" target="_blank" rel="noopener">amir-saniyan/ZFNet</a></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595278484/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-21_%E4%B8%8A%E5%8D%884.54.31_ydvnmx.png" alt=""></p><p>ZFNet是隔年的冠軍…架構上其實就是AlexNet拿來小改了一下</p><ul><li>像是將stride縮小(4-&gt;2)和kernel size縮小(11-&gt;7)</li><li>相較AlexNet，只用了一張GPU來訓練，節能減碳愛地球</li><li>ZFNet的另一個重點在於開啟了CNN可視化的大門: 透過<strong>Deconvnet來觀察模型到底學到了什麼</strong>，對於解釋性AI(Explainable AI, XAI)後續的研究打下了非常重要的基礎</li></ul><hr><h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><ul><li><a href="https://meetonfriday.com/posts/263e065d/">[論文速速讀]Going deeper with convolutions</a></li><li>Pytorch: <a href="https://pytorch.org/docs/stable/_modules/torchvision/models/googlenet.html" target="_blank" rel="noopener">Pytorch source code</a></li><li>Tensorflow: <a href="https://github.com/PanJinquan/tensorflow_models_learning" target="_blank" rel="noopener">PanJinquan/tensorflow_models_learning</a></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595626451/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-25_%E4%B8%8A%E5%8D%885.33.00_u0bj0m.png" alt=""></p><p>ILSVRC 2014的冠軍，特地把Google的”L”大寫是為了要致敬LeNet(CNN的開山始祖，有興趣的可以看<a href="https://meetonfriday.com/posts/d5321308/">[論文速速讀]Gradient Based Learning Applied to Document Recognition</a>)</p><ul><li>提出了<strong>Inception module</strong>: 由1x1, 3x3, 5x5 Conv和Maxpool組合起來的一個block<ul><li>提供了多尺度的feature extraction</li><li>讓模型自己去學哪一種Conv或Pooling比較重要</li><li>(在filter level上來實踐稀疏性，期望能逼近稀疏網路結構的效果: 這其實是他背後設計的真正目的…不過這部分有點難理解，有興趣的可以再去看這篇的論文導讀)</li></ul></li><li><strong>1x1 Conv</strong>: 在多尺度的Conv後面接上1x1 Conv來做降維，降低模型複雜度<ul><li>走在CNN路上不知道1x1 Conv? 快去看<a href="https://meetonfriday.com/posts/a151bfa2/">[論文速速讀]Network In Network</a></li></ul></li><li>在分類前使用<strong>GAP(Global Average Pooling)</strong>取代FC: 節省計算量並有更好的representation能力<ul><li>走在CNN路上不知道GAP? 快去看<a href="https://meetonfriday.com/posts/a151bfa2/">[論文速速讀]Network In Network</a></li></ul></li><li>在中間層加入2個auxiliary classifiers幫助訓練，但預測的時候不會使用</li></ul><hr><h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><ul><li><a href="https://meetonfriday.com/posts/5f745d96/">[論文速速讀]Very Deep Convolutional Networks For Large-Scale Image Recognition</a></li><li>Pytorch: <a href="https://pytorch.org/docs/stable/_modules/torchvision/models/vgg.html" target="_blank" rel="noopener">Pytorch source code</a></li><li>Tensorflow: Tensorflow 2 source code<ul><li><a href="https://github.com/tensorflow/tensorflow/blob/f67204eea0585b9f63cc7b40a20ca45e254998ec/tensorflow/python/keras/applications/vgg16.py" target="_blank" rel="noopener">vgg16</a></li><li><a href="https://github.com/tensorflow/tensorflow/blob/f67204eea0585b9f63cc7b40a20ca45e254998ec/tensorflow/python/keras/applications/vgg19.py" target="_blank" rel="noopener">vgg19</a></li></ul></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1598000889/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-08-21_%E4%B8%8B%E5%8D%885.06.02_jikm11.png" alt=""></p><p>VGG是2014年的亞軍，但也是非常知名的模型，由於他的設計考量對於後續模型在設計的時候非常重要，並且他的模型到現在也還在被很多人使用。</p><ul><li><strong>只使用3x3 Conv</strong>: 相較於之前的模型使用較大的kernel size，可以使用多次的Conv來達到同樣的感受野(Receptive fields)，並且用更少的參數量<ul><li>舉例來說2個3x3的Conv可以和1個5x5的Conv涵蓋一樣的資訊量，但參數量卻比較少(3<em>3</em>2 &lt; 5<em>5</em>2)</li></ul></li><li>打槍AlexNet的LRN: 說並沒有觀察到效果提升</li></ul><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><ul><li><a href="https://meetonfriday.com/posts/7c0020de/">[論文速速讀]Deep Residual Learning for Image Recognition</a></li><li>Pytorch: <a href="https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html" target="_blank" rel="noopener">Pytorch source code</a></li><li>Tensorflow:<a href="https://github.com/tensorflow/tensorflow/blob/f67204eea0585b9f63cc7b40a20ca45e254998ec/tensorflow/python/keras/applications/resnet.py" target="_blank" rel="noopener">Tensorflow 2 source code</a></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602230221/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%883.56.37_ag3moh.png" alt=""></p><p>2015年的冠軍ResNet提出了一個知名的Residual block，改善了模型在深層的時候無法很好訓練的問題，然後他們就很開心的把模型疊到152層了…</p><ul><li>提出<strong>Residual block</strong>: 提出的背後其實大有學問，主要是為了解決深層架構會導致<strong>退化(degradation)</strong>的問題<ul><li>簡單來說的話就是改善了在深層架構無法好好訓練的問題</li></ul></li><li>另一種層面來說，Residual block使得模型在深層的時候仍然可以有效的傳遞資訊，也減輕了gradient vanishing / exploding的現象</li><li>冷知識: 其實他們嘗試過1202層喔，可是沒有比較好…有錢就是任性</li></ul><h3 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a>SENet</h3><ul><li><a href="https://meetonfriday.com/posts/79fdff34/">[論文速速讀]Squeeze-and-Excitation Networks</a></li><li>Pytorch: <a href="https://github.com/moskomule/senet.pytorch" target="_blank" rel="noopener">moskomule/senet.pytorch</a></li><li>Tensorflow: <a href="https://github.com/taki0112/SENet-Tensorflow" target="_blank" rel="noopener">taki0112/SENet-Tensorflow</a></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603005064/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%883.10.41_ozyf3s.png" alt=""></p><p>2017年的冠軍SENet，也是最後一屆ILSVRC的冠軍得主，主要是提出SE block讓你可以到處插入別人家的模型，然後可以提升不少的效果。</p><p>(題外話，SE block也是我當初作業的題目，那時候老師就說要把某種架構加到object detection model上，後來去查才知道就是SE block…聽說那次作業完有不少人找老師說要退選…)</p><ul><li>提出<strong>SE block</strong>: 可以想成是<strong>channel-wise的注意力機制</strong>。對於分類某一個類別，不同channel萃取到的特徵可能不同，而這些特徵的重要程度也一定不同，所以透過實踐一個注意力機制來給予不同channel不同的權重，概念簡單卻很有效。</li></ul><hr><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>到這篇ILSVRC系列文總算結束了，但其實CNN還有一堆很重要的model，例如輕量化的mobilenet, shufflenet，還有EfficientNet等比較新的模型還沒讀過。之後會考慮一下要繼續往後續的模型來研讀，或是以目前的基礎開始朝向object detection了研究邁進。</p><p>不論如何，在工作之餘還是會繼續學習的，歡迎持續關注【論文速速讀】系列～</p>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux Kernel慢慢學]Different betweeen ioctl, unlocked_ioctl and compat_ioctl</title>
      <link href="/posts/736969d7/"/>
      <url>/posts/736969d7/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>會寫這一篇主要是自己在看一些網路教材的時候如果有些資源比較久你就會發現他用的還是<code>ioctl()</code>接口，但如果你實際上去碰新版的Kernel時卻發現了這個接口卻不見了，但卻多出了<code>unlocked_ioctl()</code>和<code>compat_ioctl()</code>這兩個很類似的function，然後就好奇他們到底差在哪裡、要怎麼用，學習完後就打了這篇文章做個記錄。</p><p>這篇文章就是對<code>ioctl()</code>、 <code>unlocked_ioctl()</code>和<code>compat_ioctl()</code>三個做一個簡單的介紹，讓大家更能夠知道應該如何使用和區分他們。</p><p>(以下linux source code如果沒有特別提及版本，接以<strong>v4.1.14</strong>為主)</p><h2 id="ioctl是什麼"><a href="#ioctl是什麼" class="headerlink" title="ioctl是什麼?"></a>ioctl是什麼?</h2><p><code>ioctl()</code>是撰寫driver一個很重要的接口，以字元裝置驅動(char device driver)來說，透過這個接口可以讓user來操作driver執行一些行為。</p><p>在撰寫driver code時，我們必須透過<code>register_chrdev()</code>來向kernel註冊我們的driver。為此我們需要<strong>提供該driver的file_operation相關函數實作</strong>來讓user可以透過這些接口來操控該driver。</p><p>在/include/linux/fs.h中可以看到file_operations的結構定義:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1588</span><br><span class="line">1589</span><br><span class="line">1590</span><br><span class="line">1591</span><br><span class="line">1592</span><br><span class="line">1593</span><br><span class="line">1594</span><br><span class="line">1595</span><br><span class="line">1596</span><br><span class="line">1597</span><br><span class="line">1598</span><br><span class="line">1599</span><br><span class="line">1600</span><br><span class="line">1601</span><br><span class="line">1602</span><br><span class="line">1603</span><br><span class="line">1604</span><br><span class="line">1605</span><br><span class="line">1606</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* include/linux/fs.h, line=1588 */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">file_operations</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">module</span> *<span class="title">owner</span>;</span></span><br><span class="line">    <span class="keyword">loff_t</span> (*llseek) (struct file *, <span class="keyword">loff_t</span>, <span class="keyword">int</span>);</span><br><span class="line">    <span class="keyword">ssize_t</span> (*<span class="built_in">read</span>) (struct file *, <span class="keyword">char</span> __user *, <span class="keyword">size_t</span>, <span class="keyword">loff_t</span> *);</span><br><span class="line">    <span class="keyword">ssize_t</span> (*<span class="built_in">write</span>) (struct file *, <span class="keyword">const</span> <span class="keyword">char</span> __user *, <span class="keyword">size_t</span>, <span class="keyword">loff_t</span> *);</span><br><span class="line">    <span class="keyword">ssize_t</span> (*read_iter) (struct kiocb *, struct iov_iter *);</span><br><span class="line">    <span class="keyword">ssize_t</span> (*write_iter) (struct kiocb *, struct iov_iter *);</span><br><span class="line">    <span class="keyword">int</span> (*iterate) (struct file *, struct dir_context *);</span><br><span class="line">    <span class="function"><span class="keyword">unsigned</span> <span class="title">int</span> <span class="params">(*poll)</span> <span class="params">(struct file *, struct poll_table_struct *)</span></span>;</span><br><span class="line">    <span class="keyword">long</span> (*unlocked_ioctl) (struct file *, <span class="keyword">unsigned</span> <span class="keyword">int</span>, <span class="keyword">unsigned</span> <span class="keyword">long</span>);</span><br><span class="line">    <span class="keyword">long</span> (*compat_ioctl) (struct file *, <span class="keyword">unsigned</span> <span class="keyword">int</span>, <span class="keyword">unsigned</span> <span class="keyword">long</span>);</span><br><span class="line">    <span class="keyword">int</span> (*mmap) (struct file *, struct vm_area_struct *);</span><br><span class="line">    <span class="keyword">int</span> (*mremap)(struct file *, struct vm_area_struct *);</span><br><span class="line">    <span class="keyword">int</span> (*<span class="built_in">open</span>) (struct inode *, struct file *);</span><br><span class="line">    <span class="keyword">int</span> (*<span class="built_in">flush</span>) (struct file *, <span class="keyword">fl_owner_t</span> id);</span><br><span class="line">    <span class="keyword">int</span> (*<span class="built_in">release</span>) (struct inode *, struct file *);</span><br><span class="line">    ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>常見需要實現的pointer to function包含<code>open()</code>, <code>read()</code>, <code>write()</code>, <code>close()</code>, <code>ioctl()</code>…等，細節的部份我先不提。</p><p>主要來看<code>ioctl()</code>這件事，上面是4.14版本的code，看了一下的code發現並沒有我說的<code>ioctl()</code>，反而出現了<code>unlocked_ioctl()</code>和<code>compat_ioctl()</code>，到底發生了什麼事情呢?</p><h2 id="Big-Kernel-Lock下的舊產物-ioctl"><a href="#Big-Kernel-Lock下的舊產物-ioctl" class="headerlink" title="Big Kernel Lock下的舊產物: ioctl"></a>Big Kernel Lock下的舊產物: ioctl</h2><p><code>ioctl()</code>在2.6版本以前是還有這個function的，例如你可以在<a href="https://elixir.bootlin.com/linux/v2.5.75/source/include/linux/fs.h#L718" target="_blank" rel="noopener">2.5.75的fs.h</a>中看到。但在2.6以後就被替換成<code>unlocked_ioctl()</code>了，為什麼呢?</p><p>根據<a href="https://lwn.net/Articles/119652/" target="_blank" rel="noopener">The new way of ioctl()</a>這篇文章，</p><blockquote><p>ioctl() is one of the remaining parts of the kernel which runs under the Big Kernel Lock (BKL). In the past, the usage of the BKL has made it possible for long-running ioctl() methods to create long latencies for unrelated processes. Recent changes, which have made BKL-covered code preemptible, have mitigated that problem somewhat. Even so, the desire to eventually get rid of the BKL altogether suggests that ioctl() should move out from under its protection.</p></blockquote><p>由於<code>ioctl()</code>是早期仍然在<strong>BKL(Big Kernel Lock)</strong>機制下執行的產物(BKL是甚麼可以再去google，是kernel發展史中蠻有趣的一段歷史)，BKL的機制會使得<code>ioctl()</code>的運作時間較長，可能會造成其他process的延遲。</p><p>隨著Kernel後續的改版，BKL不再是一個需要的機制了，大家開始把被BKL保護的function移除BKL。但一下子就把BKL完全移除還是會有顧慮，大家應該更加仔細的去審視是否有需要自己加入新的lock來保護自己的程序。所以此時需要一個過渡的機制: <code>unlocked_ioctl()</code>(by Michael s. Tsirkin)，該function定義如下:</p><p><code>long (*unlocked_ioctl) (struct file *filp, unsigned int cmd,                             unsigned long arg);</code></p><p>如果某個驅動的fops提供了<code>unlocked_ioctl()</code>，那麼他將優先調用<code>unlocked_ioctl()</code>而不是有BKL版本的<code>ioctl()</code>。</p><ul><li><code>unlocked_ioctl()</code>不再提供inode參數，但你仍可以透過filp-&gt;f_dentry-&gt;d_inode來取得</li><li><code>unlocked_ioctl()</code>不再使用BKL，工程師需要根據自己的需求來決定要不要加入lock的機制</li></ul><p>所以我們知道了原始<code>unlocked_ioctl()</code>的誕生是為了應付一段過渡期，讓大家能夠在這段時間趕快去修改自己的<code>ioctl()</code>，例如你在<a href="https://elixir.bootlin.com/linux/v2.6.11/source/include/linux/fs.h#L931" target="_blank" rel="noopener">2.6.11版本的fs.h</a>就可以看到這兩個接口是同時存在的。</p><ul><li>而在2.6.36後就正式將<code>ioctl()</code>移除了，大家都必須透過<code>unlocked_ioctl()</code>來提供ioctl的接口</li></ul><p>不過這就只是一段歷史，對於現在的driver開發者也沒什麼影響，就是把自己寫好的ioctl接到<code>unlocked_ioctl()</code>上面去而已。</p><h2 id="為了相容性而出現的compat-ioctl"><a href="#為了相容性而出現的compat-ioctl" class="headerlink" title="為了相容性而出現的compat_ioctl"></a>為了相容性而出現的compat_ioctl</h2><p>在Michael s. Tsirkin發布的patch提供了<code>unlocked_ioctl</code>的同時也提供了另外一個接口: <code>compat_ioctl()</code>。</p><blockquote><p>If this method exists, it will be called (without the BKL) whenever a 32-bit process calls ioctl() on a 64-bit system. It should then do whatever is required to convert the argument to native data types and carry out the request</p></blockquote><p>他出現的目的很簡單，就是相容性: 為了讓32-bit的process可以在64-bit上的system來執行<code>ioctl()</code>(沒有BKL版本)。</p><p><code>compat_ioctl()</code>實際上要怎麼寫呢? 隨便找一個example code來看一下:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> CONFIG_COMPAT</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">long</span> <span class="title">debussy_compat_ioctl</span> <span class="params">(struct file *filp, <span class="keyword">unsigned</span> <span class="keyword">int</span> cmd, <span class="keyword">unsigned</span> <span class="keyword">long</span> arg)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> debussy_ioctl(filp, cmd, (<span class="keyword">unsigned</span> <span class="keyword">long</span>)compat_ptr(arg));</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">file_operations</span> <span class="title">debussy_fops</span> = &#123;</span></span><br><span class="line">    .owner        = THIS_MODULE,</span><br><span class="line">    .unlocked_ioctl = debussy_ioctl,</span><br><span class="line">#ifdef CONFIG_COMPAT</span><br><span class="line">    .compat_ioctl = debussy_compat_ioctl,</span><br><span class="line">#endif</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* arch/arm64/include/asm/compat.h, line=227 */</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> __user *<span class="title">compat_ptr</span><span class="params">(<span class="keyword">compat_uptr_t</span> uptr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">void</span> __user *)(<span class="keyword">unsigned</span> <span class="keyword">long</span>)uptr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>會發現他實際上就是呼叫了原本的ioctl接口，只是將最後一個arg參數透過<code>compat_ptr()</code>轉換成64-bit了</p><ul><li>原本32-bit下的指標是4 byte，64-bit下是8 byte，所以會需要轉換</li><li>關於轉換的細節可以參考<a href="https://b8807053.pixnet.net/blog/post/3610561-ioctl%2Cunlocked_ioctl%E5%92%8Ccompat_ioctl" target="_blank" rel="noopener">ioctl,unlocked_ioctl和compat_ioctl</a></li></ul><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>了解了這些會有什麼幫助嗎? 其實好像也還好xD</p><p>主要就是現在在寫fops時需要提供兩個接口:</p><ul><li><code>unlock_ioctl()</code></li><li><code>compat_ioctl()</code></li></ul><p>並且這不太會影響你原本的driver ioctl寫法(除非你會使用到inode)，然後compat_ioctl的寫法也很制式化，如同上面的範例，將最後參數的指標轉換一下，呼叫你driver ioctl就好了。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://lwn.net/Articles/119652/" target="_blank" rel="noopener">The new way of ioctl()</a></li><li><a href="https://b8807053.pixnet.net/blog/post/3610561-ioctl%2Cunlocked_ioctl%E5%92%8Ccompat_ioctl" target="_blank" rel="noopener">ioctl,unlocked_ioctl和compat_ioctl</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【Linux Kernel慢慢學】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> linux kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Squeeze-and-Excitation Networks</title>
      <link href="/posts/79fdff34/"/>
      <url>/posts/79fdff34/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>paper: <a href="https://arxiv.org/pdf/1709.01507.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1709.01507.pdf</a></p><p>SENet，也就是ILSVRC 2017年的冠軍，是由自駕車公司Momenta所提出，撰寫這篇的同時也為ILSVRC的中文論文導讀系列畫下了一個句點(因為2017是ILSVRC的最後一屆)。</p><p>SENet的精神是提出了一個<strong>隨插隨用的SE block，讓你可以很沒節操的到處插入別人家的模塊中</strong>。使得model能夠針對每一層的channel產生了<strong>類似注意力機制的學習機制</strong>，廢話不多說，開始來看看SENet做了什麼事情吧！</p><a id="more"></a><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels</p></blockquote><p>有別於以往的研究都在探討怎麼在spatial domain上面透過不同的方法增強CNN的feature encoding能力，SENet則是朝向channel的角度來思考，透過提出SE block,一種基於<strong>channel-wise attention</strong>的模塊，使得模型可以針對不同的channel來給予不同的權重。</p><ul><li>spatial domain指的是空間域，也就是圖片上的pixel和pixel之間的關係</li><li>咦~最後一句話聽起來是不是有點NLP的注意力機制(attention mechanism)的意味在呢? 在NLP的注意力機制盛行後(~2016左右)，學者們也開始思考注意力機制在CV的應用摟</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>Recent research has shown that the representations produced by CNNs can be strengthened by integrating learning mechanisms into the network that help capture spatial correlations between features</p></blockquote><p>已經有許多研究表示，可以透過不同的學習機制來增強CNN的捕捉空間相關性的feature能力，例如Inception就是一種mult-scale的學習機制，也有許多人做了不同的嘗試，例如使用了空間域上的注意力機制等。</p><p>在這篇論文中提出了不同的觀點 - <strong>the relationship between channels</strong>，透過對channel上的依賴關係來進行建模，達到提升CNN的效果，為此提出了<strong>Squeeze-and-Excitation (SE) block。</strong></p><p>這種機制可以學習channel上的全局訊息，來達到<strong>channel的recalibration(重新校准)</strong>，也就是強調比較重要的channel feature，並抑制比較不重要的channel feature的權重。</p><ul><li>顧名思義，壓縮(Squeeze)後取得channel上不同程度的重要性然後再激發出來(Excitation)</li><li>最後一句話是什麼意思呢？例如不同的channel學習了不同動物的部位特徵，有腳、手、頭、嘴…之類的，那比較重要的部位就給予比較高的權重，比較不重要的(例如某些類別的腳都長得相同沒有分辨性)就降低他的權重</li><li>要如何讓模型知道哪些channel是重要的呢？這是學出來的</li></ul><p>SE block的架構如下:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603005064/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%883.10.41_ozyf3s.png" alt=""></p><p>假設前一層的輸出為$X$，透過一個Convolution運算$F_{tr}$變成了$U$。SE block的目的是希望透過channel的重新校正(Recalibration)來幫助模型提升特徵萃取的能力，總共分成了兩個步驟：</p><p><strong>Squeeze</strong>: 透過一個<script type="math/tex">F_{sq}</script>操作將這個<script type="math/tex">U</script>在channel上的global information萃取出來(squeeze)。也就是說$U$原本的shape是$(W, H, C)$，透過$F_{sq}$每一個channel取一個代表的值出來，如此就得到了一個$(1, 1, C)$的tensor，論文中稱之channel descriptor</p><ul><li>再白話一點解釋的話，每一張$H\times W$的圖片都取一個代表值(channel descriptor)，總共有$C$張圖片。這個channel descriptor代表了這一個$U$的全局訊息</li><li>怎麼取呢？ 有看過前面論文速速讀系列的你可能就想到了一個很常用的操作了: <strong>GAP(Global Average Pooling)</strong>，如果沒想到的快去看<a href="https://meetonfriday.com/posts/a151bfa2/">[論文速速讀]Network In Network</a>這篇論文</li></ul><p><strong>Excitation</strong>: 接下來透過$F_{sq}$操作，來學習這些channel descriptor的重要程度，也就反映了原本$U$上面每個channel的重要程度(大)。</p><ul><li>$F_{sq}$到底是什麼操作呢？簡單來說就是接<strong>兩個FC layer(一個降維一個升維)</strong></li></ul><p>最後將$U$乘上大小不同的權重就得到了output $\widetilde{X}$</p><p>SE block可以很無節操的隨便插入別人家的模型中，就如同Residual block一樣，<strong>block插入在淺層和深層的時候模型會自動學習並調整他的channel designators來適應不同的需求。</strong></p><ul><li>在淺層中，他會傾向去增強一些shared low-level representation</li><li>在比較後面的層中，則是會去強調一些具有分辨性的representation</li></ul><blockquote><p>As a consequence, the benefits of the feature recalibration performed by SE blocks can be accumulated through the network</p></blockquote><p>結論就是，SE block想插哪裏就插哪裡，淺層深層都可以！</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>有部分內容在介紹之前的一些model，如VGG、GoogLeNet、Resnet…等，所以跳過，有興趣的可以去看論文速速讀的其他篇。</p><h3 id="Attention-and-gating-mechanisms"><a href="#Attention-and-gating-mechanisms" class="headerlink" title="Attention and gating mechanisms"></a>Attention and gating mechanisms</h3><p>Attention機制來了！ 從2016年開始在NLP領域廣泛使用的注意力機制也有越來越多的CV應用。一個注意力機制通常是採用門控機制(sigmoid, softmax)來達成。</p><p>關於注意力機制在NLP的應用可以參考下列的論文速速讀系列：</p><ul><li><a href="https://meetonfriday.com/posts/5839a8bf/">[論文速速讀]Attention Is All You Need</a></li><li><a href="https://meetonfriday.com/posts/4f49bf9b/">[論文速速讀]NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a></li><li><a href="https://meetonfriday.com/posts/8cad39eb/">[論文速速讀]Hierarchical Attention Networks for Document Classification</a></li></ul><h2 id="Squeeze-and-Excitation-Blocks"><a href="#Squeeze-and-Excitation-Blocks" class="headerlink" title="Squeeze-and-Excitation Blocks"></a>Squeeze-and-Excitation Blocks</h2><p>這邊其實就是Introduction的細部解釋，所以會介紹快一點。</p><h3 id="Squeeze-Global-Information-Embedding"><a href="#Squeeze-Global-Information-Embedding" class="headerlink" title="Squeeze: Global Information Embedding"></a>Squeeze: Global Information Embedding</h3><p>對於convolution後得到的<script type="math/tex">\mathbf{U}=\left[\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{C}\right]</script>，將全局空間壓縮成一個值來代表該channel，有很多做法，這裡選擇了最簡單的Global Average Pooling (GAP)來達成</p><script type="math/tex; mode=display">z_{c}=\mathbf{F}_{s q}\left(\mathbf{u}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} u_{c}(i, j)</script><p>為何是GAP?</p><ul><li>其實在進行卷積操作的時候，我們得到的輸出實質上和前一層所有channel都交錯地糾纏在一起</li><li>但這個關係有點複雜，所以我們想透過GAP來進行簡化: 也就是<strong>GAP得出來的每個點都單純對應該channel而已，和其他channel並沒有任何關係</strong></li><li>而這個特性也能幫助我們來訓練模型對訊息特徵的敏感度: 單純的將重要的channel權重加大、不重要的channel權重降低</li></ul><h3 id="Excitation-Adaptive-Recalibration"><a href="#Excitation-Adaptive-Recalibration" class="headerlink" title="Excitation: Adaptive Recalibration"></a>Excitation: Adaptive Recalibration</h3><p>這個部分就是兩層FC layer，先升維再降維(為了降低模型複雜度)。</p><script type="math/tex; mode=display">\mathbf{s}=\mathbf{F}_{e x}(\mathbf{z}, \mathbf{W})=\sigma(g(\mathbf{z}, \mathbf{W}))=\sigma\left(\mathbf{W}_{2} \delta\left(\mathbf{W}_{1} \mathbf{z}\right)\right)</script><p>對於GAP後的output $Z \in R^{1 \times 1 \times C}$</p><ul><li>透過$W_{1} \in R^{1 \times 1 \times (\frac{C}{R} \times C)}$來降維，會得到$R^{1 \times 1 \times \frac{C}{R}}$，$\delta$是Relu function</li><li>$W_{2} \in R^{1 \times 1 \times (C \times \frac{C}{R})}$是用來升維的FC layers，$\sigma$是sigmoid function，用來將值調整在0~1的門控機制</li></ul><p>最後將他們乘起來，<script type="math/tex">\mathbf{F}_{\text {scale}}\left(\mathbf{u}_{c}, s_{c}\right)</script> 是 <strong>channel-wise multiplication</strong></p><script type="math/tex; mode=display">\tilde{\mathbf{x}}_{c}=\mathbf{F}_{\text {scale}}\left(\mathbf{u}_{c}, s_{c}\right)=s_{c} \mathbf{u}_{c}</script><h3 id="Instantiations"><a href="#Instantiations" class="headerlink" title="Instantiations"></a>Instantiations</h3><p>這裡告訴你怎麼無節操的去插入別人家的網路ლ(╹◡╹ლ)</p><p>插入Inception model:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603011357/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%884.55.26_vrwmcr.png" alt=""></p><p>插入ResNet model:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603011373/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%884.56.03_kjtehn.png" alt=""></p><p>簡單輕鬆又好用！</p><h2 id="Model-and-Computational-Complexity"><a href="#Model-and-Computational-Complexity" class="headerlink" title="Model and Computational Complexity"></a>Model and Computational Complexity</h2><p>下面表格列出了ResNet、SE-ResNet-50和SE-ResNeXt-50的架構</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603011574/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%884.59.19_jdas3k.png" alt=""></p><ul><li>fc中括號的兩個數字分別代表SE block的兩個FC Layer output</li></ul><p>以及他的error rate和FLOPS(Floating Point Operations Per Second)<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603011775/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%885.02.44_bq7i96.png" alt=""></p><ul><li>可以發現ResNet-50搭配SENet的效果可以堪比ResNet-101，但計算量只多出了一點點(3.86-&gt;3.87)</li><li>其他的部分請自己看圖說故事</li></ul><p>而對於模型所增加的參數量都來自SE block的兩個FC layer</p><script type="math/tex; mode=display">\frac{2}{r} \sum_{s=1}^{S} N_{s} \cdot C_{s}^{2}</script><ul><li>$r$: reduction ratio</li><li>$S$:  number of stages (a stage refers to the collection of blocks operating on feature maps of a common spatial dimension)</li><li>$C_{s}$:  dimension of the output channels</li><li>$N_{s}$: number of repeated blocks for stage $s$</li></ul><p>以SE-ResNet-50為例，參數量大約增加了10%左右，並且SE block在<strong>越深層的部分會使用越多的參數</strong>(因為越深層dimension越大)。對此他們嘗試在最後一個stage把SE block都移除，降低了4%的參數量，但相對的accuracy只降低了1%。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a>Image Classification</h3><h4 id="Network-depth"><a href="#Network-depth" class="headerlink" title="Network depth"></a>Network depth</h4><p>在不同深度的ResNet上加入SENet，證實不同的models都可以有效提升效果</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603013527/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%885.31.47_jyybkf.png" alt=""></p><h4 id="Integration-with-modern-architectures"><a href="#Integration-with-modern-architectures" class="headerlink" title="Integration with modern architectures"></a>Integration with modern architectures</h4><p>這次配上了比較先進的SOTA models</p><ul><li>Inception-ResNet-v2 </li><li>ResNeXt</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603013529/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%885.31.54_gp4iz3.png" alt=""></p><h4 id="Mobile-setting"><a href="#Mobile-setting" class="headerlink" title="Mobile setting"></a>Mobile setting</h4><p>針對行動端最佳化的模型設計來測試</p><ul><li>MobileNet</li><li>ShuffleNet</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603013890/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%885.37.58_bhiv2t.png" alt=""></p><h4 id="Additional-datasets"><a href="#Additional-datasets" class="headerlink" title="Additional datasets"></a>Additional datasets</h4><p>測試ImageNet以外的dataset，跳過，有興趣的再自己去看。</p><h3 id="Scene-Classification"><a href="#Scene-Classification" class="headerlink" title="Scene Classification"></a>Scene Classification</h3><p>在場景分類任務的dataset上訓練，跳過。</p><h3 id="ILSVRC-2017-Classification-Competition"><a href="#ILSVRC-2017-Classification-Competition" class="headerlink" title="ILSVRC 2017 Classification Competition"></a>ILSVRC 2017 Classification Competition</h3><p>ILSVRC的數據，和以前的冠軍相同的，都採用了ensemble的策略</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603014038/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%885.40.19_dp6yeo.png" alt=""></p><p><strong>【下方高能預警】</strong> 到這邊差不多惹，上面已經很清楚的介紹了SENet，下面是一堆針對設計上的實驗跟分析，如果你只是想知道SENet到底在幹嘛的那我建議看到這裡就夠惹，如果你是很想了解細節內容的就跟隨我繼續往下吧！</p><h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><h3 id="Reduction-ratio"><a href="#Reduction-ratio" class="headerlink" title="Reduction ratio"></a>Reduction ratio</h3><p>在SE block的FC layer降維(reduction ration)到底要降到多少？</p><p>這裡基於SE-ResNet-50做了不同r的實驗，發現結果並不會隨著提升r而上升，所以最後採用<strong>r=16</strong></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603014380/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%885.45.10_yjbcgm.png" alt=""></p><h3 id="Squeeze-Operator"><a href="#Squeeze-Operator" class="headerlink" title="Squeeze Operator"></a>Squeeze Operator</h3><p>比較了Global Average Pooling 和Global Maximum Pooling，發現GAP比較好，所以採用GAP。</p><h3 id="Excitation-Operator"><a href="#Excitation-Operator" class="headerlink" title="Excitation Operator"></a>Excitation Operator</h3><p>對於第二個FC layer的activation function，比較了ReLU、tanh和sigmoid，發現sigmoid最好，ReLU特差。因此他們建議選擇一個好的Excitation Operator對於SE block是很重要的。</p><h3 id="Different-stages"><a href="#Different-stages" class="headerlink" title="Different stages"></a>Different stages</h3><p>單純將SE block部分加入model某個stage進行分析的實驗，跳過。</p><p><strong>有錢人當然是全都加加爆。</strong></p><h3 id="Integration-strategy"><a href="#Integration-strategy" class="headerlink" title="Integration strategy"></a>Integration strategy</h3><p>身為一個要插入別人家模型的模塊，當然要研究一下插在哪裡比較好！</p><p>所以除了原本的架構，還多實驗了3種不同的位置:</p><ol><li>SE-PRE block, in which the SE block is moved before the residual unit</li><li>SE-POST block, in which the SE unit is moved after the summation with the identity branch (after ReLU)</li><li>SE-Identity block, in which the SE unit is placed on the identity connection in parallel to the residual unit</li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603015665/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%886.07.29_qyldu3.png" alt=""></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603015961/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%886.12.31_ezkps3.png" alt=""></p><p>此外，上面的架構都是將SE block放在Residual block外面。他們也實驗了將這個設計直接做在Residual block內，不過效果並沒有比較好，具體可以去看原文。</p><h2 id="Role-of-SE-Blocks"><a href="#Role-of-SE-Blocks" class="headerlink" title="Role of SE Blocks"></a>Role of SE Blocks</h2><p>(寫到快死掉了…怎麼做了這麼多分析…)</p><p>儘管SE block被證明可以提升效果，但他們想更進一步去看Squeeze和Excitation到底是怎麼影響模型的。所以做了分析的實驗。</p><h3 id="Effect-of-Squeeze"><a href="#Effect-of-Squeeze" class="headerlink" title="Effect of Squeeze"></a>Effect of Squeeze</h3><p>“使用GAP將channel獨立出來到底有沒有用呢？”</p><p>為了驗證這個想法，他們將GAP換成了1x1 Conv，如此一來output之間還是channel dependent的然後做實驗。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603016302/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%886.18.09_m2t5h3.png" alt=""></p><p>結果就是GAP還是比較好，並且GAP不用額外的計算參數。</p><h3 id="Role-of-Excitation"><a href="#Role-of-Excitation" class="headerlink" title="Role of Excitation"></a>Role of Excitation</h3><p>觀察Excitation在不同類別和不同layer時的分佈。</p><p>取了四個類別: goldfish, pug, plane and cliff，每一類採樣了50個樣本，並觀察他們在最後一個stage中SE block uniformly sampled channels的average activations(也做了1000類的average activations作為比較基準)。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_800/v1603017087/blog_posts/%E6%88%AA%E5%9C%96_2020-10-18_%E4%B8%8B%E5%8D%886.31.14_lwdnlm.png" alt=""></p><p>圖片的名字是這樣命名的: <code>SE_&lt;stageID&gt;_&lt;blockID&gt;</code></p><p>在這個實驗中發現了三件個現象：</p><ol><li>不同類別的分佈在較淺層都差不多，這代表模型在淺層學到的都是比較共同的初階特徵(Ex:點線面)，<strong>這些特徵每個類別都會用到，所以此時SE block的權重分佈會比較一致</strong><ul><li>例如SE_2_3, SE_3_4</li></ul></li><li>在比較深層的時候，不同類別的channel值開始出現差異性。因為這時候<strong>不同類別的高階特徵可能就具有較大的差異性</strong><ul><li>例如SE_4_6, SE_5_1</li></ul></li></ol><p>上面兩個觀察驗證了我們在Introduction講的: SE block在淺層和深層學習的任務不同，他會自己根據block所在的位置去學習對應的目標。</p><ol><li>最後，在最後一個stage的後半block觀察到了一些現象<ul><li>SE_5_2有大部分的activation都趨近於1，少部分的channel值是0</li><li>SE_5_3中每個類別的activations分佈都很類似</li></ul></li></ol><p>對於activation都是1的部分，該block就會退化成standard residual block，因為都是1的話就等於SE block沒有作用。</p><p>而對於activations分佈類似的SE block也不會造成模型太大的影響，因為SE block並沒有達到根據不同類別給予不同的權重的效果。</p><p>這些觀察代表<strong>在深層的時候SE block可能已經趨近飽和，做不太到什麼事情但又造成了較大的計算複雜度</strong>(因為深層的dimension都比較大，所以SE block的參數量也會變大)。所以他們就參考了這項實驗，把最後stage的SE block移除，結果如同Model and Computational Complexity章節提到的，準確度只降了1%。</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>終於寫完了嗚嗚嗚…這篇的Ablation Study世界長欸…寫到懷疑人生。</p><p>快速做個總結，SENet並不是一個神經網路模型，而是提出一個新的SE block用來加入其他現有模型，以達到更好效果的研究。</p><p>通過SE block，我們可以讓模型有著類似注意力機制的效果: 對於分類某個類別來說，哪些channel的特徵是比較有用的呢?</p><p>而為此所耗費的計算量也是非常少的，卻大大提升了現有模型的結果。</p><p>隨插隨用棒棒噠！</p>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux Kernel慢慢學]Linux modules載入及載入順序</title>
      <link href="/posts/c4426b79/"/>
      <url>/posts/c4426b79/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我們都知道(或是我知道)，Linux modules的入口點在<code>module_init()</code>，透過<code>module_init()</code>把我們的程式掛載到kernel去。</p><p>不過有時候會看到有些module用的是<code>late_initcall()</code>或是<code>arch_initcall()</code> 這類macro，他們跟一般的<code>module_init()</code>又差在哪裡?</p><p>本文會透過trace linux source code介紹以下幾個概念</p><ol><li><code>module_init()</code>這個macro背後做了什麼事情</li><li>不同的init macro背後的差異是什麼</li><li>怎麼透過不同的init macro來控制modules的載入先後順序，以及為何要控制載入的順序</li></ol><p>(本文的linux code 版本為<a href="https://elixir.bootlin.com/linux/v4.14.201/source/include/linux/module.h" target="_blank" rel="noopener">linux4.14.201</a>)</p><a id="more"></a><h2 id="module-init-code-trace"><a href="#module-init-code-trace" class="headerlink" title="module_init code trace"></a>module_init code trace</h2><p>首先我們來看一下每個module的entry，<code>module_init()</code>到底在幹嘛，如果不想跟我一起追code的<del>可以追我喔</del>…不是，我下面有懶人包總結，可以直接跳下去看。</p><p><code>module_init()</code>的macro被定義在linux/include/module.h中</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* linux/include/module.h, line=85 */</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * module_init() - driver initialization entry point</span></span><br><span class="line"><span class="comment"> * @x: function to be run at kernel boot time or module insertion</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * module_init() will either be called during do_initcalls() (if</span></span><br><span class="line"><span class="comment"> * builtin) or at module insertion time (if a module).  There can only</span></span><br><span class="line"><span class="comment"> * be one per module.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> module_init(x)__initcall(x);</span></span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* linux/include/init.h, line=85 */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __initcall(fn) device_initcall(fn)</span></span><br></pre></td></tr></table></figure><p>發現實際上是調用了<code>device_initcall(fn)</code>，所以來追一下這個部分會發現有許多相關的func都同樣地呼叫了<code>__define_initcall(fn, id)</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* linux/include/init.h, line=176 */</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Early initcalls run before initializing SMP.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Only for built-in code, not modules.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> early_initcall(fn)__define_initcall(fn, early)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * A "pure" initcall has no dependencies on anything else, and purely</span></span><br><span class="line"><span class="comment"> * initializes variables that couldn't be statically initialized.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This only exists for built-in code, not for modules.</span></span><br><span class="line"><span class="comment"> * Keep main.c:initcall_level_names[] in sync.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> pure_initcall(fn)__define_initcall(fn, 0)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> core_initcall(fn)__define_initcall(fn, 1)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> core_initcall_sync(fn)__define_initcall(fn, 1s)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> postcore_initcall(fn)__define_initcall(fn, 2)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> postcore_initcall_sync(fn)__define_initcall(fn, 2s)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> arch_initcall(fn)__define_initcall(fn, 3)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> arch_initcall_sync(fn)__define_initcall(fn, 3s)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> subsys_initcall(fn)__define_initcall(fn, 4)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> subsys_initcall_sync(fn)__define_initcall(fn, 4s)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> fs_initcall(fn)__define_initcall(fn, 5)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> fs_initcall_sync(fn)__define_initcall(fn, 5s)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> rootfs_initcall(fn)__define_initcall(fn, rootfs)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> device_initcall(fn)__define_initcall(fn, 6)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> device_initcall_sync(fn)__define_initcall(fn, 6s)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> late_initcall(fn)__define_initcall(fn, 7)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> late_initcall_sync(fn)__define_initcall(fn, 7s)</span></span><br></pre></td></tr></table></figure><h3 id="懶人包總結-module-init到底做了什麼"><a href="#懶人包總結-module-init到底做了什麼" class="headerlink" title="懶人包總結: module_init到底做了什麼"></a>懶人包總結: module_init到底做了什麼</h3><p>好，先到這裡停住，先講結論，講完有興趣的可以繼續往下追。</p><p>簡單說<code>module_init()</code>透過實際上呼叫<code>__device_initcall(fn, id)</code>來<strong>將對應的init function放到linker script的某個section中，而kernel在載入的時候會根據數字的順序來依序init所有的init functions完成各個modules的初始化。</strong></p><p>而我們可以發現我們常用的<code>module_init()</code>實際上優先權是6(<code>device_initcall(fn)</code>的id是6)，所以如果有人用了比<code>module_init()</code>還要前面的，那他就會比較早被載入，反之依然。</p><ul><li>對於一些載入順序很重要的modules來說，透過不同的xxxx_initcall來控制載入順序很重要</li></ul><h3 id="繼續追-define-initcall-fn-id"><a href="#繼續追-define-initcall-fn-id" class="headerlink" title="繼續追__define_initcall(fn, id)"></a>繼續追__define_initcall(fn, id)</h3><p>上面各個不同的initcall都都呼叫了<code>__define_initcall()</code>，所以來看一下</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* include/linux/init.h. line=172 */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __define_initcall(fn, id) \</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">initcall_t</span> __initcall_#<span class="meta">#fn##id __used \</span></span><br><span class="line">    __attribute__((__section__(<span class="string">".initcall"</span> #id <span class="string">".init"</span>))) = fn;</span><br></pre></td></tr></table></figure><p>這個一坨真的很可怕，舉個例子來個別解釋，如果fn=hello, id=6的話:</p><ul><li><code>__initcall_##fn##id</code>就會變成<code>__initcall_hello6</code></li><li><code>__attribute__（__section__）</code>是編譯器的編譯屬性，就是說將fn放到linker script的”.initcall6.init” section中</li></ul><p>比方說，觀察arch/powerpc/kernel/vmlinux.lds.S的initcall.init section:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* arch/powerpc/kernel/vmlinux.lds.S, line=215 */</span></span><br><span class="line">    .initcall.init : AT(ADDR(.initcall.init) - LOAD_OFFSET) &#123;</span><br><span class="line">        INIT_CALLS</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>INIT_CALLS被定義在include/asm-generic/vmlinux.lds.h</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* include/asm-generic/vmlinux.lds.h, line=749 */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INIT_CALLS_LEVEL(level)\</span></span><br><span class="line">        VMLINUX_SYMBOL(__initcall##level##_start) = .;\</span><br><span class="line">        KEEP(*(.initcall##level##.init))\</span><br><span class="line">        KEEP(*(.initcall##level##s.init))\</span><br><span class="line">        </span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INIT_CALLS\</span></span><br><span class="line">        VMLINUX_SYMBOL(__initcall_start) = .;\</span><br><span class="line">        KEEP(*(.initcallearly.init))\</span><br><span class="line">        INIT_CALLS_LEVEL(<span class="number">0</span>)\</span><br><span class="line">        INIT_CALLS_LEVEL(<span class="number">1</span>)\</span><br><span class="line">        INIT_CALLS_LEVEL(<span class="number">2</span>)\</span><br><span class="line">        INIT_CALLS_LEVEL(<span class="number">3</span>)\</span><br><span class="line">        INIT_CALLS_LEVEL(<span class="number">4</span>)\</span><br><span class="line">        INIT_CALLS_LEVEL(<span class="number">5</span>)\</span><br><span class="line">        INIT_CALLS_LEVEL(rootfs)\</span><br><span class="line">        INIT_CALLS_LEVEL(<span class="number">6</span>)\</span><br><span class="line">        INIT_CALLS_LEVEL(<span class="number">7</span>)\</span><br><span class="line">        VMLINUX_SYMBOL(__initcall_end) = .;</span><br></pre></td></tr></table></figure><p>發現他做的事情就是將*(.initcall0.init), *(.initcall0s.init)…依照順序去放在.initcall.init section中。</p><p>實際上，arch/powerpc/kernel/vmlinux.lds.S的initcall.init section展開後是長下面這樣:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* arch/powerpc/kernel/vmlinux.lds.S, line=215 */</span></span><br><span class="line">    .initcall.init : AT(ADDR(.initcall.init) - LOAD_OFFSET) &#123;</span><br><span class="line">        __initcall_start = .;</span><br><span class="line">        __initcall0_start = .;</span><br><span class="line">        *(.initcall0.init);</span><br><span class="line">        *(.initcall0s.init);</span><br><span class="line">        __initcall1_start = .;</span><br><span class="line">        *(.initcall1.init);</span><br><span class="line">        *(.initcall1s.init);</span><br><span class="line">        __initcall2_start = .;</span><br><span class="line">        *(.initcall2.init);</span><br><span class="line">        *(.initcall2s.init);</span><br><span class="line">        ...</span><br><span class="line">        __initcall7_start = .;</span><br><span class="line">        *(.initcall7.init);</span><br><span class="line">        *(.initcall7s.init);</span><br><span class="line">        __initcall_end = .;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>好，到這裡回顧一下，<code>__define_initcall(fn, id)</code>會根據你的id將你的init function放到linker script中對應的section內。</p><p>到這裡<code>module_init()</code>終於全部追完了，比追20集的韓劇還累。</p><p>如果有跟我一起追到這邊的朋友，可以再回去回顧一下懶人包，是不是對<code>module_init()</code>做的事情更加清楚了呢?</p><p>到這裡我們已經可以回答一開始的兩個問題了</p><ol><li><code>module_init()</code>做的事情是將init func放到對應優先權的init section中，等kernel載入時依序呼叫</li><li>不同<code>xxxx_initcall()</code>給了不同的優先順序，kernel載入時會根據id來依序載入，所以可以達到控制module載入先後順序的效果</li></ol><h2 id="為何要控制module載入的順序"><a href="#為何要控制module載入的順序" class="headerlink" title="為何要控制module載入的順序"></a>為何要控制module載入的順序</h2><p>引用一些網站上遇到的內容來說明模組先後順序會有什麼影響:</p><blockquote><p>举个例子，在2.6.24的内核中：gianfar_device使用的是arch_initcall，而gianfar_driver使用的是module_init，因为arch_initcall的优先级大于module_init，所以gianfar设备驱动的device先于driver在总线上添加。<br><br\><br>这是我在调背光的时候出现的问题，因为键盘驱动是会在一个遥控手柄之前加载，导致驱动出现冲突；把两者先后顺序换一下就可以了；</p></blockquote><h2 id="補充-Kernel載入init-function的過程"><a href="#補充-Kernel載入init-function的過程" class="headerlink" title="補充: Kernel載入init function的過程"></a>補充: Kernel載入init function的過程</h2><p>在kernel載入的過程中，會依序呼叫<code>start_kernel()</code>-&gt;<code>rest_init()</code>-&gt;<code>kernel_init()</code>-&gt;<code>kernel_init_freeable()</code>-&gt;<code>do_basic_setup()</code>-&gt;<code>do_initcalls()</code>來依序載入init function</p><p>不過這個要trace下去也很痛苦，我就把東西放在這裡了，有興趣的自己去慢慢看: <a href="https://blog.csdn.net/richard_liujh/article/details/46758073" target="_blank" rel="noopener">Linux内核很吊之 module_init解析 （下）</a></p><h2 id="platform-device有多個module時加載的順序"><a href="#platform-device有多個module時加載的順序" class="headerlink" title="platform_device有多個module時加載的順序"></a>platform_device有多個module時加載的順序</h2><p>以下引用<a href="https://www.cnblogs.com/linhaostudy/p/9112264.html" target="_blank" rel="noopener">linux 设备驱动加载的先后顺序</a>的內容:</p><p>【問題】背光驱动初始化先于LCD驱动初始化，导致LCD驱动初始化时出现闪屏的现象。</p><p>考慮下面的platform_device array</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* platform devices */</span></span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">platform_device</span> *<span class="title">athena_evt_platform_devices</span>[] __<span class="title">initdata</span> = &#123;</span></span><br><span class="line"><span class="comment">//&amp;xxx_led_device,</span></span><br><span class="line">&amp;xxx_rtc_device,</span><br><span class="line">&amp;xxx_uart0_device,</span><br><span class="line">&amp;xxx_uart1_device,</span><br><span class="line">&amp;xxx_uart2_device,</span><br><span class="line">&amp;xxx_uart3_device, </span><br><span class="line">&amp;xxx_nand_device,</span><br><span class="line">&amp;xxx_i2c_device,</span><br><span class="line"></span><br><span class="line">&amp;xxx_lcd_device,</span><br><span class="line">&amp;xxxpwm_backlight_device,</span><br><span class="line">        ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>發現lcd_device在backlight_device前面，但加載順序並不一致</p><ul><li>注意PM resume/suspend ops和這個順序是一致的</li></ul><p>看了一下該module的Makefile: backlight在display前面<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">obj-$(CONFIG_VT)   +&#x3D; console&#x2F;</span><br><span class="line">obj-$(CONFIG_LOGO)   +&#x3D; logo&#x2F;</span><br><span class="line">obj-y   +&#x3D; backlight&#x2F; display&#x2F;</span><br><span class="line">...</span><br><span class="line">obj-$(CONFIG_FB_xxx)   +&#x3D; xxxfb.o ak_logo.o</span><br><span class="line">obj-$(CONFIG_FB_AK88)   +&#x3D; ak88-fb&#x2F;</span><br></pre></td></tr></table></figure></p><p>產生的System.map如下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c001f540 t __initcall_pwm_backlight_init6</span><br><span class="line">c001f544 t __initcall_display_class_init6</span><br><span class="line">c001f548 t __initcall_xxxfb_init6</span><br></pre></td></tr></table></figure><br>如果把Makefile順序改一下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">obj-$(CONFIG_VT)   +&#x3D; console&#x2F;</span><br><span class="line">obj-$(CONFIG_LOGO)   +&#x3D; logo&#x2F;</span><br><span class="line">obj-y   +&#x3D; display&#x2F;</span><br><span class="line">...</span><br><span class="line">obj-$(CONFIG_FB_xxx)   +&#x3D; xxxfb.o ak_logo.o</span><br><span class="line">obj-$(CONFIG_FB_AK88)   +&#x3D; ak88-fb&#x2F;</span><br><span class="line">obj-y   +&#x3D; backlight&#x2F;</span><br></pre></td></tr></table></figure></p><p>System.map順序就對了<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c001f53c t __initcall_display_class_init6</span><br><span class="line">c001f540 t __initcall_xxxfb_init6</span><br><span class="line">c001f544 t __initcall_genericbl_init6</span><br><span class="line">c001f548 t __initcall_pwm_backlight_init6</span><br></pre></td></tr></table></figure></p><p>加載後就會發現xxxpwm_backlight_device的probe会在xxx_lcd_device的probe之后执行，即LCD初始化先于PWM的初始化。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://blog.csdn.net/richard_liujh/article/details/45669207" target="_blank" rel="noopener">linux驱动 之 module_init解析 （上）</a></li><li><a href="https://blog.csdn.net/richard_liujh/article/details/46758073" target="_blank" rel="noopener">Linux内核很吊之 module_init解析 （下）</a></li><li><a href="https://www.cnblogs.com/linhaostudy/p/9112264.html" target="_blank" rel="noopener">linux 设备驱动加载的先后顺序</a></li><li><a href="https://cloud.tencent.com/developer/article/1169502" target="_blank" rel="noopener">late_initcall和module_init的区别</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【Linux Kernel慢慢學】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> linux kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux Kernel慢慢學]likely and unlikely macro</title>
      <link href="/posts/cecba4ef/"/>
      <url>/posts/cecba4ef/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在linux kernel source code中，有時候會看到在if內的邏輯判斷加上<strong>likely</strong>或<strong>unlikely</strong>的敘述來幫助compiler做最佳化，例如下方的兩個例子，我們從<a href="https://github.com/spotify/linux/blob/6eb782fc88d11b9f40f3d1d714531f22c57b39f9/drivers/video/display/display-sysfs.c" target="_blank" rel="noopener">linux kernel source code</a>中找了一些部分程式碼:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">ssize_t</span> <span class="title">display_show_contrast</span><span class="params">(struct device *dev,</span></span></span><br><span class="line"><span class="function"><span class="params">struct device_attribute *attr, <span class="keyword">char</span> *buf)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">display_device</span> *<span class="title">dsp</span> = <span class="title">dev_get_drvdata</span>(<span class="title">dev</span>);</span></span><br><span class="line">    <span class="keyword">ssize_t</span> rc = -ENXIO;</span><br><span class="line"></span><br><span class="line">    mutex_lock(&amp;dsp-&gt;lock);</span><br><span class="line">    <span class="keyword">if</span> (likely(dsp-&gt;driver) &amp;&amp; dsp-&gt;driver-&gt;get_contrast)</span><br><span class="line">        rc = <span class="built_in">sprintf</span>(buf, <span class="string">"%d\n"</span>, dsp-&gt;driver-&gt;get_contrast(dsp));</span><br><span class="line">    mutex_unlock(&amp;dsp-&gt;lock);</span><br><span class="line">    <span class="keyword">return</span> rc;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/* linux/drivers/video/display/display-sysfs.c, line=139 */</span></span><br><span class="line"><span class="keyword">if</span> (unlikely(!driver))</span><br><span class="line">    <span class="keyword">return</span> ERR_PTR(ret);</span><br></pre></td></tr></table></figure><p>到底這樣寫有什麼用呢? 又是如何幫助compiler做最佳化的?</p><p>下面將會做一些簡單的介紹</p><a id="more"></a><h2 id="likely-unlikely-macro-introduction"><a href="#likely-unlikely-macro-introduction" class="headerlink" title="likely / unlikely macro introduction"></a>likely / unlikely macro introduction</h2><p>在Linux kernel 2.6之後提供了likely, unlikely 這兩個macro，他們被定義在/include/linux/compiler.h中</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">define</span> likely(x)__builtin_expect(!!(x), 1)            </span></span><br><span class="line"><span class="meta"># <span class="meta-keyword">define</span> unlikely(x)__builtin_expect(!!(x), 0)</span></span><br></pre></td></tr></table></figure><p><code>__built_expect()</code>是gcc的內建function，用來將branch的相關資訊提供給compiler，告訴compiler設計者期望的比較結果，以便對程式碼改變分支順序來進行優化。</p><p>而<code>!!(x)</code>是什麼意思，為什麼要這樣寫?</p><ul><li>透過兩次NOT op來確保值一定是0 或 1</li><li>因為if內邏輯敘述的值可以是0或是非0的整數的，所以如果不做<code>!!(x)</code>就無法確保值一定是0或1</li><li>類似的技巧在<a href="https://meetonfriday.com/posts/58e72281/">[Linux Kernel慢慢學]ARRAY_SIZE macro in linux kernel</a>這篇中也有用到</li></ul><p>透過這兩個macro，可以讓compiler在編譯assembly code的時候做一些最佳化，考慮以下pseudo code:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (likely(x)) &#123;</span><br><span class="line">    <span class="comment">/* execute when x is true */</span> </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">/* execute when x is false */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><ul><li>使用likely macro表示這段敘述(x)為true的機率比較大(比較常發生)，告訴compiler將x==ture的對應執行程式碼接在判斷後面</li><li>使用unlikely macro表示這段敘述(x)為true的機率比較小(不常發生)，告訴compiler將x==false的對應執行程式碼接在判斷後面</li></ul><p>這樣做有什麼好處呢?</p><h2 id="Advantage-of-using-likely-unlikely"><a href="#Advantage-of-using-likely-unlikely" class="headerlink" title="Advantage of using likely / unlikely"></a>Advantage of using likely / unlikely</h2><p>這和cache和memory的機制有關，有上過計算機組織的可能就比較了解，今天當我們需要某些資料時，我們會先去檢查cache是否有我們需要的資料:</p><ul><li>cache hit: 如果有則從cache直接拿</li><li>cache miss: 如果沒有則從memory搬到cache中，但這裡一次是搬一個區塊的資料</li></ul><p>而Spatial Locality這個特性就是在說，一記憶體位置被存取後，其附近的記憶體位置也極可能一同被存取。</p><ul><li>題外話，cache還有一種特性叫做Temporal Locality，不過這裡就不講了</li></ul><p>聰明的你應該知道了，如果程式碼被放到比較相近的位置，那他們就可能一起被搬到cache中，增加cache hit的機率，有可能可以提升程式的執行效能。</p><p>而likely / unlikely macro就是在將比較可能的部分往前移 / 比較不可能的部分往後移。</p><h2 id="Assembly-code-analysis"><a href="#Assembly-code-analysis" class="headerlink" title="Assembly code analysis"></a>Assembly code analysis</h2><p>實際上assembly code到底會發生什麼改變呢? 我們來做個簡單的觀察</p><p>給一段簡單的c code，使用了likely macro:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//#include &lt;stdio.h&gt;</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> likely(x)    __builtin_expect(!!(x), 1)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> unlikely(x)  __builtin_expect(!!(x), 0)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bar</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">char</span> *argv[], <span class="keyword">int</span> argc)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">if</span> (likely (argc == <span class="number">2</span>))</span><br><span class="line">      foo();</span><br><span class="line">   <span class="keyword">else</span></span><br><span class="line">      bar();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>組譯出來的結果會是(ARM gcc 8.2, with O2 optimization):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">main:</span><br><span class="line">        cmp     r1, #2</span><br><span class="line">        push    &#123;r4, lr&#125;</span><br><span class="line">        bne     .L2</span><br><span class="line">        bl      foo</span><br><span class="line">.L3:</span><br><span class="line">        mov     r0, #0</span><br><span class="line">        pop     &#123;r4, pc&#125;</span><br><span class="line">.L2:</span><br><span class="line">        bl      bar</span><br><span class="line">        b       .L3</span><br></pre></td></tr></table></figure></p><ul><li>可以發現在<code>bne</code>後面接的是foo</li></ul><p>那如果我們將likely改成unlikely，我們會得到:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">main:</span><br><span class="line">        cmp     r1, #2</span><br><span class="line">        push    &#123;r4, lr&#125;</span><br><span class="line">        beq     .L6</span><br><span class="line">        bl      bar</span><br><span class="line">.L3:</span><br><span class="line">        mov     r0, #0</span><br><span class="line">        pop     &#123;r4, pc&#125;</span><br><span class="line">.L6:</span><br><span class="line">        bl      foo</span><br><span class="line">        b       .L3</span><br></pre></td></tr></table></figure></p><ul><li>變成bar被移上去了，不過為了將bar放在前面，指令變成了<code>beq</code></li></ul><p>最後，這是gcc的optimization，所以如果在編譯的時候沒有開最佳化，<code>__builtin_expect()</code>並不會有任何效果，詳見<a href="https://stackoverflow.com/a/52913889/13466691" target="_blank" rel="noopener">gcc likely() unlikely() macros and assembly code</a>。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://hellpuppetanna.pixnet.net/blog/post/268059620-%E8%A8%88%E7%AE%97%E6%A9%9F%E7%B5%90%E6%A7%8B---08-cache%28%E4%B8%8A%29" target="_blank" rel="noopener">計算機結構 - 08 Cache(上)</a></li><li><a href="https://www.twblogs.net/a/5bb276df2b71770e645de5c9" target="_blank" rel="noopener">內核的likely和unlikely</a></li><li><a href="https://godbolt.org/z/UDzvf0" target="_blank" rel="noopener">https://godbolt.org/z/UDzvf0</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【Linux Kernel慢慢學】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> linux kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2020台灣設計展 in Hsinchu 參訪心得</title>
      <link href="/posts/957fed6c/"/>
      <url>/posts/957fed6c/</url>
      
        <content type="html"><![CDATA[<h2 id="2020台灣設計展-in-Hsinchu"><a href="#2020台灣設計展-in-Hsinchu" class="headerlink" title="2020台灣設計展 in Hsinchu"></a>2020台灣設計展 in Hsinchu</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602518372/blog_posts/2020-10-10-114521195_rjgnnz.jpg" alt=""></p><p>2020年的台灣設計展在新竹舉辦，不同於以往的策展方式，今年的台灣設計展是一個面向未來的非典型城市展，帶大家登入新竹市這個以「設計」導入城市治理的典範城市，讓設計翻轉城市，連結我們和城市之間的歷史記憶、空間地景和生活體驗！</p><p>這次的設計展總共分成三大展區，分別為</p><ul><li><strong>新竹公園 Terminal Park</strong> ，我們讓設計創新迴游在公園、動物園、孔廟、體育館和兒童遊戲場之間</li><li><strong>城市步行廊道 Public Line</strong>，我們一起漫遊在城市當中，將轉運站、東門城、護城河和幸福廣場連起來</li><li><strong>舊城區 Living Museum</strong>，我們在舊城街弄中，尋找著老州廳、老市場、老戲院和州圖書館的風華</li></ul><p>以及<strong>各種神奇又可愛的的新竹獸！</strong></p><p><br/><br/><br/><br>咦…我什麼時候變得可以打出這麼文青的文稿了？</p><p>沒有，這是引用官網的介紹內容~大家可以去官網看更詳細的內容(<a href="https://www.designexpo.org.tw/designnews" target="_blank" rel="noopener">2020 台灣設計展| CHECK in HSINCHU — 人來風</a>)</p><a id="more"></a><p>這次的展覽除了官網可以得到豐富的介紹和導覽外，也有一個很好的App可以下載，讓你可以根據自己喜好去設計自己的逛展路線，我覺得相當不錯。</p><p>我是在展覽最後一週，也就是國慶假日的時候去的，那個人潮真的 <strong>爆 炸 多 人</strong></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602516437/blog_posts/20201010_141801_dbh6oo.jpg" alt=""></p><p>上圖是我在排隊的狀況，那小小黑黑的都是人阿，而且照片這裡還不是最前端，前面還繼續彎下去排了一大串…新竹人平常太無聊沒地方去，難得有活動全部都跑出來了，而且這次展來還有大量的外縣市朋友一起來參觀，所以人數就overflow了。</p><p>所以儘管這次展場很多，不過最後我只逛了四個展區(都在新竹公園)，即使如此我也覺得逛得很開心，免費的展可以辦得這樣我覺得超級厲害！</p><p>下面是我參觀過的展區和所花費的排隊時間:</p><ul><li><strong>T1【城市終端機】</strong>: 大約排了一個小時</li><li><strong>T2【美感電域】</strong>: 我在晚上即將閉展前去的，大約只排了十幾分鐘</li><li><strong>T4【超連結】</strong>: 這個展沒排到隊，直接進去就好，痛哭流涕</li><li><strong>T5【○ 循環設計展】</strong>: 這是我排最久的，排了兩個多小時</li></ul><p>(貼心提醒: 下方有大量圖片，所以在瀏覽時會需要稍微等待一下圖片載入，可憐仔只能用免費空間放圖片，體諒一下QQ)</p><h2 id="T1【城市終端機】"><a href="#T1【城市終端機】" class="headerlink" title="T1【城市終端機】"></a>T1【城市終端機】</h2><p>終端機(Terminal)是工程師用來控制設備的一個介面，透過終端機的介面可以接收或是傳遞一些資訊。我認為城市終端機的名字應該是指，讓大家可以透過資訊科技的角度來重新認識新竹這個城市。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602519381/blog_posts/2020-10-13-120727863_qnazvu.jpg" alt=""></p><div class="img-desc">  在門口的部分有著新竹獸的大腳ㄚ可以拍照</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602519400/blog_posts/2020-10-13-120741490_n8ulci.jpg" alt=""></p><div class="img-desc">  當你歷盡千辛萬苦排完隊後，一進展覽就會看到一個很漂亮的電子屏幕 <br/>  身為一個好的終端機(?)，漂亮的Login介面是不可缺少的</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602519724/blog_posts/2020-10-10-113257231_adsqlu.jpg" alt=""></p><p>裡面有很多單元，時間關係不一一介紹(?)，挑幾個比較有趣的來分享</p><p>像是一日生活掃描機的單元，你有想像過你的日常生活其實所做的每一件事情都可以用資訊量(位元, bit)來表達嗎? </p><p>這個展覽記錄了不同角色一日生活中所做的行動，以及試著去數據化背後的資訊量。這概念挺有趣的，我曾經和朋友討論過，世界上任何事情其實都可以用資訊來表達。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602519415/blog_posts/2020-10-13-120710387_zx9cli.jpg" alt=""></p><div class="img-desc">  逛完生活掃描機後，會進到一個很酷的隧道，有種在訊號中流動的感覺</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602519729/blog_posts/2020-10-13-120803655_s1pjjz.jpg" alt=""></p><div class="img-desc">  一些歷史重大的里程碑</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602519665/blog_posts/2020-10-13-120815702_ivfdlr.jpg" alt=""></p><p>下面這個區域蠻有趣的，是透過數據的形式來介紹新竹，告訴你各種你不知道的神奇數據，例如<strong>新竹人平均三個人就有一個人會寫程式喔～</strong></p><p>真棒，什麼雞排店老闆會幫你Debug的笑話看來不是假的，以後我程式寫不出來我就去路上問路人。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602519681/blog_posts/2020-10-13-120841178_xnquu4.jpg" alt=""></p><div class="img-desc">  各種手機上的App</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_700/v1602518947/blog_posts/20201010_153930_tox6ol.jpg" alt=""></p><div class="img-desc">  新竹獸的心臟</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602519745/blog_posts/2020-10-10-114713253_kijli6.jpg" alt=""></p><div class="img-desc">  介紹完善的數據平台，這個UI我可以</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602519732/blog_posts/2020-10-13-120630722_vw81mr.jpg" alt=""></p><div class="img-desc">  終端機本體 - 透過投影在紗幕上，讓你走進沙幕時有種走進終端機的感覺</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602519747/blog_posts/2020-10-10-113644030_tln356.jpg" alt=""></p><div class="img-desc">  展場的最後有各種新竹獸的照片，對於沒有時間去把各個新竹獸部位都逛過一次的人可以在這裡一次滿足！</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602519750/blog_posts/2020-10-10-113658086_ardsnt.jpg" alt=""></p><h3 id="新竹獸的身體們"><a href="#新竹獸的身體們" class="headerlink" title="新竹獸的身體們"></a>新竹獸的身體們</h3><p>腳ㄚ前面有放過照片了，所以接下來放其他的部位</p><div class="img-desc">  新竹獸的手</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602611569/blog_posts/2020-10-10-115015568_bmqkcz.jpg" alt=""></p><div class="img-desc">  新竹獸的牙齒</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602611566/blog_posts/2020-10-10-115038806_z2b6g5.jpg" alt=""></p><div class="img-desc">  新竹獸的...背鰭?</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602611577/blog_posts/2020-10-10-115026980_w0j9ci.jpg" alt=""></p><div class="img-desc">  超巨大的尾巴!</div><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602611583/blog_posts/2020-10-10-115050825_iomuad.jpg" alt=""></p><h2 id="T2【美感電域】"><a href="#T2【美感電域】" class="headerlink" title="T2【美感電域】"></a>T2【美感電域】</h2><p>台電的展場，外觀做的美美的，裡面科普了各種變電箱的小知識，例如的配色、擺放、造型…等，其實都是經過仔細的設計的喔！</p><p>原本沒打算來這個展的，因為我的時間都花在T1跟T5了，不過回家前發現好像排隊的人蠻少的，感覺不會排太久就去排了。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602685123/blog_posts/2020-10-10-114827799_pg6dan.jpg" alt=""></p><h2 id="T4【超連結】"><a href="#T4【超連結】" class="headerlink" title="T4【超連結】"></a>T4【超連結】</h2><p>這也是個不太需要排隊的展，由Youtuber 志祺七七和設計師團隊一起策畫，可以在裡面看到許多Youtuber的一些介紹和他們的影片。</p><p>此外這區的亮點是放大連結的互動，透過科技刺青及時投影在大屏幕上，有點像特大型的濾鏡xD</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602685483/blog_posts/2020-10-10-113551078_enpf1a.jpg" alt=""></p><h2 id="T5【○-循環設計展】"><a href="#T5【○-循環設計展】" class="headerlink" title="T5【○ 循環設計展】"></a>T5【○ 循環設計展】</h2><p>最後也是讓我排最久的一個展，循環設計展透過玻璃讓大家去發現目前的地球存在著什麼樣的問題，然後去思考別人做了什麼努力，以及反思我們可以怎麼做。</p><p>裡面最有名的部分就是這個玻璃珠球池沙灘了，將玻璃打磨成圓珠的形狀真的很漂亮，還可以進去體驗踩踩看~</p><p>我想應該是一堆人想在這裡拍網美照才會那麼塞，為了看這個展我排了兩個多小時…腳都快斷了。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602686180/blog_posts/2020-10-10-114654928_o9y9en.jpg" alt=""></p><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>在撰寫這篇文章的時候展覽已經結束了，所以算是做個回顧這樣</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_400/v1602686207/blog_posts/2020-10-10-114545753_b7rjqs.jpg" alt=""></p><p>儘管沒有參觀完全部的活動(很想去看市集，可是真的是排隊排到沒時間)，不過也是看得很開心。T1的城市終端機是我看下來最喜歡的一個展區，有很多很棒很有創意的內容，真的讓人有種進入到科技世界內的感覺。</p><p>新竹能有這種藝文活動真的蠻棒的，不然大家假日都只能往巨城塞…而且對於一個免費的展來說，真的是覺得策畫的很棒，會排這麼久也是意想之中(畢竟可憐上班族只有假日能出來看…)</p><p>希望未來新竹能夠有越來越多的藝文活動可以參加！</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> travel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Deep Residual Learning for Image Recognition</title>
      <link href="/posts/7c0020de/"/>
      <url>/posts/7c0020de/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>paper: <a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1512.03385.pdf</a></p><p>ResNet，ILSVRC 2015年的冠軍，透過有名的Residual block降低了梯度在深層時會gradient vanish的問題，成功的達到了歷代CNN model都不能到的深度(152層)。</p><p>這篇paper發表於CVPR 2016，搜尋這篇paper，會發現被cite的次數高達了<strong>57446</strong>次！</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_450/v1602165108/blog_posts/phplBGAjV_n8umpx.jpg" alt=""></p><a id="more"></a><p>(和之前文章相同，本篇論文導讀主要在架構和classification上，所以沒有提到ResNet在detection的部分，有興趣的讀者可以再自己去看原文。)</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.</p></blockquote><p>提出了一個殘差訓練框架，使得能夠訓練更深的模型。同時他們證明了residual network可以更好地優化和透過達到更深的深度來提高準確度。</p><p>最後他們成功達到VGG層數的8倍，也就是152層，並使用了ensemble model的情況下達到了該年的ILVRC第一名。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>近代的研究指出模型的深度是設計時一個非常重要的考量，許多學者們透過較多層數的模型取得了一些較好的研究成果。</p><blockquote><p>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers?<br>An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [1, 9], which hamper convergence from the beginning</p></blockquote><p>隨著模型深度變成一個很重要的issue，大家開始思考：</p><p><strong>是不是只要簡單的加深層數，就能學習到一個好的網路？</strong></p><p>這個問題會牽涉到模型在深層的情況下容易造成gradient vanishing / exploding的問題，在近期的一些研究中，發現可以透過normalization有效的降低這種問題，使得模型仍然能夠有效運作。</p><p>當深層模型能夠有效收斂的時候，大家發現了另一個問題：<strong>退化(degradation)，當深度增加的時候，準確度會到達飽和並且快速地下降</strong></p><blockquote><p>Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error</p></blockquote><p>意外的是，這種現象並不是overfitting引起的，而是在深層的時候，模型參數的誤差造成的影響會越來越大，如同蝴蝶效應般所導致。</p><ul><li>就自己的理解來解釋一下這一段，如果是overfitting現象，我們合理的猜測我們應該可以得到一個training loss越來越低但validate loss越來越高的loss曲線</li><li>但就下面的實驗我們並沒有發現這件事，loss在train/valid都持續在下降，但明顯就是比20層的高了一個error gap</li></ul><p>下圖是簡單的用3x3 Conv來進行疊加模型，然後實驗了在20層和56層時的結果，可以看到56層的error都比20層高</p><ul><li>那個error突然下降的地方並不是這張圖想呈現的重點，不過其實我蠻好奇這個部分的，我猜想可能是因為在訓練一定的epoch後走到了一個山谷導致error瞬間降低很多的緣故</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1601212075/blog_posts/%E6%88%AA%E5%9C%96_2020-09-27_%E4%B8%8B%E5%8D%889.07.22_awlf4w.png" alt=""></p><p>這個現象很奇怪ㄋㄟ，為什麼深層的模型error curve硬是多了一個gap？</p><p>讓我們從一個簡單的case來思考：</p><ol><li>首先，先考慮一個較淺層的神經網路架構做為對照組</li><li>和一個較深層的神經網路架構</li></ol><p>該case存在一個solution可以來達成建構這個較深層的架構: <strong>也就是新添加一個identity mapping來增加層數。由於深層的模型具有更大的solution space，理論上他不應該有更高的training error，至少也應該有個跟淺層一樣的training error(也就是identity mapping case)</strong></p><ul><li>identity mapping到底是什麼概念? 可以想成我新增了一層，但新增一層後output卻和沒新增是相同的(該層單純將資訊完整地往下一層傳遞)，我們可以稱該層是一個identity mapping</li></ul><p>這個solution搭配實驗顯示了其實在深層模型訓練時<strong>最佳化模型是有一定的困難的</strong>，並且目前的solutions都不太容易解決這個現象。</p><p>所以他們就提出Residual block來想辦法解決這個問題。</p><h3 id="Residual-learning-framework"><a href="#Residual-learning-framework" class="headerlink" title="Residual learning framework"></a>Residual learning framework</h3><p>Residual block的架構如下：</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602230221/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%883.56.37_ag3moh.png" alt=""></p><p>假設我們的input x經過兩層layer後我們期望他學到的特徵是$H(x)$ (desired underlying mapping)</p><p>實際上x經過兩層後不一定能完全學到$H(x)$，我們把理想跟實際上學到的差距稱之Residual</p><script type="math/tex; mode=display">Residual = H(x) - x</script><p>我們把Residual用$F(x)$ (residual mapping)來稱呼</p><script type="math/tex; mode=display">F(x) = H(x) - x</script><script type="math/tex; mode=display">H(x) = F(x) + x</script><p>所以現在模型的目標變成學習residual mapping了。</p><p>…</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602230683/blog_posts/Li_si_leh_kong_sann_siau__kjp2pp.jpg" alt=""></p><p>如果你看到這邊是這個表情的話，下面讓我試著來用白話一點的方式來敘述:</p><p>原本我們希望我們的網路層可以學習到我們期望的特徵$H(x)$，不過上面也說了其實學習上是有困難的，尤其在深層網路的時候</p><ul><li>就像爸爸總是希望你考100分，所以請了一個家教希望能把你交到100分的程度，<strong>那對老師來說他的目標是”把你教到100分”</strong></li></ul><p>所以我們不如轉換思維，把目標分解，希望添加的網路層只需要學習自身$x$到$H(x)$的差距就好，學習$H(x)-x$，也就是$F(x)$的這個部分，這邊作者假設相較於學習$H(x)$，學習$F(x)$更加容易優化</p><ul><li>如果你本身就可以考70分了，那新來的家教就可以只需要<strong>把目標放在”把你成績提高30分”就好了</strong></li></ul><blockquote><p>To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.</p></blockquote><p>值得一提的是，最極端的狀況下，$x$如果已經完美的fit $H(x)$ (也就是說這層什麼都不學就是最佳解時)，那麼$F(x)$當作0即可，因為他根本不用學習任何東西。把$F(x)$當作0比透過一堆非線性層來直接fit $x \rightarrow H(x)$容易多了。</p><ul><li>也就是如果你本來就可以考100分了，那老師就很開心，直接薪水小偷當起來，他根本不用教你什麼</li></ul><p>上面介紹完了residual learning framework的部分，在實作上也非常簡單，透過<strong>“shortcut connection”</strong>就可以達成</p><ul><li>在deep learning框架中簡單的透過<code>torch.add()</code> or <code>tf.add()</code>來完成</li><li>shortcut connection幾乎<strong>不會帶來額外的cost</strong>，因為沒有任何新增的參數，頂多也就是那少少的一個法運算</li><li>實作上要<strong>注意shape不對等的case</strong>(因為有些層的stride=2會縮小shape)，此時就會搭配萬能的1x1 Conv來解決，下面會提到</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Residual-Representations"><a href="#Residual-Representations" class="headerlink" title="Residual Representations"></a>Residual Representations</h3><p>介紹了一些之前的相關研究，我覺得我上面Residual講得夠清楚了，所以這裡跳過！</p><blockquote><p>These methods suggest that a good reformulation or preconditioning can simplify the optimization.</p></blockquote><p>相關related works指出，好的重構或預處理可以簡化最佳化的困難度。</p><h3 id="Shortcut-Connection"><a href="#Shortcut-Connection" class="headerlink" title="Shortcut Connection"></a>Shortcut Connection</h3><p>shortcut其實也在不少的研究上被提出過，<strong>用來解決gradient vanishing / exploding的問題。</strong></p><ul><li>如果還記得<a href="https://meetonfriday.com/posts/263e065d/">GoogleNet</a>的朋友應該可以很直覺地想到他們就是在auxiliary classifiers的設計中就是把中間層的output直接接到classifier上</li><li>還有另一個related work是highway networks，一個gated-shortcut機制，有興趣的再去看吧</li></ul><p>再強調一次，在本文中他們認為深層網路學不好的主因不是gradient vanishing / exploding沒(問題可以透過parameter normalization / initialization來達到一定程度的克服)，但不可否認的是<strong>shutcut的設計本身也可以一定程度的解決該現象</strong>，來看一下梯度的計算：</p><p>$H(x) = F(x) + x$，$H(x)$對$x$的梯度變成了</p><p>$\frac{\delta{H}}{\delta{x}}$上再額外加上$\frac{\delta{x}}{\delta{x}}=1$的部分</p><h2 id="Deep-Residual-Learning"><a href="#Deep-Residual-Learning" class="headerlink" title="Deep Residual Learning"></a>Deep Residual Learning</h2><p>Introduction解釋過的部分這裡就會跳過，主要來介紹網路架構的部分。</p><h3 id="Network-Architectures"><a href="#Network-Architectures" class="headerlink" title="Network Architectures"></a>Network Architectures</h3><p>先放架構，網路那麼長截圖很難欸…想看的人請自己放大後把你的頭往右旋轉90度，不然就給我乖乖去看原文</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/a_90/v1602235479/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%885.24.21_rhhpgw.png" alt=""></p><p>為了和ResNet比較，需要有個對照組，也就是Plain Network</p><ul><li>基於VGG的架構設計，大部分都是3x3 Conv</li><li>相同input shape的layer具有相同數量的filter</li><li>如果shape減半，則filter size加倍，以保持相同的時間複雜度</li><li>最後接個GAP和softmax<ul><li>不知道GAP的去看NIN那篇論文吧…什麼你還沒看過NIN這篇論文?! 快去看<a href="https://meetonfriday.com/posts/a151bfa2/">[論文速速讀]Network In Network</a></li></ul></li></ul><p>而對於ResNet</p><ul><li>就是在Plain Network上加上shortcut connection</li><li>值得一提的是，<strong>架構中的黑色虛線代表input shape改變了(stride=2)</strong>，此時$x$和$H(x)$會因為input shape(或者說dimension)不同無法直接相加，他們考慮了兩個做法：<ul><li>(A) 硬加：dimension不足的地方我補0給你，給我硬加喔</li><li>(B) <strong>萬能的1x1 Conv</strong>: 什麼你還沒看過NIN這篇論文?! 快去看<a href="https://meetonfriday.com/posts/a151bfa2/">[論文速速讀]Network In Network</a><ul><li>1x1 Conv實作細節可以參考我寫的<a href="https://meetonfriday.com/posts/fb19d450/">[Pytorch]逐步解釋ResNet34程式碼</a></li></ul></li></ul></li></ul><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>前處理和參數設置的部分，跳過。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="ImageNet-Classification"><a href="#ImageNet-Classification" class="headerlink" title="ImageNet Classification"></a>ImageNet Classification</h3><p>做ImageNet分類時的網路架構圖，這個是用表格呈現的，不用再看到瞎掉</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602236091/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%885.33.44_amkih1.png" alt=""></p><p>這是Top1 error rate<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602236154/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%885.35.24_leo0gt.png" alt=""></p><p>這是error curve<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602236327/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%885.38.19_ropcow.png" alt=""></p><p>左邊的圖示對照組，右邊是ResNet，細線代表training error而粗線代表valid error，好了我們來看圖說故事：</p><ul><li>在左邊的圖中，深層網路的error比淺層還高，發現了<strong>模型退化</strong>的問題<ul><li>和前面討論過的一樣：在具有較大solution space的深層網路不應該學的比淺層差，在optimization過程中遇到了困難</li></ul></li><li>在右邊的圖中，深層模型的error比淺層低了！代表網路有好好最佳化喔～棒棒噠</li></ul><p>他們認為這個現象跟vanishing gradients無關，因為他們有加入BN，並且做了一些驗證。實驗數據其實也表明，儘管是plain network，仍然有一定程度的準確率，所以模型仍然是work的。關於造成的原因會在以後繼續研究。</p><p>ImageNet講完了，對於其他dataset的實驗就不在提起(我好懶喔…放過我吧QQ)，有興趣的可以參考原文。下面將會講一些和dataset無關的實驗分析。</p><h3 id="Deeper-Bottleneck-Architectures"><a href="#Deeper-Bottleneck-Architectures" class="headerlink" title="Deeper Bottleneck Architectures"></a>Deeper Bottleneck Architectures</h3><h4 id="Identity-vs-Projection-Shortcuts"><a href="#Identity-vs-Projection-Shortcuts" class="headerlink" title="Identity vs. Projection Shortcuts"></a>Identity vs. Projection Shortcuts</h4><p>對於shortcut的實作方式他們也比較了三種做法</p><ol><li>zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameterfree</li><li>projection shortcuts are used for increasing dimensions, and other shortcuts are identity</li><li>all shortcuts are projections</li></ol><p>1的方法是我們最熟悉的，也就是沒有新增任何參數的版本，但在實驗中方案3的效果比較好。</p><p>儘管在實驗中發現3的效果最好(可能是因為projections額外的參數帶來的好處)，但他們<strong>在後續的實驗還是採用了1的方案，因為沒有額外參數的設計對於接下來要把模型變得更深的實驗非常重要。</strong></p><h4 id="Deeper-Bottleneck-Architectures-1"><a href="#Deeper-Bottleneck-Architectures-1" class="headerlink" title="Deeper Bottleneck Architectures"></a>Deeper Bottleneck Architectures</h4><p>接下來他們考慮把模型弄的跟長頸鹿一樣長，可是礙於資源有限，慾望無窮(公民課本有教)，所以他們把ResNet的building block結構改了一下，變成了<strong>bottlenet building block</strong></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602237599/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%885.58.15_csqgbj.png" alt=""></p><p>簡單來說，就是先透過1x1 Conv降維，降低3x3 Conv計算上的負擔，然後再用1x1 Conv升維變回原本的shape。</p><ul><li>在這種設計下，如果是使用Projection Shortcuts，你會發現shortcut mapping連接的都是高維度的dimension，會有大量的參數需要被計算</li></ul><p>透過bottlenet building block，ResNet-50、ResNet-101就誕生了，然後還是要來嗆一下VGG: </p><p><strong>儘管我們都這麼深了(?)，複雜度仍然比你的VGG16 / VGG19低喔</strong></p><p>最後放個實驗結果：<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602238018/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%886.06.46_yr5j8r.png" alt=""></p><h3 id="Analysis-of-Layer-Responses"><a href="#Analysis-of-Layer-Responses" class="headerlink" title="Analysis of Layer Responses"></a>Analysis of Layer Responses</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602238534/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%886.15.22_y3scos.png" alt=""></p><p>這張圖顯示了各種模型的response deviations </p><ul><li>The responses are the outputs of each 3×3 layer, after BN and before other nonlinearity (ReLU/addition)</li></ul><blockquote><p>These results support our basic motivation (Sec.3.1) that the residual functions might be generally closer to zero than the non-residual functions</p></blockquote><p>可以發現到ResNet系列的response相對較小，應證了之前的假設: residual function比non-residual functions更能學到identify mapping的關係(也就是residual趨近0的效果)</p><p>此外，越深層的ResNet有越小的response，代表層數越深的時候，透過resdidual block的機制，越來越多層會將residual mapping的部分降低(也就是越來越多層開始選擇做少少事就好，當個薪水小偷)來控制模型的最佳化機制。</p><h3 id="Exploring-Over-1000-layers"><a href="#Exploring-Over-1000-layers" class="headerlink" title="Exploring Over 1000 layers"></a>Exploring Over 1000 layers</h3><p><strong>“好想知道能不能再深入喔&gt;&lt;”</strong></p><p>有錢人的幸福就是這麼樸實無華，他們最後設計了一個1202層的ResNet。</p><p>結果發現效果比152層的差，他們認為這是overfitting造成的(這麼小的dataset幹嘛用這麼大的模型)</p><p>是說…你真的搞出1202層我們也沒辦法用啦..</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>哇喔終於打完了，其實這些論文的概念都大概知道，可是要把一篇論文讀完所花的時間還是很久QQ</p><p>最後，來做個總結吧！</p><p>在ResNet這篇文章中探討了深層神經網路模型會遇到的問題</p><ul><li>不是梯度爆炸/消失所造成的，儘管確實會影響，但在Batch Normalization等技術出來後該issue被有效的改善</li><li>也不是overfitting，因為模型確實有一定程度的效果</li><li>他們認為這個現象起因於模型退化(degradation)，在深層模型中造成最佳化的複雜度大大增加</li></ul><p><strong>透過identity mapping來分解原本要優化的目標</strong>，基於這個想法提出了ResNet，ResNet可以帶來什麼好處？</p><ul><li><strong>沒有額外參數</strong>: identity mapping只是一個簡單的shortcut connection，沒有任何需要訓練的參數，降低了模型的複雜度並可以訓練更多層</li><li><strong>自適應深度</strong>: 根據前面的敘述你會發現，根據$x$的狀況，模型可以動態調整每一層$F(x)$要學習的目標<ul><li>如果網路模型夠淺: 那可以讓每一層的模型專注在學習特徵表達，也就是$F(x)$要學到一些representation</li><li>如果網路模型很深: 那可以讓某些層當薪水小偷，也就是$F(x)\rightarrow0$，做個identity mapping的功用將$x$往下傳就好。反正深層模型的solution space那麼大，讓其他人做事就好</li></ul></li><li><strong>減輕gradient vanishing / exploding的現象</strong>: 儘管論文說這不是造成深層模型訓練困難的主因，但shortcut機制確實能夠減緩梯度爆炸/消失的問題</li></ul><p>在模型的實踐上</p><ul><li>ResNet-18, ResNet-34採用的是一般的building block</li><li>ResNet-50, ResNet-101, ResNet-152採用的是bottleneck build block，為了降低模型的複雜度</li><li>關於ResNet-34的Pytorch實作分析，可以參考<a href="https://meetonfriday.com/posts/fb19d450/">[Pytorch]逐步解釋ResNet34程式碼</a></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/54072011?utm_source=com.tencent.tim&amp;utm_medium=social&amp;utm_oi=41268663025664" target="_blank" rel="noopener">【AI Talking】CVPR2016 最佳论文， ResNet 现场演讲</a></li><li><a href="https://zhuanlan.zhihu.com/p/56961832" target="_blank" rel="noopener">ResNet论文笔记及代码剖析</a></li><li><a href="https://medium.com/@hupinwei/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-resnet%E4%B9%8B%E6%AE%98%E5%B7%AE%E5%AD%B8%E7%BF%92-f3ac36701b2f" target="_blank" rel="noopener">(深度學習)ResNet之殘差學習</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux Kernel慢慢學]探討Designated Initializers</title>
      <link href="/posts/39485259/"/>
      <url>/posts/39485259/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在看別人c語言寫的程式碼中，有時可能會別人在struct會是array中使用到了”.”(點或是英文叫做dot)來進行初始化，例如下面這段struct student的宣告: </p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">student</span> <span class="title">s</span> = &#123;</span></span><br><span class="line">    .id = <span class="number">1</span>;</span><br><span class="line">    .name = <span class="string">"Just John"</span>;</span><br><span class="line">    .<span class="built_in">height</span> = <span class="number">180</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>又或是，在linux kernel code中你可以看到一堆這樣的用法<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* drivers/clk/clk-aspeed.c */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">aspeed_clk_soc_data</span> <span class="title">ast2500_data</span> = &#123;</span></span><br><span class="line">    .div_table = ast2500_div_table,</span><br><span class="line">    .eclk_div_table = ast2500_eclk_div_table,</span><br><span class="line">    .mac_div_table = ast2500_mac_div_table,</span><br><span class="line">    .calc_pll = aspeed_ast2500_calc_pll,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>可是如果沒用過的人可能就不知道這是什麼意思，然後這個偏偏又不是很好下關鍵字去google，因為”.”很容易被關聯到其他的問題，所以這篇就專門針對這個來進行介紹。</p><a id="more"></a><h2 id="Designated-Initializers"><a href="#Designated-Initializers" class="headerlink" title="Designated Initializers"></a>Designated Initializers</h2><p>上面的用法其實稱作Designated Initializers，在Using the GNU Compiler Collection (GCC)的<a href="https://gcc.gnu.org/onlinedocs/gcc/Designated-Inits.html?fbclid=IwAR1RJmNpWM0MRnU4fd29brOkRYKe-S60ZGoXUFqFx_J5b6F2kF9d6ZtIR20#:~:targetText=6.29%20Designated%20Initializers,array%20or%20structure%20being%20initialized.&amp;targetText=To%20initialize%20a%20range%20of,This%20is%20a%20GNU%20extension" target="_blank" rel="noopener">6.29 Designated Initializers</a>中你可以看到詳盡的介紹，下面透過中文的方式針對裡面比較重要的內容進行簡介。</p><p>在C90以前的規格，原本規定是必須按照固定的順序來對structure或array進行初始化的。</p><p>不過在C99開始，你可以透過指定array indeices (or structure field names in strure)來進行初始化。</p><h3 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h3><p>首先針對Array來介紹，例如你可以透過<strong>[index] = </strong>來初始化陣列的部分元素<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[<span class="number">6</span>] = &#123; [<span class="number">4</span>] = <span class="number">50</span>, [<span class="number">5</span>] = <span class="number">100</span>&#125;;</span><br></pre></td></tr></table></figure></p><p>上面的寫法和下列寫法相等</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[<span class="number">6</span>] = &#123; <span class="number">0</span>, <span class="number">0</span>, <span class="number">50</span>, <span class="number">0</span>, <span class="number">100</span>, <span class="number">0</span> &#125;;</span><br></pre></td></tr></table></figure><p>其實這裡還扯到了一件事情: array沒有被給值的elements都會是0嗎?</p><p>這個問題跟我們常用的陣列初始化寫法本質是一樣的事情，只是可能都沒有想過，考慮以下的陣列宣告</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> arr[<span class="number">100</span>] = &#123;<span class="number">0</span>&#125;;</span><br></pre></td></tr></table></figure><p>為什麼這樣就是讓arr的100個element都變成了0呢? </p><p>這樣的寫法實際上只給予了arr[0]值，其他的arr都是沒有被給值初始化的，有些人會稱這個叫做<strong>Partical Initialization</strong></p><ul><li>不過這個詞其實並不標準，Standard並沒有定義什麼是Partical Initialization，，所以還是盡量不要用會比較好，以免被誤導</li></ul><p>對於上面的狀況，在C99 Standard 6.7.8.21中是這樣寫的:</p><blockquote><p>If there are fewer initializers in a brace-enclosed list than there are elements or members of an aggregate, or fewer characters in a string literal used to initialize an array of known size than there are elements in the array, the remainder of the aggregate shall be initialized implicitly the same as objects that have static storage duration.</p></blockquote><p>也就是說，當我們宣告一個struct, array or string (with known size)時，但給定的initializers數量小於實際上該struct, array, string的大小時，<strong>剩餘的部分會被當作static storage duration來初始化，而static  variable的initialization是0</strong></p><ul><li>大家應該都有印象static和global variable初始化的值都是0，其實這與variable在memory layout中放置的位置有關，但這裡就不繼續往下追了</li></ul><p>好了我們把話題扯回Designated Initializers，對於Array，GNU extension還允許你透過<br><strong>[first … last] = value</strong><br>的方式來宣告</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> widths[] = &#123; [<span class="number">0</span> ... <span class="number">9</span>] = <span class="number">1</span>, [<span class="number">10</span> ... <span class="number">99</span>] = <span class="number">2</span>, [<span class="number">100</span>] = <span class="number">3</span> &#125;;</span><br></pre></td></tr></table></figure><p>注意此時array的長度會是最大值的index+1。</p><p>此外，在宣告時如果沒有給定index的initializer，element會根據他旁邊的index來決定他的index，例如:</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[<span class="number">6</span>] = &#123; [<span class="number">1</span>] = v1, v2, [<span class="number">4</span>] = v4 &#125;;</span><br></pre></td></tr></table></figure><p>等價於</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[<span class="number">6</span>] = &#123; <span class="number">0</span>, v1, v2, <span class="number">0</span>, v4, <span class="number">0</span> &#125;;</span><br></pre></td></tr></table></figure><p>最後來看兩個例子，示範Array在C99的designator initization中可以達到什麼樣的好處</p><h4 id="Example-1"><a href="#Example-1" class="headerlink" title="Example 1"></a>Example 1</h4><p>假設我們原本有以下的資料:</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum</span> data_name &#123;</span><br><span class="line">  CHINESE = <span class="number">0</span>,</span><br><span class="line">  ENGLISH = <span class="number">1</span>,</span><br><span class="line">  KOREAN = <span class="number">2</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">char</span> *say_hello[] = &#123;</span><br><span class="line">  <span class="string">"你好"</span>,</span><br><span class="line">  <span class="string">"hello"</span>,</span><br><span class="line">  <span class="string">"안녕하십니까"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在這樣的設計中，我們可以透過say_hello[CHINESE]來取得中文版的你好字串，但當enum的順序一有變動，對應的say_hello就要整個重改。</p><p>如果我們改成這樣的寫法:</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">char</span> *say_hello[] = &#123;</span><br><span class="line">  [CHINESE] = <span class="string">"你好"</span>,</span><br><span class="line">  [ENGLISH] = <span class="string">"hello"</span>,</span><br><span class="line">  [KOREAN] = <span class="string">"안녕하십니까"</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>就不會有上述問題了，如此為設計上增添了許多靈活性。</p><h4 id="Example-2"><a href="#Example-2" class="headerlink" title="Example 2"></a>Example 2</h4><p>在要做字元表的時候，使用這種方式會特別方便，我們可以直接給予char index來進行初始化:</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> whitespace[<span class="number">256</span>]</span><br><span class="line">  = &#123; [<span class="string">' '</span>] = <span class="number">1</span>, [<span class="string">'\t'</span>] = <span class="number">1</span>, [<span class="string">'\h'</span>] = <span class="number">1</span>,</span><br><span class="line">      [<span class="string">'\f'</span>] = <span class="number">1</span>, [<span class="string">'\n'</span>] = <span class="number">1</span>, [<span class="string">'\r'</span>] = <span class="number">1</span> &#125;;</span><br></pre></td></tr></table></figure><h3 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h3><p>對於structure，C99允許你透過指定field name的方式來對該struct進行初始化，這樣的好處免去了宣告時一定要按照順序的麻煩，並且未來更改struct時不用整個宣告重寫</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">point</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> x;</span><br><span class="line">  <span class="keyword">int</span> y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">point</span> <span class="title">p</span> = &#123;</span>.x = <span class="number">1</span>, .y = <span class="number">2</span>&#125;;</span><br></pre></td></tr></table></figure><p>上述的p宣告效果和</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">point</span> <span class="title">p</span> = &#123;</span> x = <span class="number">1</span>, y = <span class="number">2</span>&#125;;</span><br></pre></td></tr></table></figure><p>效果相同</p><blockquote><p>Omitted fields are implicitly initialized the same as for objects that have static storage duration.</p></blockquote><p>沒有宣告的fields一樣會被初始化成0(與陣列相同)。</p><p>並且這個方法也可以被用在union，不過這裡就不詳述，有興趣的可以再去看。</p><h2 id="Combine-array-amp-struct"><a href="#Combine-array-amp-struct" class="headerlink" title="Combine array &amp; struct"></a>Combine array &amp; struct</h2><p>把array和struct混合起來一起用就變成</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">point</span> <span class="title">ptarray</span>[10] = &#123;</span> </span><br><span class="line">  [<span class="number">2</span>].y = yv2, </span><br><span class="line">  [<span class="number">2</span>].x = xv2, </span><br><span class="line">  [<span class="number">0</span>].x = xv0 &#125;;</span><br></pre></td></tr></table></figure><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>最後，補充一些小部分:</p><blockquote><p>If the same field is initialized multiple times, or overlapping fields of a union are initialized, the value from the last initialization is used</p></blockquote><p>如果在初始化時，有相同的field被重複initialize，則最後使用最後一次的initization。</p><blockquote><p>The ‘[index]’ or ‘.fieldname’ is known as a designator.</p></blockquote><p>最後，<strong>[index]和.fieldname都叫做designator</strong>，這也是為什麼這個方法叫做designator initization。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://stackoverflow.com/questions/10828294/c-and-c-partial-initialization-of-automatic-structure" target="_blank" rel="noopener">C and C++ : Partial initialization of automatic structure</a></li><li><a href="http://hungmingwu-blog.logdown.com/posts/70061-designated-initializerr-in-c99" target="_blank" rel="noopener">Designated initializer in C99</a></li><li><a href="https://medium.com/cubemail88/c-struct%E5%88%9D%E5%A7%8B%E5%8C%96%E9%80%B2%E9%9A%8E%E7%94%A8%E6%B3%95designated-initializers-b6705968ede5" target="_blank" rel="noopener">[c]struct初始化進階用法designated initializers</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【Linux Kernel慢慢學】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> linux kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>研發替代役替第九十梯-15天生存紀實-2</title>
      <link href="/posts/53e1ff1f/"/>
      <url>/posts/53e1ff1f/</url>
      
        <content type="html"><![CDATA[<p>這篇是接續<a href="https://meetonfriday.com/posts/612e94f1/">研發替代役替第九十梯-15天生存紀實-1</a>的後半部分，如果沒看過第一篇的朋友建議先看過第一篇再回來看才銜接的上。</p><h2 id="5天的EMT1證照-9-10-9-14"><a href="#5天的EMT1證照-9-10-9-14" class="headerlink" title="5天的EMT1證照(9/10-9/14)"></a>5天的EMT1證照(9/10-9/14)</h2><h3 id="9-14-一"><a href="#9-14-一" class="headerlink" title="9/14(一)"></a>9/14(一)</h3><h4 id="EMT1證照考試"><a href="#EMT1證照考試" class="headerlink" title="EMT1證照考試"></a>EMT1證照考試</h4><p>五天的課程終於上完了，今天是EMT1的考試日。</p><p>考試總共分為學科跟術科（翻身+CPR, AED），一開始還有點擔心會記不住，不過其實不論是術科或是學科都有考前複習。尤其是學科考前複習，根本是把考試題目從頭念過一遍，所以考起來根本沒有難度xD</p><h4 id="替歌練習"><a href="#替歌練習" class="headerlink" title="替歌練習"></a>替歌練習</h4><p>晚上的時候就是練習替歌競賽跟基本教練，說起替歌競賽，<strong>其實一開始我們的中隊長是說我們研替不用比的</strong>，結果過幾天突然改口跟我們說我們又要比了，才很匆忙的選出了替歌、道具跟動作小老師才開始練習。</p><p>在成功嶺的期間真的是覺得很瞎，似乎上面的人什麼事情都沒辦法好好決定跟傳達的感覺。</p><a id="more"></a><h2 id="那些期末測驗前的日子-9-15-9-17"><a href="#那些期末測驗前的日子-9-15-9-17" class="headerlink" title="那些期末測驗前的日子(9/15-9/17)"></a>那些期末測驗前的日子(9/15-9/17)</h2><h3 id="9-15-二"><a href="#9-15-二" class="headerlink" title="9/15(二)"></a>9/15(二)</h3><h4 id="關於用餐的三兩事"><a href="#關於用餐的三兩事" class="headerlink" title="關於用餐的三兩事"></a>關於用餐的三兩事</h4><p>關於很趕的用餐時間，其實過了幾天後終於有人跟長官反映，我還記得那個同學當時是這樣說的:</p><p><strong>「用餐時間太短會消化不良，我是認真的」</strong></p><p>之後我們得到的用餐時間充裕了許多，他真的是大家的救星QQ</p><p>(後來長官才說其實他們以前是分批進餐廳的，我們是第一梯全中隊一起進餐廳，所以他們用餐時間也不知道到底該抓多久，真棒。)</p><p>不過今天要說的是我和鄰員決定改變用餐的方式，<strong>一開始先迅速吃完餐盤內的東西，然後去洗餐盤，洗完後再慢慢用碗去裝甜湯喝</strong>。由於這樣會讓你比別人還快吃完，所以不用花很多時間在排隊洗餐具跟沖水，可以省下更多的時間慢慢喝甜湯到飽(然後喝完再把碗偷偷帶回本部去洗，反正那個時間長官跟學長幾乎都不在所以不會被發現xD)。</p><p>用了這個方法後從此吃飯的時間更加充裕了，接下來的每一天我們幾乎都是這樣用餐的。</p><h4 id="期末測驗太差要重訓"><a href="#期末測驗太差要重訓" class="headerlink" title="期末測驗太差要重訓?"></a>期末測驗太差要重訓?</h4><p>在今天長官突然放話跟我們說叫我們皮繃緊一點，<strong>說如果期末測驗成績太差的話是有可能重訓再留一梯的</strong>。</p><p>…三小？</p><p>大家聽到的反應都是你在供三小，從來就沒聽說過有人重訓這種事情，何況我們這群可撥仔之後要馬上去公司報到了，如果被重訓是要怎麼辦……儘管大家都覺得不可能，但是還是都稍微提高注意力面對期末測驗了。</p><h3 id="9-16-三"><a href="#9-16-三" class="headerlink" title="9/16(三)"></a>9/16(三)</h3><h4 id="讀程式的同學"><a href="#讀程式的同學" class="headerlink" title="讀程式的同學"></a>讀程式的同學</h4><p>上完EMT1後我們又恢復了無聊的外師上課生活，就當某一節我實在是無聊到快睡著的時候，瞄到隔壁的同學居然帶了一疊厚厚的C#教材進來讀。真是太用心了，近來成功嶺都還在學程式，突然湧起了對他滿滿的敬佩。</p><h4 id="替歌競賽"><a href="#替歌競賽" class="headerlink" title="替歌競賽"></a>替歌競賽</h4><p>下午去比替歌競賽，總共有七個中隊比賽的樣子，原本我覺得我們隊的道具組已經很厲害了，沒想到當天看到其他隊伍扛了紙箱做的超大坦克出來，類似下圖</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1601189962/blog_posts/fe8cc15ba95e4794bada6452b4cb9470_tplv-crop-center_422_236_kssva6.jpg" alt=""></p><p>好…好強…</p><p>(聽說有些道具是中隊一直交接延續下去的，為啥我們中隊什麼都沒留下來= =)</p><p>每隊只有三分鐘的時間要完成兩首歌包含進場出場，結果我們不小心超時了一點，最後只得到第四名。<strong>不過也因為這個第四名，讓我們全中隊保底榮二</strong>(題外話，隔壁的天堂七中是第一名，全隊保底榮四，至於稱呼他們為天堂七中的原因後面會說)。</p><h3 id="9-17-四"><a href="#9-17-四" class="headerlink" title="9/17(四)"></a>9/17(四)</h3><h4 id="期末測驗"><a href="#期末測驗" class="headerlink" title="期末測驗"></a>期末測驗</h4><p>終於來到了期末測驗這一天，這天一整個早上的行程都被拿來用在期末測驗上了，首先一大早從測驗伏地挺身先開始測驗。</p><p>歐對，在這之前必須先交代一下為什麼跑3000跑著跑著，期末測驗變成了伏地挺身了呢？</p><p>原本的晨間體能都是在跑3000，可是從某天開始才突然加上了伏地挺身的訓練。我們猜測長官們自己也不知道這件事，在某天突然得知我們期末是考伏地挺身後才趕緊在晨間體能的時候多少訓練我們一下。</p><p>接下來吃完早餐是筆試測驗，當初網路上的心得文大多都是一般替代役的，所以我也不太清楚這個筆試要考什麼，只好先印著網路上一般替代役的“考古題”進去無聊的時候加減看。</p><p>結果筆試考卷一發才發現我根本讀錯了，研發替代役期末測驗的筆試考的是前幾天和<strong>未來幾天</strong>的上課內容，由於我們的課程和一般研替不同，所以考古題根本沒用。（是的你沒看錯，包含了未來幾天根本還沒上課的課程題目也一起考出來了，<strong>我還記得後來幾天的講師在上課的時候還很吃驚的問我們：「阿你們這些都還沒上過，題目怎麼可能會寫？」我他媽的怎麼會知道。</strong>）</p><p>總之，由於前幾天那種很無聊的課程根本都沒在聽，筆試只能硬著頭皮去寫，不過好險最後還是有及格啦。</p><p>最後是基本教練，當天仍然有不少人因為太緊張而放槍，不過測驗都結束了基本上也不太會被長官再罵什麼，所以就順順的過去了。</p><h4 id="暴走的阿嬤"><a href="#暴走的阿嬤" class="headerlink" title="暴走的阿嬤"></a>暴走的阿嬤</h4><p>下午是成功嶺很有名的戰鬥阿嬤，教體適能帶動跳的<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1601192905/blog_posts/20140203194618294_x93p5u.jpg" alt=""></p><p>事前透過其他人的心得文就已經知道她很兇很火爆，兇到會連長官一起罵的那種，所以多少有了心理準備。原本現場看她在那邊噹分隊長看得很爽，<strong>結果當阿嬤開始噹同學們的時候，我們有個同學忍受不住，居然回嗆了回去</strong> </p><p>Σ(*ﾟдﾟﾉ)ﾉ</p><p>這一嗆下去不得了，阿嬤開始暴走，現場氣氛一度很火爆。大家都很緊張，因為一個搞不好被處罰的話，很可能會影響那個同學後續公司的工作</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1601193165/blog_posts/yIJVcM2_xuhhdp.jpg" alt=""></p><p>不過好險後來是和平收尾，那位同學似乎也沒有被處分，也就是被大隊長關切了一下這樣。當時聽說那個同學很堅持要提告，不知道後來到底有沒有行動就是了…</p><h2 id="最煎熬的是等待-9-18-9-21"><a href="#最煎熬的是等待-9-18-9-21" class="headerlink" title="最煎熬的是等待(9/18-9/21)"></a>最煎熬的是等待(9/18-9/21)</h2><p>這個階段是我覺得除了前三天外，最難熬的幾天。由於期末測驗完了，基本上每天就是上上課、掃掃地，這段時間會覺得時間特別漫長，大家每天都在倒數…</p><p>因為這幾天除了一些活動其實蠻無聊的，所以日記的部分也沒紀錄太多。</p><h3 id="9-18-五"><a href="#9-18-五" class="headerlink" title="9/18(五)"></a>9/18(五)</h3><h4 id="天堂七中"><a href="#天堂七中" class="headerlink" title="天堂七中"></a>天堂七中</h4><p>是時候來寫寫天堂七中的故事了。</p><p>其實研替總共被分成五六中隊以及七八中隊聯合訓練，不過我們習慣都用五中、七中來稱呼。<strong>你的出生地決定了你會在哪一個中隊，同時也決定了你在天堂與地獄。</strong></p><p>相對於地獄五中的長官們，七中的長官聰明多了，據說他們會搞分流讓大家輪流去洗澡跟用手機，也因此省下了很多無謂的等待時間。聽他們說用手機的時間常常都是半小時到一小時起跳，對比可憐的我們每每只能用一下下。</p><p>再來說個綜合生活評比得名的事情好了，七中他們拿到了第一名，所以長官給他們福利，讓他們每隊可以派人去ok採購想買的東西，那天他們從ok回來的時候我們還在拔草，還被嘲笑…真的是很可憐QQ</p><p>(後來才知道我們五中綜合生活評比是第二名，不過沒有一個長官跟我們說，也完全沒有任何獎勵。)</p><h4 id="肝膽相罩"><a href="#肝膽相罩" class="headerlink" title="肝膽相罩"></a>肝膽相罩</h4><p>照網路上的慣例我們會有個肝膽相罩的時間拿來整當月壽星(壽星在第一天的時候長官有先調查)，這幾天大家看剩下的日子也沒剩多少，其實多少都有心理準備可能就是今晚要被搞了，只是不知道會被怎麼整。</p><p>晚上的時候大家都被叫去餐廳裡，然後分隊長們跟學長們開始裝兇說<strong>有人在寢室偷藏香菸被發現，勸大家自首可以從輕處理</strong>(這個理由也太瞎了吧xD)，然後就在一陣叫罵聲中點了一群嫌疑人起來，罵完之後叫大家幫他們唱生日快樂歌慶生，接下來就是讓大家吃蛋糕餅乾和喝飲料。</p><p>當天晚上算是一個比較放鬆的時段，大家開始拱同學&amp;長官在台上唱歌、講笑話，好笑的是我們的分隊長拿著麥克風突然就開始無情工商</p><p>「其實我有在twitch開台玩遊戲，大家之後可以來看我們玩」</p><p>下圖就是咱們兼任實況主的分隊長，我有幫他簡單的上了一下馬賽克(?)</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1601214118/blog_posts/120075178_3966999303316291_168051815457880888_o_k5oupq.jpg" alt=""></p><h3 id="9-19-六"><a href="#9-19-六" class="headerlink" title="9/19(六)"></a>9/19(六)</h3><h4 id="成長營"><a href="#成長營" class="headerlink" title="成長營"></a>成長營</h4><p>這天的下午是成長營，讓大家分組去體驗一些攀岩、垂降等設施。成功嶺裡面的設施其實不少，有些是平常也看不到的。</p><p>我們這組玩了垂降和兩個團體活動，其實有點小失望，因為想玩一些比較刺激的高空設施的。</p><h4 id="可憐的打飯班"><a href="#可憐的打飯班" class="headerlink" title="可憐的打飯班"></a>可憐的打飯班</h4><p>上一篇一開始有提到打飯班很辛苦，每次都要最早集合去幫同學們打飯，理應要加不少分才對，不過最後聽說打飯班只有加4分。這其實很少，<strong>因為要加滿8分才是一個榮譽假(8分榮二，16分榮四)</strong>，像我們公差班就加了8分，然後替歌又保底榮二，所以我們總共拿到了榮四。</p><p>而打飯班的同學們為了要爭取榮四，在最後幾天很努力的去當公差想要加分，可是最後聽說長官們都沒有幫他們上報，結果那些分數都沒加到，使得他們只能榮二，於15:30離開成功嶺，這使得他們極為憤怒，不過也是可以想像的。</p><h3 id="9-21-一"><a href="#9-21-一" class="headerlink" title="9/21(一)"></a>9/21(一)</h3><h4 id="逃出成功嶺！"><a href="#逃出成功嶺！" class="headerlink" title="逃出成功嶺！"></a>逃出成功嶺！</h4><p>早上在結訓典禮結束後，大家開始整理東西，吃完午餐後榮四的就被帶出去準備於13:30離開成功嶺。還記得在前往出口的時候，我們都在大聲歡呼、開心地跟長官們以及學長們道別，那時候的自己有點小感動，想著</p><p><strong>「啊…終於結束了啊…」</strong></p><p>走到成功嶺出口的路其實很長，大家又背著很重的黑大在走，不過想到即將可以奔向自由大家都沒什麼怨言，十五天的一個生活也在這裡告了一個段落。</p><p>題外話，長官們禁止我們在高鐵站亂丟鞋子等垃圾，據說因為以前結訓的時候高鐵站的垃圾桶，包含廁所的垃圾桶到處都塞滿了皮鞋和運動鞋導致被投訴xD 所以前一天晚上有先開放讓我們丟一部分的衣物</p><p>『研替90梯，於9/21（一） 13:30正式逃出成功嶺』</p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>最後，簡短的總結這十五天的心得:</p><ol><li><strong>在成功嶺生活，你需要放空你的腦袋和降低你的智商</strong><ul><li>遇到不知道的事情不要去思考，一個口令一個動作，如果擅自動作反而還會被吼</li><li>被吼了也就聽聽就好，反正保持腦袋空空就不會有什麼感覺</li></ul></li><li><strong>卡多利亞其實沒那麼難</strong>: 反正平常就在美食沙漠了，吃卡多利亞意外的習慣</li><li><strong>手錶一定要帶</strong>: 雖然他不是必備品，但沒帶這十五天你會很痛苦，因為沒有時間觀念然後各種緊張</li><li><strong>蚊帳不好折?想辦法換新的吧</strong>: 有些蚊帳超級舊很難折出摺痕，不過其實某一天我們有開放給換蚊帳，我的鄰員再換了新蚊帳後天天內務穩定加分，你就知道新舊蚊帳的差距了<ul><li>沒有理由換？把自己的蚊帳戳出個洞就可以換了</li></ul></li><li><strong>能苟且就苟且</strong>: 這是我在床板上看到前輩們寫給我們的留言(?) 真的是受用無窮，學好怎麼苟且讓你這十五天會過得輕鬆點</li><li><strong>多認識朋友</strong><ul><li>首先，在成功嶺最重要的朋友就是你的左右鄰員，好的鄰員帶你上天堂，不論是叫你起床、幫你拿東西、提醒事務都能夠一手包辦</li><li>再來就是多認識同梯的其他同學吧！大家都是科技新貴，這個圈子說大也不大，拓展多一些人脈對於自己未來或多或少都是有幫助的</li></ul></li></ol><hr><p>成功嶺的文章寫到這裡終於告一段落，從9/21(一)出來開始寫，到了9/27(日)才寫完。原本打算出來的當天就寫完的，沒想到要寫的內容比我想像中的還要多，而且上班回家根本累到只想躺床沒什麼時間寫。</p><p>最後，如果問我覺得去成功嶺值不值得，我會跟你說雖然很幹很累，但是</p><p>一 點 都 不 值 得 。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>研發替代役替第九十梯-15天生存紀實-1</title>
      <link href="/posts/612e94f1/"/>
      <url>/posts/612e94f1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言-研役制度的回歸"><a href="#前言-研役制度的回歸" class="headerlink" title="前言 - 研役制度的回歸"></a>前言 - 研役制度的回歸</h2><p>2020/7月從碩士畢業後，由於今年政府再度<strong>重起了研發替代役制度</strong>，一年半的研替(83年次以後的役期為一年半)總共分成三個階段:</p><ul><li>第一階段: 於成功嶺受訓15天</li><li>第二階段: 於用人單位開始工作，為期四個月</li><li>第三階段: 一樣於用人單位工作，為期一年兩個月</li></ul><p>第二階段和第三階段都是餘用人單位工作，不同的地方是第二階段的雇人單位仍然是政府，直至第三階段和用人單位才是雇傭關係。也因此有些公司第二階段和第三階段的薪資福利會不同……剩下的細節就不講了，我又沒收錢幹嘛幫他們工商。</p><p>總之在今年初我決定申請當研發替代役，成為了<strong>第90梯的研發替代役</strong>並於9/7號進入台中成功嶺受訓兩週(9/7-9/21)。在成功嶺受訓儘管只有15天，但是在裡面真的令人覺得度日如年，每天都在倒數還有幾天出來。如果網路上的文章在那邊跟你說什麼”其實還好”、”一點都不後悔”…那都是騙人的你各位啊</p><p>“畢竟當他們正在撰寫文章的當下，早已經離開成功嶺並和現在的我一樣，開心地敲著鍵盤了”</p><p>當我在成功嶺內聽著講師那有如白噪音般的語調講述的無聊課程時，我實在是受不了了，作為打發時間的消遣，我決定開始寫日記來記錄這十五天的生活，等到出來後將這些日記統整成Blog發布。</p><p>不過為了避免篇幅過長，所以我將這十五天的日記分成兩篇文章來撰寫:</p><ol><li><a href="https://meetonfriday.com/posts/612e94f1/">研發替代役替第九十梯-15天生存紀實-1</a>（也就是本篇文章）</li><li><a href="https://meetonfriday.com/posts/53e1ff1f/">研發替代役替第九十梯-15天生存紀實-2</a></li></ol><a id="more"></a><p>最後，在文章開始前，附上我離開成功嶺後於台中高鐵站購買的第一杯飲料 - Starbucks的”摩卡可可碎片”</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_300/v1600794496/blog_posts/119754364_3950743794941842_1505712109084150924_o_nmtzkp.jpg" alt=""></p><h2 id="閱讀前的建議服用事項"><a href="#閱讀前的建議服用事項" class="headerlink" title="閱讀前的建議服用事項"></a>閱讀前的建議服用事項</h2><p>相較於網路上很多文章是以重點、主題為章節來介紹，我更想以日記的形式來呈現每一天發生了什麼事情，讓讀者能更知道在裡頭的每一天是如何度過的。所以這不會是一篇教你應該準備什麼、注意甚麼的攻略文，但我還是有附上一些相關資料給有需要的朋友:</p><p>首先，下圖是我們這梯次的<strong>時程訓練表</strong>(可透過右鍵-&gt;新分頁開啟圖片放大檢視)，可以搭配服用。其實會照著時程走的只有課程和一些重要的事項(例如開訓、結訓、驗尿、期末測驗)，其他吃飯時間、基本教練什麼的，全端看隊長和學長們，誰管你時程表上怎麼寫。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1600794494/blog_posts/adwyi4M_uamqgo.jpg" alt=""></p><p>如果你是即將進入成功嶺並且剛好點到這篇的學弟，那以下提供幾個當初我進去前所看到還蠻完整的資源，看完應該可以更知道進去前該準備什麼:</p><ul><li><a href="https://www.dcard.tw/f/military/p/232599662" target="_blank" rel="noopener">研84梯(一般207梯)研發替代役成功嶺攻略與心得</a></li><li><a href="https://shienchia1996.pixnet.net/blog/post/295434896-%5B%E6%9B%BF%E4%BB%A3%E5%BD%B9%E6%96%B0%E8%A8%93%5D200t%E6%88%90%E5%8A%9F%E5%B6%BA%E6%97%A5%E8%A8%98day-0-----%E5%85%A5%E4%BC%8D%E4%B9%8B%E5%89%8D" target="_blank" rel="noopener">替代役新訓 200t成功嶺日記day0-入伍之前</a></li></ul><p>就我來說，因為我們中隊規定了加洗每天只能一件內衣，所以我會建議加洗內衣內褲襪子的多買一套備用就好，內褲襪子都不能加洗。儘管如此還是建議多買一套的原因是:</p><p><strong>你的衣物有可能洗著洗著就不見了，所以還是要有一套備用不然到時候可能會沒衣服換。</strong></p><p>然後關於期末測驗考古題的部分:</p><ul><li>如果你是一般替代役，請參考<a href="https://drive.google.com/drive/folders/1wlE9Y_0YHSyQSbtMtJHKMxy0mFfv0jZx" target="_blank" rel="noopener">成功嶺替代役新訓考古題</a></li><li><strong>如果你是研發替代役則不用印考古題</strong>，研替考上課內容和一班替代役不同，所以看考古也沒用，多印一點小說進去打發時間比較實在</li></ul><h2 id="前三天總是最痛苦的-9-7-9-9"><a href="#前三天總是最痛苦的-9-7-9-9" class="headerlink" title="前三天總是最痛苦的(9/7-9/9)"></a>前三天總是最痛苦的(9/7-9/9)</h2><p>網路上的文章都說前三天是最累的，這句話是真的。主要是因為前三天要做的事情最多，所以各種趕時程，此外有部分原因也是由於不習慣營中的生活而覺得很累。</p><h3 id="9-7-一"><a href="#9-7-一" class="headerlink" title="9/7(一)"></a>9/7(一)</h3><h4 id="啟程"><a href="#啟程" class="headerlink" title="啟程"></a>啟程</h4><p>第一天從左營高鐵站集合出發，出發前各種議員代表前來送了一些小東西給你。有人送了雙面膠有人送了小零錢袋，有些還不錯用。在火車上小睡了一下後，由彰化轉乘遊覽車到達成功嶺，而下車後其實也沒有被大吼什麼的，只是會叫你快點集合這樣。</p><p>然後接下來宣布我們是<strong>第二大隊第五中隊</strong>(後來才知道我們被稱為地獄五中)，然後帶我們開始進行身體檢查 -&gt; 剃頭 -&gt; 讓你去採購需要的物品。</p><ul><li>內衣的部分當初我參考網路上的心得都加購了兩套，結果後來長官他們說每天只能加洗一件內衣，有一套根本用不到</li></ul><h4 id="違禁品檢查"><a href="#違禁品檢查" class="headerlink" title="違禁品檢查"></a>違禁品檢查</h4><p>買完後就是由分隊長將各隊帶去各隊本部，把東西都倒出來給他們檢查。</p><p>印象蠻深刻的是我那排的在檢查的時候，我聽到了分隊長很大聲的對我那排的某個人大吼</p><p><strong>“你帶水果刀來幹嘛啦!”</strong></p><p>到底為什麼要帶水果刀進來成功嶺啦xD</p><h4 id="公差班的徵選"><a href="#公差班的徵選" class="headerlink" title="公差班的徵選"></a>公差班的徵選</h4><p>接下來開始問有沒有人自願公差班，公差班有: 器材、打飯、過水、體委(體溫委員)</p><p>由於在詢問的時候還有一批人還沒到齊，當時估算了一下如果不自願當器材班的話，剩下的人數很有可能就自動湊成打飯過水班了，所以我就自願出來當器材班。</p><p>後來真的剩下的自動變成了打飯過水班…這兩個公差班真的是屎缺，而且打飯班後來又被長官坑沒有拿到榮四(後面會說)，真的很衰。</p><h4 id="寢室的搖搖床"><a href="#寢室的搖搖床" class="headerlink" title="寢室的搖搖床"></a>寢室的搖搖床</h4><p>寢室分成上下舖，<strong>那個床組根本是搖搖床</strong>，只要上面或下面的人稍微動一下就會搖得跟誇張，如果本身就淺眠的人應該會睡的很痛苦，不過還好對我沒什麼影響xD</p><h3 id="9-8-二"><a href="#9-8-二" class="headerlink" title="9/8(二)"></a>9/8(二)</h3><h4 id="尿液檢查"><a href="#尿液檢查" class="headerlink" title="尿液檢查"></a>尿液檢查</h4><p>今天一早開始尿液檢查，他會要你尿到杯子八分滿，很多人尿不滿的都會被叫去做著灌水繼續努力，所以當天早上起床真的不要先去上廁所不然會很痛苦。</p><h4 id="無聊到想睡覺的課程們"><a href="#無聊到想睡覺的課程們" class="headerlink" title="無聊到想睡覺的課程們"></a>無聊到想睡覺的課程們</h4><p>然後今天開始上課，可是上課的老師實在是有夠無聊，我又不敢趴下睡覺。真心建議要先<strong>印一些有趣的東西讓你上課帶著打發時間用，像是數獨、小說</strong>(後來我跟其他同學借了「我想吃掉你的胰臟」來看，覺得蠻好看的，看得很開心)，不用怕印太多會很奇怪，我覺得二十幾張左右都還嫌少。</p><h3 id="9-9-三"><a href="#9-9-三" class="headerlink" title="9/9(三)"></a>9/9(三)</h3><h4 id="開始晨跑3000"><a href="#開始晨跑3000" class="headerlink" title="開始晨跑3000"></a>開始晨跑3000</h4><p>今天開始跑3000，聽說上一梯還不用跑，偏偏我們這梯又重起了…3000公尺的動線有三圈，然後有上坡跟下坡，那個上坡我真的是跑得很 痛 苦 </p><p>然後跑完回去馬上又要吃早餐，真的完全吃不下。</p><p>說到早餐，成功嶺的食物(卡多利亞)很多人都說很難吃，但其實我覺得還行啦，不致於完全吃不下的程度，也就是甜湯很水、鹹湯很鹹、飯菜還能入口這樣。</p><h2 id="5天的EMT1證照-9-10-9-14"><a href="#5天的EMT1證照-9-10-9-14" class="headerlink" title="5天的EMT1證照(9/10-9/14)"></a>5天的EMT1證照(9/10-9/14)</h2><h3 id="9-10-四"><a href="#9-10-四" class="headerlink" title="9/10(四)"></a>9/10(四)</h3><h4 id="EMT1"><a href="#EMT1" class="headerlink" title="EMT1"></a>EMT1</h4><p>在課程規劃中，接下來有整整五天的時間都要上EMT1(初級救護技術員)課程，聽說在外面考一張需要好幾千。</p><p>EMT1會請消防局的教官和助教來上課(消防的人真的很辛苦，有些才剛下班又馬上來幫我們上課)，消防的教官都很有趣人也很好，<strong>有時候上完課還會請同學們喝飲料</strong>。飲料對當時的我們來說就代表自由的味道，還記得在成功嶺內第一次喝到可樂的時候感覺超爽。</p><p>對了，由於像是術科需要操作CPR，頸椎骨定術，AED這些都必須要跪在地上真的是很痛，幾天下來大家的膝蓋都瘀血了（也覺得消防真的是件很辛苦的工作）。</p><h4 id="衣服加洗"><a href="#衣服加洗" class="headerlink" title="衣服加洗"></a>衣服加洗</h4><p>聽說昨天(9/9)跑完3000後其它隊的狀況普遍挺慘，所以之後變成兩天跑一次，今天改成開合跳30下*2，不過通常開合跳不會留什麼汗，所以今天晨間體能做完大家都不會換衣服。</p><p>當晚聽到了一個很傻眼的事情，學長說今天也要加洗一件內衣。公發的內衣總共就三件，今天要洗兩件，通常開合跳沒流汗大家也不會換，所以我們等於要加洗一件”乾淨”的內衣，然後一件晚上洗完澡換上。<strong>如果當初沒有多買的話，等於明天跑3000就沒多的內衣可以換了(並且加洗的時候如果沒有交出加洗的衣服也會被罵)。</strong></p><p>雖然我是有多買一件所以比較沒有這個困擾，但還是不懂上面的人到底是在想什麼…當天晚上整個寢室都在為了這件事抱怨連連。</p><h4 id="抱怨大會"><a href="#抱怨大會" class="headerlink" title="抱怨大會"></a>抱怨大會</h4><p>晚上還有另外一件事情: 我們被帶去做適性測驗，然後裡面的人在做的時候長官<strong>開放外面的人可以出來Diss任何他不爽的事情，並且不會受到任何處罰</strong>。所以陸續有一些勇者就會跳出來開砲，有些人火力比較小，不過也有些撿到大砲的，這種時候場面就很火爆，下面列了幾個我覺得當晚聽到最經典的:</p><ol><li>「長官們事情都不溝通好在下達，害我們一直改，然後又在那邊罵我們動作慢，到底是誰的問題?」</li><li>「長官在那邊說用餐的時候要有禮貌，板凳拿的時候不要有聲音。結果被看到長官離開位置的時候居然是用腳把板凳踹進去」</li><li>「我覺得研發替代役和在座的學長長官都是制度下的受害者，想想我們出去都是要幫科技公司做事的，大家都百萬年薪起跳，現在還要在這邊關十五天，這段期間不知道讓國家損失了多少錢去了xD」</li></ol><h3 id="9-11-五"><a href="#9-11-五" class="headerlink" title="9/11(五)"></a>9/11(五)</h3><h4 id="變得很短的用餐時間"><a href="#變得很短的用餐時間" class="headerlink" title="變得很短的用餐時間"></a>變得很短的用餐時間</h4><p>在前一天的抱怨大會的時後，其實還有過水班的同學跳出來抱不平說「每次過水班都要等大家用餐完才能開始清洗餐具，沒什麼休息時間晚回去又要被長官念」，不知道是不是因為這個緣故，從隔天開始我們的用餐時間開始被壓縮到非常短。</p><p>到底有多短呢？</p><p>以今天早上來說好了，再跑完3000累到喘不過來，被帶進餐廳後分隊長宣布用餐時間只有15分鐘。</p><p>你可能會覺得這15分鐘還好，但其實<strong>這15分鐘還包含了洗餐具的時間</strong>，一群人在排隊+洗餐具要花掉的時間至少就快10分鐘了，所以實際上只有大約5分鐘可以吃東西。我那時東西根本是搭配豆漿用吞的，時間急到吃得有點不舒服。</p><h4 id="團體照拍攝"><a href="#團體照拍攝" class="headerlink" title="團體照拍攝"></a>團體照拍攝</h4><p>今天午餐的甜湯居然是珍珠奶茶，雖然喝起來像珍珠奶茶水，不過有珍珠可以咬還是很開心。然後由於要去拍團體照，所以中午用餐時間更趕了，大家都趕快吃一吃回去換制服，然後去拍團體照。</p><p>團體照會拍三種Pose，第一張是正常的照片，二三張可以比動作，動作自己想，而坐在第一排的長官們會配合我們，於是我們就比了下面這兩個動作</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1601186736/blog_posts/images_tyeejx.jpg" alt=""></p><p>關於第二個動作…其實…我沒想到長官們真的會跟我們一起比…</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_300/v1601186839/blog_posts/1561674586_nuo8ur.jpg" alt=""></p><h3 id="9-12-六"><a href="#9-12-六" class="headerlink" title="9/12(六)"></a>9/12(六)</h3><h4 id="夜間站哨"><a href="#夜間站哨" class="headerlink" title="夜間站哨"></a>夜間站哨</h4><p>印象中似乎是從第三天開始，大家要輪流夜間站哨，然後終於在這天輪到我跟我鄰員。要站哨的人要提早大概半小時起床，然後換上制服站一個小時。其實站哨本身並不會很累，只是如果輪班的時間在半夜兩三點這種…回去可以睡的時間其實也沒剩多少了。</p><h4 id="基本教練"><a href="#基本教練" class="headerlink" title="基本教練"></a>基本教練</h4><p>這幾天晚上開始在練基本教練，然後帶我們的分隊長是五中裡面最兇的一個分隊長，練基教期間只要一個小錯誤就會被吼到臭頭。那天晚上他心情特不好，然後就開始狂飆</p><p>「爛！」「你們有夠爛！」「研替爛！」「沒帶過這麼爛的！」</p><p>回寢室後大家就開始拿這個來自嘲，「反正我們就爛嘛…」，算是苦中作樂吧xD</p><p>題外話，在基教分隊長罵人的時候，有個學長都會在後面當跟屁蟲跟著罵「懷疑啊！分隊長會騙你嗎！」（反骨男孩梗)</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_300/v1601188014/blog_posts/%E6%88%AA%E5%9C%96_2020-09-27_%E4%B8%8B%E5%8D%882.26.31_r5olhd.png" alt=""></p><p>不過那個學長實在是講得很好笑，所以每次我們聽到的時候其實都在偷偷憋笑。</p><hr><p>為了避免篇幅過長，後半部分的日記，請至<a href="https://meetonfriday.com/posts/53e1ff1f/">研發替代役替第九十梯-15天生存紀實-2</a>繼續閱讀。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>[Tensorflow]從Pytorch到TF2的學習之路 - Different Padding Algorithms</title>
      <link href="/posts/1244cf1f/"/>
      <url>/posts/1244cf1f/</url>
      
        <content type="html"><![CDATA[<p>【[Tensorflow]從Pytorch到TF2的學習之路】所有文章:</p><ul><li><a href="https://meetonfriday.com/posts/1244cf1f/">[Tensorflow]從Pytorch到TF2的學習之路 - Different Padding Algorithms</a></li><li><a href="https://meetonfriday.com/posts/877f4063/">[Tensorflow]從Pytorch到TF2的學習之路 - Training mode v.s. Inference mode</a></li><li><a href="https://meetonfriday.com/posts/3e428c8a/">[Tensorflow]從Pytorch到TF2的學習之路 - Custom Model &amp; Custom training</a></li></ul><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在Conv layer和Pooling layer的時候，由於kernel size和stride的設置，Input有可能會在操作過程中越變越小。為了使得圖片在這個過程中保持相同的輸出，此時我們就會對input加上padding(通常是補0操作)。<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1598519313/blog_posts/0rs9l_jjogjj.gif" alt=""></p><p>不過<strong>在Tensorflow和Pytorch中對於padding這件事有一點小差異</strong>，像是Tensorflow的padding參數就提供了<code>SAME</code>和<code>VALID</code>，但在Pytorch的文件中我們並沒看到類似的參數，究竟這不同框架之間padding的差異到底在哪裡呢? </p><p>這篇文章透過簡單的例子搭配程式碼進行講解，最後搭配一些網路上的討論跟介紹幫助大家釐清TF/Pytorch的Padding觀念。</p><a id="more"></a><h2 id="Padding-in-Tensorflow2"><a href="#Padding-in-Tensorflow2" class="headerlink" title="Padding in Tensorflow2"></a>Padding in Tensorflow2</h2><p>Padding在TF1/TF2中似乎沒有做太大改動，不過下面我們以TF2(下面統稱TF)的程式搭配來輔助講解。</p><p>首先，在TF的Conv系列(例如<a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv1d" target="_blank" rel="noopener">tf.nn.conv1d</a>)和Pooling系列(例如<a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool1d" target="_blank" rel="noopener">tf.nn.max_pool1d</a>)都可以看到關於Padding參數有兩個方法可以使用:</p><ul><li><strong>SAME</strong></li><li><strong>VALID</strong></li></ul><p>不過對於這兩個方法的細節似乎沒有太多著墨，下面針對這兩個方法來進行介紹。</p><p>首先為了方便介紹我們先考慮一維的資料，然後下面我們都使用這組作為我們的input。假設我們的輸入為:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs: <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span></span><br></pre></td></tr></table></figure><p>我們先用幾行的程式碼把資料建立起來<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">data = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]) <span class="comment"># shape: (dimension, )</span></span><br><span class="line">data = tf.reshape(data, (<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>)) <span class="comment"># reshape to (batch, dimension, channel)</span></span><br><span class="line">print(<span class="string">'input: '</span>, data)</span><br><span class="line"><span class="comment"># input:  tf.Tensor(</span></span><br><span class="line"><span class="comment"># [[[1]</span></span><br><span class="line"><span class="comment">#   [2]</span></span><br><span class="line"><span class="comment">#   [3]</span></span><br><span class="line"><span class="comment">#   [4]</span></span><br><span class="line"><span class="comment">#   [5]</span></span><br><span class="line"><span class="comment">#   [6]</span></span><br><span class="line"><span class="comment">#   [7]</span></span><br><span class="line"><span class="comment">#   [8]</span></span><br><span class="line"><span class="comment">#   [9]]], shape=(1, 9, 1), dtype=int32)</span></span><br></pre></td></tr></table></figure></p><p>儘管一開始我們說資料維度是一維的，但我們仍然轉換到一個三維的space以符合Tensorflow的格式。<strong>這邊的維度(1, 9, 1)分別代表了(batch, dimension, channel)，符合TF data format的預設NHWC格式(channel last)</strong></p><ul><li>如果是二維的資料則是(batch, width, height, channel)</li><li>關於TF的data format可以參考<a href="https://stackoverflow.com/questions/56754574/channels-first-vs-channels-last-what-do-these-mean" target="_blank" rel="noopener">Channels first vs Channels last - what do these mean?</a></li></ul><h3 id="padding-’VALID’"><a href="#padding-’VALID’" class="headerlink" title="padding=’VALID’"></a>padding=’VALID’</h3><p>對於padding=VALID，就是<strong>不做padding</strong>的意思，不夠的部分我就直接砍掉，所以資料長度很理所當然的會變小。</p><ul><li>其實覺得選用VALID這個字很容易讓人搞混，網路上也有人說或許用NO-PADDING會更加清楚</li><li>在這個情況下，output shape為$\left\lceil\frac{W-K+1}{S}\right\rceil$，$W$是input shape、$K$是kernel size，而$S$則是stride</li></ul><p>現在考慮在padding=’VALID’下，以下設置搭配max_pool1d的padding結果:</p><ul><li>kernel=4, stride=4</li></ul><figure class="highlight"><table><tr><td class="code"><pre><span class="line">inputs: <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> (<span class="number">9</span>)</span><br><span class="line">        |-----|             -&gt; output 4 </span><br><span class="line">                |-----|     -&gt; output 8</span><br></pre></td></tr></table></figure><p>可以發現多餘的部分，也就是9，直接被捨棄掉了。對照一下程式的結果發現是吻合的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(tf.nn.max_pool1d(data, ksize=<span class="number">4</span>, strides=<span class="number">4</span>, padding=<span class="string">'VALID'</span>))</span><br><span class="line"><span class="comment"># tf.Tensor(</span></span><br><span class="line"><span class="comment"># [[[4]</span></span><br><span class="line"><span class="comment">#   [8]]], shape=(1, 2, 1), dtype=int32)</span></span><br></pre></td></tr></table></figure></p><p>再來另外一個例子想想看，以下設置的時候輸出應該是什麼?</p><ul><li>kernel=4, stride=1<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(tf.nn.max_pool1d(data, ksize=<span class="number">4</span>, strides=<span class="number">1</span>, padding=<span class="string">'VALID'</span>))</span><br><span class="line"><span class="comment"># tf.Tensor(</span></span><br><span class="line"><span class="comment"># [[[4]</span></span><br><span class="line"><span class="comment">#   [5]</span></span><br><span class="line"><span class="comment">#   [6]</span></span><br><span class="line"><span class="comment">#   [7]</span></span><br><span class="line"><span class="comment">#   [8]</span></span><br><span class="line"><span class="comment">#   [9]]], shape=(1, 6, 1), dtype=int32)</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="padding-’SAME’"><a href="#padding-’SAME’" class="headerlink" title="padding=’SAME’"></a>padding=’SAME’</h3><p>對於padding=’SAME’，代表<strong>使用padding(default zero padding)來調整shape</strong></p><ul><li>output shape公式為$\left\lceil\frac{W}{S}\right\rceil$</li></ul><p>實際來看一下幾個例子，首先考慮和上面相同的case在padding=’SAME’下會是怎樣?</p><ul><li>kernel size=4, stride=4<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(tf.nn.max_pool1d(data, ksize=<span class="number">4</span>, strides=<span class="number">4</span>, padding=<span class="string">'SAME'</span>))</span><br><span class="line"><span class="comment"># tf.Tensor(</span></span><br><span class="line"><span class="comment"># [[[3]</span></span><br><span class="line"><span class="comment">#   [7]</span></span><br><span class="line"><span class="comment">#   [9]]], shape=(1, 3, 1), dtype=int32)</span></span><br></pre></td></tr></table></figure></li></ul><p>…咦? </p><p>為什麼會是這種神奇的輸出結果? 實際上TF在padding的時候把我們的input變成了下面這樣<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">inputs: <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">        |-----|                 -&gt; output 3 </span><br><span class="line">                |-----|         -&gt; output 7</span><br><span class="line">                        |-----| -&gt; output 9</span><br></pre></td></tr></table></figure><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595269674/blog_posts/Zb4LabP_j8hkod.jpg" alt=""></p><p>這個神奇的補0方式是什麼鬼? 為什麼右邊多補了1個0?</p><p>要理解這個首先就必須知道output shape真正的計算公式其實是長怎樣的:</p><script type="math/tex; mode=display">\mathrm{output\_shape}=\frac{\mathrm{W}+2 \times \text { padding }-\text { dilation } \times(\text { kernel_size }-1)-1}{\text { stride }}+1</script><p>dilation是另外一種捲積操作，一般預設為1，這裡先忽略他。於是把上面式子簡化一下可以得到</p><script type="math/tex; mode=display">\mathrm{output\_shape}=\frac{\mathrm{W}+2 \times \text { padding }-\text { kernel_size }}{\text { stride }}+1</script><ul><li>TF2透過這個公式反推出總共要補0的數量($2 \times \text { padding }$)</li><li>然後對input的左右各補padding行(二維的話則是上下左右)</li><li>然而，<strong>當($2 \times \text { padding }$)為奇數時，TF會對右邊(二維時則是右邊跟下面)多補一行0，確保output shape是整數</strong><ul><li>你應該也發現了這其實並不是一個對稱的padding，所以這種方式也被稱之<strong>Asymmetric padding</strong></li><li>TF對右下多補0的操作和caffe不同(caffe會對左上補)，所以在框架轉換的時候可能會出現問題，詳見<a href="https://stackoverflow.com/questions/42924324/tensorflows-asymmetric-padding-assumptions" target="_blank" rel="noopener">Tensorflow’s asymmetric padding assumptions</a></li></ul></li></ul><p>了解了padding=’SAME’所做的事情後，整個流程其實可用下面這個更新公式來得到，透過公式直接算出左右補0所需要的增加的欄位數量</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pad_width: 寬度方向填充0的行數</span></span><br><span class="line"><span class="comment"># pad_left, pad_right: 分别代表左右方向填充0的行數</span></span><br><span class="line"><span class="comment"># 二維的pad_top, pad_bottom依此類推</span></span><br><span class="line">pad_width = max((out_width<span class="number">-1</span>) * strides_width + kernel_size - in_width, <span class="number">0</span>)</span><br><span class="line">pad_left = pad_width // <span class="number">2</span> <span class="comment"># 向下取整</span></span><br><span class="line">pad_right  = pad_width - pad_left</span><br></pre></td></tr></table></figure><p>這個你也可以在TF的source code看到相關的code:</p><ul><li><a href="https://github.com/tensorflow/tensorflow/blob/8b722cf5ef234c187d6e96345c8715548a13c4f1/tensorflow/python/ops/nn_ops.py#L2718-L2728" target="_blank" rel="noopener">tf.nn.atrous_conv2d_transpose()</a>，implemented in python<ul><li>看標註區段(計算pad_left和pad_right)的部分就好，前面的計算其實是再做另外一種padding(FULL padding)，常被用在DeConv中，用來放大input shape</li><li>不過padding=’FULL’這個參數只在原本的Keras(不是指TF2的keras API)有，在TF中是透過直接設置pad_width=kernel_size-1來達成，細節這裡不做太多著墨</li></ul></li><li><a href="https://github.com/tensorflow/tensorflow/blob/8eaf671025e8cd5358278f91f7e89e2fbbe6a26b/tensorflow/core/kernels/conv_ops.cc#L274-L301" target="_blank" rel="noopener">tf.nn.conv2d()</a>，implemented in C </li></ul><p>好了，講了這麼多，再看一次剛剛問題的解答，現在應該知道為什麼剛剛的例子最右邊會補2個0，而左邊只補了1個0了，這裡我們再貼一次剛剛的padding結果幫助你複習<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">inputs: <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">        |-----|                 -&gt; output 3 </span><br><span class="line">                |-----|         -&gt; output 7</span><br><span class="line">                        |-----| -&gt; output 9</span><br></pre></td></tr></table></figure></p><p>再來回顧VALID的第二個case，在padding=’SAME’的時候下列設置輸出應該是什麼?</p><ul><li>kernel=4, stride=1<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(tf.nn.max_pool1d(data, ksize=<span class="number">4</span>, strides=<span class="number">1</span>, padding=<span class="string">'SAME'</span>))</span><br><span class="line"><span class="comment"># tf.Tensor(</span></span><br><span class="line"><span class="comment"># [[[3]</span></span><br><span class="line"><span class="comment">#   [4]</span></span><br><span class="line"><span class="comment">#   [5]</span></span><br><span class="line"><span class="comment">#   [6]</span></span><br><span class="line"><span class="comment">#   [7]</span></span><br><span class="line"><span class="comment">#   [8]</span></span><br><span class="line"><span class="comment">#   [9]</span></span><br><span class="line"><span class="comment">#   [9]</span></span><br><span class="line"><span class="comment">#   [9]]], shape=(1, 9, 1), dtype=int32)</span></span><br></pre></td></tr></table></figure></li></ul><p>在這個例子中，你會發現其實padding的狀況和前一個例子相同。</p><p>到這裡你應該對於TF的兩種padding方法有了充足的理解。最後再次強調，根據公式，<strong>padding=’SAME’只有當stride=1時才會確保output shape和input shape相同</strong>。</p><h2 id="Padding-in-Pytorch"><a href="#Padding-in-Pytorch" class="headerlink" title="Padding in Pytorch"></a>Padding in Pytorch</h2><p>Pytorch的padding參數預設是0，也就是<strong>預設不做padding</strong>(TF的padding=’Valid’)。</p><p>不過要做padding的話，就是需要按照Pytorch文件的公式來計算出正確output shape(寫Pytorch的都要對資料的維度很敏感，不然一個不小心shape弄錯模型就炸了…)，在<a href="https://pytorch.org/docs/master/generated/torch.nn.MaxPool1d.html" target="_blank" rel="noopener">torch.nn.MaxPool1d</a>中寫到output shape公式如下:</p><script type="math/tex; mode=display">\mathrm{L}_{\text {out }}=\left\lfloor\frac{\mathrm{L}_{\text {in }}+2 \times \text { padding }-\text { dilation } \times(\text { kernel size }-1)-1}{\text { stride }}+1\right\rfloor</script><ul><li>題外話，當初在那算二維的output shape算到崩潰，後來直接把公式變成了一個function，可以參考<a href="https://meetonfriday.com/posts/a418962a/">[Python]Utility function of calculate convolution output shape</a></li></ul><p>Pytorch的padding方法是Symmetric padding，也就是一維的時候對左右(二維則是對上下左右)補齊的數量是相同的，後續的操作出餘的部分就捨棄掉。</p><p>接下來我們來用code實際跑一下，首先我們準備和TF章節中相同的輸入:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs: <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span></span><br></pre></td></tr></table></figure></p><p>和TF不同的地方是data format要轉換一下，Pytorch預設採用的格式是channel first<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line">data = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]).view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>) <span class="comment"># (batch, channel, dimension)</span></span><br><span class="line">print(<span class="string">'input:'</span>, data, <span class="string">'shape:'</span>, data.shape)</span><br><span class="line"><span class="comment"># input: tensor([[[1., 2., 3., 4., 5., 6., 7., 8., 9.]]]) shape: torch.Size([1, 1, 9])</span></span><br></pre></td></tr></table></figure></p><p>然後一樣用max_pool1d來看一下在TF中測試過的兩組設置</p><ul><li>kernel=4, stride=4</li><li>kernel=4, stride=1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(torch.nn.functional.max_pool1d(data, kernel_size=<span class="number">4</span>, stride=<span class="number">4</span>))</span><br><span class="line"><span class="comment"># tensor([[[4., 8.]]])</span></span><br><span class="line">print(torch.nn.functional.max_pool1d(data, kernel_size=<span class="number">4</span>, stride=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[[ 4., 5., 6., 7., 8., 9.]]])</span></span><br></pre></td></tr></table></figure><p>由於預設參數padding=0，所以就行為和TF的VALID padding一模一樣。</p><p>再來，測試有指定padding參數的狀況下結果是什麼，考慮以下設置: </p><ul><li>kernel=4, stride=4, padding=1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.nn.functional.max_pool1d(c, kernel_size=<span class="number">4</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[[3., 7.]]])</span></span><br></pre></td></tr></table></figure><p>此時padding的情況如下<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">inputs: 0 1 2 3 4 5 6 7 (8 9 0) </span><br><span class="line">        |-----|                 -&gt; output 3 </span><br><span class="line">                |-----|         -&gt; output 7</span><br></pre></td></tr></table></figure></p><p>左右對稱補0後，多出來的部分(8 9 0)被捨棄了(也可以想成，Pytorch的作法是symmetric padding後，將output當作新的input進行VALID padding操作)</p><p>最後，Pytorch要怎麼做到TF的SAME padding(Asymmetric padding)呢?<br>你可以自行算好input周遭的padding數，透過<code>torch.nn.ZeroPad2d()</code>來實踐。不過這個功能目前似乎沒打算到<code>torch.nn.Conv2d()</code>等Conv function中。對此論壇上討論了很久了，目前基於效能等的考量所以不打算加進去，有興趣了解來龍去脈的可以參考下列Issue:</p><ul><li><a href="https://github.com/pytorch/pytorch/issues/3867" target="_blank" rel="noopener">[Feature Request] Implement “same” padding for convolution operations?</a></li><li><a href="https://github.com/DAA233/learning-notes/issues/16" target="_blank" rel="noopener">如何在 PyTorch 中实现 TensorFlow 里的 ‘SAME’ padding 和 ‘VALID’ padding？</a></li></ul><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>前面辛苦寫了這麼久，其實懶人包就幾行: </p><ul><li>在TF中，有兩種padding algorithm: SAME padding &amp; VALID padding<ul><li>VALID代表不做任何padding，在運算中多餘的部分就會直接捨去<ul><li>output shape: $\left\lceil\frac{W-K+1}{S}\right\rceil$</li></ul></li><li>SAME代表對input周遭進行padding，並且當stride=1的時候會使output保持跟input相同的大小<ul><li>output shape: $\left\lceil\frac{W}{S}\right\rceil$</li></ul></li></ul></li><li>在Pytorch中，預設是padding參數是0，也就是TF的VALID padding<ul><li>如果有指定padding參數，則會對四周做對稱補0，並且運算中多餘的部分會直接捨去(周圍對稱補0後再做VALID padding)<ul><li>output shape: <script type="math/tex">\mathrm{L}_{\text {out }}=\left\lfloor\frac{\mathrm{L}_{\text {in }}+2 \times \text { padding }-\text { dilation } \times(\text { kernel size }-1)-1}{\text { stride }}+1\right\rfloor</script></li></ul></li></ul></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://oldpan.me/archives/tf-keras-padding-vaild-same" target="_blank" rel="noopener">深度学习-TF、keras两种padding方式：vaild和same</a></li><li><a href="https://www.cnblogs.com/zyly/p/8697985.html" target="_blank" rel="noopener">第三节，TensorFlow 使用CNN实现手写数字识别(卷积函数tf.nn.convd介绍)</a></li><li><a href="https://stackoverflow.com/questions/34835503/tensorflow-where-is-tf-nn-conv2d-actually-executed" target="_blank" rel="noopener">Tensorflow: Where is tf.nn.conv2d Actually Executed?</a></li><li><a href="https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t/37674568#37674568" target="_blank" rel="noopener">What is the difference between ‘SAME’ and ‘VALID’ padding in tf.nn.max_pool of tensorflow?</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> pytorch </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Very Deep Convolutional Networks For Large-Scale Image Recognition</title>
      <link href="/posts/5f745d96/"/>
      <url>/posts/5f745d96/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>VGG，ILSVRC 2014的亞軍(同年冠軍為GoogLeNet)，儘管是亞軍但他提出來的設計構想對於CNN系列也有著很重要的影響，至今仍然是非常知名的一個Model。</p><p>paper網址: <a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1409.1556.pdf</a></p><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><blockquote><p>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3×3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers</p></blockquote><p>本篇文章主要的貢獻在於: <strong>透過小的Conv filter(3x3)不斷疊加，使得CNN模型可以達到一個更深的層數且得到更好的精準度</strong>。</p><p>儘管這個概念很簡單，但是非常重要，當時對於filter參數到底要設大還有著許多的疑問和研究，例如AlexNet採用了極大的size(11x11)、ZFNet將size調小了但仍然使用到了7x7，GoogLeNet同時使用了不同的filter size…</p><p>但VGG這篇卻告訴你: <strong>3x3就夠了！</strong></p><a id="more"></a><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><blockquote><p>In this paper, we address another important aspect of ConvNet architecture design – its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 × 3) convolution filters in all layers.</p></blockquote><p>這篇主要關注在ConvNet的設計架構上一個很重要的觀點: 深度，透過不斷堆疊小的Conv，我們可以走得更高更遠</p><p>這時候應該要再把NIN那個meme拿出來用<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595358824/blog_posts/a88_pc8dhu.jpg" alt=""></p><h2 id="CONVNET-CONFIGURATIONS"><a href="#CONVNET-CONFIGURATIONS" class="headerlink" title="CONVNET CONFIGURATIONS"></a>CONVNET CONFIGURATIONS</h2><h3 id="ARCHITECTURE"><a href="#ARCHITECTURE" class="headerlink" title="ARCHITECTURE"></a>ARCHITECTURE</h3><ul><li>輸入的圖像都是224x224的RGB image，唯一做的preprocessing就是減去RGB的mean</li><li>然後圖片會經過一堆3x3 Conv with stride 1, padding 1<ul><li>為什麼是3x3? <strong>因為這是捕捉周圍(上下左右)關係的最小size</strong></li><li>stride跟padding這樣的設置使得Conv後圖片大小仍然不變</li><li>每個Conv後面都有接activation function</li></ul></li><li>這邊論文提出了多種不同的架構，其中一種架構也引入了NIN的1x1Conv(參考<a href="https://meetonfriday.com/posts/a151bfa2/">[論文速速讀]Network In Network</a>)來提高Non-linearity</li><li>經過總共5次的maxpooling，maxpooling使用2x2的size, with stride 2<ul><li>但<strong>不是所有Conv後面都會接maxpooling</strong>，參照後面架構圖就知道了</li></ul></li></ul><p>Conv後面的Classifier的部分都由相同的架構組成: 3層的FC layer</p><ul><li>前兩層分別是4096個neurons</li><li>最後一層是1000類with softmax</li></ul><p>最後，關於其他的設置:</p><ul><li>Activation function都統一採用ReLU</li><li>除了其中一種架構外，其餘的架構都沒採用<a href="https://meetonfriday.com/posts/e54c12ea/">AlexNet的LRN(Local Response Normalization)</a>，他們發現<strong>LRN並不會提升效果反而增加了計算量</strong></li></ul><h3 id="CONFIGURATIONS"><a href="#CONFIGURATIONS" class="headerlink" title="CONFIGURATIONS"></a>CONFIGURATIONS</h3><p>總共有6種不同的架構被設計出來實驗，粗體字代表該架構比左邊的架構多出來的部分</p><ul><li>Conv3-64代表3x3的Conv，然後該層有64個channel</li><li>model D &amp; E 就是知名的<strong>VGG16</strong>和<strong>VGG19</strong></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1598000889/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-08-21_%E4%B8%8B%E5%8D%885.06.02_jikm11.png" alt=""></p><p>接下來關於參數量的部分:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1598001079/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-08-21_%E4%B8%8B%E5%8D%885.11.08_fstjkx.png" alt=""></p><p>儘管層數很深，但使用的參數量也沒超過像GoogLeNet這種使用較大kernel size的模型(GoogLeNet使用了144M)</p><h3 id="DISCUSSION"><a href="#DISCUSSION" class="headerlink" title="DISCUSSION"></a>DISCUSSION</h3><p>相較於以往在淺層使用較大的Conv使得模型學習到較大的感受野(Receptive fields)，如AlexNet和ZFNet，VGG透過<strong>使用多個3x3的Conv疊加一樣能夠學到大的Receptive fields</strong>，例如:</p><ul><li>兩層3x3 Conv可以學習到5x5的區塊</li><li>三層3x3 Conv可以學到7x7的區塊</li></ul><p>如此使用的參數量更少，下面這張圖我覺得就解釋得足夠清楚了，這張圖是本文精華，懂了就差不多惹:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1598001575/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-08-21_%E4%B8%8B%E5%8D%885.19.20_kykuco.png" alt=""></p><h2 id="CLASSIFICATION-FRAMEWORK"><a href="#CLASSIFICATION-FRAMEWORK" class="headerlink" title="CLASSIFICATION FRAMEWORK"></a>CLASSIFICATION FRAMEWORK</h2><p>關於一些訓練階段的細節，參數設置的部分我就跳過了。執得一提的是關於參數初始化的部分，文章中提到:</p><blockquote><p>The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fullyconnected layers with the layers of net A (the intermediate layers were initialised randomly)</p></blockquote><p>糟糕的參數初始化會使得深層網路學不好甚至不好收斂，所以他們從層數較淺的模型A開始訓練(shallow enough to be trained with random initialization)，然後拿A的參數去初始化其他模型的前四層Conv和後三層FC layer，其餘的部分仍然是random初始化。</p><p>關於training image的部分，為了得到224x224他們做了randomly crop，並且做了random horizontal flipping &amp; random RGB colour shift。同時，他們也考慮了多尺度下的模型訓練，這幾個部分有興趣的再去看原文。</p><h2 id="CLASSIFICATION-EXPERIMENTS"><a href="#CLASSIFICATION-EXPERIMENTS" class="headerlink" title="CLASSIFICATION EXPERIMENTS"></a>CLASSIFICATION EXPERIMENTS</h2><p>首先是針對單個模型的評估<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1598003085/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-08-21_%E4%B8%8B%E5%8D%885.43.59_ild1rk.png" alt=""></p><ul><li>LRN的效果並沒有比較好</li><li>加入了1x1 Conv確實效果有提升(C&gt;B)，但包含Non-linearity又同時能捕捉周圍關係的3x3 Conv能有更好的表現(D&gt;C)</li><li>其實模型D&amp;E在這個深度下，錯誤率無法再繼續往下降低了，畢竟層數越深information越難向下傳遞下去，<strong>面臨了Gradient Vanishing的問題</strong></li></ul><p>此外，他們也將B模型的2個3x3 Conv置換成一個由5層5x5 Conv組成的淺層網路(具有相同的Receptive field)，發現這樣做top-1 error上升了7%，證實了<strong>使用小filter的深層網路比大filter的淺層網路效果還要好</strong>。</p><p>對於圖像處理，他們也比較了使用multi-scale和multi-crop下的效果，數據證實效果確實有比較好的，不過這算是數據前處理的部份所以我就沒放上來做太多著墨。</p><p>最後在競賽上，和GoogLeNet相同VGG也採用了訓練多個模型然後做ensemble的做法來提昇精準度。下圖是跟其他模型的比較數據。</p><ul><li>在競賽結束後，VGG又有做了一些實驗，可以看到最後在單一個模型的效果上VGG是優於GoogLeNet的。</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1598004087/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-08-21_%E4%B8%8B%E5%8D%886.01.08_a0mcm8.png" alt=""></p><h3 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h3><ol><li>VGG很大的一個設計思維就是: 透過淺而深的卷積神經網路來做到高準確度的表現</li><li>為什麼用3x3 Conv? 因為這是最小能夠包含周遭資訊的size</li><li>AlexNet的LRN在這裡被證實沒有幫助，淘汰</li><li>看似模型越小越深越好，但其實在VGG19的時候深度就已經接近飽和了(因為Gradient Vanishing)，此時就需要考慮如何解決這個問題<ul><li>所以明年的冠軍ResNet準備跳出來摟，解決完問題就給你來個100多層真棒！</li></ul></li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://medium.com/@danjtchen/vgg-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%8E%9F%E7%90%86-d31d0aa13d88" target="_blank" rel="noopener">VGG<em>深度學習</em>原理</a></li><li><a href="https://meetonfriday.com/posts/18a141c2/">[DL]淺談CNN在Object Classification上的各種架構</a></li><li><a href="https://meetonfriday.com/posts/e54c12ea/">[論文速速讀]ImageNet Classification with Deep Convolutional Neural Networks</a></li><li><a href="https://meetonfriday.com/posts/a151bfa2/">[論文速速讀]Network In Network</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Tensorflow]從Pytorch到TF2的學習之路 - Custom Model &amp; Custom training</title>
      <link href="/posts/3e428c8a/"/>
      <url>/posts/3e428c8a/</url>
      
        <content type="html"><![CDATA[<p>【[Tensorflow]從Pytorch到TF2的學習之路】所有文章:</p><ul><li><a href="https://meetonfriday.com/posts/1244cf1f/">[Tensorflow]從Pytorch到TF2的學習之路 - Different Padding Algorithms</a></li><li><a href="https://meetonfriday.com/posts/877f4063/">[Tensorflow]從Pytorch到TF2的學習之路 - Training mode v.s. Inference mode</a></li><li><a href="https://meetonfriday.com/posts/3e428c8a/">[Tensorflow]從Pytorch到TF2的學習之路 - Custom Model &amp; Custom training</a></li></ul><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>TF2.0中採用了和Pytorch相同的<strong>Eager Mode</strong>，並且使用了大量的Keras API，使得我們可以像Pytorch一樣透過建立動態圖來操作我們的運算的同時，能夠更有效率地來設計&amp;訓練我們的深度學習模型。但在寫法上，Pytorch和TF2的到底有著什麼樣的差異呢? </p><p>這篇文章針對<strong>客製化模型</strong>和<strong>自定義訓練循環</strong>這兩個部分來撰寫，分別介紹使用Pytorch和TF2的不同，期望讓大家能夠快速地切換到不同的深度學習框架上。</p><a id="more"></a><h2 id="客製化模型的不同之處"><a href="#客製化模型的不同之處" class="headerlink" title="客製化模型的不同之處"></a>客製化模型的不同之處</h2><p>不囉So，咱們直接上code，先看一下兩個版本的程式碼，然後一併介紹他們的差異</p><p>首先先看Pytorch版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomModel</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(self).__init__()</span><br><span class="line">        <span class="comment"># define your layer</span></span><br><span class="line">        self.fc1 = torch.nn.Linear(...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        output = self.fc1(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>再來看TF2版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomModel</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(self).__init__()</span><br><span class="line">        <span class="comment"># define your layer</span></span><br><span class="line">        self.fc1 = tf.keras.layers.Dense(...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        output = self.fc1(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>這邊簡單介紹一下差異的部分:</p><ul><li>Pytorch<ol><li>在Pytorch中，如果要自定義一個模型，必須要繼承<code>torch.nn.Module</code>，然後實作<code>__init__(self)</code>和<code>forward(self)</code>兩個function</li><li>layer的定義是透過<code>torch.nn</code>來宣告</li></ol></li><li>TF2<ol><li>而在TF2中，則是透過了Keras Model API來實作，因此必須繼承<code>tf.keras.Model</code>並且實作<code>__init__(self)</code>和<code>call(self)</code>兩個function<ul><li>實際上<code>tf.keras.Model</code>是繼承自<code>tf.Module</code>，所以也可以直接繼承<code>tf.Module</code>來實作自己的Model</li><li>不過繼承<code>tf.keras.Model</code>還可以使用<code>Model.fit()</code>, <code>Model.evaluate()</code>和<code>Model.save()</code>這些操作，比方說<a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class" target="_blank" rel="noopener">官方文件的範例code</a>，繼承<code>tf.Module</code>的話就要自己寫訓練循環</li></ul></li><li>layer的定義是透過<code>tf.keras.layers</code>來宣告</li><li><code>tf.Module</code>以及被他繼承的類別皆提供很方便的properties: <strong>variables</strong> and <strong>trainable_variables</strong>，使得我們能夠很好的管理模型的參數</li></ol></li></ul><p>最後，在一篇<a href="https://tf.wiki/zh_hans/basic/models.html#keras" target="_blank" rel="noopener">教學文章</a>中看到了下面一個問題: <strong>为什么模型类是重载 call() 方法而不是 __call__() 方法？</strong>，覺得這個觀念也值得一提，所以把內容直接複製上來</p><blockquote><p>在 Python 中，对类的实例 myClass 进行形如 myClass() 的调用等价于 myClass.__call__() （具体请见本章初 “前置知识” 的 __call__() 部分）。那么看起来，为了使用 y<em>pred = model(X) 的形式调用模型类，应该重写 \</em>_call<em>_() 方法才对呀？原因是 Keras 在模型调用的前后还需要有一些自己的内部操作，所以暴露出一个专门用于重载的 call() 方法。 tf.keras.Model 这一父类已经包含 \</em>_call__() 的定义。 __call__() 中主要调用了 call() 方法，同时还需要在进行一些 keras 的内部操作。这里，我们通过继承 tf.keras.Model 并重载 call() 方法，即可在保持 keras 结构的同时加入模型调用的代码。</p></blockquote><h2 id="自定義訓練循環的不同之處"><a href="#自定義訓練循環的不同之處" class="headerlink" title="自定義訓練循環的不同之處"></a>自定義訓練循環的不同之處</h2><p>再來來看一下一個訓練的起手式寫法上，兩者有著什麼樣的差異:</p><p>首先從Pytorch看起，在<a href="https://meetonfriday.com/posts/18392404/">[Pytorch]zero_grad()和backward()使用技巧</a>中，我們提到了Pytorch訓練的起手式:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx, (batch_x, batch_y) <span class="keyword">in</span> enumerate(data_loader):</span><br><span class="line">    output = model(batch_x)</span><br><span class="line">    loss = criterion(output, batch_y)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><p>短短幾行裡面做了很多事情:</p><ol><li>將data傳入model進行forward propagation</li><li>計算loss</li><li>清空前一次的gradient</li><li>根據loss進行back propagation，計算gradient</li><li>做gradient descent</li></ol><p>接下來我們來看一下TF2.0是如何撰寫的，儘管繼承<code>tf.keras.Model</code>的Model可以無腦<code>model.fit()</code>和<code>model.predict()</code>，但我們先看如果要自己寫訓練循環時應該怎麼做:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch_idx <span class="keyword">in</span> range(batch_num):</span><br><span class="line">    <span class="comment"># 由於我還不太熟Pytorch的dataloader在TF2要怎麼寫，所以這裡寫得跟上面有點不同</span></span><br><span class="line">    <span class="comment"># 主要就是取出資料的意思</span></span><br><span class="line">    batch_X, batch_y = data_loader.get_batch(batch_size)</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        output = model(batch_X)</span><br><span class="line">        loss = criterion(batch_y, output) <span class="comment"># calculate loss</span></span><br><span class="line">    grads = tape.gradient(loss, model.variables)</span><br><span class="line">    optimizer.apply_gradients(grad_and_vars=zip(grads, model.variables))</span><br></pre></td></tr></table></figure><p>TF2做了以下的事情:</p><ol><li>透過<code>tf.GradientTape()</code>紀錄並建構正向傳播的計算圖被包覆的操作</li><li>將data傳入model進行forward propagation</li><li>計算loss</li><li>根據loss計算對model.variables的梯度(使用<code>tf.GradientTape()</code>)</li><li>做gradient descent更新梯度<ul><li>透過<code>tf.keras.optimizer</code>更新，並且<code>apply_gradients</code>需要接收grad_and_vars參數(需要將gradient和參數用zip包起來餵進去)</li></ul></li></ol><p>對於<code>tf.GradientTape()</code>，我覺得知乎上面這篇文章: <a href="https://zhuanlan.zhihu.com/p/102207302" target="_blank" rel="noopener">tensorflow计算图与自动求导——tf.GradientTape</a>寫得很詳細，以下節錄幾個重點:</p><ol><li>預設watch_accessed_variables=True，也就是會記錄所有可訓練變數(trainable=True的tf.Variable)</li><li>也可以用<code>tape.watch()</code>來觀測特定的變數，即使他trainable=False</li><li>預設persistent = False，也就是<strong>和Pytorch一樣，計算圖一但執行完就會銷毀</strong>(為了節省記憶體)，如果需要保留計算圖可以設置persistent = True</li><li>在用<code>tape.gradient()</code>計算梯度時，如果計算失敗(例如計算圖中兩個點根本沒有連接)，預設會返回None，但其實也可以指定參數unconnected_gradients=tf.UnconnectedGradients.ZERO來設置返回0</li></ol><p>到這裡，了解了模型的寫法和起手式的寫法後，原本對於Pytorch較熟悉的人應該就能較輕鬆的寫出一個可以跑的TF2程式(也希望對於TF2的使用者這篇能夠幫助你快速熟悉Pytorch的寫法)。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models?hl=en" target="_blank" rel="noopener">Making new Layers &amp; Models via subclassing</a></li><li><a href="https://tf.wiki/zh_hans/basic/models.html#keras" target="_blank" rel="noopener">简单粗暴 TensorFlow 2</a></li><li><a href="https://meetonfriday.com/posts/18392404/">[Pytorch]zero_grad()和backward()使用技巧</a></li><li><a href="https://zhuanlan.zhihu.com/p/102207302" target="_blank" rel="noopener">tensorflow计算图与自动求导——tf.GradientTape</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> pytorch </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Tensorflow]從Pytorch到TF2的學習之路 - Training mode v.s. Inference mode</title>
      <link href="/posts/877f4063/"/>
      <url>/posts/877f4063/</url>
      
        <content type="html"><![CDATA[<blockquote><p>「這個故事是描寫一位從原本在寫Pytorch的熱血少年，因為工作需求所以開始跳槽Tensorflow2，立志寫出厲害的TF2程式碼，在台灣締造的偉大抒情史詩」 (改寫自烘焙王開頭旁白)</p></blockquote><p>【[Tensorflow]從Pytorch到TF2的學習之路】所有文章:</p><ul><li><a href="https://meetonfriday.com/posts/1244cf1f/">[Tensorflow]從Pytorch到TF2的學習之路 - Different Padding Algorithms</a></li><li><a href="https://meetonfriday.com/posts/877f4063/">[Tensorflow]從Pytorch到TF2的學習之路 - Training mode v.s. Inference mode</a></li><li><a href="https://meetonfriday.com/posts/3e428c8a/">[Tensorflow]從Pytorch到TF2的學習之路 - Custom Model &amp; Custom training</a></li></ul><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在pytorch中會使用<code>train()</code>, <code>eval()</code>來控制一些在訓練(training mode)/測試階段(inference mode)下執行不同的操作，比方說Dropout, BatchNormalization</p><ul><li>Dropout在inference mode下就不會在屏蔽neuron</li><li>BatchNormalization在inference mode下會使用training時得到的平均值作為alpha, beta的參數</li></ul><p>但TF沒有這兩個function，那應該要怎麼去操控training mode / inference mode呢? 本文透過Dropout, BatchNormalization的TF官方文件搭配Source code來深入研究。</p><a id="more"></a><h2 id="Dropout-training參數"><a href="#Dropout-training參數" class="headerlink" title="Dropout: training參數"></a>Dropout: training參數</h2><p>在TF中則是透過參數來控制，例如Dropout可以透過training參數來控制，在<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout" target="_blank" rel="noopener">官方文件</a>中寫到:</p><blockquote><p>Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit, training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer.</p></blockquote><p>也就是說透過<strong>設置training=True來開啟Dropout功能</strong>，在testing的時候設置training=False即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line">layer = tf.keras.layers.Dropout(<span class="number">.2</span>, input_shape=(<span class="number">2</span>,))</span><br><span class="line">data = np.arange(<span class="number">10</span>).reshape(<span class="number">5</span>, <span class="number">2</span>).astype(np.float32)</span><br><span class="line">print(data)</span><br><span class="line"><span class="comment"># [[0. 1.]</span></span><br><span class="line"><span class="comment">#  [2. 3.]</span></span><br><span class="line"><span class="comment">#  [4. 5.]</span></span><br><span class="line"><span class="comment">#  [6. 7.]</span></span><br><span class="line"><span class="comment">#  [8. 9.]]</span></span><br><span class="line"></span><br><span class="line">outputs = layer(data, training=<span class="literal">True</span>)</span><br><span class="line">print(outputs)</span><br><span class="line"><span class="comment"># [[ 0.    1.25]</span></span><br><span class="line"><span class="comment">#  [ 2.5   3.75]</span></span><br><span class="line"><span class="comment">#  [ 5.    6.25]</span></span><br><span class="line"><span class="comment">#  [ 7.5   8.75]</span></span><br><span class="line"><span class="comment">#  [10.    0.  ]], shape=(5, 2), dtype=float32)</span></span><br></pre></td></tr></table></figure><p>此外，文件中也提到<strong>training這個參數跟trainable這個參數是不同的，trainable是只說在propagation過程中要不要更新參數，但Dropout並沒有參數，所以Dropout設置trainable是沒有用的</strong>。</p><blockquote><p>(This is in contrast to setting trainable=False for a Dropout layer. trainable does not affect the layer’s behavior, as Dropout does not have any variables/weights that can be frozen during training.)</p></blockquote><p>最後，training參數的預設值又是什麼呢? </p><p>首先先看Dropout的文件中提到:</p><blockquote><p>When using model.fit, training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer.</p></blockquote><p><strong>除了呼叫model.fit()的情況下，不然需要手動設置training=True</strong></p><p>所以這樣是說training預設是False嗎? 在<a href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/base_layer.py#L945" target="_blank" rel="noopener">tensorflow/tensorflow/python/keras/engine/base_layer.py</a>中有提到對於training的設置考慮順序<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">945</span><br><span class="line">946</span><br><span class="line">947</span><br><span class="line">948</span><br><span class="line">949</span><br><span class="line">950</span><br><span class="line">951</span><br><span class="line">952</span><br><span class="line">953</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training mode for `Layer.call` is set via (in order of priority):</span></span><br><span class="line"><span class="comment"># (1) The `training` argument passed to this `Layer.call`, if it is not None</span></span><br><span class="line"><span class="comment"># (2) The training mode of an outer `Layer.call`.</span></span><br><span class="line"><span class="comment"># (3) The default mode set by `tf.keras.backend.set_learning_phase` (if set)</span></span><br><span class="line"><span class="comment"># (4) Any non-None default value for `training` specified in the call</span></span><br><span class="line"><span class="comment">#  signature</span></span><br><span class="line"><span class="comment"># (5) False (treating the layer as if it's in inference)</span></span><br><span class="line">args, kwargs, training_mode = self._set_training_mode(</span><br><span class="line">    args, kwargs, call_context)</span><br></pre></td></tr></table></figure></p><h2 id="BatchNormalization-trainable參數"><a href="#BatchNormalization-trainable參數" class="headerlink" title="BatchNormalization: trainable參數"></a>BatchNormalization: trainable參數</h2><p>而對於BatchNormalization，則是透過trainable來控制(因為BN有需要訓練的參數)，而在BN中預設trainable=True<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.BatchNormalization(</span><br><span class="line">    axis=<span class="number">-1</span>, momentum=<span class="number">0.99</span>, epsilon=<span class="number">0.001</span>, center=<span class="literal">True</span>, scale=<span class="literal">True</span>,</span><br><span class="line">    beta_initializer=<span class="string">'zeros'</span>, gamma_initializer=<span class="string">'ones'</span>,</span><br><span class="line">    moving_mean_initializer=<span class="string">'zeros'</span>, moving_variance_initializer=<span class="string">'ones'</span>,</span><br><span class="line">    beta_regularizer=<span class="literal">None</span>, gamma_regularizer=<span class="literal">None</span>, beta_constraint=<span class="literal">None</span>,</span><br><span class="line">    gamma_constraint=<span class="literal">None</span>, renorm=<span class="literal">False</span>, renorm_clipping=<span class="literal">None</span>, renorm_momentum=<span class="number">0.99</span>,</span><br><span class="line">    fused=<span class="literal">None</span>, trainable=<span class="literal">True</span>, virtual_batch_size=<span class="literal">None</span>, adjustment=<span class="literal">None</span>, name=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>在<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization" target="_blank" rel="noopener">官方文件</a>中也提到:</p><blockquote><p><strong>About setting layer.trainable = False on a BatchNormalization layer:</strong><br>The meaning of setting layer.trainable = False is to freeze the layer, i.e. its internal state will not change during training: its trainable weights will not be updated during fit() or train_on_batch(), and its state updates will not be run.</p><p>Usually, this does not necessarily mean that the layer is run in inference mode (which is normally controlled by the training argument that can be passed when calling a layer). “Frozen state” and “inference mode” are two separate concepts.</p><p>However, in the case of the BatchNormalization layer, setting trainable = False on the layer means that the layer will be subsequently run in inference mode (meaning that it will use the moving mean and the moving variance to normalize the current batch, rather than using the mean and variance of the current batch).</p><p>This behavior has been introduced in TensorFlow 2.0, in order to enable layer.trainable = False to produce the most commonly expected behavior in the convnet fine-tuning use case.</p></blockquote><p>儘管一般來說trainable=False只是代表凍結參數更新(Frozen state和inference mode是兩個不同的概念)，但對於BN來說則比較特殊，<strong>BN中設置trainable = False代表使用inference mode，也就是BN中的兩個參數alpha, beta會使用平均的值</strong>。</p><ul><li><strong>注意這是在TF2之後才引入的</strong>，對於TF1中，trainable=False只是凍結了參數，並不會變成inference mode(也就是不會使用平均的值帶入BN參數)</li></ul><p>至於trainable參數預設是什麼呢?</p><ul><li>在<a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank" rel="noopener">tf.Variable</a>中寫到預設Variable的trainable都是True</li><li>在<a href="https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/base_layer.py#L945" target="_blank" rel="noopener">tensorflow/tensorflow/python/keras/engine/base_layer.py</a>對於layer的預設也是True<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">             trainable=True,</span></span></span><br><span class="line"><span class="function"><span class="params">             name=None,</span></span></span><br><span class="line"><span class="function"><span class="params">             dtype=None,</span></span></span><br><span class="line"><span class="function"><span class="params">             dynamic=False,</span></span></span><br><span class="line"><span class="function"><span class="params">             **kwargs)</span>:</span></span><br></pre></td></tr></table></figure></li></ul><p>其他關於trainable參數要注意的地方</p><ul><li>對某一個layer設置trainable=True會連帶影響內部的所有layer的trainable參數</li><li>如果再compile()後才更改trainable參數，那要等到再次呼叫compile()才會更新</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout" target="_blank" rel="noopener">tf.keras.layers.Dropout</a></li><li><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization" target="_blank" rel="noopener">tf.keras.layers.BatchNormalization</a></li><li><a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank" rel="noopener">tf.Variable</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> pytorch </tag>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Network In Network</title>
      <link href="/posts/a151bfa2/"/>
      <url>/posts/a151bfa2/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>paper: <a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.4400.pdf</a></p><p>人在Computer Vision的江湖上，不能不知道的一篇論文: Network in Network (NIN)，在2014 ICLR上發表出來，至今已經有4000多次的cite。</p><p>Paper中首次提出兩個重要的概念，這兩個概念至今仍然被廣泛地使用，分別是:</p><ol><li><strong>1x1 Convolution</strong></li><li><strong>Global Average Pooling (GAP)</strong></li></ol><p>這些名詞很眼熟吧，不論是GoogLeNet, VGG, ResNet都可見到他們的身影，接下來就來看看原始論文是怎麼說的~</p><a id="more"></a><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ol><li>Convolution: 一般的CNN就是線性的卷積，然後透過非線性的激活函數來學習到非線性的特徵。作為替代，NIN提出了一個新的卷積結構: MLPConv，在Conv後接上非線性的MLP，來增強學習到的特徵複雜度</li><li>提出了使用Global Average Pooling(GAP)來替代原始Fully Connected Layer，GAP在使用上有著能降低參數量、更好解釋以及更不容易overfitting的特性</li></ol><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>The convolution filter in CNN is a generalized linear model (GLM) for the underlying data patch, and we argue that the level of abstraction is low with GLM. By abstraction we mean that the feature is invariant to the variants of the same concept</p></blockquote><p>Conv可以視為是一個廣義的線性模型，generalized linear model (GLM)，也就是說Conv背後假設input是線性可分的。但資料通常是非線性的，所以會搭配非線性的activation function來進行非線性變換，期望資料至少在feature space中是可以被分開的。</p><p>從上面這段來看我們知道，GLM對於非線性的表達能力較差，儘管我們透過了activation function來增加非線性的變換，但當資料足夠複雜的時候我們就必須增加更多的Conv+Activation的次數，如此增加了模型的複雜度。</p><p>故此作者提出了使用了一種微網路(micro network)的結構來取代GLM，使得Conv變成了一個非線性的操作，這邊使用的是<br>多層感知機(multilayer perceptron)</p><ul><li>首先他是個非線性函數</li><li>再來，他是可以透過back-propagation進行訓練的</li></ul><p>一般Conv和MLPConv的差異如下圖:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1597254515/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-08-13_%E4%B8%8A%E5%8D%881.47.45_kuaryt.png" alt=""></p><blockquote><p> In traditional CNN, it is difficult to interpret how the category level information from the objective cost layer is passed back to the previous convolution layer due to the fully connected layers which act as a black box in between.</p></blockquote><p>並且在最後，使用了GAP來取代CNN和FC層之間的連結，也就是每一個feature map變成一個score，總共有class個score。</p><p>使用GAP有幾個好處:</p><ul><li>原本使用CNN和FC層使得模型就像黑盒子，很難做出一些解釋性的分析，使用GAP使得<strong>每一張feature map有著一個對應的score，如此增加了feature與class之間的關係</strong>，在解釋層面上更加直觀</li><li><strong>GAP並不用任何的訓練參數</strong>，可以減少overfitting，但相對的也必須要比較多的訓練迭代</li><li>GAP融合了spatial information，使得模型更佳robust</li></ul><h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><p>先定義一下數學式: 一個Conv+activation function(ReLU)的操作可以比表達成如下公式</p><script type="math/tex; mode=display">f_{i, j, k}=\max \left(w_{k}^{T} x_{i, j}, 0\right)</script><p>(i, j)是圖中的pixel，k是channel</p><p>其他部分是在介紹線性CNN所面臨的問題，在上面Introduction講了，跳過。</p><h2 id="Network-In-Network"><a href="#Network-In-Network" class="headerlink" title="Network In Network"></a>Network In Network</h2><p>NIN其實就是在Conv+activation function之後，還多了一個MLP</p><script type="math/tex; mode=display">\begin{aligned}f_{i, j, k_{1}}^{1} &=\max \left(w_{k_{1}}^{1} x_{i, j}+b_{k_{1}}, 0\right) \\& \vdots \\f_{i, j, k_{n}}^{n} &=\max \left(w_{k_{n}}^{n} f_{i, j}^{n-1}+b_{k_{n}}, 0\right)\end{aligned}</script><blockquote><p>This cascaded cross channel parameteric pooling structure allows complex and learnable interactions of cross channel information.</p><p>The cross channel parametric pooling layer is also equivalent to a convolution layer with 1x1 convolution kernel. This interpretation makes it straightforawrd to understand the structure of NIN.</p></blockquote><p>而這個MLP其實就<strong>等價於一個跨通道(channel)的1x1 Conv，增加了非線性的變換以及學習到了跨通道之間的information</strong>。</p><p>所以整個CNN架構就變成醬子La:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1597404499/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-08-14_%E4%B8%8B%E5%8D%887.22.46_nxinge.png" alt=""></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>比較了CIFAR-10, CIFAR-100, Street View House Numbers, MNIST的dataset，都得到了較低的Test Error，數據的部分可以去細看論文。</p><p>比較值得一提的是GAP的實驗，在CIFAR-10上測試GAP是否真的比FC / FC+Dropout好</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1597406371/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-08-14_%E4%B8%8B%E5%8D%887.58.23_zfnv2p.png" alt=""></p><p>然後他們對了MLPConv的最後一層做了視覺化<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1597406744/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-08-14_%E4%B8%8B%E5%8D%888.03.51_hg4khr.png" alt=""></p><ul><li>因為用了GAP，所以最後一層有跟類別數相同的feature map</li><li>GAP將最後一層直接輸出與目標數量相同的feature maps，再將其映射到相對應的機率空間中，所以可以預期某個類別對應activations反應最大的feature map就在對應的編號上<ul><li>Ex: 圖中類別1所對應activations最大的就是在第一張feature map</li></ul></li></ul><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>NIN這篇最最最重要的事提出了兩個想法:</p><ul><li>1x1 Conv</li><li>GAP</li></ul><p>1x1 Conv除了能增加非線性變換，也能很方便地做升維/降維。</p><p>GAP替換掉了Conv和FC中間這一層的連接，並且起到一定程度的robust效果。此外，讓每一個feature map對應到一個類別這個想法也對後續的一些XAI(Explainable AI)起到了不少幫助(CAM我就是在說你)。</p><p>這兩個技術在後來大大的被用在各種Model中，所以說走Computer Vision的人不能不讀這篇R</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://allen108108.github.io/blog/2019/10/07/%5B%E8%AB%96%E6%96%87%5D%20Network%20In%20Network/" target="_blank" rel="noopener">[論文] Network In Network</a></li><li><a href="https://blog.csdn.net/ouyangfushu/article/details/90212925?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param" target="_blank" rel="noopener">NIN(Network in Network)学习笔记</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[OpenCV]Optical Flow介紹</title>
      <link href="/posts/e2795e5a/"/>
      <url>/posts/e2795e5a/</url>
      
        <content type="html"><![CDATA[<p>以下介紹來自<a href="https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html" target="_blank" rel="noopener">OpenCV的官方文獻</a>，這份文章是我在閱讀完畢後使用中文整理的筆記。</p><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><ol><li>了解Optical Flow的概念，以及如何使用<a href="https://zh.wikipedia.org/wiki/%E5%8D%A2%E5%8D%A1%E6%96%AF-%E5%8D%A1%E7%BA%B3%E5%BE%B7%E6%96%B9%E6%B3%95" target="_blank" rel="noopener">Lucas-Kanade method</a>來計算它</li><li>使用<code>cv.calcOpticalFlowPyrLK()</code>來追蹤影片的特徵點</li><li>使用<code>cv.calcOpticalFlowFarneback()</code>來計算稠密光流</li></ol><a id="more"></a><h2 id="Optical-Flow"><a href="#Optical-Flow" class="headerlink" title="Optical Flow"></a>Optical Flow</h2><p>光流(Optical Flow)，是透過物體或照相機的移動而造成連續兩幀之間圖像的移動過程，他是一個二維的向量場(vector field)，其中每一個vector都是從前一幀到下一幀的移動向量。</p><p>現在考慮下圖：</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1597219299/blog_posts/optical_flow_basic1_smlqo2.jpg" alt=""></p><p>圖中顯示了一顆球在連續五幀中的移動過程，白色箭頭為他的移動向量。</p><p>光流可以被應用在以下領域:</p><ol><li>Structure from Motion</li><li>Video Compression</li><li>Video Stabilization</li></ol><p>但光流法的限制其實也蠻強的，要使用他必須滿足下列的假設條件:</p><ol><li><strong>連續幀之間物體的像素強度(pixel intensities)是不變的</strong></li><li><strong>相鄰的像素之間具有相似的運動</strong><ul><li>這點其實蠻直觀的，除非在物體邊界上，不然同一個物體的相鄰pixel運動方向應該是一致的</li></ul></li></ol><p>假設，在第t幀某個pixel我們表示為$I(x, y, t)$，在t+1幀時它移動了$(dx, dy)$，由於像素點仍然是相同的那個點，我們可以這樣表示:</p><script type="math/tex; mode=display">I(x, y, t)=I(x+d x, y+d y, t+d t)</script><p>再來將右項透過泰勒展開，假設該移動很小，我們會得到(不考慮常數項)</p><script type="math/tex; mode=display">I(x, y, t)=I(x, y, t)+ \frac{\partial I}{\partial x} d x+\frac{\partial I}{\partial y} d y+\frac{\partial I}{\partial t} d t</script><p>移項後再同除與$dt$</p><script type="math/tex; mode=display">\frac{\partial I}{\partial x} \frac{d x}{d t}+\frac{\partial I}{\partial y} \frac{d y}{d t}+\frac{\partial I}{\partial t} \frac{d t}{d t}=0 \\\implies \frac{\partial I}{\partial x} \frac{d x}{d t}+\frac{\partial I}{\partial y} \frac{d y}{d t}+\frac{\partial I}{\partial t} =0</script><p>這個式子稱之Optical Flow equation，其中</p><ul><li>$\frac{\partial I}{\partial x}, \frac{\partial I}{\partial y}, \frac{\partial I}{\partial t}$分別是該pixel對於x, y以及時間的偏導，這都是已知的(只要給連續的兩幀就可以計算出來)</li><li><strong>兩個未知數</strong>: $\frac{\partial x}{\partial t}, \frac{\partial y}{\partial t}$ ，也就是該pixel對於x, y方向的瞬時速度</li></ul><p>所以現在有兩個未知數，但只有一個方程式。雖然我的高中數學大部分在學測考完的時候還給老師了，不過我還記得這樣是解不開的呢！</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="http://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_300/v1597224409/blog_posts/1e90ab90fb28f177add1b76daba30b77_u0kqqj.jpg" alt=""></p><p>我們需要更多的方程式來求解，為此有很多種方法被提出來解決這個問題，而其中一種就是Lucas-Kanade</p><h2 id="Lucas-Kanade-method"><a href="#Lucas-Kanade-method" class="headerlink" title="Lucas-Kanade method"></a>Lucas-Kanade method</h2><p>還記得光流的前提假設: <strong>相鄰的像素之間具有相似的運動</strong>，所以我們可以透過相鄰的點來一起計算相同的兩個未知數。Lucas-Kanade對於每一個pixel都取附近的3x3 patch，如此我們就有<strong>九個方程式求解兩個未知數</strong>，可以使用Least Square Method解出來。</p><p>但這樣只能處理小動作，遇到大幅的移動就失敗了，所以還結合了金字塔方法(pyramids)，使得在大尺度下大動作可以被視為小動作，小動作將會被移除。透過金字塔方法可以同時考慮到大尺度和小尺度的移動。</p><p>這邊注意到使用LS的時候需要確保矩陣是可逆的，如果不可逆就解不出來了。但很巧的在角點(corner)上的矩陣大多可逆(其實不巧，我還沒研究這邊QQ)，所以corner是一個很好做光流法的特徵點。</p><h2 id="Lucas-Kanade-Optical-Flow-in-OpenCV"><a href="#Lucas-Kanade-Optical-Flow-in-OpenCV" class="headerlink" title="Lucas-Kanade Optical Flow in OpenCV"></a>Lucas-Kanade Optical Flow in OpenCV</h2><p>官網中簡單的範例程式碼如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'This sample demonstrates Lucas-Kanade Optical Flow calculation. \</span></span><br><span class="line"><span class="string">                                              The example file can be downloaded from: \</span></span><br><span class="line"><span class="string">                                              https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'image'</span>, type=str, help=<span class="string">'path to image file'</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">cap = cv.VideoCapture(args.image)</span><br><span class="line"><span class="comment"># params for ShiTomasi corner detection</span></span><br><span class="line">feature_params = dict( maxCorners = <span class="number">100</span>,</span><br><span class="line">                       qualityLevel = <span class="number">0.3</span>,</span><br><span class="line">                       minDistance = <span class="number">7</span>,</span><br><span class="line">                       blockSize = <span class="number">7</span> )</span><br><span class="line"><span class="comment"># Parameters for lucas kanade optical flow</span></span><br><span class="line">lk_params = dict( winSize  = (<span class="number">15</span>,<span class="number">15</span>),</span><br><span class="line">                  maxLevel = <span class="number">2</span>,</span><br><span class="line">                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, <span class="number">10</span>, <span class="number">0.03</span>))</span><br><span class="line"><span class="comment"># Create some random colors</span></span><br><span class="line">color = np.random.randint(<span class="number">0</span>,<span class="number">255</span>,(<span class="number">100</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment"># Take first frame and find corners in it</span></span><br><span class="line">ret, old_frame = cap.read()</span><br><span class="line">old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)</span><br><span class="line">p0 = cv.goodFeaturesToTrack(old_gray, mask = <span class="literal">None</span>, **feature_params)</span><br><span class="line"><span class="comment"># Create a mask image for drawing purposes</span></span><br><span class="line">mask = np.zeros_like(old_frame)</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    ret,frame = cap.read()</span><br><span class="line">    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)</span><br><span class="line">    <span class="comment"># calculate optical flow</span></span><br><span class="line">    p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, <span class="literal">None</span>, **lk_params)</span><br><span class="line">    <span class="comment"># Select good points</span></span><br><span class="line">    good_new = p1[st==<span class="number">1</span>]</span><br><span class="line">    good_old = p0[st==<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># draw the tracks</span></span><br><span class="line">    <span class="keyword">for</span> i,(new,old) <span class="keyword">in</span> enumerate(zip(good_new, good_old)):</span><br><span class="line">        a,b = new.ravel()</span><br><span class="line">        c,d = old.ravel()</span><br><span class="line">        mask = cv.line(mask, (a,b),(c,d), color[i].tolist(), <span class="number">2</span>)</span><br><span class="line">        frame = cv.circle(frame,(a,b),<span class="number">5</span>,color[i].tolist(),<span class="number">-1</span>)</span><br><span class="line">    img = cv.add(frame,mask)</span><br><span class="line">    cv.imshow(<span class="string">'frame'</span>,img)</span><br><span class="line">    k = cv.waitKey(<span class="number">30</span>) &amp; <span class="number">0xff</span></span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">27</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># Now update the previous frame and previous points</span></span><br><span class="line">    old_gray = frame_gray.copy()</span><br><span class="line">    p0 = good_new.reshape(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p><p>針對裡面的幾個重點來說:<br>1.<code>cv.VideoCapture()</code>來讀取影片，一開始先讀兩幀才能拿來計算前一幀跟後一幀的關係</p><ol><li><code>cv.goodFeaturesToTrack()</code>來找到要追蹤的特徵點，也就是角點</li><li>每讀取一幀，就和上一幀的特徵點透過LK method<code>cv.calcOpticalFlowPyrLK()</code>來算出新特徵點的位置，然後繪圖</li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1597222796/blog_posts/opticalflow_lk_jxl1as.jpg" alt=""></p><h2 id="Dense-Optical-Flow-in-OpenCV"><a href="#Dense-Optical-Flow-in-OpenCV" class="headerlink" title="Dense Optical Flow in OpenCV"></a>Dense Optical Flow in OpenCV</h2><p>LK方法適用於計算稀疏光流，而要計算稠密光流(也就是對於每一個pixel都計算他的光流)的時候就會使用Gunner Farneback’s algorithm (which is explained in “Two-Frame Motion Estimation Based on Polynomial Expansion” by Gunner Farneback in 2003)，</p><p>這是一個基於多項式的求解方法，對於x, y將它轉換成一個二項式，然後去座位移估計，在文獻中並沒有做太多的介紹，有興趣的可以參考<a href="https://blog.csdn.net/xholes/article/details/79894340" target="_blank" rel="noopener">光流法：Farneback</a></p><p>OpenCV文檔提供的範例code如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">cap = cv.VideoCapture(cv.samples.findFile(<span class="string">"vtest.avi"</span>))</span><br><span class="line">ret, frame1 = cap.read()</span><br><span class="line">prvs = cv.cvtColor(frame1,cv.COLOR_BGR2GRAY)</span><br><span class="line">hsv = np.zeros_like(frame1)</span><br><span class="line">hsv[...,<span class="number">1</span>] = <span class="number">255</span></span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    ret, frame2 = cap.read()</span><br><span class="line">    next = cv.cvtColor(frame2,cv.COLOR_BGR2GRAY)</span><br><span class="line">    flow = cv.calcOpticalFlowFarneback(prvs,next, <span class="literal">None</span>, <span class="number">0.5</span>, <span class="number">3</span>, <span class="number">15</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">1.2</span>, <span class="number">0</span>)</span><br><span class="line">    mag, ang = cv.cartToPolar(flow[...,<span class="number">0</span>], flow[...,<span class="number">1</span>])</span><br><span class="line">    hsv[...,<span class="number">0</span>] = ang*<span class="number">180</span>/np.pi/<span class="number">2</span></span><br><span class="line">    hsv[...,<span class="number">2</span>] = cv.normalize(mag,<span class="literal">None</span>,<span class="number">0</span>,<span class="number">255</span>,cv.NORM_MINMAX)</span><br><span class="line">    bgr = cv.cvtColor(hsv,cv.COLOR_HSV2BGR)</span><br><span class="line">    cv.imshow(<span class="string">'frame2'</span>,bgr)</span><br><span class="line">    k = cv.waitKey(<span class="number">30</span>) &amp; <span class="number">0xff</span></span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">27</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">elif</span> k == ord(<span class="string">'s'</span>):</span><br><span class="line">        cv.imwrite(<span class="string">'opticalfb.png'</span>,frame2)</span><br><span class="line">        cv.imwrite(<span class="string">'opticalhsv.png'</span>,bgr)</span><br><span class="line">    prvs = next</span><br></pre></td></tr></table></figure><ol><li>一樣先連續讀取兩幀，然後呼叫<code>cv.calcOpticalFlowFarneback()</code></li><li>基於HSV色彩空間來配色</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html" target="_blank" rel="noopener">Optical Flow</a></li><li><a href="https://zhuanlan.zhihu.com/p/44859953" target="_blank" rel="noopener">Optical Flow介绍与代码实现</a></li><li><a href="https://blog.csdn.net/xholes/article/details/79894340" target="_blank" rel="noopener">光流法：Farneback</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Pytorch]Pack the data to train variable length sequences</title>
      <link href="/posts/4d6a906a/"/>
      <url>/posts/4d6a906a/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在NLP task中常常會遇到input data長度不固定的問題，一般來說此時有兩種方法處理: <strong>對資料padding補齊到相同長度，或是截斷超過某個長度的data</strong>。不過其實還有一種更有效率的方法，就是PackSequence，這篇文章將會針對Pytorch的PackSequence做一系列的介紹，包含概念和使用技巧。</p><p>關於PackSequence，在<a href="https://pytorch.org/docs/master/nn.html#utilities" target="_blank" rel="noopener">Pytorch的document</a>中<code>torch.nn.utils.rnn</code>定義了以下的object / function:</p><ul><li>nn.utils.rnn.PackedSequence</li><li><code>nn.utils.rnn.pad_sequence()</code>: Pad a list of variable length Tensors with padding_value</li><li><code>nn.utils.rnn.pack_padded_sequence()</code>: Packs a Tensor containing padded sequences of variable length.</li><li><code>nn.utils.rnn.pad_packed_sequence()</code>: Pads a packed batch of variable length sequences.</li><li><code>nn.utils.rnn.pack_sequence()</code>: Packs a list of variable length Tensors</li></ul><p>主要重點在於<code>pad_sequence()</code>, <code>pack_padded_sequence()</code>, <code>pad_packed_sequence()</code>，而<code>pack_sequence()</code>其實就是<code>pad_sequence()</code>+<code>pack_padded_sequence()</code>，最後再來談。</p><a id="more"></a><p>首先，讓我們先把該import的都先寫好：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> torch.nn.utils.rnn <span class="keyword">as</span> rnn_utils</span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br></pre></td></tr></table></figure></p><h2 id="pad-sequence"><a href="#pad-sequence" class="headerlink" title="pad_sequence"></a>pad_sequence</h2><p><code>pad_sequence</code>用來將list of tensor用0補齊，這用在你的資料輸入長度不一致的時候，由於模型輸入要求長度固定所以必須要將長度變成一樣長。</p><p>在NLP相關的task最常看到這種現象，舉例來說: <strong>當你的輸入是句子，而每句長度都不同的時候</strong></p><p>假設我們的資料沒有做embedding，也就是說句子裡每一個字都只是一個數字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_x = [torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]),</span><br><span class="line">           torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]),</span><br><span class="line">           torch.tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]),</span><br><span class="line">           torch.tensor([<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]),</span><br><span class="line">           torch.tensor([<span class="number">5</span>])]</span><br><span class="line">pprint(train_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># pad the list of sequences</span></span><br><span class="line">pad_train_x = rnn_utils.pad_sequence(train_x, batch_first=<span class="literal">True</span>)</span><br><span class="line">pprint(pad_train_x)</span><br></pre></td></tr></table></figure><p>進行padding後，pad_train_x如下，將長度小於最大長度的都用0補齊了:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure></p><p>如果句子中的每個字都經過embedding了? 比方說現在句子中的每個字都是一個4維的向量時: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_x1 = [torch.ones(<span class="number">3</span>, <span class="number">4</span>),</span><br><span class="line">            torch.ones(<span class="number">1</span>, <span class="number">4</span>),</span><br><span class="line">            torch.ones(<span class="number">5</span>, <span class="number">4</span>)]</span><br><span class="line">pprint(train_x1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># pad the list of sequences</span></span><br><span class="line">pad_train_x1 = rnn_utils.pad_sequence(train_x1, batch_first=<span class="literal">True</span>)</span><br><span class="line">pprint(pad_train_x1)</span><br></pre></td></tr></table></figure><p>padding後得到的pad_train_x1會是:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]])</span><br></pre></td></tr></table></figure></p><p>關於<code>pad_sequence()</code>內的參數batch_first會造成什麼影響:</p><ul><li>batch_first=True時，return的維度是: [Batch, Sequence, Features] </li><li>batch_first=False時，return的維度是: [Sequence, Batch, Features]</li></ul><p>通常我們會設置batch_first=True，因為比較符合一般我們在思考的格式</p><ul><li>不過這個格式餵入RNN相關模型時其實並不利於平行化計算(<strong>但其實Pytorch內部又會把他轉回batch_first=False，所以用我們比較好理解的表達形式就好</strong>)，在最後面的時候再來補充說明</li></ul><p>對於DataLoader，DataLoader無法處理不定長度的輸入，來看一下長度不相同的時候DataLoader會花生什麼4: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">train_x = [torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]),</span><br><span class="line">           torch.tensor([<span class="number">2</span>, <span class="number">2</span>]),</span><br><span class="line">           torch.tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]),</span><br><span class="line">           torch.tensor([<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]),</span><br><span class="line">           torch.tensor([<span class="number">5</span>])]</span><br><span class="line">pprint(train_x)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mydataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(Mydataset(train_x), batch_size=<span class="number">2</span>)</span><br><span class="line">batch_x = iter(train_loader).next()</span><br><span class="line">pprint(batch_x)</span><br></pre></td></tr></table></figure><p>恭喜！ 你會得到下面的Error msg:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 6 and 2 in dimension 1 at &#x2F;tmp&#x2F;pip-req-build-4baxydiv&#x2F;aten&#x2F;src&#x2F;TH&#x2F;generic&#x2F;THTensor.cpp:689</span><br></pre></td></tr></table></figure></p><p>所以要嘛在前處理就先padding好，不然就是透過DataLoader的collate_fn參數來處理從Dataset return的data:</p><ul><li>在這邊的collate_fn中對資料先做了降序排列，原因後面會說到</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="comment"># sort data with descending order</span></span><br><span class="line">    data.sort(key=<span class="keyword">lambda</span> x: len(x), reverse=<span class="literal">True</span>)</span><br><span class="line">    data = rnn_utils.pad_sequence(data, batch_first=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(Mydataset(train_x), batch_size=<span class="number">2</span>, collate_fn=collate_fn)</span><br><span class="line">batch_x = iter(train_loader).next()</span><br><span class="line">pprint(batch_x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><h2 id="pack-padded-sequence"><a href="#pack-padded-sequence" class="headerlink" title="pack_padded_sequence"></a>pack_padded_sequence</h2><h3 id="為何需要pack"><a href="#為何需要pack" class="headerlink" title="為何需要pack?"></a>為何需要pack?</h3><p>在上一節，使用了<code>pad_sequence()</code>將長度變成一致後，其實就可以餵入RNN系列的model了。</p><p>但我們來考慮一下一組有padding的句子在RNN model中是怎麼樣訓練的，假設input sequence如下:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="string">'a'</span>, <span class="string">'b'</span>, &lt;PAD&gt;, &lt;PAD&gt;, &lt;PAD&gt;]</span><br></pre></td></tr></table></figure><br>由於要與其他sequence長度一致，假設我們將這個句子補齊到長度為5，並用<code>&lt;PAD&gt;</code>代表padding</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RNN(h1) -&gt; RNN(h2) -&gt; RNN(h3) -&gt; RNN(h4) -&gt; ...</span><br><span class="line"> ^          ^           ^          ^  </span><br><span class="line"><span class="string">'a'</span>        <span class="string">'b'</span>        &lt;PAD&gt;      &lt;PAD&gt;</span><br></pre></td></tr></table></figure><p>我們通常都拿最後一個hidden state作為模型輸出，而此時的輸出是經過了許多padding sybmol的，理想上我們要拿的hidden state應該是輸入’b’當下的output(也就是h2)。</p><ul><li>當然你可以想辦法拿出h2作為模型的output</li><li>或是說可以預期模型學得好的時候，應該學到padding symbol是無意義的，也就是說理論上h4應該要跟h2差不多才對</li></ul><p>另外，一個很重要的議題是: 在雙向的RNN系列模型中，如果不用pack則會很麻煩(因為你需要handle兩個padding sequence: 一個往前padding + 一個往後padding)，所以雙向的RNN模型都會使用pack sequence來實作。</p><p>現在，對於上述的Issue，我們有更好的一個方式來處理: <strong>透過pack sequence來避免訓練多餘的padding symbol</strong></p><h3 id="pack-a-sequence"><a href="#pack-a-sequence" class="headerlink" title="pack a sequence"></a>pack a sequence</h3><p>應該怎麼做呢，這又扯到了model在餵入資料的時候是怎麼吃data的，用上面的example data來說明(這裡先不考慮batch_first的影響，單純先假設每一列都是一筆sequence)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><p>RNN系列的model每一次的循環是先餵每一個sequence內第一個timestamp的data，也就是<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[<span class="number">1</span>],</span><br><span class="line"> [<span class="number">2</span>],</span><br><span class="line"> [<span class="number">3</span>],</span><br><span class="line"> [<span class="number">4</span>],</span><br><span class="line"> [<span class="number">5</span>]]</span><br></pre></td></tr></table></figure><br>然後產生下一個時間的hidden state，再計算每一個sequence內下一個timestamp的hidden state<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">[[<span class="number">1</span>],</span><br><span class="line"> [<span class="number">2</span>],</span><br><span class="line"> [<span class="number">3</span>],</span><br><span class="line"> [<span class="number">4</span>],</span><br><span class="line"> [<span class="number">0</span>]]</span><br></pre></td></tr></table></figure><br>依此類推，你會發現<strong>每個循環都是餵入相同筆數的data，然後產生下一個hidden state。但以第二個timestamp來看，最後一筆的[0]明明只是個padding，根本不用計算的，因此造成了額外的計算資源浪費</strong>。</p><p>所以pack在做的事情就是讓每一次的筆數省掉計算多餘的padding，你可以想像成<strong>在原本batch下我們又透過一個mini-batch來紀錄沒有padding symbol下要餵的長度</strong>，為此透過一個Class(PackedSequence)來封裝我們所需要的資訊:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PackedSequence(</span><br><span class="line">  data=tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>]),</span><br><span class="line">  batch_sizes=tensor([<span class="number">5</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p><strong>PackedSequence的data是原始的資料去除掉padding後串接起來的一維data，而batch_sizes是說在原本batch下的資料透過pack轉換後，每一次的mini-batch長度</strong>(下面除非特地提到參數，不然統一mini-batch代替pack裡面的batch_sizes以免搞混)</p><ol><li>第一個batch_sizes參數是5，代表第一個mini-batch應該從data中取出5筆data([1, 2, 3, 4, 5])來作為第一個timestamp的input</li><li>第二個batch_sizes參數的值是4，代表第二個mini-batch應該從data中取出4筆data([1, 2, 3, 4])來作為第二個timestamp的input</li></ol><p>如此，我們的model就可以接受長度不同的input了，並且還不用考慮padding。並且這邊的mini-batch是指在同一個batch下的所有sequence去進行pack的結果，並不會影響其他batch，所以仍然可以用GPU平行計算</p><ul><li>舉個例子，batch size=5，那可以想像成訓練的時候每張顯卡都拿不同的5筆sequences資料</li><li>然後每個GPU在各自對自己的這5比去做pack sequence取得不包含padding下的mini-batch</li></ul><p>最後，我們仔細看一下PackedSequence這一個Class，在Pytorch doc中提到:</p><blockquote><p>Holds the data and list of batch_sizes of a packed sequence.</p><p>All RNN modules accept packed sequences as inputs.</p></blockquote><ul><li>也就是說這一個類別他同時紀錄了資料和packed sequence的batch size</li><li>對於所有的RNN model，input都是可以接受packed seq的</li><li>文件中也提到，這一個Class不應該被手動產生，而應該透過對應的function來生成(pack_padded_sequence)</li></ul><h3 id="write-a-code-for-packing-sequence"><a href="#write-a-code-for-packing-sequence" class="headerlink" title="write a code for packing sequence"></a>write a code for packing sequence</h3><p>懂了pack的概念後，來看一下怎麼寫，<code>pack_padded_sequence()</code>顧名思義，是對有padding的sequence來進行pack這個操作，所以要先做<code>pad_sequence()</code></p><ul><li>並且要注意<strong>sequence必須是降序排列</strong>，為了避免在串接中有padding被夾在中間的原因<ul><li>所以前面在寫<code>collate_fn()</code>的時候才會先對sequences做排序 </li></ul></li><li><code>pack_padded_sequence()</code>同樣具有batch_first的參數，所以也要跟<code>pad_sequence()</code>的設置相同</li><li>還有一個參數lengths，用來告訴每一個sequence下真正的長度(不包含padding)是多少<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_x = [torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]),</span><br><span class="line">           torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]),</span><br><span class="line">           torch.tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]),</span><br><span class="line">           torch.tensor([<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]),</span><br><span class="line">           torch.tensor([<span class="number">5</span>])]</span><br><span class="line">pprint(train_x)</span><br><span class="line"><span class="comment"># [tensor([1, 1, 1, 1, 1, 1]),</span></span><br><span class="line"><span class="comment">#  tensor([2, 2, 2, 2]),</span></span><br><span class="line"><span class="comment">#  tensor([3, 3, 3, 3]),</span></span><br><span class="line"><span class="comment">#  tensor([4, 4, 4]),</span></span><br><span class="line"><span class="comment">#  tensor([5])]</span></span><br><span class="line"></span><br><span class="line">rnn_utils.pack_padded_sequence(rnn_utils.pad_sequence(train_x, batch_first=<span class="literal">True</span>), lengths=[<span class="number">6</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>], batch_first=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># PackedSequence(</span></span><br><span class="line"><span class="comment">#   data=tensor([1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 1]), </span></span><br><span class="line"><span class="comment">#   batch_sizes=tensor([5, 4, 4, 3, 1, 1]), sorted_indices=None, unsorted_indices=None)</span></span><br></pre></td></tr></table></figure></li></ul><p>對於data loader，由於長度仍然要是相同的，所以一樣會先padding，等到load進來之後才做pack，但這樣就無法得知seq的真正長度了，所以<code>collate_fn()</code>必須也要回傳長度的部分:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">train_x = [torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]),</span><br><span class="line">           torch.tensor([<span class="number">2</span>, <span class="number">2</span>]),</span><br><span class="line">           torch.tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]),</span><br><span class="line">           torch.tensor([<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]),</span><br><span class="line">           torch.tensor([<span class="number">5</span>])]</span><br><span class="line"><span class="comment"># pprint(train_x)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mydataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(data)</span>:</span></span><br><span class="line">    data.sort(key=<span class="keyword">lambda</span> x: len(x), reverse=<span class="literal">True</span>)</span><br><span class="line">    seq_lens = [len(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> data]</span><br><span class="line">    data = rnn_utils.pad_sequence(data, batch_first=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> data, seq_lens</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(Mydataset(train_x), batch_size=<span class="number">2</span>, collate_fn=collate_fn)</span><br><span class="line">batch_x, seq_lens = iter(train_loader).next()</span><br><span class="line">pprint(batch_x)</span><br><span class="line"><span class="comment"># tensor([[1, 1, 1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [2, 2, 0, 0, 0, 0]])</span></span><br><span class="line"></span><br><span class="line">pprint(seq_lens)</span><br><span class="line"><span class="comment"># [6, 2]</span></span><br><span class="line"></span><br><span class="line">packed_batch_x = rnn_utils.pack_padded_sequence(batch_x, lengths=seq_lens, batch_first=<span class="literal">True</span>)</span><br><span class="line">pprint(packed_batch_x)</span><br><span class="line"><span class="comment"># PackedSequence(</span></span><br><span class="line"><span class="comment">#   data=tensor([1, 2, 1, 2, 1, 1, 1, 1]), </span></span><br><span class="line"><span class="comment">#   batch_sizes=tensor([2, 2, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)</span></span><br></pre></td></tr></table></figure><h2 id="pad-packed-sequence"><a href="#pad-packed-sequence" class="headerlink" title="pad_packed_sequence"></a>pad_packed_sequence</h2><p><code>pad_packed_sequence()</code>就是<code>pack_padded_sequence()</code>的逆操作，返回原本padding的形式以及對應的length</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rnn_utils.pad_packed_sequence(packed_batch_x)</span><br><span class="line"><span class="comment"># (tensor([[1, 2],</span></span><br><span class="line"><span class="comment">#          [1, 2],</span></span><br><span class="line"><span class="comment">#          [1, 0],</span></span><br><span class="line"><span class="comment">#          [1, 0],</span></span><br><span class="line"><span class="comment">#          [1, 0],</span></span><br><span class="line"><span class="comment">#          [1, 0]]),</span></span><br><span class="line"><span class="comment">#  tensor([6, 2]))</span></span><br></pre></td></tr></table></figure><h2 id="用PackedSequence寫一個RNN訓練的流程"><a href="#用PackedSequence寫一個RNN訓練的流程" class="headerlink" title="用PackedSequence寫一個RNN訓練的流程"></a>用PackedSequence寫一個RNN訓練的流程</h2><p>寫一個RNN model來實際跑跑看。</p><p>首先，還記得我們說過batch_first=True時，return的維度是: [Batch, Sequence, Features]，上面最後的程式碼維度其實只是[Batch, Sequence]，因為這樣的輸出比較容易理解，不過實際上在餵入模型的時候要改一下data loader的<code>__getitem__()</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mydataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        self.data = data</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[idx].unsqueeze(<span class="number">-1</span>) <span class="comment"># return dimension [Batch, Sequence, Features]</span></span><br></pre></td></tr></table></figure><br>改成這樣得到的pad packed sequence會變成:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[[<span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">2</span>],</span><br><span class="line">         [<span class="number">2</span>],</span><br><span class="line">         [<span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>]]])</span><br><span class="line">[<span class="number">6</span>, <span class="number">2</span>]</span><br><span class="line">PackedSequence(data=tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>]]), batch_sizes=tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]), sorted_indices=<span class="literal">None</span>, unsorted_indices=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p><p>然後放個LSTM model，記得也要設置batch_first。並將我們的tensor轉成float<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LSTM model</span></span><br><span class="line">net = nn.LSTM(<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, batch_first=<span class="literal">True</span>) </span><br><span class="line">out, (h, c) = net(packed_batch_x.float())</span><br><span class="line"></span><br><span class="line">pprint(out)</span><br><span class="line"><span class="comment"># PackedSequence(</span></span><br><span class="line"><span class="comment">#   data=tensor([[-0.0417, -0.1202,  0.0748,  0.0658],</span></span><br><span class="line"><span class="comment">#         [-0.0434, -0.1283,  0.0659,  0.0617],</span></span><br><span class="line"><span class="comment">#         [-0.0873, -0.1682,  0.1064,  0.0991],</span></span><br><span class="line"><span class="comment">#         [-0.0899, -0.1807,  0.0927,  0.0926],</span></span><br><span class="line"><span class="comment">#         [-0.1233, -0.1861,  0.1222,  0.1136],</span></span><br><span class="line"><span class="comment">#         [-0.1483, -0.1918,  0.1307,  0.1192],</span></span><br><span class="line"><span class="comment">#         [-0.1647, -0.1926,  0.1354,  0.1211],</span></span><br><span class="line"><span class="comment">#         [-0.1752, -0.1915,  0.1380,  0.1214]], grad_fn=&lt;CatBackward&gt;), </span></span><br><span class="line"><span class="comment">#   batch_sizes=tensor([2, 2, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)</span></span><br></pre></td></tr></table></figure></p><p>可以看到output也是一個PackedSequence</p><ul><li>batch_sizes和packed_batch_x的相同(這也很合理，想想看就知道了)</li><li>out的shape是(8, 4)<ul><li>packed_batch_x總共有8個非0的data</li><li>LSTM的hidden size為4</li></ul></li></ul><p>如果把out透過<code>pad_packed_sequence()</code>還原的話<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">pad_out, out_len = rnn_utils.pad_packed_sequence(out, batch_first=<span class="literal">True</span>)</span><br><span class="line">pprint(pad_out)</span><br><span class="line"><span class="comment"># tensor([[[0.0872, 0.0783, 0.1465, 0.1197],</span></span><br><span class="line"><span class="comment">#          [0.1331, 0.1032, 0.2627, 0.1835],</span></span><br><span class="line"><span class="comment">#          [0.1564, 0.1090, 0.3387, 0.2184],</span></span><br><span class="line"><span class="comment">#          [0.1685, 0.1088, 0.3853, 0.2377],</span></span><br><span class="line"><span class="comment">#          [0.1750, 0.1073, 0.4137, 0.2484],</span></span><br><span class="line"><span class="comment">#          [0.1787, 0.1058, 0.4312, 0.2543]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[0.0753, 0.0784, 0.1402, 0.1334],</span></span><br><span class="line"><span class="comment">#          [0.1139, 0.1052, 0.2527, 0.2110],</span></span><br><span class="line"><span class="comment">#          [0.0000, 0.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="comment">#          [0.0000, 0.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="comment">#          [0.0000, 0.0000, 0.0000, 0.0000],</span></span><br><span class="line"><span class="comment">#          [0.0000, 0.0000, 0.0000, 0.0000]]], grad_fn=&lt;TransposeBackward0&gt;)</span></span><br><span class="line"></span><br><span class="line">pprint(out_len)</span><br><span class="line"><span class="comment"># tensor([6, 2])</span></span><br></pre></td></tr></table></figure></p><ul><li>pad_out的shape為(2, 6, 4)<ul><li>2是batch size </li><li>6是sequence長度，因為變回有padding版本所以每個sequence都一樣長</li><li>4是LSTM的hidden size</li></ul></li><li>out_len的輸出則是原始padding版本下data的shape</li></ul><p>好，完畢！</p><h2 id="所以我說…那個pack-sequence勒"><a href="#所以我說…那個pack-sequence勒" class="headerlink" title="所以我說…那個pack_sequence勒?"></a>所以我說…那個pack_sequence勒?</h2><p>如果你去看<a href="https://pytorch.org/docs/master/_modules/torch/nn/utils/rnn.html#pack_sequence" target="_blank" rel="noopener">source code</a>，你會發現<code>pack_sequence()</code>其實就是padding + pack一起用而已<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pack_sequence</span><span class="params">(sequences, enforce_sorted=True)</span>:</span></span><br><span class="line">    lengths = torch.as_tensor([v.size(<span class="number">0</span>) <span class="keyword">for</span> v <span class="keyword">in</span> sequences])</span><br><span class="line">    <span class="keyword">return</span> pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)</span><br></pre></td></tr></table></figure></p><p>pack_sequence我目前看到比較少在使用(如果有看到也歡迎提供，我會再補上)，大部分都是<code>pad_sequence</code>, <code>pack_padded_sequence</code>, <code>pad_packed_sequence</code>的搭配</p><ul><li>然後注意source code都沒寫到batch_first，所以default都是False</li></ul><h2 id="延伸討論-batch-size-True到底在幹嘛"><a href="#延伸討論-batch-size-True到底在幹嘛" class="headerlink" title="延伸討論: batch_size=True到底在幹嘛?"></a>延伸討論: batch_size=True到底在幹嘛?</h2><p>前面提到過:</p><ul><li>batch_first=True時，return的維度是: [Batch, Sequence, Features] </li><li>batch_first=False時，return的維度是: [Sequence, Batch, Features]</li></ul><p>阿這兩個到底差在哪裡，都幾?</p><p>我們先放上一張Pytorch doc的圖來幫助理解:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1596016176/blog_posts/20190820164135998_as9ikt.png" alt=""></p><p>通常第一個維度會被稱之循環維度，也就是機器單次是怎麼抓取和處理數據的</p><ul><li>當資料格式是[Batch, Sequence, Features]，代表每次都是抓取同一個sequence內不同timestamp的data</li><li>當資料格式是[Sequence, Batch, Features]，代表每次都是抓取同不同sequences下，同一個timestamp的data</li></ul><p><strong>而對於後者，很直觀地想到這樣的格式是可以平行化計算的，因為RNN model必須先計算完第t個timestamp的數據後才能計算t+1個數據，所以後者這種cross-batch的方式更加適合。</strong></p><p>咦既然batch_first=False比較好，那為啥前面都設置成True呢?</p><p><strong>因為Pytorch內做了設置，不管設置True或False他內部都會以[Sequence, Batch, Features]的方式來處理數據</strong>(<a href="https://zhuanlan.zhihu.com/p/32103001" target="_blank" rel="noopener">读PyTorch源码学习RNN(1)</a>)，看到有些資料說，該參數的設置只是提醒有這個trick</p><p>所以在撰寫程式的時候還是以我們平常思考的模式來寫就好。</p><h2 id="延伸討論-PackedSequence搭配DataParallel的問題"><a href="#延伸討論-PackedSequence搭配DataParallel的問題" class="headerlink" title="延伸討論: PackedSequence搭配DataParallel的問題"></a>延伸討論: PackedSequence搭配DataParallel的問題</h2><p>這是之前實作上遇到的問題，當初有記錄下來，這個問題主要是說:</p><p>將data放到不同的gpu上跑時，由於使用了PackedSequence，每個batch的長度都是不固定的，<strong>在每張gpu上執行pad_packed_sequence()時，會取它當下batch的最大長度來對其他句子進行padding，這時因為每個gpu上data不同導致當下的最大長度都會不同，在gather的時候就會產生維度不匹配的問題。</strong></p><p>關於詳細內容可以看這篇文章: <a href="https://meetonfriday.com/posts/d9cbeda0/#%E5%9D%91-2">[Pytorch]當DataParallel碰上RNN的那些坑</a></p><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>這篇文章提及了下列幾個問題，可以一邊思考下面的問題來幫助自己確認觀念熟悉了沒:</p><ol><li>NLP資料在訓練時長度不同應該怎麼處理?</li><li>PackedSequence的概念是什麼? 為何這樣可以增加計算效率?</li><li>為什麼<code>pack_padded_sequence()</code>要先對sequences排序?</li><li>RNN系列模型常常會有一個參數batch_first，這個參數的作用是什麼?</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://pytorch.org/docs/master/nn.html#utilities" target="_blank" rel="noopener">TORCH.NN</a></li><li><a href="https://zhuanlan.zhihu.com/p/59772104" target="_blank" rel="noopener">PyTorch 训练 RNN 时，序列长度不固定怎么办？</a></li><li><a href="https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch" target="_blank" rel="noopener">why do we “pack” the sequences in pytorch?</a></li><li><a href="http://news.migage.com/articles/%E3%80%90Pytorch%E3%80%91%E8%AF%A6%E8%A7%A3RNN%E7%BD%91%E7%BB%9C%E4%B8%AD%E6%96%87%E6%9C%AC%E7%9A%84pack%E5%92%8Cpad%E6%93%8D%E4%BD%9C_3529246_csdn.html" target="_blank" rel="noopener">【Pytorch】详解RNN网络中文本的pack和pad操作</a></li><li><a href="https://www.zhihu.com/question/41949741/answer/318771336" target="_blank" rel="noopener">LSTM神经网络输入输出究竟是怎样的？</a></li><li><a href="https://zhuanlan.zhihu.com/p/32103001" target="_blank" rel="noopener">读PyTorch源码学习RNN（1）</a></li><li><a href="https://meetonfriday.com/posts/d9cbeda0/#%E5%9D%91-2">[Pytorch]當DataParallel碰上RNN的那些坑</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> python </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Going deeper with convolutions</title>
      <link href="/posts/263e065d/"/>
      <url>/posts/263e065d/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>paper: <a href="https://arxiv.org/pdf/1409.4842.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1409.4842.pdf</a></p><p><strong>GoogLeNet，Google 2015年在CVPR發表的paper，也是ILSVRC 2014的冠軍</strong>(Classification &amp; Detection)，會這樣取名是為了致敬LeNet，所以特地把google的l給大寫了。</p><p>這篇介紹會聚焦在Classification和模型的部分，所以對於Detection會跳過或快速帶過，有興趣可以去看原文。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant.</p></blockquote><p>ILSVRC 2014的冠軍，恭喜恭喜( ﾟ∀ﾟ)o彡ﾟ</p><p>Google透過提出了Inception的深度神經網路結構，主要改進了網路內計算資源的使用效率，並能在計算資源固定下能夠訓練得更深更廣。</p><a id="more"></a><blockquote><p>To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing</p></blockquote><p>此外還遵循了<strong>Hebbian原則和多尺度的處理</strong>的想法來設計..</p><p>好喔…所以我說…Hebbian原則是蝦咪挖歌?? </p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>Our GoogLeNet submission to ILSVRC 2014 actually uses 12× fewer parameters than the winning architecture of Krizhevsky et al [9] from two years ago, while being significantly more accurate.</p></blockquote><p>首先，GoogLeNet使用的參數量只有AlexNet的1/12倍，但卻有著更好的準確度。</p><blockquote><p>In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al [12] in conjunction with the famous “we need to go deeper” internet meme [1].</p></blockquote><p>Inception結構是從Network in Network, NIN這篇論文發展而來，然後名字的由來來自NIN中的meme “<strong>we need to go deeper</strong>“…棒喔，還cite一個meme</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595358824/blog_posts/a88_pc8dhu.jpg" alt=""></p><p>(20200814更新)NIN的文章出來啦! 有興趣的可以看這篇: <a href="https://meetonfriday.com/posts/a151bfa2">[論文速速讀]Network In Network</a></p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ol><li>介紹了CNN的background</li><li>受到靈長類神經模型的啟發，Serre et al，使用了不同大小的Gabor filter來學習不同尺度的特徵，這類似於Inception的做法，但不同的是Inception的所有filter都是可以被學習的</li><li>在NIN這篇論文中，透過了1x1的Conv來增加模型的表達能力和計算複雜度。<strong>在Inception中1x1 Conv最主要被用來做dimension reduction以降低記算複雜度，這使得模型可以在不增加計算複雜度的情況下增加寬度</strong>(變胖了!)</li><li>這是在講Detection的部分，R-CNN中將檢測任務分成了兩個階段: Region Proposal和Classification。GoogLeNet也用到了類似的方法並加以改良，然後得到了較好的結果。</li></ol><h2 id="Motivation-and-High-Level-Considerations"><a href="#Motivation-and-High-Level-Considerations" class="headerlink" title="Motivation and High Level Considerations"></a>Motivation and High Level Considerations</h2><p>其實單看GoogLeNet架構並不難理解，但如果要試著去理解這個設計背後的動機和考量並沒有那麼簡單。這邊就從原文的角度出發來看一下設計背後的思維:</p><p>提升神經網路效果最直覺的方法就是<strong>增加網路的深度跟寬度</strong>，暴力但有效，但這樣有兩個主要的缺點:</p><ul><li><strong>在數據不夠大量的時候，模型參數過多往往容易overfitting</strong><ul><li>你說那為何不給多點數據就好了? 孩子你去跟你老闆這樣講講看啊</li><li>在論文中還特地給了一張圖片來告訴你如果對於一些需要精細標記的類別，標記是一件多麼費工的事情</li></ul></li><li><strong>計算資源的問題，模型越大計算資源越大，而得到的效益往往不成正比</strong>，如果學到了一堆拉基(例如weight都是0)那根本就是在浪費資源</li></ul><blockquote><p>The fundamental way of solving both issues would be by ultimately moving from fully connected to sparsely connected architectures, even inside the convolutions.</p></blockquote><p>解決上述兩個議題的方式是可以<strong>建立一個稀疏的神經網路結構</strong>，也就是用稀疏的層取代全連接層以及Conv層(為什麼Conv不是稀疏層後面會說)</p><p>根據Arora的研究: 如果可以透過一個大且稀疏的神經網路結構來表示一個dataset的機率分佈，那麼可以透過逐層分析然後將相關性高的元素群聚起來，進而去找到一個最優的網路拓樸結構(optimal network topology)，這個研究與著名的Hebbian principle不謀而合，Hebbian principle說到: “neurons that fire together, wire together”</p><blockquote><p>Most current vision oriented machine learning systems utilize sparsity in the spatial domain just by the virtue of employing convolutions. However, convolutions are implemented as collections of dense connections to the patches in the earlier layer.</p></blockquote><p><strong>卷積其實就是在空間域上實現了稀疏性的一種操作</strong>，圖片中相鄰的點其實相關性最高，而這些相關性高的點透過卷積給群聚起來了，是不是跟上面的研究很像呢，所以卷積是符合Hebbian principle的。</p><p>但卷積的操作本身還是密集連接而不是稀疏連接，因為卷積本質上還是上一層的每個neuron和這一層的每個neuron去逐點計算(早期在LeNet的時候對於卷積有隨機和稀疏連接的設計以打破對稱性(參見<a href="https://meetonfriday.com/posts/d5321308/">[論文速速讀]Gradient Based Learning Applied to Document Recognition</a>中C3的Conv layer)，但後來AlexNet又改回一般的全連接卷積，由於這樣能夠更好的優化跟平行計算)。</p><p>那重點來了，既然稀疏的神經網路結構無法建立，那要怎麼解決原本的議題呢？</p><blockquote><p>This raises the question whether there is any hope for a next, intermediate step: an architecture that makes use of the extra sparsity, even at filter level, as suggested by the theory, but exploits our current hardware by utilizing computations on dense matrices. </p></blockquote><p>既然無法達到真正完整的稀疏網路結構，那就退而求其次，<strong>在filter level上來實踐稀疏性，期望能逼近稀疏網路結構的效果</strong></p><ul><li>利用filter level的稀疏能力</li><li>但層和層之間還是全連接的，如此仍舊確保了計算上的效率</li></ul><p>filter level是什麼意思呢，看了幾篇文章後我覺得<a href="https://zhuanlan.zhihu.com/p/32702031" target="_blank" rel="noopener">深入理解GoogLeNet结构（原创）</a>講的是我比較能夠接受的講法，以下引用該篇文章的解釋:</p><blockquote><p>这个原理应用到inception上就是要在特征维度上进行分解！传统的卷积层的输入数据只和一种尺度（比如3x3）的卷积核进行卷积，输出固定维度（比如256个特征）的数据，所有256个输出特征基本上是均匀分布在3x3尺度范围上，这可以理解成输出了一个稀疏分布的特征集；而inception模块在多个尺度上提取特征（比如1x1，3x3，5x5），输出的256个特征就不再是均匀分布，而是相关性强的特征聚集在一起（比如1x1的的96个特征聚集在一起，3x3的96个特征聚集在一起，5x5的64个特征聚集在一起），这可以理解成多个密集分布的子特征集。这样的特征集中因为相关性较强的特征聚集在了一起，不相关的非关键特征就被弱化，同样是输出256个特征，inception方法输出的特征“冗余”的信息较少。用这样的“纯”的特征集层层传递最后作为反向计算的输入，自然收敛的速度更快。</p></blockquote><p>也就是將特徵透過不同size的kernel來萃取再合併起來，使得高相關性的特徵都能夠被群聚在一起(1x1一群、3x3一群、5x5一群)，這也符合了Hebbian principle</p><p>最後paper也提到，儘管發現Inception可以提升結果，但究竟是不是因為這樣的架構設計才造成提升的還有待實驗證實。</p><p>…歐這一章節好難懂喔，沒看論文前明明覺得概念其實不難，仔細讀論文才知道背後的意涵這麼複雜= =</p><h2 id="Architectural-Details"><a href="#Architectural-Details" class="headerlink" title="Architectural Details"></a>Architectural Details</h2><blockquote><p>The main idea of the Inception architecture is based on finding out how an optimal local sparse structure in a convolutional vision network can be approximated and covered by readily available dense components.</p></blockquote><p>所以Inception架構的目的就是要找出一個局部最佳的稀疏結構，並且這結構可以簡單的透過全連接串起來。</p><h3 id="naive-version"><a href="#naive-version" class="headerlink" title="naive version"></a>naive version</h3><ul><li>在CNN淺層的時候會學到一些初階的局部特徵，這些特徵通常都集中在某些的區域且相關性較高，而我們可以透過1x1 Conv將他們涵蓋到下一層<ul><li>可以想成圖片同個位置上不同channel的相關性較高，而我們可以使用1x1 Conv給涵蓋著</li></ul></li><li>此外，有些稀疏的特徵群也可以被kernel size較大的Conv給涵蓋到，並且kernel size越大涵蓋的範圍也理當越大，所以也加入了3x3 Conv和5x5 Conv</li><li>使用1x1, 3x3, 5x5的Conv是為了設計上的方便性，因為在固定stride=1的情況下，我們只要設定pad=0,1,2就可以得到相同size的輸出，輕鬆地解決了patch alignment的問題</li><li>並且隨著Inception在模型中越深層，應該會學到越高階的抽象特徵，所以3x3和5x5 Conv的數量會隨著層數越深而提高，而學習局部特徵的1x1 Conv數量則會越來越少</li><li>此外發現，Pooling也有助於提升結果，所以再加入Pooling進來</li></ul><p>最後naive version架構如下:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595582404/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-24_%E4%B8%8B%E5%8D%885.19.49_fwkfbh.png" alt=""></p><p>這個架構的問題是，即使做到了局部稀疏的架構，但在性能上非常的差，因為同時使用了不同的scale的Conv又使用了Pooling，可想而知最後合併得到的filter數量(或者說dimension)會非常的大，所以有了第二個版本。</p><h3 id="Inception-module-with-dimension-reductions"><a href="#Inception-module-with-dimension-reductions" class="headerlink" title="Inception module with dimension reductions"></a>Inception module with dimension reductions</h3><p>很直觀的想法就是<strong>在計算量大的地方之前使用1x1 Conv來做降維</strong>，由於dimensional embeddings的成功，我們仍然能夠在維度減少的情況下保留大部分的資訊。</p><blockquote><p> However, embeddings represent information in a dense, compressed form and compressed information is harder to model. We would like to keep our representation sparse at most places (as required by the conditions of [2]) and compress the signals only whenever they have to be aggregated en masse.</p></blockquote><p>但是根據Arora的研究，所有的地方都降維也不是明智之舉，作者仍然想要讓整體的網路結構還是應該保持稀疏，所以只在需要被3x3和5x5之前、和max pooling之後使用。</p><ul><li>1x1 Conv後面是接著ReLU的，使得能夠有更多的非線性變換</li><li>透過1x1 Conv降維使得儘管卷積的操作變多了，但整體的計算量並不會超出太多</li></ul><p>最後架構如下:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595626451/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-25_%E4%B8%8A%E5%8D%885.33.00_u0bj0m.png" alt=""></p><p>由於memory考量，作者他們只在GoogLeNet中後半加入Inception，前幾層還是按照CNN的一般建模方式來設計。</p><h3 id="直覺上的設計考量"><a href="#直覺上的設計考量" class="headerlink" title="直覺上的設計考量"></a>直覺上的設計考量</h3><p>前面的看不懂嗎，那看從這節開始看就好了(?)</p><p>在Motivation中提到作者最原始的設計出發點是想要設計一個稀疏的局部最優網路結構，但其實Inception有個大家應該都比較熟悉的、也比較直覺的設計思維:</p><ol><li><strong>多尺度提取特徵</strong>: 透過1x1, 3x3, 5x5的Conv進行卷積，使得在下一層能夠從不同尺度上進行特徵提取</li><li><strong>架構要怎麼設計，不如讓模型自己決定</strong>: 大家一定想過如果要自己設計一個CNN模型，那要怎麼去設計Conv層和Pooling層的搭配呢? 那不如讓模型自己決定吧! 同時給出不同的卷積和Pooling讓模型自己去訓練，然後模型在更新的過程中就自然的會給予他認為比較好的操作較大的權重</li></ol><h3 id="1x1-Conv是如何降低計算量的"><a href="#1x1-Conv是如何降低計算量的" class="headerlink" title="1x1 Conv是如何降低計算量的?"></a>1x1 Conv是如何降低計算量的?</h3><p>1x1的Conv就是一個kernel(或者稱filter)掃過去，那到底要如何降低計算量?</p><p>關鍵在於該kernel的channel(或者說dimension)的數量是可以控制的，所以只要output dimension參數設置的比原本前一層Conv小就可以做降維，反之也可以提升維度。</p><ul><li>在這操作中，可以想成1x1 Conv整合了同個位置不同channel之間的訊息</li></ul><p>舉個例子: </p><p>假設上一層的輸出是(100, 100, 128)</p><p>經過一個5x5的Conv(有256個filter, stride=1, padding=2, 如此的設置保持了output size大小不變)，我們可以得到的輸出為(100, 100, 256)</p><p>在這個例子下，卷積的參數為128 x 5 x 5 x 256 = 819200 (不考慮bias)</p><ul><li>input channel x kernel size x output channel</li><li>想了解更多如何計算參數量可以參考我以前的文章: <a href="https://meetonfriday.com/posts/4647b68d/">[DL]Calculate Parameter Numbers of MLP &amp; CNN</a></li></ul><p>不過如果中間先經過了1x1 Conv降維(假設32個filter)，在接5x5的Conv，我們的參數量就會是: 128 x 1 x 1 x 32 + 32 x 5 x 5 x 256 = 208896</p><ul><li>可以看到參數量少了4倍，而最後的輸出仍然是(100, 100, 256)</li></ul><p>那中間經過降維會不會影響模型精度?<br>其實模型在高維度中很容易找到一個hyperplane去fit我們的data，所以適當的降維其實不太會降低我們的模型結果，可以想成將原本較分散的資訊量都集中在這些維度上了(當然你也不能降維的太誇張啦)</p><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>慢慢看論文好累啊，我要快速<del>飆車</del>帶過了</p><p>最後的架構如下，我把架構圖壓縮倒過來放不然太大了，看的再去paper看原圖ˊ＿&gt;ˋ</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595629450/blog_posts/%E6%93%B7%E5%8F%96_epvtfg.png" alt=""></p><p>最後對GoogLeNet稍微整理一下:</p><ol><li>總共27層</li><li><strong>在分類前使用NIN的Gobal Average Pooling(GAP)</strong><ul><li>GAP不需要參數，並且能有更好的representation</li><li>實驗結果顯示比用FC layer提升了1%的Acc</li></ul></li><li>還是有使用dropout的regularization技術</li><li>加入了兩個auxiliary classifiers: 隨著網路變深，可能會產生gradient vanishing的問題。於是想透過中間層來幫助預測，在訓練的過程中會透過給予auxiliary classifiers權重的方式來一起去分類<ul><li>但是在預測的時候則不會使用 </li></ul></li></ol><h3 id="Training-Methodology"><a href="#Training-Methodology" class="headerlink" title="Training Methodology"></a>Training Methodology</h3><p>跳過，覺得比較沒有重要的，有興趣自己看原文</p><h3 id="ILSVRC-2014-Classification-Challenge-Setup-and-Results"><a href="#ILSVRC-2014-Classification-Challenge-Setup-and-Results" class="headerlink" title="ILSVRC 2014 Classification Challenge Setup and Results"></a>ILSVRC 2014 Classification Challenge Setup and Results</h3><p>訓練了7個GoogLeNet來做Ensemble，最後結果如下:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595630447/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-25_%E4%B8%8A%E5%8D%886.40.20_s0om7e.png" alt=""></p><p>有沒有看到熟悉的VGG呀，當年的第二名，也是CNN發展史上非常著名的一個模型呢</p><p>沒意外下一篇就是寫他了哈哈哈嗚嗚嗚嗚</p><h3 id="ILSVRC-2014-Detection-Challenge-Setup-and-Results"><a href="#ILSVRC-2014-Detection-Challenge-Setup-and-Results" class="headerlink" title="ILSVRC 2014 Detection Challenge Setup and Results"></a>ILSVRC 2014 Detection Challenge Setup and Results</h3><p>GoogLeNet也是當年Detection的冠軍呢，牛逼啊大佬</p><p>採用的方式和R-CNN類似，目前對於detection的著墨還不多，所以先不讀這部分，有興趣的可以再去看原文。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595630687/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-25_%E4%B8%8A%E5%8D%886.44.39_ng8hph.png" alt=""></p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>GoogLeNet提出了一個隨插隨用的區塊結構: Inception，直覺的想法就是讓模型自己去學最佳的結構(在該層中1x1, 3x3, 5x5 或是pooling誰比較重要就給予比較大的權重)。儘管設計看似很直覺，但在設計的背後考量並不是那麼簡單，要了解背後的設計動機需要花點時間看這篇論文。</p><p>然後這篇其實是GoogLeNet v1，之後還有v2 v3 v4的不同改進，有時間再來看看後續的發展。</p><p>而這篇文章中大量使用到了1x1 Conv，也使用到了GAP，這些技術都來自CV經典的論文: <strong>2014年ICLR的”Network in Network, NIN”</strong>，之後也會閱讀這篇文章。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://blog.csdn.net/lhanchao/article/details/55804968" target="_blank" rel="noopener">[深度学习] Going Deeper with Convolutions（GooLeNet）阅读笔记</a></li><li><a href="https://zhuanlan.zhihu.com/p/34326914" target="_blank" rel="noopener">论文阅读GoogLeNet Inception V1</a></li><li><a href="https://meetonfriday.com/posts/d5321308/">[論文速速讀]Gradient Based Learning Applied to Document Recognition</a></li><li><a href="https://zhuanlan.zhihu.com/p/32702031" target="_blank" rel="noopener">深入理解GoogLeNet结构（原创）</a></li><li><a href="https://zhuanlan.zhihu.com/p/32882744" target="_blank" rel="noopener">Going Deeper with Convolutions</a></li><li><a href="https://zhuanlan.zhihu.com/p/68654281" target="_blank" rel="noopener">论文精读与分析：Inception V1:Going Deeper with Convolution</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Visualizing and Understanding Convolutional Networks</title>
      <link href="/posts/3013fdb9/"/>
      <url>/posts/3013fdb9/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>paper: <a href="https://arxiv.org/pdf/1311.2901.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1311.2901.pdf</a></p><p>ZFNet可說是<strong>CNN可視化的開山始祖</strong>，儘管AlexNet中作者也簡單地拿出第一層的feature map做出了可視化(<a href="https://meetonfriday.com/posts/e54c12ea/">看我看我</a>)，不過這種簡單的方法只能用在第一層，因為後面層隨著Conv和Pooling維度降低，此時就沒辦法輕易的看出來feature map是什麼東西了。</p><p>ZFNet這篇文章中透過反卷積，或者叫做DeConvnet（或者Transpose Convolution…網路上很多種叫法…）來做到可以對每一層的Conv layer進行可視化，並透過此技術來觀察並調整AlexNet的架構，然後就贏下了ILSVRC 2013年的冠軍，恭喜恭喜!</p><p>並且，除了這些還做了一堆實驗，真的是很用心ㄋㄟ，<del>所以我才會看這麼久(ヾﾉ･ω･`)</del></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues.</p></blockquote><p>CNN架構在圖像分類領域上取得了巨大的成功，但是，還沒有研究去說他們為何可以有如此好的表現，以及應該如何針對模型架構去改進。</p><p>這篇文章中主要解決了這兩個問題，第一是提出了一種新的可視化技術可以更加了解中間層的運作，透過這個技術他們去微調了AlexNet的模型並得到了更好的結果。此外他們也透過ablation study找出不同層對於model的貢獻。</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>近年來CNN興起的三個因素:</p><ol><li>越來越多的有label的圖片訓練資料集</li><li>越來越強的GPU</li><li>更好的正則化策略，例如dropout</li></ol><p>但是對於CNN背後的原理其實還不是很明瞭，就像個黑盒子一樣。</p><p>這篇文章透過<strong>反卷積(DeConv)的技術提出了一種視覺化方法，使得可以去觀察每一層的Conv layer。</strong></p><p>此外這篇文章還透過遮擋圖片的一部分來觀察模型輸出，藉此找出圖片中哪些區塊對於模型分類是比較重要的部分。</p><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>一開始對CNN model做了一下介紹，大部分的設置都與AlexNet相同</p><h2 id="Visualization-with-a-Deconvnet"><a href="#Visualization-with-a-Deconvnet" class="headerlink" title="Visualization with a Deconvnet"></a>Visualization with a Deconvnet</h2><blockquote><p>We present a novel way to map these activities back to the input pixel space, showing what input pattern originally caused a given activation in the feature maps.</p></blockquote><p>要去解釋CNN的一種方式就是看中間層的feature map哪些被激活了，作者透過Deconvnet來將特定的feature map映射回到輸入層，這樣就可以看到feature map和input layer之間的對應關係。</p><p><strong>反卷積最早由(Zeiler et al., 2011)提出，一開始適用在unsupervisied learning上，不過這邊是將訓練完成的model使用Deconvnet已做出視覺化的效果，所以並沒有訓練的效果。</strong></p><p>先上paper中重點架構圖，然後再來細講每一個部分:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595088962/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-18_%E4%B8%8B%E5%8D%8811.30.44_ekusji.png" alt=""></p><p>這張圖分成上下兩個部分:</p><ul><li>上半部中，右半部分是Conv的過程: Conv -&gt; ReLU -&gt; MaxPool</li><li>上半部中，左半部是DeConvnet的過程: MaxUnPool -&gt; ReLU -&gt; DeConv</li><li>下半部中則是介紹了Unpooling的概念(文章後面會介紹)</li></ul><blockquote><p>To examine a given convnet activation, we set all other activations in the layer to zero and pass the feature maps as input to the attached deconvnet layer.</p></blockquote><p>此外，為了檢查在某一層feature map中某一個點(activation)對於input的關聯，他們會先將這個點之外的值都設成0然後再傳到DeConvnet層裡面。</p><h3 id="Unpooling"><a href="#Unpooling" class="headerlink" title="Unpooling"></a>Unpooling</h3><p>Maxpool的運算是<strong>不可逆</strong>的，但可以想辦法逼近他。</p><p>方法就是在每次Maxpool的時候將最大的位置都記錄下來，Unpool的時候把值放回到feature map對應的最大位置上。</p><ul><li>paper中稱之switch，在流程圖中也可以看到，灰色的代表最大值的位置，其他沒有填的位置則都是黑色(0)</li></ul><h3 id="Rectification"><a href="#Rectification" class="headerlink" title="Rectification"></a>Rectification</h3><p>在Conv時，為了確保feature map的值都是正的，會使用ReLU</p><p>在DeConvnet時也為了讓值都是正的，所以也使用ReLU</p><p>看似簡單，我在這一塊卡了很久…囧</p><p>我一直在想: 既然Conv完會經過ReLU和Pool，確保了feature map值都是正的，那從feature map拿回來的也都是正的，為何需要再加上ReLU呢?</p><p>後來想到的原因是，第一回DeConvnet的feature map的確沒這個問題(此時的Unpool是從Convnet的最後一次feature map取得，所以一定是正的，所以經過ReLU後也必然是正的)，問題在<strong>經過了一回後就不保證值還都是正的了(因為經過了一次DeConv可能會產生負值)，所以才要加上ReLU來限制</strong>。</p><h3 id="Filtering"><a href="#Filtering" class="headerlink" title="Filtering"></a>Filtering</h3><blockquote><p>To invert this, the deconvnet uses transposed versions of the same filters, but applied to the rectified maps, not the output of the layer beneath. In practice this means flipping each filter vertically and horizontally.</p></blockquote><p>Conv是對上一層的feature map來做卷積，Deconvnet為了逆這個操作，則是對經過ReLU的output使用Conv矩陣的轉置(Transposed Convolution)來做卷積(也就是將原始的參數垂直和水平翻轉)，並且這個部分並不干涉模型的訓練過程。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595269674/blog_posts/Zb4LabP_j8hkod.jpg" alt=""></p><p>阿勒…？ 為什麼這裡突然跑出了一個Conv矩陣的轉置呢？用卷積的轉置矩陣就可以做出逆矩陣的效果嗎？？？</p><p>我覺得<a href="https://www.zhihu.com/question/43609045/answer/120266511" target="_blank" rel="noopener">如何理解深度学习中的deconvolution networks？</a>講的蠻詳細的，下面根據內文的解釋和我的理解簡單整理一遍:</p><p><strong>DeConv，或是逆卷積，在做的事情做出和Conv(卷積)在正向/反向傳播中相反的運算，主要的目標是使得shape是可以被上下層給對應起來的。</strong></p><p>首先想一下正常的卷積在正向/反向傳播的時候的行為:<br>考慮一個$4\times4$的input經過一個$3\times3$的kernel做卷積後，在沒有pooling和stride=1的情況下，output shape=$2\times2$</p><ul><li>input可以看成一個16維的向量$x$</li><li>output則是4維的向量$y$</li><li>卷積運算用$C$表示</li></ul><p>那其實一次的Conv就是下面這個式子:</p><script type="math/tex; mode=display">y=Cx</script><p>$C$其實是一個$4\times16$的matrix:</p><script type="math/tex; mode=display">\left(\begin{array}{ccccccccccccccc}c_{0,0} & c_{0,1} & c_{0,2} & 0 & c_{1,0} & c_{1,1} & c_{1,2} & 0 & c_{2,0} & c_{2,1} & c_{2,2} & 0 & 0 & 0 & 0 & 0 \\0 & c_{0,0} & c_{0,1} & c_{0,2} & 0 & c_{1,0} & c_{1,1} & c_{1,2} & 0 & c_{2,0} & c_{2,1} & c_{2,2} & 0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 & c_{0,0} & c_{0,1} & c_{0,2} & 0 & c_{1,0} & c_{1,1} & c_{1,2} & 0 & c_{2,0} & c_{2,1} & c_{2,2} & 0 \\0 & 0 & 0 & 0 & 0 & c_{0,0} & c_{0,1} & c_{0,2} & 0 & c_{1,0} & c_{1,1} & c_{1,2} & 0 & c_{2,0} & c_{2,1} & c_{2,2}\end{array}\right)</script><p>那為了從$y$還原$x$，先不論還原矩陣的值，我們必須要有一個$16\times4$的矩陣。</p><p>所<strong>以反卷積其實在做的事情只是想辦法還原shape，而這個還原矩陣的值有很多種方式，這篇文章中的DeConvnet由於不牽涉到訓練過程，採用的方式就是簡單的原始卷積矩陣的轉置$C^{T}$</strong></p><p>其實關於逆矩陣或是反矩陣，英文的DeConv或是transpose convolution…blabla，不只被用在是CNN視覺化，還有很多應用。這邊要認真研究有有很多坑可以去講，在我花了兩天看了一堆資料後，下面是我認為的幾個重要概念:</p><ol><li>transpose convolution並不能從feature map還原原始的input data，他只是還原出原始的input shape。(就這點來說其實逆矩陣，或是DeConv是個不太適切的講法，因為他並沒有還原原始的input，這個名詞會讓人誤會。)</li><li>這個矩陣的參數不一定一定要是原本Conv matrix的轉置，是可以被學出來的</li><li>就pytorch的實作上，的weight似乎是隨機初始化的，詳見<a href="https://blog.csdn.net/LoseInVain/article/details/81098502?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">一文搞懂反卷积，转置卷积</a></li><li>tensorflow的實作上則是真的有使用到了卷積的轉置，詳見<a href="https://medium.com/@dboyliao/dl-transposed-conv-%E5%88%B0%E5%BA%95%E6%98%AF%E5%95%A5-8e2a2192058b" target="_blank" rel="noopener">Deep Learning: Transpose Convolution 到底是?</a></li></ol><h2 id="Training-Details"><a href="#Training-Details" class="headerlink" title="Training Details"></a>Training Details</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595278484/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-21_%E4%B8%8A%E5%8D%884.54.31_ydvnmx.png" alt=""></p><p>跟AlexNet差不多，不過原本sprase connection(也就是使用到兩張GPU的部分)被替換掉了，ZFNet重頭到尾只用了一張GPU，節能愛地球，讚。</p><p>其他更改的地方像是第一層使用了更小的stride(4-&gt;2)和kernel size(11-&gt;7)。</p><p>細節就在仔細去看paper吧，接下來要來看本文重點了。</p><h2 id="Convnet-Visualization"><a href="#Convnet-Visualization" class="headerlink" title="Convnet Visualization"></a>Convnet Visualization</h2><h3 id="Feature-Visualization"><a href="#Feature-Visualization" class="headerlink" title="Feature Visualization"></a>Feature Visualization</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595279722/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-21_%E4%B8%8A%E5%8D%885.14.55_vzibp4.png" alt=""></p><p>從validation set隨機取了一些feature maps，然後針對9個最大的點(activations)來看這些點映射到input space下的結果，也就是input image哪些區塊激活了這些點。</p><p>圖片中左半邊是DeConv的視覺化，右半邊的是該點(activation)對應圖片的感受野區塊(receptive field)</p><ul><li>也就是從該點往回推，當推回input layer時被影響到的範圍</li><li>比方說input layer -&gt; (3x3 Conv) -&gt; output layer，那在output layer的一個點實際上對應到了input layer的3x3區塊</li></ul><p>從這張圖中我們觀察到:</p><ul><li>show 9個activation的原因是，透過多張圖來觀察，可以發現Conv具備空間不變性: 即使圖片有差異，但學到的特徵都很類似</li><li>前幾層學到的特徵都在一些顏色或邊角特徵、越後面則會學到較複雜的紋理、局部、整體特徵</li></ul><h3 id="Feature-Evolution-during-Training"><a href="#Feature-Evolution-during-Training" class="headerlink" title="Feature Evolution during Training"></a>Feature Evolution during Training</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595298366/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-21_%E4%B8%8A%E5%8D%8810.25.51_ovaeml.png" alt=""></p><p>這個實驗室show了不同層在訓練過程中的可視化圖，可以發現越淺層學習到特徵所需的時間越短，層數越深，則需要越多次迭代才能學到好的特徵。</p><h3 id="Feature-Invariance"><a href="#Feature-Invariance" class="headerlink" title="Feature Invariance"></a>Feature Invariance</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595327602/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-21_%E4%B8%8B%E5%8D%886.32.11_ci90rn.png" alt=""><br>這個實驗是在說對於CNN垂直翻轉、縮放比較具有不變性，但對於旋轉則比較不具有不變性(除非圖片本身有對稱性，例如娛樂中心那張)。</p><ul><li>a2-c2是第一層的feature space距離，a3-c3是第七層的距離，可以看到在越深層的地方翻轉和縮放的影響相對旋轉較小。</li></ul><h3 id="Architecture-Selection"><a href="#Architecture-Selection" class="headerlink" title="Architecture Selection"></a>Architecture Selection</h3><p>從Feature Visualization實驗中觀察到:</p><ul><li>第一層主要都是學習高頻和低頻的訊息，比較少中頻的訊息</li><li>第二層有些混疊效應(aliasing artifacts)，由於第一層中使用到了較大的stride</li></ul><p>因此對AlexNet的設置做了一些修改，修改的部分上面提過了。</p><h3 id="Occlusion-Sensitivity"><a href="#Occlusion-Sensitivity" class="headerlink" title="Occlusion Sensitivity"></a>Occlusion Sensitivity</h3><p>觀察遮蔽圖片某個部分的話，對於模型分類的影響。從圖中可以看到，模型確實是定位到了場景中的物體，如果該物體被遮蔽的話那分類的準確度就會大大降低。<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1595328196/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-21_%E4%B8%8B%E5%8D%886.42.52_bsuoys.png" alt=""></p><h3 id="Correspondence-Analysis"><a href="#Correspondence-Analysis" class="headerlink" title="Correspondence Analysis"></a>Correspondence Analysis</h3><blockquote><p>Deep models differ from many existing recognition approaches in that there is no explicit mechanism for establishing correspondence between specific object parts in different images (e.g. faces have a particular spatial configuration of the eyes and nose). However, an intriguing possibility is that deep models might be implicitly computing them.</p></blockquote><p>深度學習沒辦法像傳統的辨識技術那樣顯式地建立特地物件之間的關係(例如臉部中眼睛和鼻子之間的關係)，但是一個有趣的可能是模型可能隱式地學習到了這些關係。為了驗證，他們選擇了5張圖片，然後遮蔽相同的部分來做觀察。下面內容引用至<a href="https://zhuanlan.zhihu.com/p/24833574" target="_blank" rel="noopener">Deep Visualization:可视化并理解CNN</a></p><blockquote><p>觀察方式是: 首先计算图片遮蔽前后特征，两个特征向量做差，通过sign来确定符号；然后通过海明距离来计算5张照片两两之间的一致性，然后求和。</p><p>由实验结果可以得出，layer5的特征一致性中，眼和鼻子的数值较低，说明眼和鼻子比其他部分，有更强的相关性，这也说明深度网络能够隐式地建立对应关系。但是在layer7中眼，鼻子和随机的数值较为相似，可能是因为高层尝试去区分狗的种类。</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>值得一提的是，CNN學到的特徵SVM也一樣可以用，越深層學習到的特徵分類效果越好。</p><p>其他跳過。</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>跳過</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>其實說白了ZFNet只是AlexNet對於架構做了一些微調。</p><p>不過這篇論文的貢獻在於提出了一個CNN可視化的方法，透過反卷積，DeConvnet，Transpose Convolution隨便什麼名字啦，使得我們可以去觀察模型不同層對應到input時哪些是比較重要的部分，這對於後續的解釋性AI(Explainable AI, XAI)有非常重要的影響。</p><p>然後那個反卷積跟ReLU那邊我真的是搞了很久，明明才11頁的paper我居然搞了好幾天才搞完(ヾﾉ･ω･`)</p><p>對了，對於實作有興趣的人，無意中挖到了這個pytorch實作的網站，歡迎去看看可能會對ZFNet更加了解: <a href="https://hackmd.io/@bouteille/ByaTE80BI" target="_blank" rel="noopener">ZFNet/DeconvNet: Summary and Implementation</a></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.zhihu.com/question/41529286" target="_blank" rel="noopener">CNN中feature map可视化有哪些思路和方法吗？</a></li><li><a href="https://zhuanlan.zhihu.com/p/47784506" target="_blank" rel="noopener">卷积神经网络可视化的探索</a></li><li><a href="https://www.zhihu.com/question/43609045/answer/120266511" target="_blank" rel="noopener">如何理解深度学习中的deconvolution networks？</a></li><li><a href="https://datascience.stackexchange.com/questions/20565/why-we-use-transposed-filter-as-the-deconvolution-operation-instead-of-the-pseud" target="_blank" rel="noopener">Why we use transposed filter as the deconvolution operation instead of the pseudo inverse of filter?</a></li><li><a href="https://blog.csdn.net/LoseInVain/article/details/81098502?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">一文搞懂反卷积，转置卷积</a></li><li><a href="https://medium.com/@dboyliao/dl-transposed-conv-%E5%88%B0%E5%BA%95%E6%98%AF%E5%95%A5-8e2a2192058b" target="_blank" rel="noopener">Deep Learning: Transpose Convolution 到底是?</a></li><li><a href="https://zhuanlan.zhihu.com/p/24833574" target="_blank" rel="noopener">Deep Visualization:可视化并理解CNN</a></li><li><a href="https://hackmd.io/@bouteille/ByaTE80BI" target="_blank" rel="noopener">ZFNet/DeconvNet: Summary and Implementation</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>碩二生活總結</title>
      <link href="/posts/2c756e7/"/>
      <url>/posts/2c756e7/</url>
      
        <content type="html"><![CDATA[<blockquote><p>時光冉冉，歲月如梭，記憶中初次踏入交大宿舍的那一刻仍依舊鮮明，如今的我卻已經是個即將從這間學校畢業的碩士生了。回首兩年碩士生涯，有成功也有挫折、有喜悅也有悲傷…</p></blockquote><p>這是我論文致謝的第一段，兩年其實並不長，但卻發生了很多事。</p><p>如今碩二的生活在109年7月13日正式劃下句點(我在今天正式拿到了畢業學位證書，痛哭流涕)，於是想寫一篇文章回顧一下碩二的生活，為自己的生活做個紀錄。</p><p>(想了解我碩一的生活可以看: <a href="https://meetonfriday.com/posts/79132bb1/">碩一生活總結</a>)</p><a id="more"></a><h2 id="住手不要再打了啦"><a href="#住手不要再打了啦" class="headerlink" title="住手不要再打了啦"></a>住手不要再打了啦</h2><p>暑假的時間實習完回來後，又要開始沒有盡頭的面試地獄，為了畢業後的工作重新開始慢慢的準備面試。把三四月準備實習複習的東西全部拿出來重新複習了一遍，然後開始一家一家的投履歷，面試，被電在牆上，從牆上爬出來後繼續面下一家…</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594632132/blog_posts/CoolInsistentAiredaleterrier-size_restricted_p8mljx.gif" alt=""></p><h2 id="證明自己還活著的N種方法"><a href="#證明自己還活著的N種方法" class="headerlink" title="證明自己還活著的N種方法"></a>證明自己還活著的N種方法</h2><p><strong>如果每天都在研究的話就只是一塊會研究的肉，根本稱不上自己還活著。</strong></p><p>我玩故我在，這一年還是多少參加了一些課業外的活動讓自己放鬆一下，不然在巨大的研究壓力下我覺得自己可能撐不完這一年。</p><p>這一年中除了研究之外，還參加的活動有:</p><ul><li>跑去新北耶誕城欣賞美美的裝置藝術（想拍照又沒人陪我去QQ）(<a href="https://meetonfriday.com/posts/e8dfc58f/">2019新北耶誕城</a>)</li><li>跑去參加了看起來在辦演唱會， 其實是技術發表會的LINE TAIWAN TECHPULSE(<a href="https://meetonfriday.com/posts/224202e2/">LINE TAIWAN TECHPULSE 2019參加心得</a>)</li><li>過年跟家人跑去台東走春(<a href="https://meetonfriday.com/posts/b97515db/">台東走春行程DAY-1</a>, <a href="https://meetonfriday.com/posts/54fd4849/">台東走春行程DAY-2 &amp; DAY-3</a>)</li><li>燃燒了一個月的server拿回了惡意程式競賽的第三名(<a href="https://meetonfriday.com/posts/33e680bb/">2019 SecBuzzer AI UP!人工智慧資安挑戰賽參賽心得</a>)</li><li>跑去微軟當一日實習生，在隊友凱瑞下拿了當天的RD組第一名(<a href="https://meetonfriday.com/posts/3cef1e25/">2020微軟一日實習生(RDI)</a>)</li></ul><h2 id="研究生活就好比在沙漠中行進著"><a href="#研究生活就好比在沙漠中行進著" class="headerlink" title="研究生活就好比在沙漠中行進著"></a>研究生活就好比在沙漠中行進著</h2><p>在碩二上學期其實論文數度難產，怎麼想就是想不到好的方向跟可行的想法。當時開始覺得覺得自己是不是其實並不適合走研究，因為自己可能沒有那顆頭腦，<del>差點連休學都要考慮了</del>。</p><p><strong>這其實是一個很惡性的循環，沒有想法就會讓自己沒有研究的動力，然後就會更消極的去面對自己的研究，然後時間這無形的壓力又一直在逼迫自己。</strong></p><p>上學期大概有一兩個月我都在這死循環中，找不到跳脫點。直到大概十一月左右我開始了一件事情才有所改變: </p><p><strong>每天記錄和規劃自己每天的工作日誌</strong></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594643935/blog_posts/108107792_3725068064176084_7539510083512366610_o_qzaljc.jpg" alt=""></p><p>當時配合著AI UP的競賽，我開始每天透過記事本紀錄今天做了什麼事情、以及規劃明天應該要做什麼事情。然後每天一開始先用便條紙把當天要做的事情寫好貼在位置的牆壁上，每當做完便條紙上面的事項就把他們從牆壁上撕下來，反正便條紙很多(就業博覽會拿了一堆)不用白不用。而且透過這樣撕掉一張張的便條紙其實是還蠻舒壓的，我認為這是對自己的一個鼓勵：</p><p><strong>告訴自己並不是漫無目的的在過每一天，每天我都確實都有完成一些些事項</strong></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594643984/blog_posts/109049241_3724534234229467_4976203083776939949_n_f2pfly.jpg" alt=""></p><p>透過這樣持續的紀錄自己的進度，我漸漸每天找回了一點動力，並且更加清楚每一天的自己應該做什麼。</p><p>就這樣大概持續到十二月的時候，某一天，我的論文雛形終於出來了。</p><p>那是一個很重要的里程碑。</p><h2 id="這世界每個研究生都在等一個論文"><a href="#這世界每個研究生都在等一個論文" class="headerlink" title="這世界每個研究生都在等一個論文"></a>這世界每個研究生都在等一個論文</h2><blockquote><p>「這世界每個人都在等一個人」        -《等一人咖啡》</p></blockquote><p>《等一人咖啡》這部電影說的這句話並沒有錯，只是，在研究生的世界，我們在等的不是人而是論文。</p><p>碩士論文要不就是比別人好、就是要創新，為此總是要想破頭想出縱古至今沒有人做過的事情。</p><p>在這個崩潰的階段，我覺得最需要的就是知識量: 多看論文。</p><p>因為如果論文看得不夠多，就會變成根本連前人做過了什麼都不知道，哪天你以為你終於想出了第1001種方法，結果才發現早就有人做過了怎麼辦QQ</p><p><strong>所以這個階段就是要瘋狂看論文，死看活看拼命看，把論文想像成你idol臉龐每天看</strong>(借用美食博主”黑貓廚房”的口頭禪)，看一下到底什麼東西是被做過，什麼東西還可以再改進的把它都記錄下來。</p><p>在碩二上日復一日的看論文中，有一天…</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594793426/blog_posts/images_bihyok.jpg" alt=""></p><p>…不是，沒有禿。是我從某篇論文中終於好像似乎看到了通往論文的一條道路。</p><p>以那篇論文為基礎，我開始思考不同的架構，然後嘗試各種實驗，最後終於得到了一個還可以的雛形，拿著這個雛形和老師討論也第一次得到了老師的認同。</p><p>痛哭流涕。</p><h2 id="人生就像茶几，上面充滿了悲劇"><a href="#人生就像茶几，上面充滿了悲劇" class="headerlink" title="人生就像茶几，上面充滿了悲劇"></a>人生就像茶几，上面充滿了悲劇</h2><p>講到我的口試過程真的是悲慘萬分。如果有人說把身體照顧好也是一種準備，那我對於碩士論文口試的準備實在是太不充分了。</p><p><strong>是的，我的身體在口試的前幾天掛掉了。</strong></p><p>喉嚨開始腫，然後身體狀況變得非常差，一直恍神跟不舒服，口試前幾天給我來這招我真的是傻眼貓咪。</p><p>不管如何口試還是得硬上，為此我連續兩天跑了藥局直接跟藥師說</p><p>「請給我藥效強的感冒藥跟喉嚨消炎藥!」</p><p>有幾包吃了覺得沒啥感覺我隔天就立馬跑去買了其他種藥效更強的藥頂著，下圖是我口試當天嗑的所有東西:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594794855/blog_posts/107956543_3724534190896138_4787481211585851552_n_lmeasy.jpg" alt=""></p><p>…還真的是不管嗑了什麼都給我來一點</p><p>題外話，這幾包中我覺得<strong>感冒膠囊跟去痰液是最有效的，然後再搭配一瓶紅牛給你一對翅膀</strong>，如果有可撥菸酒生不幸遇到跟我一樣的情況可以參考參考。</p><hr><p>碩二這一年雖然沒有碩一的修課地獄(很感謝碩一跟大學的自己提早把學分都搞定了，讓我碩二才不會那麼崩潰)，不過也是充滿艱辛的一年，同時身邊也有很多研究生同伴在水深火熱，有時候心情不好還可以彼此互相鼓勵安慰真的是挺溫馨的。</p><p>最後終於撐完了這兩年真的很是欣慰，只能說<strong>走研究的路是孤獨的，在努力的同時還是要找點能讓自己開心的事情，興趣也好，出去走走也好，適當的轉換一下研究上遇到的壓力與煩惱，才能夠堅持下去</strong>，與所有研究生同伴們共勉之。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594796666/blog_posts/bcd9507b5592c218026cbf2a4a529871_tkp8x3.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]ImageNet Classification with Deep Convolutional Neural Networks</title>
      <link href="/posts/e54c12ea/"/>
      <url>/posts/e54c12ea/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>paper: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p><p>AlexNet，2012年的ILSVRC冠軍，也是首度使用了CNN模型屌打傳統的ML方法，讓大家開始脫離手工提取特徵的時代。</p><p>這一篇只有9頁，對比於上一篇LeNet，真是感激萬分。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective.</p></blockquote><p>摘要主要點出了幾個使用到的技術:</p><ul><li>non-saturating neurons: 就是<strong>ReLU</strong> La~</li><li>effifient GPU implementation: 各位當時的設備很可憐阿QQ GPU的memory非常少R，所以他們把model分到兩個不同的gpu，然後再去共享data</li><li><strong>使用了dropout來避免overfitting</strong></li></ul><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don’t have. Convolutional neural networks (CNNs) constitute one such class of models…</p></blockquote><p>圖像識別任務的巨大複雜度(從數百萬張圖片分類數千個對象)在即使給予了很龐大的dataset，像是ImageNet也很難做得好，模型中應該需要包含大量的先驗知識來補償我們不具有的data。在此之上CNN就是一個很適合的方法。</p><p>不過在當時的年代，CNN的計算量仍然是一個很大的負擔，這篇論文主要的貢獻如下:</p><ul><li>寫了一個使用GPU來training CNN的implementation</li><li>透過一些技巧(ReLU, data augumentation)得到了更好的result</li><li>加入了dropout防止overfitting</li><li>AlexNet: 5層Conv layer + 3層FC layer</li><li>「快…還要更快…」實驗數據表明只要有更快更強的GPU，我們結果就能夠好<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594537508/blog_posts/UfXTSPn_u5sgr0.gif" alt=""></li></ul><h2 id="The-Dataset"><a href="#The-Dataset" class="headerlink" title="The Dataset"></a>The Dataset</h2><p>介紹ILSVRC和ImageNet，有興趣的可以再自己去看。主要做的前處理部分是:</p><ol><li>圖像的大小不同，所以統一reszie到256x256</li><li>對RGB做centering，也就是說計算所有圖片在R,G,B上面的mean跟std，然後做normalization</li></ol><p>所以這也解釋了為什麼會在<a href="https://pytorch.org/docs/stable/torchvision/models.html" target="_blank" rel="noopener">Pytorch document</a>中看到這種神奇的數字<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">normalize = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                                 std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br></pre></td></tr></table></figure></p><p><strong>[0.485, 0.456, 0.406]就是ImageNet中R, G, B的mean</strong>，後來大家習慣使用在ImageNet上pretrain的model時也會一併使用這組value來做前處理</p><h2 id="The-Architecture"><a href="#The-Architecture" class="headerlink" title="The Architecture"></a>The Architecture</h2><p>接下來他們依照重要程度來介紹它們使用了那些技術，以及模型架構。</p><h3 id="ReLU-Nonlinearity"><a href="#ReLU-Nonlinearity" class="headerlink" title="ReLU Nonlinearity"></a>ReLU Nonlinearity</h3><p>傳統的activation function，如sigmoid, tanh都是saturating nonlinearities，也就是說他們的值會有一個區間。而AlexNet<strong>使用了non-saturating nonlinearity的ReLU</strong>($f(x)=max(0,x)$)，使得訓練速度快了很多。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594576446/blog_posts/%E6%93%B7%E5%8F%96_cm3o3x.png" alt=""></p><p>這張圖是說，使用ReLU(實現)比使用tanh(虛線)的快了六倍，棒棒哒。</p><h3 id="Training-on-Multiple-GPUs"><a href="#Training-on-Multiple-GPUs" class="headerlink" title="Training on Multiple GPUs"></a>Training on Multiple GPUs</h3><p>當時他們使用了兩顆GTX 580 GPU，然後分別進行訓練，只在某些layer進行交互，細節看架構圖就知道了。</p><h3 id="Local-Response-Normalization"><a href="#Local-Response-Normalization" class="headerlink" title="Local Response Normalization"></a>Local Response Normalization</h3><p>儘管ReLU在positive沒有upper bound，他們卻發現對ReLU做Normalization也能提升效能，這個就是Local Response Normalization，公式如下:</p><script type="math/tex; mode=display">b_{x, y}^{i}=a_{x, y}^{i} /\left(k+\alpha \sum_{j=\max (0, i-n / 2)}^{\min (N-1, i+n / 2)}\left(a_{x, y}^{j}\right)^{2}\right)^{\beta}</script><p>其中$k, \alpha, \beta$都是超參數，$a_{x,y}$是(x, y)位置經過activation function後的值，也就是說以(x, y)為中心，左右共取了$n$個值一起做Normalization(左邊取i-n/2，右邊取i+n/2)。</p><p>他們發現這樣做讓testing err從13%降到11%</p><p>(題外話，後來好像很少人用LRN，所以去查了一下，結果在VGG的paper中好像提到了使用LRN對他們並沒有幫助)</p><h3 id="Overlapping-Pooling"><a href="#Overlapping-Pooling" class="headerlink" title="Overlapping Pooling"></a>Overlapping Pooling</h3><p>一般的pooling並不會overlap，不過這邊使用到stride&lt; width(height)的pooling，使得pooling之間取值是會重疊的，他們發現這樣可以降低overfitting。</p><p>使用overlapping使得error降低了0.4%</p><h3 id="Overall-Architecture"><a href="#Overall-Architecture" class="headerlink" title="Overall Architecture"></a>Overall Architecture</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594579999/blog_posts/%E6%93%B7%E5%8F%96_woxr2t.png" alt=""></p><p>很久以前看到這圖還想說是不是截圖的人沒截好，這圖怎麼被切到了，今天看完paper才恍然大悟…</p><p><strong>原來他本來就長的一副被切樣阿阿阿阿阿!</strong></p><p>注意到這個架構雖然使用到了2張GPU，但他並不是像現在的平行處理架構，而是有特別設計過的。這個架構現在應該也沒什麼人會用了，<del>因為Nivdia乾爹威猛，區區記憶體還不夠多嗎</del></p><p>值得一提的是，<strong>最後一層是使用了softmax而不是cross entropy做為activation function</strong></p><p>其他架構的細節我這裡就不講了，有興趣的可以參考<a href="https://zhuanlan.zhihu.com/p/80087776" target="_blank" rel="noopener">可能是史上最全面的AlexNet 论文笔记与复现！！</a>的內容</p><h2 id="Reducing-Overfitting"><a href="#Reducing-Overfitting" class="headerlink" title="Reducing Overfitting"></a>Reducing Overfitting</h2><p>介紹了兩種使用到的避免overfitting的方法</p><h3 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h3><p>包含了圖像平移和水平翻轉。</p><p>圖像平移的部分在256x256的圖上隨機提取224x224來當作training image，然後再搭配水平翻轉</p><ul><li>這樣的方法使得training data增加了2048倍(每一張圖都有32x32個不同的子圖，然後還可以水平翻轉所以共有32x32x2=2048)</li><li>儘管這樣取得的圖片當然具有高度的相關性(因為可能是同一張圖片然後切成不同的子圖)，但仍然有效。</li></ul><p>在testing時，他們會對同一張圖片取5張224x224的子圖和水平翻轉的圖，總共10張去predict然後在用average output做softmax取得label。</p><p>第二種方式是改變RGB channel的強度，透過PCA去取得RGB的principle component，然後對每張圖加上隨機量的RGB principle component:</p><script type="math/tex; mode=display">\left[\mathbf{p}_{1}, \mathbf{p}_{2}, \mathbf{p}_{3}\right]\left[\alpha_{1} \lambda_{1}, \alpha_{2} \lambda_{2}, \alpha_{3} \lambda_{3}\right]^{T}</script><p>這樣的方法降低了1%的error</p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>在training時，每個neuron有0.5的機率會被隱藏，這樣能使得模型更加的robust(有點類似ensemble model的效果)。testing時使用所有的neuron，但將結果乘上0.5。</p><p>AlexNet在前兩層FC中使用到了dropout，他們也提到使用dropout會使得收斂所需要的iteration加倍。</p><h2 id="Details-of-learning"><a href="#Details-of-learning" class="headerlink" title="Details of learning"></a>Details of learning</h2><p>使用SGD，然後搭配weight decay的learning rate…剩下的自己看。</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594581622/blog_posts/%E6%93%B7%E5%8F%96_cwjpvj.png" alt=""></p><p>7層的AlexNet效果最好，棒棒哒。</p><p>然後他們也觀察了在第一層Conv的kernel長怎樣(第一層Conv有2個(48,3,11,11)，所以總共有96張(3,11,11)的視覺化圖)，然後他們發現第一個GPU都是學習到邊緣的特徵，第二個GPU都是學習到顏色的特徵比較多。</p><ul><li>這也是CNN可視化的開山始祖</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594585486/blog_posts/%E6%93%B7%E5%8F%96_onkpde.png" alt=""></p><p>接下來他們做了另一件事情: 透過最後一層的output vector來觀察是不是距離相近的相似度也比較近(這裡用的是歐式距離)。下圖中第一欄都是testing set的圖片，然後二到五欄都是training set中與第一欄的歐式距離最小的圖片。<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594582003/blog_posts/%E6%93%B7%E5%8F%96_cnxmlq.png" alt=""></p><p>透過這個觀察可以說，<strong>模型在higher level如果距離比較小的vector是它認為比較相似的</strong>。儘管AlexNet最後一層是4096在計算上很沒有效率，不過他們也提到了這個是可以被優化的，例如train一個auto-encoder來壓縮vector長度，這在圖像檢索領域(image retrieval)上是一個很大的進展。</p><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>AlexNet作為第一個在ILSVRC告訴大家別再傻傻地人工取feature然後用SVM的人，這篇的出現導致從此ILSVRC正式CNN時代。</p><p>雖然裡面的架構現在很少人在用了，不過裡面使用到的技術如ReLU, Dropout等技術仍然是現在很受用的技巧。</p><p>然後這一篇只有9頁，非常簡潔有力，小弟看得很開心，希望後續要看的paper再接再厲。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/80087776" target="_blank" rel="noopener">可能是史上最全面的AlexNet 论文笔记与复现！！</a></li><li><a href="https://pytorch.org/docs/stable/torchvision/models.html" target="_blank" rel="noopener">Pytorch document</a></li><li><a href="https://zhuanlan.zhihu.com/p/32995244" target="_blank" rel="noopener">深度学习中Normalization的简介以及Batch Normalization的Alternative实现</a></li><li><a href="https://zhuanlan.zhihu.com/p/22734982" target="_blank" rel="noopener">【阅读笔记】AlexNet - ImageNet Classification with Deep Convolutional Neural Networks</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Gradient Based Learning Applied to Document Recognition</title>
      <link href="/posts/d5321308/"/>
      <url>/posts/d5321308/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>paper: <a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" target="_blank" rel="noopener">http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf</a></p><p><strong>CNN系列的開山始祖，LeNet(因為作者是Yann LeCun)</strong>，被應用在手寫辨識上，簡單的幾層Conv就可以在MNIST dataset上達到90%以上的Acc.。至今已經有兩萬以上的cite數量。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing.</p></blockquote><p>基於gradient based learning的多層神經網路可以合成一個複雜的decision surface，使得在高維下也能夠很好地區分不同的pattern，例如手寫辨識，並且相較於傳統的技術，不需要太多的前處理就可以做得很好。</p><p>這篇paper的貢獻可以簡述成下列幾項:</p><ul><li>傳統的手寫辨識技術需要較多的前處理來提取特徵，透過自動學習來建立一個pattern recognition systems，降低前處理的步驟</li><li>提出了基於梯度學習的神經網路，並和當前手寫辨識的各類方法進行了比較</li><li>手寫辨識系統不只有辨識的部分，還包含了句子的分段，paper中也提出設計了一個通用化的架構: graph transformer networks (GTN’s)來做這件事。</li></ul><p>這篇文章中會聚焦在Deep learning的部分，關於後面的系統架構不會做太多的介紹，畢竟長達46頁看得頭很痛QQ</p><a id="more"></a><h2 id="Introudction"><a href="#Introudction" class="headerlink" title="Introudction"></a>Introudction</h2><p>傳統的pattern recognition通常分成兩個部分，因為data的變異性和豐富性使得無法完全透過手工來建立一個精準的辨識系統(要建立的feature太多了)，所以以前的方法會分成兩個部分(手工+自動)，大致上可以分成下圖兩個架構:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594297396/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-07-09_%E4%B8%8B%E5%8D%888.23.02_t14lck.png" alt=""></p><p>在這個架構中: </p><ol><li>透過一個人工建立的feature extraction module取得feature，這個module將raw data轉換成低維的feature vector或是symbols，使得這些feature可以<ul><li>很簡單的拿來進行匹配或比較的操作</li><li>對於不改變input性質的一些transformations和distortions具有不變性(比較robust)</li><li><strong>這個feature extractor也包含大量的先驗知識(也就是需要大量的domain knowledge)</strong>，並且是基於特定任務的(for special case)</li></ul></li><li>透過一個自動化的classifier module來進行分類</li></ol><blockquote><p>One of the main problems with this paper is that the recognition accuracy is larger determined by the ability of the designer to come up with an appropriate set of features .This turns out to be a daunting task which,unfortunately,must be redone for each new problem</p></blockquote><p>一個pattern recognition system的好壞很大一部分取決於前面的feature extractor，很直觀的，因為如果特徵取的不好，那在後面怎麼學也是學不好的。並且這個extractor還是case by case，所以很耗費大量的人力成本</p><p>傳統的技術上，特徵萃取的技術都被限制在低維度空間上進行操作才能有比較好的表現，但近幾年來這個限制卻被突破了，作者認為基於下面的原因:</p><ol><li>高效的設備成本越來越低，使得大家越來越有能力可以做一些運算量大的計算(例如numerical brute-force，其實就是在講Neural Network)</li><li>資料的取得更加容易，所以可以透過大量的real data來減少hand-crafted feature extraction</li><li>最重要的是，machine learning technique的興起使得可以處理高維度的資料</li></ol><h3 id="Leaning-from-Data"><a href="#Leaning-from-Data" class="headerlink" title="Leaning from Data"></a>Leaning from Data</h3><p>(看這段的時候好像再重新學習deep learning的基礎xD)</p><p>近期(注意這篇paper是1998年喔，大概是我兩歲的時候)一個成功的automatic machine learning技術，叫做”numerical”或”gradient-based learning”，對於第$p$個input pattern $Z^{p}$，給予一組權重$W$，使用機器學習的方式$F(\cdot)$可以學到一個predict $Y^{p}$，公式如下:<script type="math/tex">Y^{p}=F(Z^{p}, W)</script></p><p>然後我們可以去計算這個prediction跟ground truth $D^{p}$的差異:</p><script type="math/tex; mode=display">E^{p}=\mathcal{D}\left(D^{p}, F\left(W, Z^{p}\right)\right) \\E_{t r a i n}=\frac{1}{P} \sum_{n-1}^{P} E^{p}</script><p>而一種最佳化方法就是<strong>找到一組$W$使得minimize $E_{t r a i n}$</strong>，這個performance通常會在獨立於training set的另一組data上來評估，也就是testing set。一些研究發現testing error和train error之間的差異(gap)會近似(這一項有點眼熟，好像就是VC bound? h好像就是VC dimension?)<br>(VC bound的部分推薦林軒田教授的機器學習課程，雖然很數學，不過稱過一次可以很清楚理解，讚讚)</p><script type="math/tex; mode=display">E_{\text {test}}-E_{\text {train}}=k(h / P)^{\alpha}</script><p>跟在介紹VC bound時一樣，這個公式可以幫助我們理解怎樣對於machine learning是有幫助的:</p><ul><li>當traning samples增加的時候這個gap會降低</li><li>當訓練複雜度h增加的時候， E_{train}會降低<ul><li>所以這裡會有一個trade-off，在gap和$E_train$之間</li></ul></li></ul><p>如何想辦法降低gap的同時也降低training error，這類研究稱之structural risk minimization</p><ul><li>通常會透過加一個regularization function來trade-off模型的複雜度</li></ul><h3 id="Gradient-Based-Learning"><a href="#Gradient-Based-Learning" class="headerlink" title="Gradient Based Learning"></a>Gradient Based Learning</h3><p>更新權重的公式，用到微積分的chain rule</p><script type="math/tex; mode=display">W_{k}=W_{k-1}-\epsilon \frac{\partial E(W)}{\partial W}</script><p>或用SGD</p><script type="math/tex; mode=display">W_{k}=W_{k-1}-\epsilon \frac{\partial E^{p_{k}}(W)}{\partial W}</script><p>這個都聽到爛掉不想看惹，跳過，有興趣再自己去補。</p><h3 id="Gradient-Back-Propagation"><a href="#Gradient-Back-Propagation" class="headerlink" title="Gradient Back-Propagation"></a>Gradient Back-Propagation</h3><p>論文中提到，近年來gradient based learning的興起和下面幾件事情有關:</p><ol><li>在高維度中，loss function的local minima並不是實務上的問題<ul><li><strong>在高維度中，其實收斂到鞍點的機率遠大於收斂到局部最小值</strong>，有興趣的可以參考<a href="https://www.zhihu.com/question/68109802" target="_blank" rel="noopener">梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？</a> </li></ul></li><li>透過back propagation，可以在多層非線性系統計算gradient</li><li>back propagetion搭配神經網路可以學習到複雜的任務</li></ol><h3 id="Learning-in-Real-Handwriting-Recognition-Systems"><a href="#Learning-in-Real-Handwriting-Recognition-Systems" class="headerlink" title="Learning in Real Handwriting Recognition Systems"></a>Learning in Real Handwriting Recognition Systems</h3><p>跟Deep Learning無關，跳過</p><h3 id="Global-ly-Trainable-Systems"><a href="#Global-ly-Trainable-Systems" class="headerlink" title="Global ly Trainable Systems"></a>Global ly Trainable Systems</h3><p>跳過跳過</p><h2 id="Convolutional-Neural-Network-for-Isolated-Character-Recognition"><a href="#Convolutional-Neural-Network-for-Isolated-Character-Recognition" class="headerlink" title="Convolutional Neural Network for Isolated Character Recognition"></a>Convolutional Neural Network for Isolated Character Recognition</h2><p>傳統的方式會先透過人工的一些方法萃取出feature然後去訓練，不過如果想要讓模型自己學到這些feature的話該怎麼辦？</p><p>論文中提到了幾個困難點:</p><ol><li>image size過大，如果一開始就用FC(Fully Connected Layer)會造成參數過多</li><li><strong>FC這種沒有結構的網路並不具備平移、縮放等不變性(invariance with respect to translations, or local distortions of the inputs)</strong>，也就是說如果圖片有經過一些調整轉換、放大縮小、位移，則FC學到的特徵會完全不同。理論上一個非常大的FC搭配更多的data可以考慮這些狀況(使得所有case都可以被FC學到)，不過這不make sense<ul><li>而CNN具備了平移不變性(shift invariance)，所以很適用在圖片上的相關任務</li></ul></li><li>FC忽略了input的拓墣結構，<strong>對於圖片這種二維結構，相鄰的pixel有很大的相關性</strong>，不過這在FC中都只是彼此獨立的，甚至順序還可以互相對調而不影響結果。<ul><li>在CNN中則是可以學習到一些local feature</li></ul></li></ol><h3 id="Convolutional-Networks"><a href="#Convolutional-Networks" class="headerlink" title="Convolutional Networks"></a>Convolutional Networks</h3><p>CNN的三個重要概念:</p><ul><li><strong>local receptive</strong></li><li><strong>shared weights(weight replication)</strong></li><li><strong>spatial(temporal) sub-sampling</strong></li></ul><h4 id="local-receptive"><a href="#local-receptive" class="headerlink" title="local receptive"></a>local receptive</h4><p>相較於全連接層的感受範圍是整張圖片，一點點圖片的變化就會影響萃取的feature。使用CNN的kernel(或是filter)可以做到局部感受野，使得神經網路能夠提取一些局部特徵(邊緣、角)，並交給下一層合成更高級的特徵。</p><h4 id="shared-weights"><a href="#shared-weights" class="headerlink" title="shared weights"></a>shared weights</h4><p>由於圖片可能會位移或是形變，在做convolution時使用了具有相同權重的kernel不只可以降低計算量，也可以想成<strong>在不同圖片上使用相同的局部感受野萃取相同的特徵，這樣的話物體在圖片的哪個位置就相對沒那麼重要了</strong>。</p><p>shared weights和local  receptive是CNN對位移和形變robust的原因(shift-invariance)</p><h4 id="spatial-temporal-sub-sampling"><a href="#spatial-temporal-sub-sampling" class="headerlink" title="spatial(temporal) sub-sampling"></a>spatial(temporal) sub-sampling</h4><p>一但透過convolution得到feature map，特徵的位置就不是那麼重要了，特徵跟特徵之間的相對位置才是我們需要關注的。<strong>down-sampling的方式可以降低特徵在圖片中位置的精度，也可以使得CNN更加robust</strong>，這使得CNN對於物件在圖片中的什麼位置上影響比較相對較小</p><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p>最後來看一下LeNet的架構:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594476939/blog_posts/%E6%93%B7%E5%8F%96_gqikcf.png" alt=""></p><p>比較特別的地方稍微提一下:</p><h4 id="C3的Conv-layer"><a href="#C3的Conv-layer" class="headerlink" title="C3的Conv layer"></a>C3的Conv layer</h4><p>C3的Conv layer不是現在一般的Conv，而是將前一層(S2)的數個output，也就是feature map，做convolution，對應的關係如下表，列代表每一個feature map，行代表這一個convolution使用到了哪幾個feature map<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594477290/blog_posts/%E6%93%B7%E5%8F%96_cegd4i.png" alt=""></p><p>作者認為這樣的好處在於: </p><ul><li>降低參數使用量 </li><li>利用對稱的連接方式學到不同的特徵</li></ul><p>不過現在CNN幾乎沒人這樣用了…</p><h4 id="F6為啥是84"><a href="#F6為啥是84" class="headerlink" title="F6為啥是84?"></a>F6為啥是84?</h4><p>其實這也沒啥，神經網路數字想給多少就給多少，只要接得起來就好。不過為啥是84，難道LeCun也會通靈?</p><p>F6取84的原因是因為: 對應到ASCII的bitmap，如下圖每一個符號都是7 * 12的bitmap，所以這層的FC output是84<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1594477768/blog_posts/%E6%93%B7%E5%8F%96_uwqhhi.png" alt=""></p><p>你說阿為啥要讓他等於ASCII的bitmap? 因為在原始LeNet中，output layer的計算方式是用到Euclidean Radial Basis Function ( RBF )來計算每個類別的值:</p><script type="math/tex; mode=display">y_{i}=\sum_{j}\left(x_{j}-w_{ij}\right)^{2}</script><p>$x$是output layer的輸出(共10個)，$i$是類別(0-9)，$j$是bitmap中的每一個pixel(0-83)，所以RBF的值越接近0代表預測出來的圖片和ASCII該類圖片的bitmap越相近。</p><h4 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h4><p>使用MLE(Maximum Likelihood Estimation)，希望訓所有訓練資料預測正確的「正確類別」 RBF 值之平均越小越好。</p><p>不過單純這樣設計會有一些問題，所以又補上了Maximum a posteriori(MAP)進去，細節這裡就不講了，因為後續CNN其實也不是用這個方式來設計。不過有興趣的可以參考<a href="https://allen108108.github.io/blog/2019/10/04/[%E8%AB%96%E6%96%87]%20Gradient-Based%20Learning%20Applied%20to%20Document%20Recognition/" target="_blank" rel="noopener">[ 論文 ] Gradient-Based Learning Applied to Document Recognition</a>或<a href="https://hackmd.io/@DanielChen/rka5m35NX?type=view" target="_blank" rel="noopener">Gradient-Based Learning Applied to Document Recognition</a>這篇，覺得講得非常仔細。</p><p>不過後來的model也都是用單純的cross entropy來training…</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>原本是為了重看所有的CNN系列論文才開始看這篇，結果就發現第一篇居然給我有46頁讓我很無語(雖然最後也沒有全看完)。</p><p>重看這篇有總在複習Deep Learning跟CNN的感覺，主要更加了解CNN的三個特性(local receptive, shared weights, spatial sub-sampling)。以這篇為基礎，之後會開始看其他後續的CNN paper。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://blog.csdn.net/sunshine_010/article/details/79876255" target="_blank" rel="noopener">论文阅读笔记（一）：Gradient-Based Learning Applied to Document Recognition ( LeNet )</a></li><li><a href="https://allen108108.github.io/blog/2019/10/04/[%E8%AB%96%E6%96%87]%20Gradient-Based%20Learning%20Applied%20to%20Document%20Recognition/" target="_blank" rel="noopener">[ 論文 ] Gradient-Based Learning Applied to Document Recognition</a></li><li><a href="https://www.zhihu.com/question/68109802" target="_blank" rel="noopener">梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？</a></li><li><a href="https://blog.csdn.net/qianqing13579/article/details/71076261" target="_blank" rel="noopener">LeNet论文的翻译与CNN三大核心思想的解读</a></li><li><a href="https://hackmd.io/@DanielChen/rka5m35NX?type=view" target="_blank" rel="noopener">Gradient-Based Learning Applied to Document Recognition</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]系列文章介紹</title>
      <link href="/posts/aa55d3f9/"/>
      <url>/posts/aa55d3f9/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>論文速速讀系列是從今年四月開始，我開始寫一些論文的中文讀書筆記，還記得第一篇是<a href="https://meetonfriday.com/posts/98f60d1d/">[論文速速讀]ReZero is All You Need: Fast Convergence at Large Depth</a>，之後發現儘管自己已經陸續產出了幾篇文章，可是好像都沒正式的跟大家介紹這系列文章的由來xD</p><p>所以這篇文章就是來講講這系列文章到底是什麼，以及我會和會想寫這些文章。</p><h2 id="論文速速讀系列是什麼"><a href="#論文速速讀系列是什麼" class="headerlink" title="論文速速讀系列是什麼?"></a>論文速速讀系列是什麼?</h2><p>由於在AI領域每年總是有一些非常重大的突破和應用，如果跟不上潮流很有可能就會錯失許多機會。例如，對NLP領域熟悉的話你一定聽過2013年的word2vec、2014年開始流行的attention、2018年的Bert…這些很有名的技術。</p><p>還記得Bert剛出的時候我好像剛進碩士實驗室，當時只知道這個技術屌打了當時一堆NLP的研究，但我想也想不到兩年後Bert已經造成如此大的影響力，一堆基於Bert的變形應用在各大領域上都取得了非常優異的結果。</p><p>因此，我想要藉由這系列的文章讓自己能夠更加快速的了解AI的新技術和研究，同時逼迫自己看論文xD</p><a id="more"></a><p>這個系列的文章大多都是<strong>中文的讀書筆記的形式</strong>(畢竟對於我自己在中文的文章上還是能夠更加快速的理解的)，可能會將自己認為論文中重要的部分擷取出來做引用，並且加上一些自己的認知。在過程中如果有不懂的地方也會試著將外部的一些資訊一同補充進來(題外話，真的覺得知乎是個中文友善的學術研討平台，可以學到很多東西)。</p><p>然後我希望以快速理解論文的重點為主，所以<strong>對於實驗流程、資料集、數據比較的部分我會比較省略</strong>，此時你可能就會看到下面的Jumping馬力歐出現，代表我又要跳過這一部分了xD</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.pinimg.com/originals/51/18/62/511862395ab304a6991b2917fa2bc039.jpg" alt=""></p><p>最後，所有的文章都是自己的閱讀心得、或是整理網路上其他前輩的資源後的文章，如果觀念、講解上有任何錯誤還請協助提醒指教謝謝~</p><h2 id="文章整理"><a href="#文章整理" class="headerlink" title="文章整理"></a>文章整理</h2><p>雖然再Archives也能看的到，不過那邊還有包含了其他的文章，所以我會將這篇文章置頂，並做個簡單的分類，以便能夠快速查閱論文。</p><p>目前分類的部分大致會分成CV(Computer Vision), NLP(Natural Language Processing), and Others這三個項目，之後視情況會再進行微調。排序方式由上至下分別是根據我發文的日期最新到最舊的文章這樣。</p><p><strong>注意下列順序為我撰寫文章的順序，並非論文的年份順序，此分類僅供方便查閱文章使用，並不建議按照分類順序學習Deep Learning</strong></p><h3 id="Computer-Vision"><a href="#Computer-Vision" class="headerlink" title="Computer Vision"></a>Computer Vision</h3><ul><li><a href="https://meetonfriday.com/posts/2b446abf">[論文速速讀]Learning Deep Features for Discriminative Localization</a></li><li><a href="https://meetonfriday.com/posts/79fdff34">[論文速速讀]Squeeze-and-Excitation Networks</a></li><li><a href="https://meetonfriday.com/posts/7c0020de">[論文速速讀]Deep Residual Learning for Image Recognition</a></li><li><a href="https://meetonfriday.com/posts/5f745d96">[論文速速讀]Very Deep Convolutional Networks For Large-Scale Image Recognition</a></li><li><a href="https://meetonfriday.com/posts/a151bfa2">[論文速速讀]Network In Network</a></li><li><a href="https://meetonfriday.com/posts/263e065d/">[論文速速讀]Going deeper with convolutions</a></li><li><a href="https://meetonfriday.com/posts/3013fdb9">[論文速速讀]Visualizing and Understanding Convolutional Networks</a></li><li><a href="https://meetonfriday.com/posts/e54c12ea">[論文速速讀]ImageNet Classification with Deep Convolutional Neural Networks</a></li><li><a href="https://meetonfriday.com/posts/d5321308">[論文速速讀]Gradient Based Learning Applied to Document Recognition</a></li><li><a href="https://meetonfriday.com/posts/2e56b772/">[論文速速讀]End-to-end object detection with Transformers</a></li></ul><h3 id="Natural-Language-Processing"><a href="#Natural-Language-Processing" class="headerlink" title="Natural Language Processing"></a>Natural Language Processing</h3><ul><li><a href="https://meetonfriday.com/posts/abd81088/">[論文速速讀]Efficient Estimation of Word Representation in Vector Space</a></li><li><a href="https://meetonfriday.com/posts/aa73b0a2/">[論文速速讀]PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</a></li><li><a href="https://meetonfriday.com/posts/b70f16d0/">[論文速速讀]Attention is not Explanation</a></li><li><a href="https://meetonfriday.com/posts/6a169efb/">[論文速速讀]Attention-based LSTM for Aspect-level Sentiment Classification</a></li><li><a href="https://meetonfriday.com/posts/8cad39eb/">[論文速速讀]Hierarchical Attention Networks for Document Classification</a></li><li><a href="https://meetonfriday.com/posts/4f49bf9b/">[論文速速讀]NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a></li><li><a href="https://meetonfriday.com/posts/5839a8bf/">[論文速速讀]Attention Is All You Need</a></li><li><a href="https://meetonfriday.com/posts/8a0dacdf/">[論文速速讀]Automatic Generation of Personalized Annotation Tags for Twitter Users</a></li></ul><h3 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h3><ul><li><a href="https://meetonfriday.com/posts/1e087b70/">[論文速速讀]ILSVRC系列文回顧 - 各種CNN知名模型介紹</a></li><li><a href="https://meetonfriday.com/posts/d978825d/">[論文速速讀]Attentive CutMix: An Enhanced Data Augmentation Approach for Deep Learning Based Image Classification</a></li><li><a href="https://meetonfriday.com/posts/b4202d1/">[論文速速讀]CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</a></li><li><a href="https://meetonfriday.com/posts/98f60d1d/">[論文速速讀]ReZero is All You Need: Fast Convergence at Large Depth</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Efficient Estimation of Word Representation in Vector Space</title>
      <link href="/posts/abd81088/"/>
      <url>/posts/abd81088/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>這篇就是<strong>鼎鼎大名的word2vec，在2013年由google提出</strong>。</p><p>Word2vec是一種word embedding技術，為什麼需要word embedding? 因為這樣可以<strong>將NLP task的word轉成一個numeric vector，然後加以進行分析和運算</strong>。</p><p>paper: <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Efficient Estimation of Word Representation in Vector Space</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></blockquote><p>word2vec的paper，提出了Skip-gram跟CBOW，相較於傳統的方法，能夠大幅提高訓練速度。</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary. This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data.</p></blockquote><p>傳統的一些nlp task把word視作不可分割的單元(atomic units)，詞和詞之間不具相似度，因為他們都只是vocab table的index。這種方法簡單且robust，n-gram就是其中的一個應用。</p><blockquote><p>However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less. Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques.</p></blockquote><p>但是這種方法也有他的極限，當資料詞庫過少的時候表現就會不好。</p><p>這篇paper的目標:</p><ul><li>可從大量資料(數十億level)中學習word vectors的技術<ul><li>據目前所知，還沒有方法提出可以在幾百萬的資料上學習50-100的詞向量</li></ul></li><li>希望學習到相似的詞距離相近，而且詞有不同的相似度(multiple degrees of similarity)<ul><li>名詞的多個詞尾可能在相近的空間中被找到</li></ul></li><li>新的模型可以做到<strong>向量的線性操作</strong> <ul><li>vector(”King”) - vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen</li></ul></li></ul><p>接下來介紹NNLM(Neural Network Language Model): 一個線性投影層 + 一個非線性隱藏層，用來學習詞向量表示和統計語言模型</p><blockquote><p>… Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are first learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM.</p></blockquote><p>提到說NNLM先透過single hidden layer來訓練word vectors，然後在使用word vectors來train NNLM，所以即使NNLM沒有train完也可以train好word vectors。</p><p>word2vec就是基於這個想法的擴展應用。</p><h2 id="Model-Architectures"><a href="#Model-Architectures" class="headerlink" title="Model Architectures"></a>Model Architectures</h2><p>為了比較模型好壞，先定義接下來訓練深度模型的複雜度皆為:<script type="math/tex">O = E\times T\times Q</script></p><ul><li>E: 迭代次數</li><li>T: 訓練集的詞個數</li><li>Q: 模型參數</li><li>Common choice is E = 3 − 50 and T up to one billion</li></ul><p>使用mini-batch + Adagrad + SGD來訓練</p><h3 id="Feedforward-Neural-Net-Language-Model-NNLM"><a href="#Feedforward-Neural-Net-Language-Model-NNLM" class="headerlink" title="Feedforward Neural Net Language Model (NNLM)"></a>Feedforward Neural Net Language Model (NNLM)</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1593446401/blog_posts/ClFEz8v_wgqndr.jpg" alt=""></p><p>(上圖是原始paper(A Neural Probabilistic Language Model)的圖，annotation可能會跟下面word2vec的對不起來，下面使用原本word2vec的annotation)</p><p>有4層: input, projection, hidden, output</p><ul><li>input layer: <strong>前</strong>N words使用one-hot encoding成V維的向量，V是(vocab size)<ul><li>注意這裡是用前N words，而不是用所有words來訓練，這是和word2vec中CBOW投影層的差異!!</li></ul></li><li>projection layer: input(NxV)會使用同一個projection matrix(VxD)投影到projection layer P(NxD)<ul><li>D是投影後的維度</li><li>共用一個projection matrix，所以這裡的cost還算低</li></ul></li><li>hidden layer: 隱藏層來計算整個word的機率，有H個neuron</li><li>output layer有V個neuron</li></ul><p>所以整體的模型參數量是<script type="math/tex">Q=N×D+N×D×H+H×V</script></p><ul><li>其中output layer的HxV最重要<ul><li>有一些優化的方法，例如hierarchical softmax，使用binary tree representations of the vocabulary(Huffman tree)，可以降到$\log_2(V)$</li></ul></li><li>所以其實主要的計算量在hidden layer</li></ul><h3 id="Recurrent-Neural-Net-Language-Model-RNNLM"><a href="#Recurrent-Neural-Net-Language-Model-RNNLM" class="headerlink" title="Recurrent Neural Net Language Model (RNNLM)"></a>Recurrent Neural Net Language Model (RNNLM)</h3><p>原始paper: Linguistic Regularities in Continuous Space Word Representations</p><p>只有input, hidden, output層，訓練複雜度是<script type="math/tex">Q=D×H+H×V</script></p><ul><li>D和隱藏層H有相同的維度<ul><li>使用hierarchical softmax + huffman tree，H×V可以降低為H×$\log_2V$，所以大部分的複雜度來自D×H</li></ul></li></ul><h3 id="Parallel-Training-of-Neural-Networks"><a href="#Parallel-Training-of-Neural-Networks" class="headerlink" title="Parallel Training of Neural Networks"></a>Parallel Training of Neural Networks</h3><p>使用DistBelief框架，該框架可以使用平行計算</p><h2 id="New-Log-linear-Models"><a href="#New-Log-linear-Models" class="headerlink" title="New Log-linear Models"></a>New Log-linear Models</h2><p>提出了兩個新模型，之前的研究表示<strong>大部分的複雜度是由於模型的非線性層導致的</strong>，所以這裡試圖減少非線性層的參數量</p><h3 id="Continuous-Bag-of-Words-Model"><a href="#Continuous-Bag-of-Words-Model" class="headerlink" title="Continuous Bag-of-Words Model"></a>Continuous Bag-of-Words Model</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1593446401/blog_posts/eAzmaER_yg2aym.png" alt=""></p><p>和NNLM相似，但刪除了非線性層，並且投影層是所有的words共用(NNLM是前N words共用)</p><ul><li>所有的单词都投影到同一个位置（所有向量取平均值）。这样不考虑单词的位置顺序信息，叫做词袋模型<ul><li>詞的順序對於不影響投影</li></ul></li><li>会用到将来的词，例如如果窗口 windows 为 2，这样训练中心词的词向量时，会选取中心词附近的 4 个上下文词（前面 2 个后面 2 个）</li><li>输出层是一个 log-linear 分类器</li></ul><p>整體的模型參數量為:<script type="math/tex">Q=N×D+D×log(V)</script></p><ul><li>log(V)是用到了hierarchical softmax + huffman tree</li></ul><h3 id="Continuous-Skip-gram-Model"><a href="#Continuous-Skip-gram-Model" class="headerlink" title="Continuous Skip-gram Model"></a>Continuous Skip-gram Model</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1593446401/blog_posts/3rwiJtE_t8r2c1.png" alt=""></p><p>跟CBOW相似，不過是根據中心的詞去預測上下文</p><ul><li>通過實驗發現，windows越大效果越好(但cost也越大)</li><li>距離較遠的word通常關聯性較小，所以透過抽取較少的樣本(降低機率)來降低對距離較遠word的權重</li></ul><p>整體複雜度:<script type="math/tex">Q=C×(D+D×log(V))</script></p><ul><li>C是window size的2倍，也就是要預測的word個數<ul><li>也就是說預測每個word所需的參數量是D+D×log(V)</li><li>看到logV就知道用到了hierarchical softmax</li></ul></li></ul><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><blockquote><p>We follow previous observation that there can be many different types of similarities between words, for example, word big is similar to bigger in the same sense that small is similar to smaller. Example of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further denote two pairs of words with the same relationship as a question, as we can ask: ”What is the word that is similar to small in the same sense as biggest is similar to big?”</p></blockquote><p>單詞間具有相似性，並且可透過向量加減法來得到語意上的正確結果</p><h2 id="延伸討論-Word2vec-implement-detail"><a href="#延伸討論-Word2vec-implement-detail" class="headerlink" title="延伸討論: Word2vec implement detail"></a>延伸討論: Word2vec implement detail</h2><p>下面講的可能不在原始paper內，不過是實作上可能會被提及的技術。根據一些網站資源學習了之後加以整理:</p><h3 id="Hidden-layer沒有activation-function"><a href="#Hidden-layer沒有activation-function" class="headerlink" title="Hidden layer沒有activation function"></a>Hidden layer沒有activation function</h3><p>在input-hidden層，沒有非線性變換，而是簡單地把所有vector加總並取平均，以減少計算複雜度</p><h3 id="Hierarchical-Softmax-v-s-Negative-sampling"><a href="#Hierarchical-Softmax-v-s-Negative-sampling" class="headerlink" title="Hierarchical Softmax v.s. Negative sampling"></a>Hierarchical Softmax v.s. Negative sampling</h3><p>實際上，input layer有CBOW和Skip-gram兩種版本，output也有Hierarchical Softmax和Negative sampling兩種版本</p><h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p>一般正常input-hidden-output的model如果套用在embedding training，因為output層的softmax計算量很大(要去算所有詞的softmax機率，再去找機率最大值)<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1593446400/blog_posts/6m9UYxV_kjxacf.png" alt=""></p><ul><li>透過huffman tree + Hierarchical Softmax，tree的根節點是每一個word，透過一步步走到leaf來求得softmax的值<ul><li>從root到leaf只需要log(V)步</li></ul></li><li>如此可以大幅的加快求得softmax的速度</li><li>缺點: 但是如果我们的训练样本里的中心词w是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了<ul><li>於是有了Negative Sampling </li></ul></li></ul><h4 id="Negative-sampling"><a href="#Negative-sampling" class="headerlink" title="Negative sampling"></a>Negative sampling</h4><p>定義window內的為正樣本，window外的為負樣本，如此就可以不用把全部的word拿進來一起train</p><ul><li>只需要window內的所有正樣本 + sampling一定數量的負樣本就足夠訓練模型</li></ul><p>透過Unigram distribution來模擬負樣本(不在window內的word)被選中的機率</p><ul><li>設計這個分佈時希望詞被抽到的機率要跟這個詞出現的頻率有關，出現在文本中的頻率越高越高越有可能被抽到</li></ul><p>公式為:<script type="math/tex">P(w_i) = \frac{ {f(w_i)}^{3/4}  }{\sum_{j=0}^{n}\left( {f(w_j)}^{3/4} \right ) }</script><br>$f(w_i)$代表$w_i$出現次數(頻率)，3/4是實驗try出來的數據</p><ul><li>例如：有一個詞編號是 100，它出現在整個文本中 1000 次，所以 100 在 unigram table 就會出現 $1000^{0.75} = 177$ 次</li></ul><p>至於要選幾個詞當 negative sample，paper 中建議如下</p><blockquote><p>Our experiments indicate that values of k in the range 5–20 are useful for small training datasets, while for large datasets the k can be as small as 2–5.</p></blockquote><h3 id="Subsampling-of-frequent-words"><a href="#Subsampling-of-frequent-words" class="headerlink" title="Subsampling of frequent words"></a>Subsampling of frequent words</h3><p>英文中 “the”, “a”, “in”，中文中的「的」、「是」等等這種詞，其實在句子中並沒有辦法提供太多資訊但又常常出現，對訓練沒有太大幫助，所以就用一個機率來決定這個詞是否要被丟掉，公式如下<script type="math/tex">P(f_i) = (\sqrt{\frac{f(w_i)}{0.001}} + 1) \cdot \frac{0.001}{f(w_i)}</script></p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>word2vec真滴神，然後skip-gram似乎普遍比CBOW好用</p><p>現在有很多方法可以直接使用word2vec，如gensim，不過了解了背後的由來相信能夠對於word2vec更有感覺，以及更清楚應該如何調整相關參數。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://ctjoy.github.io/jekyll/update/2017/10/23/word2vec-tutorial.html" target="_blank" rel="noopener">Word2vec Tutorial</a></li><li><a href="https://www.cnblogs.com/pinard/p/7243513.html" target="_blank" rel="noopener">word2vec原理(二) 基于Hierarchical Softmax的模型</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</title>
      <link href="/posts/aa73b0a2/"/>
      <url>/posts/aa73b0a2/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>paper: <a href="https://arxiv.org/pdf/1912.08777.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1912.08777.pdf</a></p><p>Google ICML 2020的paper，一打開發現有54頁…哎呀我的媽</p><p>好險後面都是附錄，實際上只有8頁而已。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>　Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains.</p></blockquote><p>基於transformers的self-supervised模型(bert以及相關變形)已經橫掃一遍了NLP的應用，透過bert的pretrained model來fine tune後續的task已經被證實可以達到很好的結果，包含文本摘要。不過在抽象式文本摘要(abstractive text summarization)的部分還做得不夠好。所以這篇就是提出一個NLP的pre-trained model來做這件事。</p><p>抽象式文本摘要是啥，跟一般文本摘要又差在哪裡? 可以看下面Introduction一開始的介紹。</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>Text summarization aims at generating accurate and concise summaries from input document(s). In contrast to extractive summarization which merely copies informative fragments from the input, abstractive summarization may generate novel words. A good abstractive summary covers principal information in the input and is linguistically fluent.</p></blockquote><p>文本摘要有兩種類型:</p><ul><li><strong>extraction-based summarization</strong>: 僅從文章中擷取重要的詞來組成摘要</li><li><strong>abstractive summarization</strong>: 產生的摘要可能包含文章沒出現過的詞，這種方法產生的摘要往往更加流暢以及能夠表達得更加清晰</li></ul><p>重點來了:</p><blockquote><p>We find ument and generating these gap-sentences from the rest of the document works well as a pre-training objective for downstream summarization tasks. In particular, choosing putatively important sentences outperforms lead or randomly selected ones. We hypothesize this objective is suitable for abstractive summarization as it closely resembles the downstream task, encouraging whole-document understanding and summary-like generation. We call this self-supervised objective Gap Sentences Generation (GSG).</p></blockquote><p>他們發現，<strong>透過遮住部分的句子來進行pre-training，這樣的方式有助於文本摘要的task</strong>。</p><ul><li>也就是說，mask幾個句子，然後goal是要還原這些句子，透過這種方式來pre-train。 </li><li>並且如果遮住的句子是重要的效果會更好。<ul><li>哪些句子是重要的呢? paper中提到他們是透過importance來選</li></ul></li></ul><p>好結束，本篇論文就讀到這裡，大家再會。</p><p>…開個玩笑，不過重點就真的是這裡，概念懂了就通了，我們再透過google ai blog的一張gif來搭配理解:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1592500770/blog_posts/image1_iaq3w0.gif" alt=""></p><p>遮住部分句子，然後透過transformer還原那些句子(注意並不是還原整句input)。</p><p>題外話，<strong>這種self-supervised(自監督學習)的方法優勢在於，不需要人工標註label(因為label就來自你的dataset，你只要想辦法去前處理他)，所以你可以產生跟dataset一樣多的資料來train你的model</strong>，而這也是其他supervised方法的缺點。</p><p>他們<strong>假設會比較好是因為這樣的pre-training很接近他們的downstream task</strong>(也就是抽象文章摘要)想要做的事情。</p><p>他們稱這種方法為Gap Sentences Generation (GSG)，然後透過這個pre-trained方法產生的抽象文版摘要seq2seq模型稱之天馬，<strong>PEGASUS</strong>(Pre-training with Extracted Gap-sentences for Abstractive SUmmarization Sequence-to-sequence models)</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://n.sinaimg.cn/sinacn10122/217/w640h377/20191012/82b3-ifvwftk1510380.png" alt=""></p><p>…好，上面那張是亂入的，看不懂不要理我沒關係。</p><p>不過只細看全名的話會發現根本不是每個word的第一個字，很棒，延續了芝麻街時代之後，NLP取名字的傳統xD</p><h2 id="Pre-training-Objectives"><a href="#Pre-training-Objectives" class="headerlink" title="Pre-training Objectives"></a>Pre-training Objectives</h2><h3 id="Gap-Sentences-Generation-GSG"><a href="#Gap-Sentences-Generation-GSG" class="headerlink" title="Gap Sentences Generation (GSG)"></a>Gap Sentences Generation (GSG)</h3><p>這邊在講如何找出之後要mask的gap sentence。</p><p>首先，對於大量的文本dataset我們並不具有文本摘要的ground truth，按照這篇的想法我們可以用gap sentence來替代摘要當作ground truth，但要如何找到適合的gap sentence呢?</p><p>他們做了三種不同的方法:</p><ol><li>Random: 隨機選m句子</li><li>Lead: 開頭的前m個句子</li><li>Principal: 透過importance來找出前m個重要的句子，透過算ROUGE1-F1來實現，細節可以去看paper。<ul><li>在Principal的方法上總共有4種不同的策略，分別是Ind-Orig, Ind-Uniq, Seq-Orig, Seq-Uniq，細節請參閱paper</li></ul></li></ol><p>之後把gap sentence都用<code>[MASK1]</code>替換掉來做pre-training</p><h3 id="Masked-Language-Model-MLM"><a href="#Masked-Language-Model-MLM" class="headerlink" title="Masked Language Model (MLM)"></a>Masked Language Model (MLM)</h3><p>MLM透過隨機mask句子的token來學習token的上下文關係。這邊的用法跟Bert差不多，然後他們也附了一張圖幫助讀者瞭解如果要同時用GSG跟MLM的架構長怎樣。<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1592503091/blog_posts/%E6%93%B7%E5%8F%96_s1b2qc.png" alt=""></p><p>不過他們也說，在後面的實驗發像MLM的mask效果其實並不好(下方實驗有提到)，所以最終的模型版本$PEGASUS_{LARGE}$其實沒有採用MLM</p><h2 id="Pre-training-Corpus"><a href="#Pre-training-Corpus" class="headerlink" title="Pre-training Corpus"></a>Pre-training Corpus</h2><p>使用兩個dataset來做pre-training</p><ul><li>C4</li><li>HugeNews</li></ul><h2 id="Downstream-Tasks-Datasets"><a href="#Downstream-Tasks-Datasets" class="headerlink" title="Downstream Tasks/Datasets"></a>Downstream Tasks/Datasets</h2><p>接下來在12個dataset上實驗，細節去看paper啦!</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>要繼續加速了喔，請繫好安全帶…</p><blockquote><p>We refer to $PEGASUS_{BASE}$ without pre-training as $Transformer_{BASE}$</p></blockquote><p>首先他們採用了一個簡單的$PEGASUS_{BASE}$模型，然後來進行不同的實驗驗證不同策略下的好壞，最後再用最好的策略來進行$PEGASUS_{LARGE}$的建模。</p><h3 id="Ablations-on-PEGASUS-BASE"><a href="#Ablations-on-PEGASUS-BASE" class="headerlink" title="Ablations on $PEGASUS_{BASE}$"></a>Ablations on $PEGASUS_{BASE}$</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1592503982/blog_posts/%E6%93%B7%E5%8F%96_bmgj2j.png" alt=""></p><p>在兩個dataset上pre-trained後，在4個downstream dataset上進行實驗的結果，發現HugeNews的pre-train在一些新聞相關的downstream dataset效果比較好，反之則是C4的效果較好</p><ul><li>所以他們<strong>建議downstream task的類型是啥，pre-trained就最好用相同類型的dataset</strong></li></ul><h3 id="EFFECT-OF-PRE-TRAINING-OBJECTIVES"><a href="#EFFECT-OF-PRE-TRAINING-OBJECTIVES" class="headerlink" title="EFFECT OF PRE-TRAINING OBJECTIVES"></a>EFFECT OF PRE-TRAINING OBJECTIVES</h3><h4 id="GSG"><a href="#GSG" class="headerlink" title="GSG"></a>GSG</h4><p>選擇了30%的句子來做GSG，然後用了六種GSG策略來做實驗(Random, Lead, Ind-Orig, Ind-Uniq, Seq-Orig, Seq-Uniq)，發現Ind-Orig的效果最好。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1593006021/blog_posts/%E6%93%B7%E5%8F%96_oar8un.png" alt=""></p><p>並且他們也觀察到了Lead在新聞的數據集上其實表現得不錯，不過在非新聞的數據集就差的很多，這可能是由於在新聞數據集上的lead bias現象導致的(lead bias in news datasets (See et al., 2017; Zhong et al., 2019))。</p><p>總之，最後在$PEGASUS_{LARGE}$中採用了Ind-Orig的GSG策略。</p><p>接下來他們看說在不同GSR(Gap Sentences Ratio)下效果如何，GSR過低會使得訓練沒有效率，GSR過高則會喪失過多的上下文資訊。最後$PEGASUS_{LARGE}$模型中採用了30%的GSR。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1593006785/blog_posts/%E6%93%B7%E5%8F%96_jda8ji.png" alt=""></p><h4 id="MLM"><a href="#MLM" class="headerlink" title="MLM"></a>MLM</h4><p>一樣可以從上面(a)圖中來觀察，他們發現使用MLM在早期的step可以加速收斂，但在後期卻限制了收斂的效果。所以他們在$PEGASUS_{LARGE}$中決定不採用MLM。</p><h3 id="EFFECT-OF-VOCABULARY"><a href="#EFFECT-OF-VOCABULARY" class="headerlink" title="EFFECT OF VOCABULARY"></a>EFFECT OF VOCABULARY</h3><p>比較不同tokenization和size的方法差異，最後採用Unigram 96k。</p><h3 id="Larger-Model-Results"><a href="#Larger-Model-Results" class="headerlink" title="Larger Model Results"></a>Larger Model Results</h3><p>這邊就是講說$PEGASUS_{LARGE}$的detail</p><h3 id="Zero-and-Low-Resource-Summarization"><a href="#Zero-and-Low-Resource-Summarization" class="headerlink" title="Zero and Low-Resource Summarization"></a>Zero and Low-Resource Summarization</h3><p>這裡在測試說當有label的資料很少的時候，PEGASUS的表現如何。他們發現，大約在1000個example來pre-trained時就能夠比使用$Transformer_{BASE}$，在full supervised datasets但沒有pre-trained的方法還好。</p><h3 id="Qualitative-Observations-and-Human-Evaluation"><a href="#Qualitative-Observations-and-Human-Evaluation" class="headerlink" title="Qualitative Observations and Human Evaluation"></a>Qualitative Observations and Human Evaluation</h3><p>這個部分是在測試，在不同dataset上用PEGASUS和人類產生的摘要，給其他人去判斷的話是否能夠分得出來(圖靈測試?)。</p><p>然後其實效果跟人類產生的摘要還不差呢! 棒棒哒~<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1593008225/blog_posts/%E6%93%B7%E5%8F%96_db8rhj.png" alt=""></p><h3 id="A-Test-of-Comprehension-Counting-Ships"><a href="#A-Test-of-Comprehension-Counting-Ships" class="headerlink" title="A Test of Comprehension: Counting Ships"></a>A Test of Comprehension: Counting Ships</h3><p>這裡的內容是出自於google ai blog的(<a href="https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html?m=1&amp;fbclid=IwAR2kcs5k3BysvuG4JIixX6iq2nkio_DkMIjQ7yWbmQrHbs6GjNSkVDiRT8E" target="_blank" rel="noopener">PEGASUS: A State-of-the-Art Model for Abstractive Text Summarization</a>)，透過一篇船的文章來舉例，他們發現: </p><ol><li>儘管文章內沒提到船的數量，但在文章內出現兩艘船、三艘船、四艘船的時候，模型仍然可以正確的摘要出船的數量(太神奇了傑克!)</li><li>但是在這方面(邏輯推理)的能力還有待加強，比方說在文章出現了六艘船時，文章摘要出來卻出現了七艘。</li></ol><p>細節內容可以從blog去看~</p><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>其實做了很多實驗，但方法不難理解，下面簡短的講一下我認為的重點:</p><ul><li><strong>google認為現階段的transformer-based(Ex: bert)的模型在訓練pre-trained上都是訓練一個通用的pre-trained model，但是針對downstream task去客製化預訓練的方式能夠有更好的結果。</strong></li><li>這篇的PEGASUS就是抽象文章摘要的一個客製化預訓練模型。</li><li>而預訓練的方法是屬於<strong>self-supervisied的一種，所以不用人工去產生大量的label</strong>，讚讚。</li><li>在少量的pre-trained下也可以達到不錯的效果。</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html?m=1&amp;fbclid=IwAR2kcs5k3BysvuG4JIixX6iq2nkio_DkMIjQ7yWbmQrHbs6GjNSkVDiRT8E" target="_blank" rel="noopener">PEGASUS: A State-of-the-Art Model for Abstractive Text Summarization</a></li><li><a href="https://github.com/google-research/pegasus" target="_blank" rel="noopener">google-research/pegasus</a></li><li><a href="https://zhuanlan.zhihu.com/p/148225554" target="_blank" rel="noopener">谷歌开源“穷人版”摘要生成NLP模型：1000个样本就能打败人类</a></li><li><a href="https://www.linkresearcher.com/theses/7054cdb3-b934-4fd8-9e5b-b4a320f5c6c7" target="_blank" rel="noopener">PEGASUS : 新的自监督目的的大型 Transformer 预训练编码器-解码器模型</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[DL]深入了解gradient descent</title>
      <link href="/posts/c41c815d/"/>
      <url>/posts/c41c815d/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>“在深度學習中，你(設計)的(損失)函數是怎麼更新的，他可微嗎?”</p><p>最近有聽到有人提了這個問題，一開始聽到這個問題也沒太放在心上，由於deep learning framework(Tensorflow, Pytorch)的便利性，大家可以不用管求導這件事，開開心心寫code。</p><p>不過後來想了想這的確是個可以來認真思考的事情，畢竟是走這行的，總不能一直都只依靠工具而不去了解背後的原理。於是努力上網東找西找拼湊了一套我覺得還算合理的說法，不過畢竟我不是數學系背景出生，如果有講錯的地方歡迎提出指教~</p><h2 id="How-Deep-Learning-Update"><a href="#How-Deep-Learning-Update" class="headerlink" title="How Deep Learning Update?"></a>How Deep Learning Update?</h2><p>我們知道，一個Deep learning model的參數量往往很大，在更新過程中是透過我們定義好的loss function(可能是cross-entropy或mse)來做gradient descent，gradient descent會去算每個參數的gradient，搭配learning rate來更新這些參數，使得讓模型越來越好。</p><p>不過算gradient這個動作在tensorflow/pytorch都自動幫我們算好了，以pytorch為例，我們可以從他的code來看到他更新的步驟(可以參考我寫的另一篇:<a href="https://meetonfriday.com/posts/18392404/">Pytorch的zero_grad()和backward()使用技巧</a>)</p><h2 id="Why-Gradient-Descent"><a href="#Why-Gradient-Descent" class="headerlink" title="Why Gradient Descent?"></a>Why Gradient Descent?</h2><p><strong>我們知道沿著gradient的方向走可以讓模型越來越好</strong>，但為什麼?</p><p>首先先來快速了解gradient是什麼，以下介紹引用自<a href="https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6" target="_blank" rel="noopener">wiki</a>: </p><blockquote><p>考慮一座高度在$(x, y)$點是$H(x, y)$的山。$H$這一點的梯度是在該點坡度（或者說斜度）最陡的方向。梯度的大小告訴我們坡度到底有多陡。</p></blockquote><p>也就是說對於一個函數我們可以想像他有個對應的的分布圖(如果函數是高維空間你就畫不出來，可以用個三維空間的例子來想像，如下圖)，在圖上某個點我們可以透過計算梯度來找到在該點最陡的方向，然後沿著該方向繼續往下找最小值。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1592150590/blog_posts/v2-bd640ea05ec20fe2db5eb1ae7a3a6f5f_r_ctry9f.jpg" alt=""></p><p>那接下來思考一個問題，考慮一個損失函數:</p><script type="math/tex; mode=display">L_{n}(a, W)=\frac{1}{n} \sum_{i=1}^{n} l\left(y_{i}, a^{T} \sigma\left(W^{T} x_{i}\right)\right)</script><p>假設actuvation function是relu，loss function是L2，那請問這個函數是凸函數(convex)還是非凸函數(non-convex)?</p><p>先給答案，如果單看每個unit都是凸的</p><ul><li>relu是convex</li><li>L2是convex</li></ul><p>不過當他們混合起來後就不一定了，以這個例子為例其實他就變成了Non-convex。</p><h2 id="Convex-or-Non-convex"><a href="#Convex-or-Non-convex" class="headerlink" title="Convex or Non-convex"></a>Convex or Non-convex</h2><p>“蛤?我知道gradient可以找到最小值就好了，理他凸不凸幹嘛。”</p><p><strong>這很重要，因為這會牽涉到你找到的是區域最小值或是全域最小值。</strong></p><p>在講這件事情前，聽說中國那邊有些凹凸的定義跟國外相反，所以我們先來定義一下什麼是凸函數好了:</p><p>凸函數長的跟下面這個人的眼睛一樣，開口朝上然後有個最小值(眼珠的部分)<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/a_270/v1592150882/blog_posts/unnamed_itqfgh.jpg" alt=""></p><p>好啦不鬧，也可以看一下他的數學定義:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1592151186/blog_posts/30493664687a62703072_gdd09t.jpg" alt=""></p><p>反正就是畫一條線去看就可以判斷了。再來我們看一下非凸函數跟凸函數的圖長怎樣:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1592151774/blog_posts/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGV2Y2xvdWQvYXJ0aWNsZS9kZXRhaWxzLzEwMTEwNjE2Ng__aHR0cHM6Ly9pbWctYmxvZy5jc2RuaW1nLmNuLzIwMTkwOTIxMTQwOTU3Njc1LnBuZw_bqzowi.png" alt=""></p><p>可以知道:</p><ul><li>如果函數是凸函數，那我們可以很快的找到全域最小值<ul><li>傳統機器學習方法問題大多是凸函數</li></ul></li><li>但如果函數是非凸函數，模型學習到的就很容易是區域最小值而非全域最佳解，這也是我們必須透過training set/ validation set來判斷模型好不好的原因，因為你不知道你現在的位置到底在哪裡。<ul><li>深度學習方法大多是非凸函數(注意這裡用的是大多，而不是全部)</li></ul></li></ul><p>要如何知道一個function是不是non-convex，引用Ian Goodfellow的回答:</p><blockquote><p>There are various ways to test for convexity.</p><p>One is to just plot a cross-section of the function and look at it. If it has a non-convex shape, you don’t need to write a proof; you have disproven convexity by counter-example.</p><p>If you want to do this with algebra, one way is just to take the second derivatives of a function. If the second derivative of a function in 1-D space is ever negative, the function isn’t convex.</p><p>For neural nets, you have millions of parameters, so you need a test that works in high-dimensional space. In high-dimensional space, it turns out we can take the second derivative along one specific direction in space. For a unit vector $d$ giving the direction and a Hessian matrix $H$ of second derivatives, this is given by $d^{T}Hd$.</p><p>For most neural nets and most loss functions, it’s very easy to find a point in parameter space and a direction where $d^{T}Hd$ is negative.</p></blockquote><p>也就是說如果能找到一維空間中函數的二階導數始終為負，則該函數不是凸函數。而最後兩段講到，<strong>在非常高維度的神經網路中，我們很容易就可以找到一個點他的二階導數是負的，所以大部分的loss function是non-convex</strong>。</p><p>所以阿，深度學習很容易學到一半模型就學歪了，因為他可能一不小心就在廣大的沙漠中找到了一小片綠洲而停下來，儘管他不是真正的最佳解。</p><p>到這裡我們知道函數是否convexity在最佳化時的差異。不過再來考慮另一個問題，deep learning都說用gradient descent來做更新，我們假設單一的函數都是convex(如Affine transformations, relu, max…)，那這些function一定可微嗎?</p><h2 id="Is-convex-function-differentiable"><a href="#Is-convex-function-differentiable" class="headerlink" title="Is convex function differentiable?"></a>Is convex function differentiable?</h2><p>答案是<strong>convex function不一定要可微</strong>，你可以去回顧上面對於凸函數的定義，可不可微並不影響他的性質。</p><p>那問題來了，不可微的東西你Pytorch是要怎麼backward算gradient啦!!</p><p>我們來舉個例子，大家都用到爛掉的relu:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1592153681/blog_posts/1_oePAhrm74RNnNEolprmTaQ_do4nzc.png" alt=""></p><p>你知道，我知道，獨眼龍也知道，在0那個點是不可微的(不過relu確實是個convex function唷)，那在0這個點到底該怎麼辦呢?</p><p>這時候<strong>sub-gradient</strong>就跳出來了，先說結論:</p><ol><li>由於backward並不是真的去算一遍微分公式，而是會先定義好他的gradient是多少然後去propagation(可參閱autumatic differentiation技術)，所以就不會有計算錯誤的run-time error。</li><li>對於relu在0這個點的微分通常會設成0，不過實際上在[0, 1]之間選一個值也可以(我也看過有人用1/2的)</li></ol><h3 id="Subgradient"><a href="#Subgradient" class="headerlink" title="Subgradient???"></a>Subgradient???</h3><p>先上簡單版結論: <strong>Subgradient method是可以在非光滑的function(也就是有些地方不可微時)訓練更新的方法</strong>，接下來會必較詳細的介紹subgradient</p><p>有幾篇文章我覺得講得非常好，所以下面內容我會節錄部分內容來做個簡單的介紹，你可以在reference看到引用的網站:</p><blockquote><p>对于光滑的凸函数而言，我们可以直接采用梯度下降算法求解函数的极值，但是当函数不处处光滑、处处可微的时候，梯度下降就不适合应用了。因此，我们需要计算函数的次梯度。对于次梯度而言，其没有要求函数是否光滑，是否是凸函数，限定条件很少，所以适用范围更广。。官方定义就不抄了，大致就是：<br>$g$ is a subgradient of $f$ (convex or not) at $x$ if</p><script type="math/tex; mode=display">f(y) \geq f(x)+g^{T}(y-x), \forall y</script><p>这里主要包含两层意思：</p><ol><li>用次梯度对原函数做出的一阶展开估计总是比真实值要小；</li><li>次梯度可能不唯一。</li></ol></blockquote><p>也就是說，<strong>在函數不可微的地方我們仍然可以透過求subgradient，然後進行更新。</strong></p><blockquote><p>实际上，函数$f(x)$在$x$处的次梯度可以构成一个集合，通常用符号$∂f(x)$表示，这个集合是一个凸集，元素个数可能等于0或者大于0。<br>對於一個凸函數，一定可以找到一個全域最小值，那也可以想成是說他不存在subgradient:</p><script type="math/tex; mode=display">f\left(x^{*}\right)=\min _{x} f(x) \leftrightarrow 0 \in \partial f\left(x^{*}\right)</script></blockquote><p>然後來看gradient descent和subgradient method的差異:</p><ul><li>gradient descent是沿著最陡方向去迭代更新，直到收斂<ul><li>更新公式: $x^{(k)}=x^{(k-1)}-\alpha_{k} \nabla f\left(x^{(k-1)}\right)$ </li></ul></li><li>Subgradient method並不是下降算法，因為subgradient不一定是最陡的方向，從上面介紹就可以看的出來<ul><li>更新方式跟gradient descent類似，只是換成了subgradient</li></ul></li></ul><p>注意subgradient不一定是最陡的方向，那他真的會收斂嗎? 答案是會的(太神奇了傑克!)，細節可以看<a href="https://blog.csdn.net/qq_32742009/article/details/81704139?utm_medium=distribute.pc_relevant.none-task-blog-baidujs-3" target="_blank" rel="noopener">文章內的證明</a>。</p><blockquote><p>另外，因为次梯度通常不唯一，而上面并没有提到任何次梯度的选取，因此理论上，任选一个都是可以使坐标不断向最小值点靠近，只是收敛的速率会不一样</p></blockquote><p>了解subgradient是什麼，以及他可以更新不可微的函數後，我們回頭看relu的case: </p><p>在x=0時我們可以找到很多條切線通過這個點，這些線的斜率組成了subgradient的集合，所以我們可以任意取一個subgradient來作為relu在x=0該點的微分值。</p><p>懂了這個概念後，所有有尖點的函數都可以用subgradient來算了，像是L1, relu…blabla</p><ul><li>題外話，L1也是convex，參考<a href="https://www.quora.com/According-to-shape-an-l1-norm-is-not-convex-Why-do-people-say-it-is-a-convex-function" target="_blank" rel="noopener">According to shape, an l1 norm is not convex. Why do people say it is a convex function?</a></li></ul><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>這篇文章重新回顧了深度學習是如何學習的概念，然後探討了一些問題:</p><ol><li>deep learning怎麼使用gradient descent來學習 </li><li>函數是不是convex對於找最佳解的影響</li><li>convex不一定可微</li><li>使用gradient descent時如果遇上函數不可微該怎麼辦</li></ol><p>所以雖然大家都說deep learning的更新是用gradient descent，不過更仔細來說的話我感覺應該是:</p><ul><li>對於函數可微的部分的確是用gradient descent</li><li>在函數不可微的地方使用了subgradient method</li></ul><p>然後在來迭代更新，值到某個停止條件達成。</p><p>所以…下次再被問到”你的這個loss function可微嗎?”，就可以更精確的用比較裝B的方式回答對方:</p><p>“就算有不可微的地方依舊可以透過subgradient來autumatic differentiate，所以不影響updating”</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.zhihu.com/question/265516791" target="_blank" rel="noopener">神经网络的损失函数为什么是非凸的?</a></li><li><a href="https://www.quora.com/How-can-you-prove-that-the-loss-functions-in-Deep-Neural-nets-are-non-convex" target="_blank" rel="noopener">How can you prove that the loss functions in Deep Neural nets are non-convex?</a></li><li><a href="https://www.zhihu.com/question/51344481" target="_blank" rel="noopener">凹凸性定义需要函数可导吗？</a></li><li><a href="https://closure11.com/subgradient/" target="_blank" rel="noopener">次梯度(Subgradient)</a></li><li><a href="https://blog.csdn.net/qq_32742009/article/details/81704139?utm_medium=distribute.pc_relevant.none-task-blog-baidujs-3" target="_blank" rel="noopener">【机器学习】次梯度（subgradient）方法</a></li><li><a href="https://www.zhihu.com/question/275630890" target="_blank" rel="noopener">L1正则和max函数的可导性？</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Attention is not Explanation</title>
      <link href="/posts/b70f16d0/"/>
      <url>/posts/b70f16d0/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>NAACL2019 2019的一篇文章，旨在透過一系列的實驗提出一種新的看法:<strong>attention機制並不是解釋性，我們不能用attention來說作為XAI的技術</strong>。這其實是個蠻特別的觀點，Attention自從2016年崛起後，一堆研究都是基於他而擴展的，現在這篇論文出來卻是打臉了attention的可解釋性。</p><p>…所以以後不能再用attention來做XAI(Explainable AI)了嗎? QQ饅頭喔</p><p>也別難過得太早，同一年又有另一篇論文跳出來了，EMNLP 2019的”Attention is not not Explanation”，看這個標題就知道擺明就是要跟這篇對著幹，所以先看完這篇再去看如何被反駁好像也是蠻有趣的? </p><p>題外話，我是站Attention is not not Explanation這邊，希望那篇可以狠狠打臉這篇xD</p><a id="more"></a><p>論文網址: <a href="https://arxiv.org/pdf/1902.10186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1902.10186.pdf</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs.<br>In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations” for predictions</p></blockquote><p>attention雖然已經被證實可以提升效果，但是很多人也同時把他吹捧成具有<strong>透明性(transparency)</strong>，可以幫助解釋深度學習這個黑盒子。</p><p>這篇的作者認為，attention跟model之間的關係其實尚不明朗，不准你這麼說!</p><p>所以他們設計了一系列的NLP實驗來度量attention對於模型預測時提供的解釋性分數，然後他們發現:</p><ol><li>注意力的權重和使用gradient-based來評估模型好壞的方法其實沒有甚麼相關性</li><li>不同的注意力分布可以產生相同的預測效果(如果注意力是指模型的重要性的話，那為何不同的重要性分布可以有一樣的效果呢?)</li></ol><p>所以，他們認為不能也不應該把attention當作提供解釋性的方法(不過本質上他們並沒有否定attention唷)。</p><p>Github code的部分在: <a href="https://github.com/successar/AttentionExplanation" target="_blank" rel="noopener">https://github.com/successar/AttentionExplanation</a></p><h2 id="Introduction-and-Motivation"><a href="#Introduction-and-Motivation" class="headerlink" title="Introduction and Motivation"></a>Introduction and Motivation</h2><blockquote><p>Li et al. (2016) summarized this commonly held view in NLP: “Attention provides an important way to explain the workings of neural models”. Indeed, claims that attention provides interpretability are common in the literature</p></blockquote><p>2016年後，普遍認為attention是可以來幫助解釋模型運作的一種注意力機制。這項共識來自於一個假設:</p><p><strong>注意力權重高的feature比較大的程度影響了模型的output，所以我們可以說他是這個task的重要特徵</strong></p><p>不過這項假設實際上並沒有正式的被評估過。所以就有了這篇。</p><blockquote><p>Assuming attention provides a faithful explanation for model predictions, we might expect the following properties to hold.</p><ol><li>Attention weights should correlate with feature importance<br>measures (e.g., gradient-based measures); </li><li>Alternative (or counterfactual) attention weight configurations ought to yield corresponding changes in prediction (and if they do not then are equally plausible as explanations). </li></ol></blockquote><p>假設注意力機制可以為模型提供合理的解釋性，那他們期望下列的幾個性質也是同時存在的:</p><ol><li><strong>attention weight應該跟feature important measures(例如gradient-based的方法)具有相關性</strong></li><li><strong>不同的attention weight分布應該會造成不同的prediction，因為注意的地方不同了。反之，如果不會造成不同的prediction，則可以用來當作他是解釋性的一種依據</strong></li></ol><p>然後他們說他們在很多種NLP task上做實驗，可是卻沒發現上述這兩種現象:&lt;</p><p>接下來他們用了一個例子來描述2的現象:<br>使用了一個標準具有attention的BiLSTM網路來做情感分析，模型原始的attention distribution $\alpha$下發現asking, waste是造成負面情緒($y=0.01$)的重要token。</p><ol><li>他們發現這樣子的attention weight和gradient-based measures of feature importance的相關性非常的小</li><li>他們另外造了一個分布$\tilde{\alpha}$(此時重要的token變成了myself, was)卻能得到一樣的prediction。如果更換其他分布對於預測的準確度也不會降低太多(只有0.006的誤差)</li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591634093/blog_posts/01_gwf9vq.png" alt=""></p><blockquote><p>We thus caution against using attention weights to highlight input tokens “responsible for” model outputs and onstructing just-so stories on this basis.</p></blockquote><p>基於上述的發現，他們告誡大家不要再把attention當作解釋性來用惹。</p><h3 id="Research-questions-and-contributions"><a href="#Research-questions-and-contributions" class="headerlink" title="Research questions and contributions"></a>Research questions and contributions</h3><blockquote><p>We investigate whether this holds across tasks by exploring the following empirical questions.</p><ol><li>To what extent do induced attention weights correlate with measures of feature importance – specifically, those resulting from gradients and leave-one-out (LOO) methods?</li><li>Would alternative attention weights (and hence distinct heatmaps/“explanations”) necessarily yield different predictions?</li></ol></blockquote><p>作者通過以上的問題設置來進行多個NLP的實驗</p><ol><li>attention weight和使用gradient-based/LOO得出的feature importance之間的相關性?</li><li>不同的attention weight是否會導致不同的prediction?</li></ol><h2 id="Datasets-and-Tasks"><a href="#Datasets-and-Tasks" class="headerlink" title="Datasets and Tasks"></a>Datasets and Tasks</h2><p>在下面三個領域、多個資料集上來驗證上面的這兩個問題</p><ol><li>binary text classification</li><li>Question Answering (QA)</li><li>Natural Language Inference (NLI)</li></ol><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>實驗的詳細結果的detail可以在這個網站上看到: <a href="https://successar.github.io/AttentionExplanation/docs/" target="_blank" rel="noopener">Attention is not Explanation — Results</a></p><h3 id="Correlation-Between-Attention-and-Feature-Importance-Measures"><a href="#Correlation-Between-Attention-and-Feature-Importance-Measures" class="headerlink" title="Correlation Between Attention and Feature Importance Measures"></a>Correlation Between Attention and Feature Importance Measures</h3><p>比較了1. gradient-based 2. leave one out(LOO)的相關性，algo如下圖所示</p><ul><li>gradient-based的就是根據backward去回推gradient</li><li>LOO就是看移除某個token $t$時，prediction的TVD與attention weight之間的差異</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591638767/blog_posts/01_yigug7.png" alt=""></p><blockquote><p>Note that we disconnect the computation graph at the attention module so that the gradient does not flow through this layer: This means the gradients tell us how the prediction changes as a function of inputs, keeping the attention distribution fixed.</p></blockquote><p>值得注意的地方是，<strong>他們把attention layer的computer graph斷開了，也就是說在做gradient backward的時候gradient並不會經過這一層</strong></p><ul><li>這裡指的是說，forward照做，可是backward的時候attention layer的前一層gradient會直接傳到attention layer的後一層，attention layer這一層並不會去參與到backward的計算</li><li>作者說這樣做是想要考慮同樣具有attention機制下，gradient-based的評估方法跟attention weight的相關性</li></ul><p>詳細的結果如下表所示:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591640094/blog_posts/01_nivwxo.png" alt=""></p><p>可以看到有三個欄位，分別是:</p><ol><li>BiLSTM下gradient-based方法跟attention weight的相關性<ul><li>BiRNN + attention</li></ul></li><li>Average方法的gradient-based方法跟attention weight的相關性<ul><li>將token經過linear projection layer(以及ReLU)的encoding + attention</li></ul></li><li>BiLSTM下的LOO方法跟attention weight的相關性</li></ol><p>可以看到，BiLSTM下的相關性大多都低於0.5，除了一些dataset(Diabetes和一些QA corpora)相關性相對比較大。作者認為因為這些dataset提供了足夠大的樣本數，這可以讓gradient-based和attention的相關性好一點(不過還是相對差於其他的方法)</p><p>再來，數據發現基於簡單的linear projection embedding的model和gradient-based的相關性甚至還比較大。他們假設問題出在BiRNN會看過所有的input然後才產生hidden state，所以這個hidden state包含了整體的information。</p><p>接下來他們又去分析了gradient-based和LOO的相關性(這裡沒放圖，可以對照論文的Fig.3-Fig.5)，發現gradient-based和LOO的相關性確實比attention來的高</p><p>細節可以再去看原文，不過這一章節主要就是反應了作者的第一個問題，也就是，attention和gradient-based的相關性其實並不高。並且用單純的方法來做embedding+attention反倒比用什麼BiRNN得到的相關性還高。這些數據讓人重新思考注意力到底具不具有解釋性。</p><h3 id="Counterfactual-Attention-Weights"><a href="#Counterfactual-Attention-Weights" class="headerlink" title="Counterfactual Attention Weights"></a>Counterfactual Attention Weights</h3><p>接下來他們想要看的是，不同的注意力分佈是否能夠產生出相同的預測？</p><p>因為如果按照”注意力機制是解釋性”的想法，作者認為不同的分佈會對於模型的輸出產生不同的結果。</p><blockquote><p>We experiment with two means of constructing such distributions. </p><ol><li>First, we simply scramble the original attention weights $\hat{\alpha}$, re-assigning each value to an arbitrary, randomly sampled index (input feature). </li><li>Second, we generate an adversarial attention distribution: this is a set of attention weights that is maximally distinct from $\hat{\alpha}$ but that nonetheless yields an equivalent prediction (i.e., prediction within some of $\hat{y}$)</li></ol></blockquote><p>第一個測驗方法是單純的打亂注意力的排列順序，演算法如下:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591688571/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-06-09_%E4%B8%8B%E5%8D%883.41.19_emk8jt.png" alt=""></p><p>第二種為產生一組<strong>adversarial attention distribution $\alpha$</strong>(在能夠產生相同prediction下與原本分布差異最大的分佈)，產生的方法主要是解一個最佳化問題</p><script type="math/tex; mode=display">\begin{array}{ll}\underset{\alpha^{(1)}, \ldots, \alpha^{(k)}}{\operatorname{maximize}} & f\left(\left\{\alpha^{(i)}\right\}_{i=1}^{k}\right) \\\text { subject to } & \forall i \operatorname{TVD}\left[\hat{y}\left(\mathbf{x}, \alpha^{(i)}\right), \hat{y}(\mathbf{x}, \hat{\alpha})\right] \leq \epsilon\end{array}</script><p>其中， <script type="math/tex">f\left(\left\{\alpha^{(i)}\right\}_{i=1}^{k}\right)</script> 為:</p><script type="math/tex; mode=display">\sum_{i=1}^{k} \operatorname{JSD}\left[\alpha^{(i)}, \hat{\alpha}\right]+\frac{1}{k(k-1)} \sum_{i=1} \operatorname{JSD}\left[\alpha^{(i)}, \alpha^{(j)}\right]</script><p>兩個子實驗的Detail我就不講了，自己去看，不過大致上就是作者發現可以透過不同的attention distribution來產生出相同的prediction</p><h2 id="Discussion-and-Conclusions"><a href="#Discussion-and-Conclusions" class="headerlink" title="Discussion and Conclusions"></a>Discussion and Conclusions</h2><p>作者透過一系列的實驗來驗證提出的兩個問題:</p><ol><li>attention跟gradient-based相關性其實並不高</li><li>不同的attention distribution可以有相同的prediction</li></ol><p>並用了這兩個來結論說，他們認為attention儘管可以幫助模型提升效能，但他背後的機制目前尚未明朗，我們不能把它當作Explainable的技術來看</p><p>再來，這篇文章有一些<strong>important limitations</strong></p><ol><li>使用gradient-based的方法來比較相關性，這其實是把gradient-based當作ground truth來看了，但其實他們也不能完全代表真正的可解釋性</li><li>要如何評估attention和gradient-based的相關性要多少才叫好？其實誰也不知道</li><li>評估標準使用了Kendall $\tau$ measure，這個方法再有不相關的特徵時會因噪聲而帶給了不夠精準的評估</li><li>實際上這篇文章只在少部分的attention varients上進行實驗(在文章中只用了BiLSTM)</li><li>儘管不同的attention distribution可以產生相同的prediction，但他們不否認<strong>可能同時存在多種解釋性</strong>，也就是說模型今天可能透過不同的注意力組合來得到相同的推論。</li><li>最後，目前只在分類任務上做實驗，他們把其他任務當作future work了(ㆆᴗㆆ)</li></ol><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>雖然論文落落長，不過主要就是瘋狂做實驗來驗證兩個問題:</p><ol><li><strong>attention weight是否跟feature important measures(例如gradient-based的方法)具有相關性，如果有的話則可以說明具有解釋性</strong></li><li><strong>不同的attention weight分布應該會造成不同的prediction，因為注意的地方不同了；反之，如果不會造成不同的prediction，則可以用來當作他是解釋性的一種依據</strong></li></ol><p>如果這兩個性質不存在，作者認為就不能將attention作為解釋性的一種技術依據。</p><p>不過其實後來在”Attention is not not Explanation”中，有對這篇的論點還有做實驗的方法提出了一些觀點，認為這篇的論點還不足以說attention不具解釋性(注意我的用詞，<strong>並不是說他們證明了attention具有解釋性而是說認為attention不能說不具有解釋性喔</strong>…我沒有在玩繞口令)。</p><p>兩篇都看完的話，你到底站哪一邊呢~</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.zhihu.com/question/314463239" target="_blank" rel="noopener">如何评价NAACL2019 paper：Attention is not Explanation?</a></li><li><a href="https://www.jiqizhixin.com/articles/2019-09-07-4" target="_blank" rel="noopener">注意力机制不能提高模型可解释性？不，你这篇论文搞错了</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新竹租屋經驗談</title>
      <link href="/posts/5a78c86/"/>
      <url>/posts/5a78c86/</url>
      
        <content type="html"><![CDATA[<h2 id="不自殺聲明"><a href="#不自殺聲明" class="headerlink" title="不自殺聲明"></a>不自殺聲明</h2><p>這篇文章就是要分享在新竹找房子租屋的心路歷層，只能說相對於高雄，我覺得新竹的租屋環境真的很不友善…</p><p>以下為真實經歷，很多相關的資訊也可以上網找一下就能佐證了，<strong>我沒有想要拉黑誰或是斷人財路，只是覺得在新竹租屋實在是資訊太不對等了，僅分享自己找房子的經驗，希望能幫助到未來同樣要在新竹找房子的朋友</strong>。</p><h2 id="要看房-前一個禮拜再聯絡謝謝"><a href="#要看房-前一個禮拜再聯絡謝謝" class="headerlink" title="要看房?前一個禮拜再聯絡謝謝"></a>要看房?前一個禮拜再聯絡謝謝</h2><p>由於工作關係，畢業後應該也是確定待在新竹了(園區附近)，考量到之後自身要準備口試還有搬家都需要一定時間，我打算6月同時承租目前的房子跟新房子，於是，我最先的規劃如下: </p><p>4月底開始消極看房，陸續加入一些臉書的新竹租屋社團以及上591看有沒有喜歡的物件，如果有喜歡的想說就直接連絡房東預約看房，順便問他能不能讓我六月起租</p><p>不過就在我四月底開始聯絡一些房東(其實他們根本不是房東，這些後面會講到)時，他們都口頭一致的先問了我打算的起租時間，然後就叫我五月中甚至五月底才來看房。</p><p>Okay, fine. 聽完後我就大概知道在新竹就是這樣了，<strong>都是快要租的時候(大約前一到兩個禮拜)才會讓你看房</strong>。</p><a id="more"></a><p>其實我剛開始不是很習慣，因為以前在高雄的時候，大部分都是可以很早就開始看，滿意的話付個訂金這樣。而如果現在都要前一個禮拜左右才開始看，就意味著你的選擇其實變得很少(你必須在很短的時間內做好決定你要租哪一間)。</p><p>不過這也只是兩個地區的不同而已，所以我後來就等到了5月中5月底才繼續我的下一步行動。</p><h2 id="591-假的。還不如臉書社團"><a href="#591-假的。還不如臉書社團" class="headerlink" title="591?假的。還不如臉書社團"></a>591?假的。還不如臉書社團</h2><p>這邊要說明一下，我後來找的房子是靠近園區附近，所以我的目標之一就是俗稱園區新手村的金山街。</p><p>如果你在591上搜尋”新竹金山街”，你會看到非常多的物件任君挑選</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591291636/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-06-05_%E4%B8%8A%E5%8D%8812.27.16_lztmgd.png" alt=""></p><p>然後有各種看起來很高級的裝潢獨立套房，看得大家心癢癢</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591296743/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-06-05_%E4%B8%8A%E5%8D%8812.26.47_vfcyvo.png" alt=""></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591296743/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-06-05_%E4%B8%8A%E5%8D%8812.26.56_ldcscn.png" alt=""></p><p>不過呢，在我找房子期間發現了兩個現象:</p><ol><li>聯絡人都寫是<strong>屋主自租，仲介勿擾</strong>，我打電話過去也是再度確認是不是房東，對方也跟我說是。不過很多時候他們其實只是房屋託管公司的人<ul><li>其實這也不難發現，591上很多家房子不同房東的姓可是電話卻都同一隻，那就大概可以知道了…</li><li>不過他們其實也不會多收仲介費，並且還是有好的房仲的，只是覺得摁…那幹嘛要自稱房東？</li></ul></li><li>這些看起來很高級精美的房子照片<strong>很可能是騙人的</strong>，你聯絡好要看房之後他們帶你看的根本不是這間<ul><li>我那時候還問了對方說我網路上有看到你們這間照片，請問可以看嗎？ <strong>對方用很理所當然的態度跟我說”591上面的照片都是騙人的不要相信”</strong>，摁…要用這種方式來刊登我是沒意見啦，不過這麼大喇喇的承認上面都是假照片也是…所以我後面都放棄在591上面找了</li></ul></li></ol><p>其實相比起來，在FB社團上看到的照片還比較多是真實房子的樣子，所以推薦可以在社團多看看，或是直接連絡信任的房仲請他們介紹他們手上的物件給你</p><h2 id="違建多多的園區新手村"><a href="#違建多多的園區新手村" class="headerlink" title="違建多多的園區新手村"></a>違建多多的園區新手村</h2><p>這個其實是令我最無奈的部分，<strong>工程師都這麼辛苦在打拼了，卻可能連一個安全安穩的住所都很奢侈…</strong></p><p>怎麼說金山街違建多多呢，其實網路上很多資料可以去看，為了應付竹科大量的人流，這邊的房子越蓋越多，有蠻大部分都是違建的形式蓋起來的。根據2015年的一篇討論: <a href="https://www.mobile01.com/topicdetail.php?f=457&amp;t=4217605" target="_blank" rel="noopener">令人火大的違建名單</a>中提到，對於違建的手法做了一些簡單的介紹，這裡就不講了，有興趣的可以自己去討論區看。</p><p>看完之後根據討論區的介紹然後回顧一下金山街的房子都長怎樣，你就會突然好像發現了什麼東西…哎…不說了…</p><p>而且看房的時候有時候帶看的人還會強調”他們有取得建照喔”，聽起來好像是很安全的，不過依據討論區的內容，他們可能會先拿一個合法的建照後，改造成根本不是建照許可的東西出來(?)</p><p>違建如果只是沒有登記的那種我倒覺得還好，可是如果是會危及居家安全的違建我就覺得很糟糕了。</p><p>此外，這些討論2015就有了，不過今年2020年壓根沒啥解決，所以看來就是這樣了，只希望大家都能住得平平安安的，真不要出了什麼人命政府才要馬後砲來重視這塊。</p><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ul><li><a href="https://rent.591.com.tw/?kind=0&amp;region=4&amp;keywords=%E9%87%91%E5%B1%B1%E8%A1%97&amp;shType=list" target="_blank" rel="noopener">591租屋網: 金山街</a></li><li><a href="https://www.mobile01.com/topicdetail.php?f=457&amp;t=4217605" target="_blank" rel="noopener">令人火大的違建名單</a></li><li><a href="https://www.facebook.com/pages/category/Community/%E4%B8%80%E5%A4%9C%E6%B6%88%E5%A4%B1%E7%9A%84%E9%87%91%E5%B1%B1%E8%A1%97170%E6%A3%9F%E5%85%AD%E6%A8%93%E4%BB%A5%E4%B8%8A%E9%81%95%E5%BB%BA%E5%9C%A8%E5%93%AA%E8%A3%A1-%E9%87%91%E5%B1%B1%E8%A1%97102%E6%A3%9F%E9%87%8D%E5%A4%A7%E9%81%95%E5%BB%BA%E7%95%AA%E5%A4%96%E7%AF%87-587588008041431/" target="_blank" rel="noopener">一夜消失的金山街170棟六樓以上違建在哪裡? 金山街102棟重大違建番外篇</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]End-to-end object detection with Transformers</title>
      <link href="/posts/2e56b772/"/>
      <url>/posts/2e56b772/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>DETR，是DEtection TRansformer的縮寫，FB首度將NLP的transformer用在CV的object detection上，還不用做NMS。<br>FB真滴神。已經沒有任何東西可以阻擋attention了…</p><p>論文網址: <a href="https://arxiv.org/pdf/2005.12872.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2005.12872.pdf</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task.</p></blockquote><p>將object detection視作一個direct set prediction problem。並且精簡了很多object detection上的額外操作(non-maximum suppression, anchor generation)</p><blockquote><p>The main ingredients of the new framework, called DEtection TRansformer or<br>DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture.</p></blockquote><p>這段提到了這個架構的重點在於</p><ol><li>set-based global loss that forces unique predictions via bipartite matching，也就是<strong>透過二分匹配的一個global loss</strong></li><li><strong>transformer的encoder-decoder架構</strong></li></ol><a id="more"></a><blockquote><p>Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset.</p></blockquote><p>透過transformer使得DETR可以考慮object跟整體image之間的關係，然後去預測最後的結果。並且不用額外的操作(這很重要，在objection detection上有太多繁瑣的額外工作要處理，這些在DETR上都不用做了!!)，最後他們的結果比Faster RCNN好。儘管Faster RCNN已經不是最新的方法，現在有很多方法都超越了，不過你能想像這是第一次把transformer應用在CV領域上，這是一件多麼令人興奮的事情!</p><p>FB也釋出了他們的code: <a href="https://github.com/facebookresearch/detr" target="_blank" rel="noopener">github</a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>We streamline the training pipeline by viewing object detection as a direct set prediction problem. We adopt an encoder-decoder architecture based on transformers, a popular architecture for sequence predictionThe self-attention mechanisms of transformers, which explicitly model all pairwise interactions between elements in a sequence, make these architectures particularly suitable for specific constraints of set prediction such as removing duplicate predictions.</p></blockquote><p>將object detection視為一個set prediction的問題。使用transformer的encoder-decoder架構</p><p>因為transformer的self-attention機制可以對element之間的交互關係來進行建模，這個性質在一些specific constraints of set prediction上的應用非常適合，例如刪除重複的predictions</p><blockquote><p>Our DEtection TRansformer (DETR, see Figure 1) predicts all objects at once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects.</p></blockquote><p>DETR可以一次就預測出所有物體，並且他是一個end-to-end的架構，搭配了set loss function來針對prediction和ground truth進行二分匹配(bipartite matching)，下面是架構圖，對CNN跟transformer(不懂的可以去看我寫的transformer的論文介紹<a href="https://meetonfriday.com/posts/5839a8bf/">論文速速讀: Attention Is All You Need</a>)有一些概念的並不難理解，也就是:</p><ol><li>圖片經過CNN後，把學到的features丟到transformer(怎麼丟也是一門學問，後續會說)，</li><li>然後transformer透過decoder來predict set of objects，每個predict都包含了類別跟bounding box，</li><li>然後這個prediction會跟ground truth來計算loss</li></ol><p>所以重點就在於<strong>bipartite matching loss</strong>了<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591122997/blog_posts/01_wiat1e.png" alt=""></p><blockquote><p>Our matching loss function uniquely assigns a prediction to a ground truth object, and is invariant to a permutation of predicted objects, so we can emit them in parallel</p></blockquote><p>透過這個matching loss function對prediction和ground truth object進行匹配，並且在預測物件上的排列順序是無關的，所以可以平行處理</p><blockquote><p>Our experiments show that our newmodel achieves comparable performances. More precisely, DETR demonstrates significantly better performance on large objects, a result likely enabled by the non-local computations of the transformer</p></blockquote><p>他們發現DETR在較大物件上有更好的performance，可能是因為transformer的non-local computations所造成(也就是說他可以關注大範圍的information)</p><h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>DETR使用了下列多個技術:</p><ul><li>bipartite matching losses for set prediction</li><li>encoder-decoder architectures based on the transformer</li><li>parallel decoding</li><li>object detection methods.</li></ul><h3 id="Set-Prediction"><a href="#Set-Prediction" class="headerlink" title="Set Prediction"></a>Set Prediction</h3><blockquote><p>Most current detectors use postprocessings such as<br>non-maximal suppression to address this issue, but direct set prediction are postprocessing-free. They need global inference schemes that model interactions between all predicted elements to avoid redundancy</p></blockquote><p>object detection很大的一個問題就是要如何去解決重複的prediction，一個方法是透過non-maximal suppression(NMS)的後處理來刪除重複的prediction。但是這個問題在set prediction上則不用這樣做，他們可以透過對predicted elements之間的關係進行inference來解決這個issue。</p><blockquote><p>　The usual solution is to design a loss based on the Hungarian algorithm, to find a bipartite matching between ground-truth and prediction.<br>This enforces permutation-invariance, and guarantees that each target element has a unique match. We follow the bipartite matching loss approach.</p></blockquote><p><strong>(上面這一段英文是本篇重點)，透過匈牙利算法(Hungarian algorithm)去一對一的匹配prediction跟ground truth，並且這個結果預位置無關(permutation-invariance)，因為不管prediction在set的哪個位置上，都會透過algo找到與他最相似的ground。</strong></p><p>此外，這一段也講到傳統在做set prediction時，會使用RNN，不過這裡使用的是transformer。合理，RNN系列本來就可以被attention替換掉，看看NLP近年的研究，哪一個RNN成果沒被attention洗過一輪的?</p><h3 id="Transformers-and-Parallel-Decoding"><a href="#Transformers-and-Parallel-Decoding" class="headerlink" title="Transformers and Parallel Decoding"></a>Transformers and Parallel Decoding</h3><p>介紹Transformer，不懂的可以去看<a href="https://meetonfriday.com/posts/5839a8bf/">論文速速讀: Attention Is All You Need</a></p><h3 id="Object-detection"><a href="#Object-detection" class="headerlink" title="Object detection"></a>Object detection</h3><p>跳過，覺得這裡比較不重要，有興趣自己去細看</p><h2 id="The-DETR-model"><a href="#The-DETR-model" class="headerlink" title="The DETR model"></a>The DETR model</h2><blockquote><p>Two ingredients are essential for direct set predictions in detection:<br>(1) a set prediction loss that forces unique matching between predicted and ground truth boxes;<br>(2) an architecture that predicts (in a single pass) a set of objects and models their relation.<br>We describe our architecture in detail in Figure 2.</p></blockquote><p>DETR的兩個重點:</p><ol><li>一個set prediction loss來評估prediction跟ground truth boxes對應關係的好壞，說白了就是前面說的Hungarian algorithm</li><li>一個可以去預測一組objects的模型架構，說白了就是我大transformer+CNN</li></ol><h3 id="Object-detection-set-prediction-loss"><a href="#Object-detection-set-prediction-loss" class="headerlink" title="Object detection set prediction loss"></a>Object detection set prediction loss</h3><blockquote><p>DETR infers a fixed-size set of N predictions, in a single pass through the decoder, where N is set to be significantly larger than the typical number of objects in an image. One of the main difficulties of training is to score predicted objects (class, position, size) with respect to the ground truth</p></blockquote><p>一開始會設置一個$N$，代表要預測幾個object，這個$N$要被設置好才好來進行prediction跟ground truth之間的匹配(通常會設一個普遍大於image所應該有的object數量)。接下來就是需要一個好的score function來評估匹配的prediction跟ground truth的好壞(object類別、位置跟大小)</p><p>下面來介紹一下loss function，首先先定義一些notation:</p><ol><li>$y$是ground truth的objects set，$\hat{y} = { \hat{y_i} }^N_{i=1}$是predicts set</li><li>這邊會假設$N$是一個大於image所應應有的object數量，不過這樣後續匹配的時候就<strong>必須對ground truth去做padding才能湊到N組object</strong>，pad的方式是使用用$\varnothing$(代表沒有object)來padding</li><li>$y_i$是ground truth的一組object，每一個$y_i$包含了class labe和bounding box的四個值: $y_{i}=\left(c_{i}, b_{i}\right)$(center coordinates, width, height relative to image size)</li><li>對於一個$y_i$，他對應到的prediction是$\hat{y}_{\sigma(i)}$<ul><li>如何找到$y_i$應該對應哪一個$\hat{y}_{\sigma(i)}$? 用前面提到的匹配演算法Hungarian algorithm</li></ul></li></ol><p>於是loss function可以寫成:</p><script type="math/tex; mode=display">\mathcal{L}_{\text {Hungarian }}(y, \hat{y})= \\ \sum_{i=1}^{N}\left[-\log \hat{p}_{\hat{\sigma}(i)}\left(c_{i}\right)+1_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\hat{\sigma}}(i)\right)\right]</script><ul><li>$-\log \hat{p}_{\hat{\sigma}(i)}\left(c_{i}\right)$是<strong>希望predict分類分的越準越好</strong><ul><li>$\hat{p}_{\sigma(i)}\left(c_{i}\right)$是$\sigma (i)$被預測維class $c_i$的機率</li><li>當分的正確的時候(也就是機率=1)這一項就是0</li></ul></li><li><script type="math/tex">1_{\left\{c\_{i} \neq \varnothing\right\}} \mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\hat{\sigma}}(i)\right)</script>則是<strong>bounding box的重合度匹配</strong><ul><li>使用了L1 loss和IOU(單純用L1 loss會造成這個loss過於依賴bounding box的大小，所以加上了IOU)</li></ul></li></ul><p>Box Loss詳細的公式如下所示:</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)=\lambda_{\mathrm{iou}} \mathcal{L}_{\mathrm{iou}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)+\lambda_{\mathrm{L} 1}\left\|b_{i}-\hat{b}_{\sigma(i)}\right\|_{1}</script><blockquote><p>These two losses are normalized by the number of objects inside the batch</p></blockquote><p>最後這兩個loss會根據batch內objs的數量來做normalization</p><h3 id="DETR-architecture"><a href="#DETR-architecture" class="headerlink" title="DETR architecture"></a>DETR architecture</h3><p>總體架構如下:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591126333/blog_posts/01_jy1rvs.png" alt=""></p><ol><li><strong>Backbone</strong>: 先跑CNN，然後把最後一層透過Conv1d降維，得到了feature map $z_{0} \in \mathbb{R}^{d \times H \times W}$，然後轉成$(d, HW)$的shape等下準備餵進transformer</li><li><strong>Transformer encoder</strong>: 然後把1.的output丟到transformer encoder上做multi-head self-attention<ul><li>transformer只在一開始加了<strong>position encoding</strong>，TEDR覺得一次不夠，每一個encoder block都給你加好加滿</li></ul></li><li><strong>Transformer decoder</strong>: 在transformer decoder的部分，使用N個d維的vector來當作object query vector<ul><li>decoder也給你每層加position encoding，值得一提的是這裡的position encoding是直接用query vector</li></ul></li><li><strong>Prediction feed-forward networks (FFNs)</strong>: 最後接FFN(feed-forward networks)產生N個prediction，前面有提到這個N會比image object數量還來的大，所以會有一個特殊的class叫做<strong>no object</strong>，可以把他想成圖片的背景</li><li><strong>Auxiliary decoding losses</strong>: 他們發現在decoder加上auxiliary losses能夠得到比較好的結果</li></ol><p>上面2和3的部分是transfromer，可以搭配下面架構圖一同服用<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591208708/blog_posts/01_udbffn.png" alt=""></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>呼…講了這麼多，終於來到實驗章節了…我要準備加速了唷，請綁好安全帶…</p><p>在COCO 2017 dataset上實驗，然後和Faster RCNN比。CNN架構採用了ResNet，同時比較了ResNet50/101和有沒有用dilation的結果</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591210153/blog_posts/%E6%93%B7%E5%8F%96_pm70fe.png" alt=""></p><h3 id="Ablations"><a href="#Ablations" class="headerlink" title="Ablations"></a>Ablations</h3><blockquote><p>In Figure 3, we visualize the attention maps of the last encoder layer of a trained model, focusing on a few points in the image. The encoder seems to separate instances already, which likely simplifies object extraction and localization for the decoder.</p></blockquote><p>首先來看encoder最後一層的attention maps拿來做visualization，可以看到<strong>其實encoder已經可以分辨出不同的instance了</strong>，作者認為這可以幫助減輕後面decoder的工作<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591210343/blog_posts/01_zvxvt0.png" alt=""></p><blockquote><p>Similarly to visualizing encoder attention, we visualize decoder attentions in Fig. 6, coloring attention maps for each predicted object in different colors. We observe that decoder attention is fairly local, meaning that it mostly attends to object extremities such as heads or legs. We hypothesise that after the encoder has separated instances via global attention, the decoder only needs to attend to the extremities to extract the class and object boundaries.</p></blockquote><p>接下來看decoder最後一層的attention maps，可以發現此時的attention集中在一些動物的腳r、鼻子r、頭阿…他們認為，<strong>在decoder階段模型只需要去注意這些動物的肢體來決定他們的bounding box</strong><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591210561/blog_posts/01_jx8gdo.png" alt=""></p><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><p>下面這張圖可視化了N=100中的20個object query和他們查到的bounding box。可以看到不同的query vector具有不同的分布(有些總是注重左下角，有些總是注重右邊…)，可以想成: 有N個不同的人用不同的角度來詢問模型<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591211328/blog_posts/01_hmwkoa.png" alt=""></p><p>然後來看一個例子:下面這張長頸鹿圖。他想說的是在training中沒有一張圖是同時有超過13個長頸鹿的，但是在predict的時候TEDR卻能成功分辨這24隻長頸鹿。<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591211067/blog_posts/01_ac5x9h.png" alt=""></p><h4 id="DETR-for-panoptic-segmentation"><a href="#DETR-for-panoptic-segmentation" class="headerlink" title="DETR for panoptic segmentation"></a>DETR for panoptic segmentation</h4><p>DETR可以透過在decoder後面加一些mask head來達到全景分割…細節有興趣的自己看…我累了…</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591211707/blog_posts/01_mgzmxh.png" alt=""></p><h2 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h2><p>TEDR把NLP上很紅的transformer搬過來CV領域了…</p><p>想當初transfromer一出，NLP所有論文都被刷過一輪，未來CV會不會也走上這條道路呢xD</p><p>不過用image做self-attention的想法蠻有意思的，看了某個視頻分享後，覺得其實他背後的概念很符合CV上的注意力概念，我覺得後續可以另外開一篇來進一步介紹。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers" target="_blank" rel="noopener">End-to-end object detection with Transformers</a></li><li><a href="https://github.com/facebookresearch/detr" target="_blank" rel="noopener">(github)facebookresearch/detr</a></li><li><a href="https://www.zhihu.com/question/397624847/answer/1250073418" target="_blank" rel="noopener">如何看待End-to-End Object Detection with Transformers？</a></li><li><a href="https://www.bilibili.com/s/video/BV1Qg4y1B7rL" target="_blank" rel="noopener">DETR: End-to-End Object Detection with Transformers论文解读</a></li><li><a href="https://blog.csdn.net/u014754127/article/details/78086014" target="_blank" rel="noopener">Hungarian Algorithm匈牙利算法</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Pytorch]zero_grad()和backward()使用技巧</title>
      <link href="/posts/18392404/"/>
      <url>/posts/18392404/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>今天來聊聊Pytorch的gradient update這個寫法。對Pytorch不陌生的朋友應該知道，一個pytorch model training的起手式大概長這個樣子:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx, (batch_x, batch_y) <span class="keyword">in</span> enumerate(data_loader):</span><br><span class="line">    output = model(batch_x)</span><br><span class="line">    loss = criterion(output, batch_y)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></p><p>這段code看似簡單，實際上他做了下列這些事情:</p><ol><li>將data傳入model進行forward propagation</li><li>計算loss</li><li>清空前一次的gradient</li><li>根據loss進行back propagation，計算gradient</li><li>做gradient descent</li></ol><p>如果用numpy純刻forward/backward是一件很累的事情，所以deep learning framework如Pytorch, tensorflow都幫你做完了，大感恩。</p><p>這篇文章主要是針對上面這個起手式來討論一些有的沒的，在往下看之前也可以先想想看這些問題:</p><ul><li>Pytorch為什麼要手動將gradient清空(<code>optimizer.zero_grad()</code>)，不能把這一步自動化嗎?</li><li>同理，為什麼gradient也要手動計算(<code>loss.backward()</code>)，不能每一次forward做完就自動算出對應的gradient嗎?</li></ul><a id="more"></a><h2 id="Gradient-accumulation"><a href="#Gradient-accumulation" class="headerlink" title="Gradient accumulation"></a>Gradient accumulation</h2><p>Pytorch不幫你自動清空gradient，而是要你呼叫<code>optimizer.zero_grad()</code>來做這件事是因為，這樣子你可以有更大的彈性去做一些<del>黑魔法</del>，畢竟，<strong>誰規定每一次iteration都要清空gradient?</strong></p><p>試想你今天GPU的資源就那麼小，可是你一定要訓練一個很大的model，然後如果batch size不大又train不起來，那這時候該怎麼辦? </p><p>雖然沒有課金解決不了的事情，如果有，那就多課一點…不是，這邊提供另外一種設計思維:</p><p><strong>你可以將你的model每次都用小的batch size去做forward/backward，但是在update的時候是多做幾個iteration在做一次。</strong></p><p>這個想法就是梯度累加(<strong>gradient accumulation</strong>)，也就是說我們透過多次的iteration累積backward的loss，然後只對應做了一次update，間接的做到了大batch size時候的效果。</p><p>gradient accumulation寫起來也不難，我們來看一下大概的寫法:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx, (batch_x, batch_y) <span class="keyword">in</span> enumerate(data_loader):</span><br><span class="line">    output = model(batch_x)</span><br><span class="line">    loss = criterion(output, batch_y)</span><br><span class="line">    </span><br><span class="line">    loss = loss / accumulation_step</span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (idx % accumulation_step) == <span class="number">0</span>:</span><br><span class="line">        optimizer.step() <span class="comment"># update</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment"># reset</span></span><br></pre></td></tr></table></figure><p>簡單吧? 當我執行了accumulation_step次之後，才進行gradient descent然後清空gradient。</p><p>注意這邊每一次iteration都還是有呼叫<code>loss.backward()</code>，所以每一次迭代的時候gradient都會一直被累加，直到最後被呼叫了<code>optimizer.zero_grad()</code>才將他們清空。</p><ul><li>不過要注意的是，在進行這個trick的時候，learning rate也要對應的做出調整。</li></ul><p>現在我們來找一些source code看看這個技巧唄! 知名頂頂的BERT裡面也有用到這一個技巧，有用過的就知道他有一個<code>gradient_accumulation_steps</code>參數，那在code裡面怎麼寫呢? 我們來看一下<a href="https://github.com/huggingface/transformers/blob/6a17688021268fe429e78c66ea0932cb55cd03b1/src/transformers/trainer.py#L473" target="_blank" rel="noopener">transformers/src/transformers/trainer.py</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (step + <span class="number">1</span>) % self.args.gradient_accumulation_steps == <span class="number">0</span> <span class="keyword">or</span> (</span><br><span class="line">    <span class="comment"># last step in epoch but step is always smaller than gradient_accumulation_steps</span></span><br><span class="line">    len(epoch_iterator) &lt;= self.args.gradient_accumulation_steps</span><br><span class="line">    <span class="keyword">and</span> (step + <span class="number">1</span>) == len(epoch_iterator)</span><br><span class="line">):</span><br><span class="line">    <span class="keyword">if</span> self.args.fp16:</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), self.args.max_grad_norm)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_tpu_available():</span><br><span class="line">        xm.optimizer_step(optimizer)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="Gradient-accumulation-in-multi-task-training"><a href="#Gradient-accumulation-in-multi-task-training" class="headerlink" title="Gradient accumulation in multi-task training"></a>Gradient accumulation in multi-task training</h2><p>接下來我們看另一個情境，假設我們今天在做一個multi-task的訓練，那不同的task我們就會有不同的loss，於是乎我們想把這些loss加起來一起做然後得到最後的loss，在去做backward。</p><p>這樣的話，寫法大概會是下面這樣:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">for idx, (batch_x, batch_y) in enumerate(data_loader):</span><br><span class="line">    output1 &#x3D; model1(batch_x)</span><br><span class="line">    loss1 &#x3D; criterion(output1, batch_y)</span><br><span class="line">    </span><br><span class="line">    output2 &#x3D; model2(batch_x)</span><br><span class="line">    loss2 &#x3D; criterion(output2, batch_y)</span><br><span class="line">    </span><br><span class="line">    loss &#x3D; loss1 + loss2</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></p><p>要記得在變數計算的時候背後都是有著一張graph的，所以在第2行loss1計算的背後會得到一張graph，在第5行loss2也會有一張graph，直到第8行才將這兩張graph進行了合併成一張，然後在<code>backward()</code>的時候將梯度更新並釋放掉graph。</p><p>於是，<strong>在6行到第8行中間，device同時儲存了兩張graph</strong>，那如果loss一多，這樣對於memory的消耗就很大了。</p><p>但是套用gradient accumulation的概念，我們可以只存一張graph就完成這些計算，考慮以下程式碼:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> idx, (batch_x, batch_y) <span class="keyword">in</span> enumerate(data_loader):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    output1 = model1(batch_x)</span><br><span class="line">    loss1 = criterion(output1, batch_y)</span><br><span class="line">    loss1.backward() </span><br><span class="line">    </span><br><span class="line">    output2 = model2(batch_x)</span><br><span class="line">    loss2 = criterion(output2, batch_y)</span><br><span class="line">    loss2.backward()</span><br><span class="line">    </span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></p><p>我們先針對loss1去進行backward，然後算完後就將graph釋放掉然後將gradient存在變數中，之後再去計算loss2，所以從頭到尾device同個時間上最多只會有一張graph的memory。</p><p>如果這樣看不懂為什麼可以work的話，我們來看一下數學公式QQ</p><script type="math/tex; mode=display">\frac{\partial (loss_1+loss_2)}{\partial x} = \frac{\partial (loss_1)}{\partial x}+\frac{\partial (loss_2)}{\partial x}</script><p>把loss相加在去算gradient跟兩個分開來做是等價的，然後要記得我們在前一節提過的，<strong>沒做<code>optimizer.zero_grad()</code>前gradient是會被累加的</strong>，所以在第12行我們只需要呼叫一次<code>optimizer.step()</code>就可以做到同時update兩個loss。</p><h2 id="Truncated-Back-Propagation-Through-Time-in-RNN"><a href="#Truncated-Back-Propagation-Through-Time-in-RNN" class="headerlink" title="Truncated Back Propagation Through Time in RNN"></a>Truncated Back Propagation Through Time in RNN</h2><p>(這一段其實自己也不太熟，如果有錯誤再麻煩指正~)<br>在RNN訓練中，back propagation的方法叫做Back Propagation Through Time(BPTT)，因為backward會牽涉到前一個時間的hidden state。</p><p>並且，由於每一個hidden state都跟前一個時刻有關，RNN在input過長的時候會出現gradient vanish/explosion的問題，解決方法向是LSTM或是GRU等具有gated mechanism的模型。</p><p>不過在訓練的技巧上有另外一種可以幫助解決這個問題的方法，那就是<strong>Truncated Back Propagation Through Time(TBPTT)</strong>，其實他有很多種case，細節可以參閱<a href="https://machinelearningmastery.com/gentle-introduction-backpropagation-time/" target="_blank" rel="noopener">A Gentle Introduction to Backpropagation Through Time</a>。</p><p>不過這裡講的是其中一種case: 也就是為了避免input過長造成上述的問題，我們不將所有的input都用來計算gradient，也就是，對於某個長度之後的gradient我們都捨棄掉不理他。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># non-truncated</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">    out = model(out)</span><br><span class="line">    out.backward()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># truncated to the last K timesteps</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">    out = model(out)</span><br><span class="line">    <span class="keyword">if</span> T - t == K:</span><br><span class="line">        out.backward()</span><br><span class="line">        out.detach()</span><br><span class="line">out.backward()</span><br></pre></td></tr></table></figure><p><code>out.detach()</code>會在K timestamp的時候截斷graph，然後後面就從該變數之後就不會在做backward了，所以就實現了TBPTT</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.zhihu.com/question/303070254" target="_blank" rel="noopener">PyTorch中在反向传播前为什么要手动将梯度清零？</a></li><li><a href="https://www.cnblogs.com/shiyublog/p/10542682.html" target="_blank" rel="noopener">[NLP] RNN 前向传播、延时间反向传播 BPTT 、延时间截断反向传播 TBTT</a></li><li><a href="https://machinelearningmastery.com/gentle-introduction-backpropagation-time/" target="_blank" rel="noopener">A Gentle Introduction to Backpropagation Through Time</a></li><li><a href="https://discuss.pytorch.org/t/correct-way-to-do-backpropagation-through-time/11701" target="_blank" rel="noopener">Correct way to do backpropagation through time?</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> python </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Attention-based LSTM for Aspect-level Sentiment Classification</title>
      <link href="/posts/6a169efb/"/>
      <url>/posts/6a169efb/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>論文網址: <a href="https://www.aclweb.org/anthology/D16-1058.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D16-1058.pdf</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>Aspect-level sentiment classification is a fine-grained task in sentiment analysis.</p></blockquote><p>這篇文章透過attention mechanism來加強LSTM的Aspect-level sentiment classification，中文我暫且稱作觀點層級的情感分析。</p><blockquote><p>In this paper, we reveal that the sentiment polarity of a sentence is not only determined by the content but is also highly related to the concerned aspect. For instance, “The appetizers are ok, but the service is slow.”, for aspect taste, the polarity is positive while for service, the polarity is negative.</p></blockquote><p>作者發覺句子的情感分析不只跟內容有關，也跟你切入的觀點有關。這也蠻好懂的，舉個例子，”開胃菜好ㄘ，但服務很慢”，在這句話中，如果從食物的角度來看是正向的;但如果從服務的角度來看則是負面的。</p><p>所以他們希望不同的aspect被當作輸入時，可以透過attention關注到句子的不同部分。</p><p>Attentionすごい</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>The main contributions of our work can be summarized as follows:</p><ul><li>We propose attention-based Long Short-Term memory for aspect-level sentiment classification. The models are able to attend different parts of a sentence when different aspects are concerned. Results show that the attention mechanism is effective.</li><li>Since aspect plays a key role in this task, we propose two ways to take into account aspect information during attention: one way is to concatenate the aspect vector into the sentence hidden representations for computing attention weights, and another way is to additionally append the aspect vector into the input word vectors.</li><li>Experimental results indicate that our approach can improve the performance compared with several baselines, and further examples<br>demonstrate the attention mechanism works well for aspect-level sentiment classification.</li></ul></blockquote><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>Jumping<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.pinimg.com/originals/51/18/62/511862395ab304a6991b2917fa2bc039.jpg" alt=""></p><h2 id="Attention-based-LSTM-with-Aspect-Embedding"><a href="#Attention-based-LSTM-with-Aspect-Embedding" class="headerlink" title="Attention-based LSTM with Aspect Embedding"></a>Attention-based LSTM with Aspect Embedding</h2><h3 id="Long-Short-term-Memory-LSTM"><a href="#Long-Short-term-Memory-LSTM" class="headerlink" title="Long Short-term Memory (LSTM)"></a>Long Short-term Memory (LSTM)</h3><p>挖歐…重新介紹了一遍LSTM…厲害…</p><h3 id="LSTM-with-Aspect-Embedding-AE-LSTM"><a href="#LSTM-with-Aspect-Embedding-AE-LSTM" class="headerlink" title="LSTM with Aspect Embedding (AE-LSTM)"></a>LSTM with Aspect Embedding (AE-LSTM)</h3><blockquote><p>Aspect information is vital when classifying the polarity of one sentence given aspect. We may get opposite polarities if different aspects are considered.<br>To make the best use of aspect information, we propose to learn an embedding vector for each aspect.</p></blockquote><p>因為Aspect很大程度會影響句子的polarity，所以他們提出針對每一個aspect都訓練一個embedding vector。</p><ul><li>注意到這裡的aspect會在之後的task一起train</li></ul><p>$v_{\alpha_i}\in \mathbb{R}^{d_\alpha}$是aspect $i$的embedding，$d_{\alpha}$是dimension</p><p>$A\in \mathbb{R}^{d_{\alpha}\times|A|}$是所有的embeeding matrix</p><p>然後呢？沒有然後了，往下繼續看唄</p><h3 id="Attention-based-LSTM-AT-LSTM"><a href="#Attention-based-LSTM-AT-LSTM" class="headerlink" title="Attention-based LSTM (AT-LSTM)"></a>Attention-based LSTM (AT-LSTM)</h3><p>設計了一種LSTM架構，可以根據給定的aspect去捕捉句子中的不同部分。<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/aCwT3nK.png" alt=""></p><ul><li>$H \in \mathbb{R}^{d \times N}$, <script type="math/tex">[h_{1}, \cdots, h_{N}]</script> 是LSTM的hidden vector</li><li><script type="math/tex">v_{a} \in \mathbb{R}^{d_{a}}</script> 是某個aspect的embedding</li></ul><p>接下來的公式有點難打啊，可以去看原文，不過看架構圖也大致理解了他怎麼做的: <strong>在原先LSTM架構中concat某個特定的aspect embedding，然後去做attention</strong>。</p><p>題外話，原文裡面的公式沒事搞複雜，扯了一個<script type="math/tex">e_{N}</script>出來，然後<script type="math/tex">v_\alpha \otimes e_N</script>這又是什麼鬼？</p><p>其實看架構圖會更容易理解，或是要注意到原文的這個例子$v_\alpha \otimes e_N= [v;v;…;v]$，其實就是架構圖的每個$h$都會跟$v$做concat的意思而已)</p><p>做完attention得到了$r$，然後再接了一個transformation得到最後的representation $h^{*}$，值得一提的是，這裡的tranformation不是一個簡單的FC，他長下面這樣:</p><script type="math/tex; mode=display">h^{*}=tanh(W_p r + W_x h_N)</script><p>他多加了一項$W_x h_N$，因為他們發現這樣做效果會比較好(*´･д･)?</p><ul><li>好啦他們有給reference: <a href="https://arxiv.org/abs/1509.06664" target="_blank" rel="noopener">Reasoning about Entailment with Neural Attention</a></li></ul><p>最後$h^{*}$再接一層FC然後去softmax分類。</p><h3 id="Attention-based-LSTM-with-Aspect-Embedding-ATAE-LSTM"><a href="#Attention-based-LSTM-with-Aspect-Embedding-ATAE-LSTM" class="headerlink" title="Attention-based LSTM with Aspect Embedding (ATAE-LSTM)"></a>Attention-based LSTM with Aspect Embedding (ATAE-LSTM)</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/BLjnN4n.png" alt=""></p><p>剛剛在hidden layer concat了aspect的embedding，那為何input layer不行？</p><p>沒有不行，所以ATAE-LSTM就出來了！只要有心，任何地方都可以被注意！</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>訓練資料必須包含aspect和對應的polarity，這樣才能給定aspect去進行attention。</p><p>然後你就可以去看給定aspect下句子的attention score<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/pztK3h9.png" alt=""></p><p>這個例子是說，在一般情況下，有否定句，長句子下都可以做得不錯<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/24K9EgJ.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Hierarchical Attention Networks for Document Classification</title>
      <link href="/posts/8cad39eb/"/>
      <url>/posts/8cad39eb/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>論文網址: <a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener">https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics:<br>(i) it has a hierarchical structure that mirrors the hierarchical structure of documents;<br>(ii) it has two levels of attention mechanisms applied at the word and sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation.</p></blockquote><p>提出一個基於hierarchical attention architecture的模型，用於文本分類任務，abstract點出兩個特色:</p><ol><li>使用了可以反映文章結構的的階層結構</li><li>在word level和sentence level上都使用了attention mechanism，使得在建構representation時能夠注意到比較重要的內容</li></ol><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>Although neural-network–based approaches to text classification have been quite effective (Kim, 2014; Zhang et al., 2015; Johnson and Zhang, 2014; Tang et al., 2015), in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interactions of the words, not just their presence in isolation</p></blockquote><p>儘管一些方法在文本分類上已有不錯的表現，但這篇paper證明了一個假設:透過將文章結構的資訊加入模型架構中可以使文章有夠好的representation。</p><p>設計模型的直覺來自：<strong>並非文章的所有部分都對建立模型有同等的貢獻</strong>，並且我們應該<strong>對於word之間的相關性也加入考慮建模</strong>，而非將他們視作獨立的表示。</p><blockquote><p>First, since documents have a hierarchical structure (words form sentences, sentences form a document), we likewise construct a document representation by first building representations of sentences and then aggregating those into a document representation. Second, it is observed that different words and sentences in a documents are differentially informative. Moreover, the importance of words and sentences are highly context dependent.</p></blockquote><p>可以將文章分成兩個階段的結構體: “由word組成的sentence”和”由sentence組成的document”。HAN透過先建構sentence-level的representation，再將這些組合成一個document的presentation。</p><p>並且可以觀察到:</p><ol><li>不同的word跟sentence在文章中具有不同的資訊量</li><li>word和sentence的重要性很大程度上取決於上下文</li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/s0EB8zc.png" alt=""></p><h2 id="Hierarchical-Attention-Networks"><a href="#Hierarchical-Attention-Networks" class="headerlink" title="Hierarchical Attention Networks"></a>Hierarchical Attention Networks</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/rvasPc1.png" alt=""><br>HAN包含了以下幾個部分:</p><ul><li>word sequence encoder</li><li>word-level attention layer</li><li>sentence encoder</li><li>sentence-level attention layer</li></ul><h3 id="GRU-based-sequence-encoder"><a href="#GRU-based-sequence-encoder" class="headerlink" title="GRU-based sequence encoder"></a>GRU-based sequence encoder</h3><p>先介紹GRU</p><p>GRU的new state:</p><script type="math/tex; mode=display">h_t=(1-z_t)\odot h_{t-1}+z_t\odot \tilde h_t</script><p>其中candidate state$\tilde h_t$透過下面這項來更新:</p><script type="math/tex; mode=display">\tilde h_t= tanh(W_h x_t + r_t \odot (U_h h_{t-1})+b_h)</script><p>不同於LSTM，GRU只用了reset gate $r$和 update gate $z$，這兩個gate的更新如下:</p><script type="math/tex; mode=display">z_t = \sigma(W_z x_t+U_z h_{t-1}+b_z) \\ r_t = \sigma(W_r x_t+U_r h_{t-1}+b_r)</script><h3 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h3><h4 id="Word-Encoder"><a href="#Word-Encoder" class="headerlink" title="Word Encoder"></a>Word Encoder</h4><p>將word $x$做word embedding後，透過一個雙向的GRU得到encoding後的vector:</p><script type="math/tex; mode=display">x_{it}=W_e w_{it}, t\in[1, T] \\\overrightarrow h_{it}=\overrightarrow{GRU}(x_{it}), t\in[1, T]\\\overleftarrow h_{it}=\overleftarrow{GRU}(x_{it}), t\in[1, T]</script><h4 id="Word-Attention"><a href="#Word-Attention" class="headerlink" title="Word Attention"></a>Word Attention</h4><p>將<script type="math/tex">h_{it}=[\overrightarrow{h_{it}}, \overleftarrow{h_{it}}]</script>進行一次transformation，然後透過一個content vector $u_w$來進行attention，這裡的<script type="math/tex">u_w</script>可以被視為<strong>a high level representation of a fixed query “what is the imformative word” over the words like that used in menory networks</strong></p><script type="math/tex; mode=display">u_{it}=tanh(W_h h_{it}+b_w) \\\alpha_{it} = \frac{exp(u_{it}^{T}u_w)}{\sum_t exp(u_{it}^{T}u_w)} \\s_i=\sum_t\alpha_{it}h_{it}</script><p>產生出來的$s$就是sentence vector</p><h4 id="Sentence-Encoder"><a href="#Sentence-Encoder" class="headerlink" title="Sentence Encoder"></a>Sentence Encoder</h4><p>跟Word一樣，跳過<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.pinimg.com/originals/51/18/62/511862395ab304a6991b2917fa2bc039.jpg" alt=""></p><h4 id="Sentence-Attention"><a href="#Sentence-Attention" class="headerlink" title="Sentence Attention"></a>Sentence Attention</h4><p>Jumping<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.pinimg.com/originals/51/18/62/511862395ab304a6991b2917fa2bc039.jpg" alt=""></p><h3 id="Document-Classification"><a href="#Document-Classification" class="headerlink" title="Document Classification"></a>Document Classification</h3><p>最後透過Sentence Attention可以得出一個document vector，經過一層transformation後接softmax，用cross entropy去分類文章。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>在多個datasets上測試，這些dataset主要分成兩種類型: sentiment estimation and topic classification</p><ul><li>80% training, 10% validation, 10% testing</li></ul><ol><li>Yelp reviews(有三年，一年一個)</li><li>IMDB reviews </li><li>Yahoo answers</li><li>Amazon reviews</li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/XSIJORX.png" alt=""></p><h3 id="Model-configuration-and-training"><a href="#Model-configuration-and-training" class="headerlink" title="Model configuration and training"></a>Model configuration and training</h3><ul><li>使用Stanford’s CoreNLP (Manning et al., 2014)切sentence和word</li><li>word2vec做word embedding, dimension=200</li><li>frequency小於5的換成<UNK></li><li>GRU dimension=50<ul><li>所以biGRU就會是100</li></ul></li><li>word/sentence context vector dimension=100</li><li>batch size=64</li><li>optimizer use SGD</li></ul><h3 id="Results-and-analysis"><a href="#Results-and-analysis" class="headerlink" title="Results and analysis"></a>Results and analysis</h3><blockquote><p>The experimental results on all data sets are shown in Table 2. We refer to our models as HN-{AVE, MAX, ATT}. Here HN stands for Hierarchical Network, AVE indicates averaging, MAX indicates max-pooling, and ATT indicates our proposed hierarchical attention model. Results show that HNATT gives the best performance across all data sets</p></blockquote><p>同時也比較了階層架構下使用averaging, max-pooling和HAN的效果，總之</p><p><strong>HAN棒棒噠</strong></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/Vvix5sD.png" alt=""></p><h3 id="Context-dependent-attention-weights"><a href="#Context-dependent-attention-weights" class="headerlink" title="Context dependent attention weights"></a>Context dependent attention weights</h3><p>下圖是’good’對於評分的重要程度，(b)-(f)分別代表1-5分，可以發現評分越好的圖，’good’的weight越大。</p><p><strong>因為word的重要性很大部分取決於上下文</strong>，沒有注意力機制的模型可能會將word獨立的去進行判斷，但是就如’good’仍然會出現在評分低的評論中(例如只對部分產品感到滿意，或是用了否定句: ‘not good’)，此時這些模型就無法work的很好。不過就如下圖看到的，在HAN中並不會出現這個狀況。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/4XhVyiz.png" alt=""></p><p>再來看看’bad’的:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/pbE0RWm.png" alt=""></p><h3 id="Visualization-of-attention"><a href="#Visualization-of-attention" class="headerlink" title="Visualization of attention"></a>Visualization of attention</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/AIvoXvj.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</title>
      <link href="/posts/4f49bf9b/"/>
      <url>/posts/4f49bf9b/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>attention在nlp很紅眾所皆知，這篇據說是nlp第一個將attention概念引入的paper，還不來拜見衣食父母</p><p>paper: <a href="https://arxiv.org/pdf/1409.0473.pdf?fbclid=IwAR0d3Fe7egKP0zz8wAe2A-UX-ZIeUzj7MlfFmk_WwkfBeCWx-mkynfFNFls" target="_blank" rel="noopener">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a></p><p>作者的slide: <a href="https://aisc.ai.science/static/slides/20181018_XiyangChen.pdf" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></p><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><blockquote><p> In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.</p></blockquote><p>以往神經網路再翻譯的任務上都是建立在encoder-decoder的架構上，<strong>固定長度的context vector會成為效能的bottleneck</strong>，所以這篇文章提出了dynamic context vector的概念，讓model自己去搜尋input和related predict的相關部分，使得效能upupup。</p><p>這也是<strong>最早提出attention mechanism的paper</strong>。</p><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><a id="more"></a><p>一開始先後介紹了Neural machine translation, encoder-decoder架構。</p><p>接下來講了encoder-decoder的缺點: <strong>context vector必須要足以包含整個input sentence的information，否則效果就會不好，當句子長度變長時這個問題就會浮現出來</strong>。</p><p>接下來講自己的方法棒棒哒:</p><blockquote><p>In order to address this issue, we introduce an extension to the encoder–decoder model which learns to align and translate jointly. Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.</p></blockquote><p>模型會自到去找出source sentence中對當前最富有資訊量的部分，並透過這些部分與context vector結合來進行預測。</p><ul><li>直接翻有點難懂，就是注意力機制，看當前哪個部分是最重要的!</li></ul><blockquote><p>The most important distinguishing feature of this approach from the basic encoder–decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector. We show this allows a model to cope better with long sentences.</p></blockquote><p>如果對attention還不熟的可以看這段，相較原本encoder會把所有input sequence壓成一個固定的context vector，透過了attention機制選擇了一個subset區域，特別針對那一區段來動態產生context vector，使得context vector能夠更有變化。</p><p>相關閱讀</p><ul><li><a href="https://meetonfriday.com/posts/5839a8bf/">論文速速讀: Attention Is All You Need</a></li><li><a href="https://meetonfriday.com/posts/e26b7840/">[DL]Attention Mechanism學習筆記</a></li></ul><h2 id="BACKGROUND-NEURAL-MACHINE-TRANSLATION"><a href="#BACKGROUND-NEURAL-MACHINE-TRANSLATION" class="headerlink" title="BACKGROUND: NEURAL MACHINE TRANSLATION"></a>BACKGROUND: NEURAL MACHINE TRANSLATION</h2><h3 id="RNN-ENCODER–DECODER"><a href="#RNN-ENCODER–DECODER" class="headerlink" title="RNN ENCODER–DECODER"></a>RNN ENCODER–DECODER</h3><p>RNN based的encoder-decoder，有興趣的自己去paper看，想說這裡還算簡單不想打一堆annotation…QQ</p><h2 id="LEARNING-TO-ALIGN-AND-TRANSLAT"><a href="#LEARNING-TO-ALIGN-AND-TRANSLAT" class="headerlink" title="LEARNING TO ALIGN AND TRANSLAT"></a>LEARNING TO ALIGN AND TRANSLAT</h2><h3 id="DECODER-GENERAL-DESCRIPTION"><a href="#DECODER-GENERAL-DESCRIPTION" class="headerlink" title="DECODER: GENERAL DESCRIPTION"></a>DECODER: GENERAL DESCRIPTION</h3><script type="math/tex; mode=display">s_i=f(s_{i-1},y_{i-1},c_i)</script><p>$s<em>i$是RNN的在time $i$的hidden state，前一個時刻的output$y</em>{i-1}$和context vector$c_i$</p><p>也就是每一個time stamp，context vector都會不同，透過下列公式得出:</p><script type="math/tex; mode=display">c_i=\sum^{T_x}_{j=1}\alpha_{ij}h_j</script><p>$h$是intput sequence丟到RNN的output，這裡RNN用的是雙向所以output vector會是2倍。而$\alpha$則是透過下列公式計算:</p><script type="math/tex; mode=display">\alpha_{ij}=\frac{exp(e_{ij})}{\sum^{T_x}_{k=1}exp(e_{ik})}</script><p>$e<em>ij=a(s</em>{i-1}, h_j)$是一個alignment model，透過一個分數紀錄了對於第i個output，input j附近的位置的重要性</p><ul><li><strong>注意在這篇paper中這一項是被train出來的</strong>，attention機制在後面的研究不一定需要train</li><li>這個model會被加入原本的task一起train</li><li>細節在最後的附錄有提到</li></ul><blockquote><p>Intuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixed length vector.</p></blockquote><h3 id="ENCODER-BIDIRECTIONAL-RNN-FOR-ANNOTATING-SEQUENCES"><a href="#ENCODER-BIDIRECTIONAL-RNN-FOR-ANNOTATING-SEQUENCES" class="headerlink" title="ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES"></a>ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES</h3><p>Encoder使用了Bidirectional RNN，好這裡應該不是重點xD</p><p>所以全部合起來的架構就會變成下面這張圖<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/cu63oIR.png" alt=""></p><p>Encoder就是藍色跟紅色的BiRNN，產出$h_i= [ \overrightarrow{h_i} ; \overleftarrow{h_i} ]$，後面decoder做的事情就跟上面講的一樣了</p><h2 id="EXPERIMENT-SETTINGS"><a href="#EXPERIMENT-SETTINGS" class="headerlink" title="EXPERIMENT SETTINGS"></a>EXPERIMENT SETTINGS</h2><h3 id="QUALITATIVE-ANALYSIS"><a href="#QUALITATIVE-ANALYSIS" class="headerlink" title="QUALITATIVE ANALYSIS"></a>QUALITATIVE ANALYSIS</h3><h4 id="ALIGNMENT"><a href="#ALIGNMENT" class="headerlink" title="ALIGNMENT"></a>ALIGNMENT</h4><p>比較了attention(左邊)機制下和原本encoder-decoder(右邊)的效果，數據棒棒哒的部分就不提了</p><p>來提一下比較有趣的attention分析，作者把他們模型train好後把對應的$\alpha_{ij}$值畫了出來<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/VQqsBAV.png" alt=""><br>可以看到中間有一個很特別的斜槓，作者有提到由於<strong>英文跟法語在形容詞和名詞語法上的不同</strong>，所以</p><p>[European Economic Area]</p><p>會變成</p><p>[zone economique europ ´ een]</p><p>儘管如此，attention還是棒棒哒知道Area對應zone呢!</p><p>此外也提到了soft-align(attention)跟hard-align(每個位置依序對其)的優缺點:</p><ul><li>可以更有彈性</li><li>可以多對多</li><li>soft-align允許input/output長度不同<ul><li>hard-align如果長度不同就要加入一些額外的token，Ex: [NULL]，不過這是別篇論文在做的事情了</li></ul></li></ul><h4 id="LONG-SENTENCES"><a href="#LONG-SENTENCES" class="headerlink" title="LONG SENTENCES"></a>LONG SENTENCES</h4><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/6bhk5Hy.png" alt=""></p><h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2><h3 id="LEARNING-TO-ALIGN"><a href="#LEARNING-TO-ALIGN" class="headerlink" title="LEARNING TO ALIGN"></a>LEARNING TO ALIGN</h3><blockquote><p>…Our approach, on the other hand, requires computing the annotation weight of every word in the source sentence for each word in the translation. This drawback is not severe with the task of translation in which most of input and output sentences are only 15–40 words. However, this may limit the applicability of the proposed scheme to other tasks.</p></blockquote><p>由於attention會看input sequence的每一個token，在機器翻譯這個task中還好，因為每一句長度不大，不過當句子過長的時候可能就會造成計算上的cost</p><h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>哎呀，雖然attention很有名，不過這篇提出的model叫做RNNsearch唷!<br>前面我都忘記提了補一下</p><h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><p>不要覺得附錄都不重要，我們直接來看那個神奇的alignment model，也就是$e$到底是怎麼設計的:</p><script type="math/tex; mode=display">e_{ij}=v^T_{a}tanh(W_a s_{i-1}+U_a h_j)</script><p>p.s. 這裡是把$s_{i-1}$跟$h_j$加起來唷，之後再繼續看其他的attention paper會看到有很多種作法(add, concat…)</p><ul><li>題外話，關於對add跟concat的理解可以參考這篇:<a href="https://meetonfriday.com/posts/45f8d851/">[DL]Difference between add/concat operation in Neural Network</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解決NVIDIA-SMI has failed because it could not communicate with the NVIDIA driver</title>
      <link href="/posts/66a59c98/"/>
      <url>/posts/66a59c98/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近因為碩論進入了瘋狂跑實驗補數據的階段，有幾個model又要訓練的蠻久的，讓人有時覺得有點小煩躁。</p><h2 id="問題描述"><a href="#問題描述" class="headerlink" title="問題描述"></a>問題描述</h2><p>昨天，對就是昨天，在server上用gpu訓練model訓練到一半，然後我interrupt kernel後感覺失敗了，從那一刻起jupyter再也不理我，然後ssh server也連不上了…</p><p>不過萬幸的是ping server還是ping的到，所以應該不是整個server爆炸了。不過由於server在學校，也沒辦法繼續進行實驗，等隔天請人幫忙將server重開後server就可以登進去了。</p><p>儘管成功登進去了，不過又遇到了奇怪的靈異事件，那就是不論使用<code>nvidia-smi</code>或是<code>gpustat</code>均取得不到顯卡的資訊。</p><p>明明前一晚還好好的，一重開就這樣真的很讓人傻眼…這種時期server可不能出事阿…</p><a id="more"></a><h2 id="解決方法"><a href="#解決方法" class="headerlink" title="解決方法"></a>解決方法</h2><p>後來參考網路上的解決方法，先確認<code>nvcc -V</code>可以正常取得驅動的資訊，然後透過dkms重新安裝nvidia</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install dkms</span><br><span class="line">sudo dkms install -m nvidia -v [GPU_DRIVER_VERSION]</span><br></pre></td></tr></table></figure><p>驅動的版本可以透過<code>ls /usr/src</code>查看</p><p>更新完就解決了!! 可喜可賀可喜可賀</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.twblogs.net/a/5c45ceffbd9eee35b3a72c4f" target="_blank" rel="noopener">無法連接NVIDIA驅動：NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Difference average method in sklearn.metrics.classification_report()</title>
      <link href="/posts/3a69a72/"/>
      <url>/posts/3a69a72/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在評估模型好壞時，除了accuracy往往也會想看看其他的評估指標</p><p>而在sklearn中，有一個很方便的function可以快速取得評估模型的一些量化指標，例如下方程式碼:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">classification_report(test_y_true, test_y_pred, digits=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>會得到:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/SdCWB73.png" alt=""></p><p>這張圖到底要怎麼看，可以知道macro, weighted是不同的平均方法，所以最後兩行應該是<strong>對各種評估指標用不同的方法進行平均</strong>，那位什麼accuracy又放在倒數第三行，且沒有precision和recall呢?</p><ul><li>先上結論，accuracy那行其實是在做<strong>micro avg</strong></li><li>對於micro avg，precision, recall, f1-score是相同的</li></ul><p>詳細內容請看下方介紹</p><a id="more"></a><h2 id="F1-score"><a href="#F1-score" class="headerlink" title="F1-score"></a>F1-score</h2><blockquote><p>In general, we prefer classifiers with higher precision and recall scores. However, there is a trade-off between precision and recall: when tuning a classifier, improving the precision score often results in lowering the recall score and vice versa — there is no free lunch.</p></blockquote><p>通常precision和recall之間存在著trade-off，想讓一個好往往就會降低令一個</p><ul><li>no free lunch theorem</li></ul><p>想用一個評估指標同時衡量precision和recall? F1-score使用了調和平均(harmonic mean)</p><p>F1-score = 2 × (precision × recall)/(precision + recall)</p><p>調和平均對於較少的數值會有比較大的權重，例如以下的例子:</p><blockquote><p>Similar to arithmetic mean, the F1-score will always be somewhere in between precision and mean. But it behaves differently: the F1-score gives a larger weight to lower numbers. For example, when Precision is 100% and Recall is 0%, the F1-score will be 0%, not 50%. Or for example, say that Classifier A has precision=recall=80%, and Classifier B has precision=60%, recall=100%. Arithmetically, the mean of the precision and recall is the same for both models. But when we use F1’s harmonic mean formula, the score for Classifier A will be 80%, and for Classifier B it will be only 75%. Model B’s low precision score pulled down its F1-score.</p></blockquote><h2 id="F1-score-in-multi-class"><a href="#F1-score-in-multi-class" class="headerlink" title="F1-score in multi-class"></a>F1-score in multi-class</h2><p>在Multi-class task中，每一個類別都可以算出一個f1-score，那整體的f1-score要如何取得？有幾種平均的方法，假設現在有三類，各自的F1-score分別是:</p><ul><li>42.1% </li><li>30.8% </li><li>66.7%</li></ul><h3 id="macro"><a href="#macro" class="headerlink" title="macro"></a>macro</h3><p>所有f1-score的算術平均，也就是全部加起來除以數量，也就是:</p><p>Macro-F1 = (42.1% + 30.8% + 66.7%) / 3 = 46.5%</p><p>macro-f1是unweighted的，對於每個類別的權重是相同的</p><ul><li>也就是說，在imbalanced的分佈下，<strong>數量小的類別對於整體的影響也是相同的</strong></li></ul><h3 id="weighted"><a href="#weighted" class="headerlink" title="weighted"></a>weighted</h3><p>假設我們有25個樣本: 分別數量為6, 10, 9 </p><p>則Weighted-F1的算法為</p><p>Weighted-F1 = (6 × 42.1% + 10 × 30.8% + 9 × 66.7%) / 25 = 46.4%</p><p>對於每一類的f1，類別的數量會影響到該類別的權重</p><ul><li>也就是說，在imbalanced的分佈下，<strong>數量大的類別將會大大的影響整體的f1效果</strong></li></ul><h3 id="micro"><a href="#micro" class="headerlink" title="micro"></a>micro</h3><p>先算整體的percision和recall，再算micro-f1</p><p>percision = (TP/(TP+FP))</p><p>recall = (TP/(TP+FN))</p><p>考慮在多類別情況的confusion matrix</p><ul><li>TP就是對角線上的數字(正確的分對)</li><li>FP/FN就是非對角線上的其他數字，confusion matrix(A, B)可以想成: <ul><li>對於某個positive類別A他卻分錯B(FP)</li><li>對於某個negative類別B他卻分錯A(FN)</li></ul></li></ul><p>所以<strong>在micro下的percision和recall會是相同的，實際上，就連accuracy也是相同的</strong></p><ul><li>accuracy不就是confusion上所有數量中，分對的數量?</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1" target="_blank" rel="noopener">Multi-Class Metrics Made Simple, Part II: the F1-score</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> machine learning </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Attentive CutMix: An Enhanced Data Augmentation Approach for Deep Learning Based Image Classification</title>
      <link href="/posts/d978825d/"/>
      <url>/posts/d978825d/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>論文網址: <a href="https://arxiv.org/pdf/2003.13048.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2003.13048.pdf</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p> However, all of them perform this operation randomly, without capturing the most important region(s) within an object. In this paper, we propose Attentive CutMix, a naturally enhanced augmentation strategy based on CutMix [3]. In each training iteration, we choose the most descriptive regions based on the intermediate attention maps from a feature extractor, which enables searching for the most discriminative parts in an image.</p></blockquote><p>CutMix的進化版，以往的data augumentation都是random operation。Attentive CutMix透過取出中間層的attention map來挑選最具有解釋性的區域進行CutMix。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Attentive CutMix想要基於CutMix的情況下，找出最具代表性的region來進行替換。</p><p>下圖高能，非戰鬥人員請迅速撤離<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/mfFweSp.png" alt=""></p><p>這已經沒在顧及動物的感受了…求那隻狗的心理面積…</p><a id="more"></a><blockquote><p>Our goal is to learn a more robust network that can attend to the most important part(s) of an object with better recognition performance without incurring any additional testing costs. We achieve this by utilizing the attention maps generated from a pretrained network to guide the localization operation of cutting and pasting among training image pairs in CutMix.</p><p>wherein we initially discern the most important parts from an object, then use cut and paste inspired from CutMix to generate a new image which helps the networks better attend to the local regions of an image.</p></blockquote><p>透過pretrained network來得到attention maps，所以並不會造成額外的cost(對於model本身)</p><p>不過這個paper只在Cifar-100上進行了實驗而已。</p><h2 id="Proposed-Approach"><a href="#Proposed-Approach" class="headerlink" title="Proposed Approach"></a>Proposed Approach</h2><h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/LMdYdjO.png" alt=""></p><p>Regularization的做法都跟CutMix一樣: 細節可以看<a href="https://meetonfriday.com/posts/b4202d1/">論文速速讀: CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</a><br>$x\in R^{W\times H\times C}$是圖片, $y$是label，CutMix做的是合成兩張圖片$(x_A, y_A)$和$(x_B, y_B)$然後產生新圖片$(\tilde{x}, \tilde{y})$，透過以下公式:</p><script type="math/tex; mode=display">\begin{aligned}\tilde{x}&=M\odot x_A+(1-M)\odot{x_B}\\\tilde{y}&=\lambda y_A+(1-\lambda)y_B\end{aligned}</script><ul><li>$M\in {0,1}^{W\times H}$是一個binary mask，在bounding box內的值是0；否則是1</li><li>$\odot$是element-wise multiplication</li><li>combination ratio$\lambda$來自beta distribution $Beta(\alpha, \alpha)$<ul><li>$\lambda$來自Mixup的原始paper</li></ul></li></ul><p>重點是patches怎麼選，在CutMix中是來自一個uniform distribution，不過在這篇用到了attention機制:</p><blockquote><p>We first obtain a heatmap (generally a 7×7 grid map) of the first image by passing it through a pretrained classification model like ResNet-50 and take out the final 7×7 output feature map. We then select the top “N” patches from this 7×7 grid as our attentive region patches to cut from the given image. Here N can range from 1 to 49 (i.e. the entire image itself). Later, we will present an ablation study on the number of attentive patches to be cut from a given image.</p></blockquote><p>透過將一張圖片餵到一個pretrained model，把最後一層的 7x7 feature map取出，選前N個patches，這篇提到N可以是1~49</p><blockquote><p>We then map the selected attentive patches back to the original image. For example, a single patch in a 7×7 grid would map back to a 32×32 image patch on a 224×224 size input image. The patches are cut from the first image and pasted onto the second image at their respective original locations, assuming both images are of the same size. The pair of training samples are randomly selected during each training phase. For the composite label, considering that we pick the top 6 attentive patches from a 7×7 grid, λ would then be 6 49 . Every image in the training batch is augmented with patches cutout from another randomly selected image in the original batch. Please refer Fig. 2 for an illustrative representation of our method.</p></blockquote><p>把attentive patches upsampling回原本的size，然後對應到相同的位置上蓋掉(如果大小相同)</p><ul><li>Ex: 對於224x224的image，7x7的每個patch都變回32x32</li></ul><p>而對於混和label則是根據使用了幾個patches，例如N=6則$\lambda=6/49$</p><h3 id="Theoretical-Improvements-over-CutMix"><a href="#Theoretical-Improvements-over-CutMix" class="headerlink" title="Theoretical Improvements over CutMix"></a>Theoretical Improvements over CutMix</h3><p>講說其實CutMix雖然效果很好，但沒有好的理論根據佐證，其中一個假設是: <strong>在隨機蓋掉patch時可能蓋掉了圖像中的重點區塊，使得降低了模型對於特定主題的overfitting</strong>。</p><p>在這個假設下，CutMix隨機蓋掉部分的做法就不合理了，蓋掉不重要的位置並不能幫助模型robust(例如補丁的其實是一個背景區塊)，這也是為什麼Attentive CutMix會被提出來的原因，<strong>希望蓋的部分都是重點部分</strong>。</p><p>此外，Attentive CutMix的效用很大一部分取決於pretrained model能否有效的找出important part。</p><h2 id="Experiments-and-Analysis"><a href="#Experiments-and-Analysis" class="headerlink" title="Experiments and Analysis"></a>Experiments and Analysis</h2><p>在Cifar-10和Cifar-100上做測試，Attentive CutMix棒棒哒<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/vIgvYZU.png" alt=""></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/paxxSlj.png" alt=""></p><h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p>此外，N到底該選多少他們也做了實驗(N=1~15)，得到的結論是N=6的結果最好，不過</p><ul><li>過少(小於6)的話可能會造成重點區塊無法被遮蓋住</li><li>過多的話則會造成遮蔽太多，使得模型沒辦法有效學習</li></ul><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>需要額外的pretrained model，不過paper認為這項成本對於效能的提升來說小Case啦~</p>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</title>
      <link href="/posts/b4202d1/"/>
      <url>/posts/b4202d1/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><p>論文網址: <a href="https://arxiv.org/pdf/1905.04899.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.04899.pdf</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>…current methods for regional dropout removes informative pixels on training images by overlaying a patch of either black pixels or random noise.<br>Such removal is not desirable because it leads to information loss and inefficiency during training.</p></blockquote><ul><li>以往的regional dropout技術是在圖片上加上黑色的補釘(patch)或是雜訊來使得model更robust</li><li>這樣的方式會造成information loss或是訓練效率降低</li></ul><blockquote><p>We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches.</p></blockquote><ul><li>切割patch然後把其他張圖片拿來補</li></ul><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>In particular, to prevent a CNN from focusing too much on a small set of intermediate activations or on a small region on input images, random feature removal regularizations have been proposed. Examples include dropout [33] for randomly dropping hidden activations and regional dropout [2, 49, 32, 7] for erasing random regions on the input. Researchers have shown that the feature removal strategies improve generalization and localization by letting a model attend not only to the most discriminative parts of objects, but rather to the entire object region [32, 7].</p></blockquote><p>為了避免CNN關注到某些特定小區域，一些random feature removal regularizations的技術被提出，向是dropout或是feature removal strategies</p><p>研究發現feature removal strategies可以使得模型關注整體的資訊而不是只關注最重要的部分，因此提升了generalization和localization</p><blockquote><p>While regional dropout strategies have shown improvements of classification and localization performances to a certain degree, deleted regions are usually zeroed-out [2, 32] or filled with random noise [49], greatly reducing the proportion of informative pixels on training images</p></blockquote><p>但作者認為regional dropout strategies造成了被dropout的區域值全部都是zero或是noise，導致了過多的information loss</p><ul><li>既然如此，把要dropout的區域用其他張圖片來補就好了，這樣既有資訊可以學又可以robust</li></ul><h3 id="與其它的方法比較"><a href="#與其它的方法比較" class="headerlink" title="與其它的方法比較"></a>與其它的方法比較</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/ur2eRMe.png" alt=""></p><ul><li>CutOut就是單純的黑色補丁，但是會有多餘的information loss</li><li>Mixup是透過把兩張圖片作線性插值來混合，但這樣會造成混合後的圖片很不自然</li><li>CutMix是只針對patch region來補其它圖片，解決了上述兩個缺點</li></ul><h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><p>以往的CNN優化技巧有:</p><ul><li>regional dropout</li><li>data augmentation</li></ul><p>剩下的就是介紹CutMix有多棒，然後因為他是在data level進行操作，所以不會影響原本的模型架構</p><h2 id="CutMix"><a href="#CutMix" class="headerlink" title="CutMix"></a>CutMix</h2><h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><p>$x\in R^{W\times H\times C}$是圖片, $y$是label，CutMix做的是合成兩張圖片$(x_A, y_A)$和$(x_B, y_B)$然後產生新圖片$(\tilde{x}, \tilde{y})$，透過以下公式:</p><script type="math/tex; mode=display">\begin{aligned}\tilde{x}&=M\odot x_A+(1-M)\odot{x_B}\\\tilde{y}&=\lambda y_A+(1-\lambda)y_B\end{aligned}</script><ul><li>$M\in {0,1}^{W\times H}$是一個binary mask，在bounding box內的值是0；否則是1</li><li>$\odot$是element-wise multiplication</li><li>combination ratio$\lambda$來自beta distribution $Beta(\alpha, \alpha)$<ul><li>$\lambda$來自Mixup的原始paper</li></ul></li></ul><p>接下來定義bounding box $B=(r_x, r_y, r_w, r_h)$</p><ul><li>該區域會將B的內容crop下來蓋到A上面</li></ul><p>論文的實驗中使用的是rectangular masks(aspect ratio和原圖一樣)，box coordinates來自uniformly distribution: </p><script type="math/tex; mode=display">\begin{aligned}r_x&\sim Uniform(0, W), r_w=W\sqrt{1-\lambda} \\ r_y&\sim Uniform(0, H), r_y=H\sqrt{1-\lambda}\end{aligned}</script><p>實作細節簡單易懂，取一個minibatch，之後shuffle再取一次minibatch，然後根據上面公式產生新的label:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/GlLQ3k2.png" alt=""></p><p>source code: <a href="https://github.com/clovaai/CutMix-PyTorch" target="_blank" rel="noopener">clovaai/CutMix-PyTorch</a></p><ul><li><p>決定bounding box:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rand_bbox</span><span class="params">(size, lam)</span>:</span></span><br><span class="line">    W = size[<span class="number">2</span>]</span><br><span class="line">    H = size[<span class="number">3</span>]</span><br><span class="line">    cut_rat = np.sqrt(<span class="number">1.</span> - lam)</span><br><span class="line">    cut_w = np.int(W * cut_rat)</span><br><span class="line">    cut_h = np.int(H * cut_rat)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># uniform</span></span><br><span class="line">    cx = np.random.randint(W)</span><br><span class="line">    cy = np.random.randint(H)</span><br><span class="line"></span><br><span class="line">    bbx1 = np.clip(cx - cut_w // <span class="number">2</span>, <span class="number">0</span>, W)</span><br><span class="line">    bby1 = np.clip(cy - cut_h // <span class="number">2</span>, <span class="number">0</span>, H)</span><br><span class="line">    bbx2 = np.clip(cx + cut_w // <span class="number">2</span>, <span class="number">0</span>, W)</span><br><span class="line">    bby2 = np.clip(cy + cut_h // <span class="number">2</span>, <span class="number">0</span>, H)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bbx1, bby1, bbx2, bby2</span><br></pre></td></tr></table></figure></li><li><p>取一個minibatch，將該batch的所有圖片都用同一張B來合成</p></li><li>注意到實際上這張合成圖混和label的意思，是指在實作上算loss時<strong>使用不同的label來算cross entropy並加權平均</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (input, target) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    <span class="comment"># measure data loading time</span></span><br><span class="line">    data_time.update(time.time() - end)</span><br><span class="line"></span><br><span class="line">    input = input.cuda()</span><br><span class="line">    target = target.cuda()</span><br><span class="line"></span><br><span class="line">    r = np.random.rand(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> args.beta &gt; <span class="number">0</span> <span class="keyword">and</span> r &lt; args.cutmix_prob:</span><br><span class="line">        <span class="comment"># generate mixed sample</span></span><br><span class="line">        lam = np.random.beta(args.beta, args.beta)</span><br><span class="line">        rand_index = torch.randperm(input.size()[<span class="number">0</span>]).cuda()</span><br><span class="line">        target_a = target</span><br><span class="line">        target_b = target[rand_index]</span><br><span class="line">        bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), lam)</span><br><span class="line">        input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]</span><br><span class="line">        <span class="comment"># adjust lambda to exactly match pixel ratio</span></span><br><span class="line">        lam = <span class="number">1</span> - ((bbx2 - bbx1) * (bby2 - bby1) / (input.size()[<span class="number">-1</span>] * input.size()[<span class="number">-2</span>]))</span><br><span class="line">        <span class="comment"># compute output</span></span><br><span class="line">        output = model(input)</span><br><span class="line">        loss = criterion(output, target_a) * lam + criterion(output, target_b) * (<span class="number">1.</span> - lam)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># compute output</span></span><br><span class="line">        output = model(input)</span><br><span class="line">        loss = criterion(output, target)</span><br></pre></td></tr></table></figure><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/N4LRiba.png" alt=""></p><ul><li>Mixup混和了兩張圖片，造成模型無法正確知道要關注什麼部分</li><li>Cutout再只有兩個類別的時候效果不錯，因為遮掉另一個類別所以可以區分剩下的類別；不過這樣就無法有效的關注另一個類別了</li><li>CutMix棒棒哒，上面的兩個問題都解決了</li></ul><h2 id="Refeneces"><a href="#Refeneces" class="headerlink" title="Refeneces"></a>Refeneces</h2><ul><li><a href="https://blog.csdn.net/weixin_38715903/article/details/103999227" target="_blank" rel="noopener">【论文阅读笔记】CutMix：数据增强</a></li><li><a href="https://github.com/clovaai/CutMix-PyTorch" target="_blank" rel="noopener">CutMix-PyTorch</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Attention Is All You Need</title>
      <link href="/posts/5839a8bf/"/>
      <url>/posts/5839a8bf/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><h2 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h2><p>paper: <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcS4z5cGytjRDdt8JnxeTYClIXUonbrXSFIf8WTHU6f0thbH6_dd&amp;usqp=CAU" alt=""></p><p>提到nlp近年來的重點技術之一就不能不提到attention，注意力機制提出後幾乎所有nlp論文都被重新用attention掃過一輪benchmark。</p><p>雖然這篇不是第一個提出注意力機制的，不過後面的各種芝麻街是基於這篇來延伸。</p><p>關於attention一路走來的發展，可以參考之前我寫的<a href="https://meetonfriday.com/posts/e26b7840/">[DL]Attention Mechanism學習筆記</a>，這篇會主要在摘要paper重點內容。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.</p></blockquote><ul><li>以往的seqence transduction models是使用基於encoder &amp; decoder的複雜RNN/CNN模型<ul><li>最佳的模型則是使用了基於attention mechanism的encoder decoder(還是依據RNN/CNN)</li></ul></li><li>提出了<strong>transformer</strong>，不使用CNN/RNN，完全只使用attention mechanism的網路架構<ul><li>不過他的架構還是encoder decoder的概念，只是沒用到RNN/CNN<a id="more"></a></li></ul></li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>Recurrent models typically factor computation along the symbol positions of the input and output　sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden　states $h<em>t$, as a function of the previous hidden state $h</em>{t−1}$ and the input for position $t$. This inherently　sequential nature precludes parallelization within training examples, which becomes critical at longer　sequence lengths, as memory constraints limit batching across examples. Recent work has achieved　significant improvements in computational efficiency through factorization tricks [21] and conditional　computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.</p></blockquote><ul><li>RNN主要是透過將sequence的位置與time steps對齊來考慮不同sequence之間的關係</li><li>很大的一個問題: <strong>無法平行運算</strong></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/EN4377S.png" alt=""></p><blockquote><p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.</p></blockquote><ul><li>attention mechanism使得模型可以透過dependencies進行建模，而不用考慮sequence之間的關係</li><li>但早期的研究還是將attention和RNN一起使用</li><li>(其實最早提出attention mechanism的不是這篇，參見<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a>)</li></ul><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/Y06ZdLt.jpg" alt=""></p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li>N個block組成，每層有兩個sub-layers<ul><li>Multi-head attention</li><li>Fully connected</li></ul></li><li>residual connection + layer normalization</li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li>N個block組成，每層有三個sub-layers<ul><li>Masked Multi-head attention</li><li>Multi-head attention</li><li>Fully connected</li></ul></li><li>residual connection + layer normalization</li></ul><blockquote><p>We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$</p></blockquote><ul><li>透過一個mask確保attention在i時刻不會關注到i之後的資料<ul><li>因為真實情況你是不會有未來的資料的</li><li>look_ahead_mask</li></ul></li></ul><h4 id="look-ahead-mask"><a href="#look-ahead-mask" class="headerlink" title="look_ahead_mask"></a>look_ahead_mask</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立一個 2 維矩陣，維度為 (size, size)，</span></span><br><span class="line"><span class="comment"># 其遮罩為一個右上角的三角形</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_look_ahead_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">  mask = <span class="number">1</span> - tf.linalg.band_part(tf.ones((size, size)), <span class="number">-1</span>, <span class="number">0</span>)</span><br><span class="line">  <span class="keyword">return</span> mask  <span class="comment"># (seq_len, seq_len)</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]</span><br><span class="line"> [0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]</span><br><span class="line"> [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]</span><br><span class="line"> [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</span><br><span class="line"> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]</span><br></pre></td></tr></table></figure><p>這個mask在attention做完要進行softmax的時候會用到，也就是讓遮罩為1的地方加上一個趨近負無限大的值，使得softmax完的值會趨近於0<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scaled_dot_product_attention</span><span class="params">(q, k, v, mask)</span>:</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="comment"># 將遮罩「加」到被丟入 softmax 前的 logits</span></span><br><span class="line">  <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    scaled_attention_logits += (mask * <span class="number">-1e9</span>)</span><br></pre></td></tr></table></figure></p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><blockquote><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p></blockquote><ul><li>透過query vector和key-value vector的mapping<ul><li>q, k, v都是相同的vector -&gt; self-attention的原因</li></ul></li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://img-blog.csdnimg.cn/20190410093253366.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4Mzg1NTM1,size_16,color_FFFFFF,t_70" alt=""></p><h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><p>$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$</p><ul><li>$d_k$是Q, K的dimension; $d_v$是V的dimension<ul><li>其實在transformer中Q, K, V的dimension都相同，可是論文不知道位啥特地把V的annotaiton換了一個…</li><li>其實有人做了一個實驗，有沒有除$\sqrt{d_k}$好像沒啥差(參見李弘毅教學影片)</li></ul></li><li>attention常見的兩種操作<ul><li>additive attention: <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a><ul><li>最早提出attention的nlp paper</li></ul></li><li>dot attention: <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></li></ul></li><li>dot attention通常比較有效率，因為矩陣乘法-&gt;GPU</li></ul><h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/t5Uy3la.jpg" alt=""><br>$MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^o$<br>$where \space head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)$</p><ul><li>data分成Q, K, V後先做了一個linear transformation</li><li>然後attention完後concat起來，再做一次linear transformation</li><li>不同的attention可以關注不同的訊息(local, global…)</li></ul><h4 id="Applications-of-Attention-in-our-Model"><a href="#Applications-of-Attention-in-our-Model" class="headerlink" title="Applications of Attention in our Model"></a>Applications of Attention in our Model</h4><p>Multi-head attention被用在以下三個地方</p><ul><li>encoder layers: Q,K,V來自相同的input</li><li>decoder layers: Q,K,V來自相同的input，還使用了look_ahead_mask防止decoder看到未來的資料</li><li>“encoder-decoder attention” layers: Q來自上一層decoder的輸出; K跟V來自encoder最後一層的輸出<ul><li>使得decoder可以關注到encoder的所有資料</li></ul></li></ul><h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p>就是FC，不同層有不同參數</p><h3 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h3><p>就是常用的word embedding跟softmax</p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>“self-attention會看sequence的每個資料，那我資料放第一個跟最後一個其實沒差阿?”</p><ul><li>Ex: “天涯若比鄰” “比天若涯鄰” 的結果應該會是相同的</li></ul><blockquote><p>In order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.</p></blockquote><script type="math/tex; mode=display">PE(pos,2i)=sin(pos/10000^{2i/d_{model}}) \\ PE(pos,2i+1)=cos(pos/10000^{2i/d_{model}})</script><p>畫起來就是下面這種神奇的圖</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png" alt=""></p><h2 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://img-blog.csdnimg.cn/20190411095015338.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI4Mzg1NTM1,size_16,color_FFFFFF,t_70" alt=""></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://meetonfriday.com/posts/e26b7840/">Introduction of Attention Mechanism</a></li><li><a href="https://zhuanlan.zhihu.com/p/74516930" target="_blank" rel="noopener">Layer Normalization</a></li><li><a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html" target="_blank" rel="noopener">淺談神經機器翻譯 &amp; 用 Transformer 與 TensorFlow 2 英翻中</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]Automatic Generation of Personalized Annotation Tags for Twitter Users</title>
      <link href="/posts/8a0dacdf/"/>
      <url>/posts/8a0dacdf/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><h2 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h2><p>論文網址: <a href="https://www.aclweb.org/anthology/N10-1101.pdf" target="_blank" rel="noopener">Automatic Generation of Personalized Annotation Tags for Twitter Users</a></p><p>2010的nlp論文，目的是透過twitter的文章來extract keyword然後自動標註該文章的tags。文章中使用了兩種比較方法: tf-idf以及textrank(他的類似應用是google search engine的page rank)。</p><a id="more"></a><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>This paper introduces a system designed for automatically generating personalized annotation tags to label Twitter user’s interests and concerns. <strong>We applied TFIDF ranking and TextRank to extract keywords from Twitter messages to tag the user</strong>. The user tagging precision we obtained is comparable to the precision of keyword extraction from web pages for content-targeted advertising.</p></blockquote><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>…These Twitter messages contain rich information about an individual user, including what s/he is interested in and concerned about. Identifying an individual user’s interests and concerns can help potential commercial applications. For instance, this information can be employed to produce “following” suggestions, either a person who shares similar interests (for expanding their social network) or a company providing products or services the user is interested in (for personalized advertisement)</p></blockquote><p>Twitter訊息中包含了使用者的許多資訊，這些資訊具有一些淺在的商業應用，例如可以提供客製化的廣告或是協助公司提供相關的產品行銷。</p><blockquote><p>We formulate this problem as a keyword extraction task, by selecting words from each individual user’s Twitter messages as his/her tags. Due to the lack of human generated annotations, we employ an unsupervised strategy.</p></blockquote><p>定義keyword extraction問題為: 從使用者的twitter訊息中去找出屬於該訊息的tags。但由於標籤數量不足，所以採用了unsupervised的策略。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>提到了一些之前的研究，不過也提到這似乎是<strong>第一篇透過twitter message來找出對應的tag的研究</strong>，使用twitter會有一些問題，例如: twitter message相對於之前的研究比較口語化(包含表情符號、俚語、縮寫…)的訊息，傳統研究的兩個方法(tf-idf和textrank)是否仍舊有效仍待驗證。</p><h2 id="System-Architecture"><a href="#System-Architecture" class="headerlink" title="System Architecture"></a>System Architecture</h2><p>系統架構，要將twitter message變成可以train的data要經過一連串的preprocessing pipeline。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/04/001.png" alt="001"></p><ol><li>Remove reply message: 因為這些回應訊息是比較針對對方的觀點而不是針對自己的言論</li><li>Remove emoticons: 對於keyword analysis比較沒有幫助</li><li>Substituting / removing internet slangs(厘語) and abbreviations: 這類詞彙可以分成三類 a. 可以被正名的有意義的詞彙: bff(best friend forever), fone(phone) b. 文法上的縮寫: im(i’m), abt(about)，如果刪除會影響POS tagging的結果 c. 用來斷句的詞彙，通常沒有實質意義: lol(laugh out loud), clm(cool like me)，在這篇的方法中會被刪除</li><li>Part-of-Speech tagging and filtering: 透過<a href="https://nlp.stanford.edu/software/tagger.shtml" target="_blank" rel="noopener">Stanford POS tagger</a>只篩選出<strong>名詞</strong>和<strong>形容詞</strong></li><li>Stemming and stopword removing：use Porter stemmer去除字尾，然後去除stop words</li></ol><p>接下來就去做TFIDF跟Textrank，tfidf就不細講了，textrank前身是google search engine以前使用的一個algorithm pagerank。textrank更新公式如下:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/04/002.png" alt="002"></p><p><strong>前處理後每個word都當成圖中的一個節點，點和點之間的edge取決於這兩個word在同一篇文章中共同出現(co-exist)的次數</strong>。</p><p>$d$來自pagerank這篇文章，damping factor，在原本的意思中來自使用者在這一頁會點擊下一頁的機率，這邊直接用了原始的數據(0.85)。</p><p>也就是說這個公式在不斷的更新點之間的排名，透過更新連接你的點$V_j$中，你在這些點中佔有的重要程度是多少，最後排名會收斂到一個狀態。</p><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p>其實他還有做了一些分析，像是</p><ul><li>Top-N代表選出的前N個標籤都要經過三個專家判斷是否這些tags<strong>都</strong>符合該文章內容，所以很合理的，當N越大的時候precision會越低</li><li>Standard deviation of the top-10 TextRank :一個text rank graph，每一個rank所計算出的標準差。<strong>越高代表說每一個word之間的rank差異很大，因此找出的tag比較能反應user的偏好；相反越低代表每一個word之間rank差異不大，因此找出的tag就比較不具有鑑別性。</strong></li><li>Text entropy: 刻劃user message 的豐富度。<strong>越高表示message 內容越豐富。當user message 內容越豐富，越能找出具有代表性的tag。</strong></li></ul><p>然後另一個點就是Textrank普遍比TFIDF好。  </p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>其實就兩個方法tfidf跟textrank，如果以前沒碰過google的pagerank的可以順便學習下，蠻有趣的概念，不過google現在不完全靠這個做SEO了。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://dsmilab.github.io/dsmi-lab-website/2020/03/24/Automatic%20Generation%20of%20Personalized%20Annotation%20Tags%20for%20Twitter%20Users/#more" target="_blank" rel="noopener">Automatic Generation of Personalized Annotation Tags for Twitter Users</a></li><li><a href="https://zhuanlan.zhihu.com/p/37957649" target="_blank" rel="noopener">{论文笔记}Automatic Generation of Personalized Annotation Tags for Twitter Users</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Device rebooting during deep learning training</title>
      <link href="/posts/df742442/"/>
      <url>/posts/df742442/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一直有個壞習慣就是看到值得一看的文章就會先加到我的最愛蒐藏起來，想說以後再看，不過總是沒有行動，日積月累下我的最愛就跟個垃圾桶沒什麼兩樣……</p><p>打這篇文章的當下本人坐在實驗室裡面，覺得剛吃飽好累喔不想做正事，所以開始清我的最愛清單，清阿清啊發現了有兩篇是之前遇到的問題，原本打算要來寫blog記錄下來的可是居然忘了，所以來寫篇廢文記錄一下順便讓自己不要那麼睏(對我來說寫blog比做正事好多了)。</p><h2 id="問題敘述"><a href="#問題敘述" class="headerlink" title="問題敘述"></a>問題敘述</h2><p>前陣子lab server發生了一個很奇怪的現象，當有<strong>多人同時在train deep learning model的時候，可能train個一陣子server就會自動重開</strong>，一開始也不知道為什麼會這樣，不斷的透過排除法來尋找問題在哪裡，只差沒拿綠色乖乖放在server上了。後來參考下面這兩篇文章找到了問題：<br><a id="more"></a></p><ul><li><a href="https://stackoverflow.com/questions/39122984/system-auto-reboot-when-tensorflow-model-is-too-large" target="_blank" rel="noopener">system auto reboot when tensorflow model is too large</a></li><li><a href="https://github.com/tensorflow/tensorflow/issues/8858" target="_blank" rel="noopener">Machine restarts when running TensorFlow with GPU</a></li></ul><p>好滴，來畫個重點，<strong>就是我們實驗室用的電供(power)瓦數太低啦！小小的550W電供承擔不起咱們那高貴的RTX2080 Ti</strong>。當初在列設備規格的學長沒有靠慮到這點，所以當超載到一個極限後為了保護設備他就會自動重開機(不同電供有不同的對應措施)。</p><p>根據文章討論的內容，解決方法有兩種，一種是限制GPU的瓦數(文章內有提供程式碼參考)，一種是當高級課長：砸錢換個好電供。</p><p>.</p><p>. .</p><p>我們後來選擇當課長了，所以新的電供來了之後就再也沒重新開機過了呢！有錢真棒！！</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[C/C++]Optimize string use: a case study閱讀筆記</title>
      <link href="/posts/21e694bd/"/>
      <url>/posts/21e694bd/</url>
      
        <content type="html"><![CDATA[<p>文章網址: <a href="https://www.oreilly.com/content/optimize-string-use-case-study/" target="_blank" rel="noopener">Optimize string use: a case study</a><br>Hackmd好讀版: <a href="https://hackmd.io/@sXG2cRDpRbONCsrtz8jfqg/SkQkY4FS8" target="_blank" rel="noopener">[閱讀筆記]Optimize string use: a case study</a></p><p>最近真是挫折，修了C越覺得自己其實不懂C，修了OSDI的課發現越來越不懂OS…不過越是不懂就越要努力把知識補起來!<br>這篇是一篇介紹如何<strong>最佳化string coding(C/C++)的文章的閱讀心得&amp;重點摘要，以及自己的一些深入研究</strong>。</p><p>該篇文章很長，我也還沒看完，不過這裡記錄到了原作者的第一個最佳化case study的重點和心得。以下有著大量的文字敘述以及程式碼，請小心服用，不過看完應該能學到不少。<br><a id="more"></a></p><h2 id="Why-Strings-Are-a-Problem"><a href="#Why-Strings-Are-a-Problem" class="headerlink" title="Why Strings Are a Problem"></a>Why Strings Are a Problem</h2><blockquote><p>Furthermore, the behavior of std::string has been changed over the years to keep up with changes in the C++ standard. This means that a conforming std::string implementation from a C++98 compiler may not behave the same way as a std::string implementation after C++11.</p><p>Strings have some behaviors that make them expensive to use, no matter the implementation. They are dynamically allocated, they behave as values in expressions, and their implementation requires a lot of copying.</p></blockquote><h2 id="Strings-Are-Dynamically-Allocated"><a href="#Strings-Are-Dynamically-Allocated" class="headerlink" title="Strings Are Dynamically Allocated"></a>Strings Are Dynamically Allocated</h2><p>std:string是動態分配memory的，使用到了大量的複製，動態配置的成本極高，在C裡面動態配置的寫法類似下方這樣:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span>* p = (<span class="keyword">char</span>*) <span class="built_in">malloc</span>(<span class="number">7</span>);</span><br><span class="line"><span class="built_in">strcpy</span>(p, <span class="string">"string"</span>);</span><br><span class="line">   ...</span><br><span class="line"><span class="built_in">free</span>(p);</span><br></pre></td></tr></table></figure><br>不過，string的內部mem space(internal character buffer)仍然是固定的，當操作string造成空間超出原本配置的大小時，會重新分配一段空間，再將內容複製過去。</p><blockquote><p>std::string implementations do a trick to amortize the cost of reallocating storage for the character buffer as the string grows. Instead of making a request to the memory manager for the exact number of characters needed, the string implementation rounds up the request to some larger number of characters. For instance, some implementations round up the request to the next power of 2. The string then has the capacity to grow to twice its current size before needing to call the into the memory manager again. The next time an operation needs to extend the length of the string, there is room in the existing buffer, avoiding the need to allocate a new buffer. The benefit of this trick is that the cost of appending characters to a string approaches a constant asymptotically as the string grows longer. The cost of this trick is that strings carry around some unused space. If the string implements a policy of rounding up requests to a power of 2, up to half the storage in a string may be unused.</p></blockquote><ul><li>一種技巧是，每次都為請求的空間配置搭約多兩倍的記憶體(round up the request to the next power of 2)，從而降低了重新配置的次數</li><li>不過缺點就是可能會有一些空間沒使用到</li></ul><h2 id="Strings-Are-Values"><a href="#Strings-Are-Values" class="headerlink" title="Strings Are Values"></a>Strings Are Values</h2><p>strings是values，如同下方integer是values的範例:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i,j;</span><br><span class="line">i = <span class="number">3</span>; <span class="comment">// i has the value 3</span></span><br><span class="line">j = i; <span class="comment">// j also has the value 3</span></span><br><span class="line">i = <span class="number">5</span>; <span class="comment">// i now has the value 5, j still has the value 3</span></span><br></pre></td></tr></table></figure><p>string也有相同的特性</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">string</span> s1, s2;</span><br><span class="line">s1 = “hot”;  <span class="comment">// s1 is "hot"</span></span><br><span class="line">s2 = s1;     <span class="comment">// s2 is "hot"</span></span><br><span class="line">s1[<span class="number">0</span>] = <span class="string">'n'</span>; <span class="comment">// s2 is still "hot", s1 is "not"</span></span><br></pre></td></tr></table></figure><blockquote><p>Because strings are values, the results of string expressions are also values. If you concatenate strings, as in the statement s1 = s2 + s3 + s4;, the result of s2 + s3 is a newly allocated temporary string value. The result of concatenating s4 to this temporary string is another temporary string value. This value replaces the previous value of s1. Then the dynamically allocated storage for the first temporary string and the previous value of s1 are freed. This adds up to a lot of calls into the memory manager.</p></blockquote><ul><li><strong>所以在對string做操作的時候，並不是更改原本memory的value，而是先去產生一個新的string儲存結果，然後在把原本的memory釋放掉。因此string的相關操作會大量的呼叫到memory manager</strong></li></ul><h3 id="Strings-Do-a-Lot-of-Copying"><a href="#Strings-Do-a-Lot-of-Copying" class="headerlink" title="Strings Do a Lot of Copying"></a>Strings Do a Lot of Copying</h3><p><strong>Copy on Write: 讓string都先指向相同的空間，透過counter來統計複製的數量，有需要修改時在另外開一個空間配置</strong></p><blockquote><p>There is a well-known programming idiom for things that behave as values but are expensive to copy. It is called “copy on write,” and often abbreviated COW in C++ literature (see not available). In a COW string, the dynamically allocated storage can be shared between strings. A reference count lets each string know if it is using shared storage. When one string is assigned to another, only a pointer is copied, and the reference count is incremented. Any operation that changes a string’s value first checks to see that there is only one pointer to that storage. If multiple strings point to the storage, any mutating operation (any operation that may change the contents of the string) allocates new storage and makes a copy of the string before making its change</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">COWstring s1, s2;</span><br><span class="line">s1 = <span class="string">"hot"</span>;  <span class="comment">// s1 is "hot"</span></span><br><span class="line">s2 = s1;     <span class="comment">// s2 is "hot" (s1 and s2 point to the same storage)</span></span><br><span class="line">s1[<span class="number">0</span>] = <span class="string">'n'</span>; <span class="comment">// s1 makes a new copy of its storage before </span></span><br><span class="line">             <span class="comment">// changing anything</span></span><br><span class="line">             <span class="comment">// s2 is still "hot", s1 is "not"</span></span><br></pre></td></tr></table></figure><ul><li><strong>儘管這樣在assignment和argument-passing時cost很低，但non-const references和any call to a mutating function的cost就很大</strong></li><li><strong>在平行化時，Cow的cost也很大，需要確保同時間不會有人更改共同的空間</strong></li></ul><blockquote><p>In C++11 and later, the burden of copying is somewhat reduced by the presence of rvalue references and the move semantics (see not available) that go with them. If a function takes an rvalue reference as argument, the string can do an inexpensive pointer copy when the actual argument is an rvalue expression, saving one copy.</p></blockquote><h2 id="Case-Study-First-Attempt-at-Optimizing-Strings"><a href="#Case-Study-First-Attempt-at-Optimizing-Strings" class="headerlink" title="Case Study: First Attempt at Optimizing Strings"></a>Case Study: First Attempt at Optimizing Strings</h2><p>考慮以下code:</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">remove_ctrl</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> result;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;s.length(); ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (s[i] &gt;= <span class="number">0x20</span>)</span><br><span class="line">            result = result + s[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>糟糕透了，<strong>這僅僅只是一段可以執行的危險代碼，因為有著對大量記憶體的更動，每次更改string都會重新產生一個memory去更改，然後再把原本的刪除</strong><ul><li>假設string長度100，則會create storage和release storage各100次</li></ul></li><li>此外，根據assignment的實作不同，可能會有更多的memory call</li></ul><blockquote><ul><li>If strings are implemented using the copy-on-write idiom, then the assignment operator performs an efficient pointer copy and increments the reference count.</li><li>If strings have a non–shared buffer implementation, then the assignment operator must copy the contents of the temporary string. If the implementation is naïve, or result‘s buffer does not have enough capacity, then the assignment operator also allocates a new buffer to copy into. This results in 100 copy operations and as many as 100 additional allocations.</li><li>If the compiler implements C++11-style rvalue references and move semantics, then the fact that the concatenation expression’s result is an rvalue allows the compiler to call result‘s move constructor instead of its copy constructor. The result is that the program performs an efficient pointer copy.</li></ul></blockquote><p>在作者的設備上，each call took 24.8 microseconds，數字沒有意義，僅作為後續優化的比較參考值，接下來作者會介紹如何一步步的優化</p><h2 id="Use-Mutating-String-Operations-to-Eliminate-Temporaries"><a href="#Use-Mutating-String-Operations-to-Eliminate-Temporaries" class="headerlink" title="Use Mutating String Operations to Eliminate Temporaries"></a>Use Mutating String Operations to Eliminate Temporaries</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">remove_ctrl_mutating</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> result;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;s.length(); ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (s[i] &gt;= <span class="number">0x20</span>)</span><br><span class="line">            result += s[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>This improvement comes from eliminating all the calls to allocate temporary string objects to hold the concatenation expression result, and the associated copying and deleting of temporaries. Depending on the string implementation, allocation and copying on assignment are also eliminated.</p></blockquote><ul><li>修改第五行的concatenation expression，省去了暫時儲存計算結果的 temporary object allocate, 存回去的copying和釋放的deleting。</li></ul><p>現在each call是1.72 microseconds per call，進步了13倍</p><h2 id="從source-code理解上述兩種寫法的差異"><a href="#從source-code理解上述兩種寫法的差異" class="headerlink" title="從source code理解上述兩種寫法的差異"></a>從source code理解上述兩種寫法的差異</h2><p>看不懂上面這兩個差在哪?來看source code</p><ol><li>從<a href="https://gcc.gnu.org/onlinedocs/gcc-4.8.2/libstdc++/api/a01053_source.html" target="_blank" rel="noopener">libstdc++</a>可以看到<code>operator+=</code>其實就是呼叫了<code>string::append()</code></li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">926</span><br><span class="line">927</span><br><span class="line">928</span><br><span class="line">929</span><br><span class="line">930</span><br><span class="line">931</span><br><span class="line">932</span><br><span class="line">933</span><br><span class="line">934</span><br><span class="line">935</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">// Modifiers:</span></span><br><span class="line">/\*\*</span><br><span class="line"> \*  @brief  Append a <span class="built_in">string</span> to <span class="keyword">this</span> <span class="built_in">string</span>.</span><br><span class="line"> \*  @param __str  The <span class="built_in">string</span> to append.</span><br><span class="line"> \*  @<span class="keyword">return</span>  Reference to <span class="keyword">this</span> <span class="built_in">string</span>.</span><br><span class="line"> */</span><br><span class="line">basic_string&amp;</span><br><span class="line"><span class="keyword">operator</span>+=(<span class="keyword">const</span> basic_string&amp; __str)</span><br><span class="line">&#123; <span class="keyword">return</span> <span class="keyword">this</span>-&gt;append(__str); &#125;</span><br></pre></td></tr></table></figure><ol><li>來看看<a href="https://android.googlesource.com/platform/external/astl/+/9cb0478662a7c988146fff0d868bba2839ea80f2/src/string.cpp" target="_blank" rel="noopener"><code>string:append()</code></a>都在做啥</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">string::Append</span><span class="params">(<span class="keyword">const</span> value_type *str, size_type n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> size_type total_len = mLength + n;</span><br><span class="line">    <span class="comment">// n &gt; 0 and no overflow for the string length + terminating null.</span></span><br><span class="line">    <span class="keyword">if</span> (n &gt; <span class="number">0</span> &amp;&amp; (total_len + <span class="number">1</span>) &gt; mLength)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (total_len &gt; mCapacity)</span><br><span class="line">        &#123;</span><br><span class="line">            reserve(total_len);</span><br><span class="line">            <span class="keyword">if</span> (total_len &gt; mCapacity)</span><br><span class="line">            &#123;  <span class="comment">// something went wrong in the reserve call.</span></span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">memcpy</span>(mData + mLength, str, n);</span><br><span class="line">        mLength = total_len;</span><br><span class="line">        mData[mLength] = '\\0';</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>原來就是先用<code>reserve()</code>string的capacity變大，然後memcpy過去，這樣就不用產生一個占存的memory space先記錄改變的結果，然後在assign了(這就是<code>operator+</code>在做的事情)</p><ol><li>來看看<a href="https://gcc.gnu.org/onlinedocs/gcc-4.6.2/libstdc++/api/a01075_source.html" target="_blank" rel="noopener"><code>operator+</code></a>在幹啥</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// operator+</span></span><br><span class="line">/\*\*</span><br><span class="line">\*  @brief  Concatenate two strings.</span><br><span class="line">\*  @param __lhs  First <span class="built_in">string</span>.</span><br><span class="line">\*  @param __rhs  Last <span class="built_in">string</span>.</span><br><span class="line">\*  @<span class="keyword">return</span>  New <span class="built_in">string</span> with value of @a __lhs followed by @a __rhs.</span><br><span class="line">*/</span><br><span class="line">   <span class="keyword">template</span>&lt;<span class="keyword">typename</span> _CharT, <span class="keyword">typename</span> _Traits, <span class="keyword">typename</span> _Alloc&gt;</span><br><span class="line">     basic_string&lt;_CharT, _Traits, _Alloc&gt;</span><br><span class="line">     <span class="keyword">operator</span>+(<span class="keyword">const</span> basic_string&lt;_CharT, _Traits, _Alloc&gt;&amp; __lhs,</span><br><span class="line">           <span class="keyword">const</span> basic_string&lt;_CharT, _Traits, _Alloc&gt;&amp; __rhs)</span><br><span class="line">     &#123;</span><br><span class="line">       basic_string&lt;_CharT, _Traits, _Alloc&gt; __str(__lhs);</span><br><span class="line">       __str.append(__rhs);</span><br><span class="line">       <span class="keyword">return</span> __str;</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure><p>注意到他先創造了一個新的variable <code>basic_string</code>，然後初始值放其中一個string，才把另一個變數進行<code>append</code></p><h2 id="Reduce-Reallocation-by-Reserving-Storage"><a href="#Reduce-Reallocation-by-Reserving-Storage" class="headerlink" title="Reduce Reallocation by Reserving Storage"></a>Reduce Reallocation by Reserving Storage</h2><p>上面提到，string空間不足時的re-allocate也是成本，透過reserve()是確認好大小以減少re-allocate的次數。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">remove_ctrl_reserve</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> result;</span><br><span class="line">    result.reserve(s.length());</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;s.length(); ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (s[i] &gt;= <span class="number">0x20</span>)</span><br><span class="line">            result += s[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Not only does use of reserve() eliminate reallocation of the string buffer, but it also improves the cache locality of the data accessed by the function</p></blockquote><ul><li>除了降低重新配置的cost，也增加了cache locality，使得data能夠更快速的存取</li></ul><p>A test using remove_ctrl_reserve() consumes 1.47 microseconds per call, an improvement of 17% over remove_ctrl_mutating().</p><h2 id="Eliminate-Copying-of-String-Arguments"><a href="#Eliminate-Copying-of-String-Arguments" class="headerlink" title="Eliminate Copying of String Arguments"></a>Eliminate Copying of String Arguments</h2><blockquote><p>When a string expression is passed into a function by value, the formal argument (in this case, s) is copy-constructed.</p></blockquote><p>在function傳遞參數時，參數會被copy，根據string的implementation，copy的動作有多種可能的情況:</p><blockquote><ul><li>If strings are implemented using the copy-on-write idiom, then the compiler generates a call to the copy constructor, which performs an efficient pointer copy and increments the reference count.</li><li>If strings have a nonshared buffer implementation, then the copy constructor must allocate a new buffer and copy the contents of the actual argument.</li><li>If the compiler implemented C++11-style rvalue references and move semantics, then if the actual argument is an expression, it will be an rvalue, so the compiler will generate a call to the move constructor, resulting in an efficient pointer copy. If the actual argument is a variable, then the formal argument’s copy constructor is called, resulting in an allocation-and-copy. Rvalue references and move semantics are described in more detail in not available.</li></ul></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">remove_ctrl_ref_args</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> <span class="keyword">const</span>&amp; s)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> result;</span><br><span class="line">    result.reserve(s.length());</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;s.length(); ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (s[i] &gt;= <span class="number">0x20</span>)</span><br><span class="line">            result += s[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>由於s不會被修改，所以不用另外copy一份，透過const reference來節省空間</strong></li></ul><blockquote><p>The result is a surprise. The timing test of remove_ctrl_ref_args() took 1.60 microseconds per call, 8% worse than remove_ctrl_reserve(). …Reference variables are implemented as pointers. So, everywhere s appears in remove_ctrl_ref_args(), the program dereferences a pointer that it did not have to dereference in remove_ctrl_reserve(). I hypothesize that this extra work might be enough to account for the reduced performance.</p></blockquote><ul><li>但在這一步的結果卻沒有優化到code(在Visual Studio上)，作者推測是VS對於copy有額外做一些事情，還有因為reference是透過dereference pointer實作的，在dereference上可能有一些額外的工作增加了時間</li></ul><h2 id="Eliminate-Pointer-Dereference-Using-Iterators"><a href="#Eliminate-Pointer-Dereference-Using-Iterators" class="headerlink" title="Eliminate Pointer Dereference Using Iterators"></a>Eliminate Pointer Dereference Using Iterators</h2><blockquote><p>String iterators are simple pointers into the character buffer. That saves two dereference operations versus the non-iterator code in the loop.</p></blockquote><ul><li>string iterators直接指向string的char buffer，所以不用在額外做兩次的dereference(loop一次合併一次)</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">remove_ctrl_ref_args_it</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span> <span class="keyword">const</span>&amp; s)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> result;</span><br><span class="line">    result.reserve(s.length());</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> it=s.<span class="built_in">begin</span>(),<span class="built_in">end</span>=s.<span class="built_in">end</span>(); it != <span class="built_in">end</span>; ++it) &#123;</span><br><span class="line">        <span class="keyword">if</span> (*it &gt;= <span class="number">0x20</span>)</span><br><span class="line">            result += *it;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>The timing test for remove_ctrl_ref_args_it() produced a satisfying result of 1.04 microseconds per call.</p><p>remove_ctrl_ref_args_it() contains one other optimization of note. The value s.end(), used to control the for loop, is cached on loop initialization. This saves another 2n indirections, where n is the length of the argument string.</p></blockquote><ul><li>另一種版本的優化是，把<code>s.end()</code>位址先記錄下來，就不用每次loop都重找一次</li></ul><h3 id="為什麼是2n"><a href="#為什麼是2n" class="headerlink" title="為什麼是2n?"></a>為什麼是2n?</h3><p>來看看<code>string::end()</code>的<a href="https://code.woboq.org/llvm/libcxx/include/string.html" target="_blank" rel="noopener">source code</a>，可以看到每次呼叫<code>end()</code>，都會取得原始的pointer + size()，這個別需要一次operation(可以在追下去看<code>__get_pointer</code>和<code>size()</code>各別做了什麼事情)，迴圈執行了n次，所以2n可能是這樣來的</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">910</span><br><span class="line">911</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">const_iterator <span class="title">end</span><span class="params">()</span> <span class="keyword">const</span> _NOEXCEPT</span></span><br><span class="line"><span class="function">    </span>&#123;<span class="keyword">return</span> const_iterator(<span class="keyword">this</span>, __get_pointer() + <span class="built_in">size</span>());&#125;</span><br></pre></td></tr></table></figure><h2 id="Eliminate-Copying-of-Returned-String-Values"><a href="#Eliminate-Copying-of-Returned-String-Values" class="headerlink" title="Eliminate Copying of Returned String Values"></a>Eliminate Copying of Returned String Values</h2><p>為了回傳結果，function內必須要create一個變數，最後在將該變數copy給等待接收的變數中。有些compiler會幫忙做優化(though the compiler is permitted to elide (that is, simplify by removing) the copy construction if it can)，不過如果要確保沒有copy，有兩種方法，一種是所有C++版本都適用的，也就是<strong>call function的時候同時reference一個保存結果的string進來(這也是編譯器優化時在做的事情)</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">remove_ctrl_ref_result_it</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="built_in">std</span>::<span class="built_in">string</span>&amp; result,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="built_in">std</span>::<span class="built_in">string</span> <span class="keyword">const</span>&amp; s)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    result.<span class="built_in">clear</span>();</span><br><span class="line">    result.reserve(s.length());</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> it=s.<span class="built_in">begin</span>(),<span class="built_in">end</span>=s.<span class="built_in">end</span>(); it != <span class="built_in">end</span>; ++it) &#123;</span><br><span class="line">        <span class="keyword">if</span> (*it &gt;= <span class="number">0x20</span>)</span><br><span class="line">            result += *it;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Measured performance of remove_ctrl_ref_result_it() is 1.02 microseconds per call, about 2% faster than the previous version.</p></blockquote><p>不過這種用法要注意，比方說下列的情況就會得到不正確的結果(return empty string)</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">foo</span><span class="params">(<span class="string">"this is a string"</span>)</span></span>;</span><br><span class="line">remove_ctrl_ref_result_it(foo, foo);</span><br></pre></td></tr></table></figure><h2 id="Use-Character-Arrays-Instead-of-Strings"><a href="#Use-Character-Arrays-Instead-of-Strings" class="headerlink" title="Use Character Arrays Instead of Strings"></a>Use Character Arrays Instead of Strings</h2><p>要效能?用C++幹嘛，用C阿</p><blockquote><p>To use the C-style string functions, the programmer must choose either to manually allocate and free character buffers, or to use static arrays dimensioned to worst-case sizes</p></blockquote><ul><li><strong>不過C需要手動配置和釋放記憶體，以及以worst-case決定static array size</strong></li></ul><blockquote><p>Declaring a bunch of static arrays is problematic if memory is at all constrained. However, there is usually room to statically declare large temporary buffers in local storage (that is, on the function call stack). These buffers are reclaimed at negligible runtime cost when the function exits. Except in the most constrained embedded environments, it is no problem to declare a worst-case buffer of 1,000 or even 10,000 characters on the stack.</p></blockquote><ul><li><strong>在記憶體有限的狀況下，宣告一堆static array是不好的，不過在function內宣告的memory由於function結束時會釋放出來，除非是真的記憶體非常受限的狀況下(embedded system)，不然宣告個1000~10000的 char空間還是沒問題的</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">remove_ctrl_cstrings</span><span class="params">(<span class="keyword">char</span>* destp, <span class="keyword">char</span> <span class="keyword">const</span>* srcp, <span class="keyword">size_t</span> <span class="built_in">size</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i=<span class="number">0</span>; i&lt;<span class="built_in">size</span>; ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (srcp[i] &gt;= <span class="number">0x20</span>)</span><br><span class="line">            *destp++ = srcp[i];</span><br><span class="line">    &#125;</span><br><span class="line">    *destp = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>remove_ctrl_cstrings() took 0.15 microseconds per call in the timing test. This is 6 times faster than its predecessor, and an astonishing 170 times faster than the original function. One reason for the improvement is the elimination of several function calls, with a corresponding improvement in cache locality.</p></blockquote><ul><li>比上一版快了6倍，比最原始版本快了170倍，我的老天鵝阿!<ul><li>減少了大量的function call和增加了cache locality(char* 是連續的memory space)</li></ul></li></ul><h2 id="Stop-and-Think"><a href="#Stop-and-Think" class="headerlink" title="Stop and Think"></a>Stop and Think</h2><p>儘管過程中一步步的優化了程式碼，但<strong>這些效能可能是犧牲了簡單性和安全性換來的</strong>，比方說</p><ul><li>remove_ctrl_ref_result_it()具有潛在的不正確使用方式</li><li>remove_ctrl_cstrings()需要手動管理char*的記憶體空間，設計上較複雜</li></ul><blockquote><p>C++ offers developers a range of choices between simple and safe code that is slow and radically fast code that must be used carefully. Advocates of other languages may call this a weakness, but for optimization, it is one of the greatest strengths of C++.</p><p>In the end, the team must answer the question, “How much do we need this performance improvement?”</p></blockquote><h2 id="Summary-of-First-Optimization-Attempt"><a href="#Summary-of-First-Optimization-Attempt" class="headerlink" title="Summary of First Optimization Attempt"></a>Summary of First Optimization Attempt</h2><p>作者也比較了debug mode 和 release mode的差異，能看出好的coding對於程式效能真的影響很大</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/w1QD7au.png" alt=""></p><hr><p>你知道花為什麼會笑嗎?</p><p>: 因為它有梗。</p><p>…好啦，雖然不知道有多少人能看到最後這裡，不過恭喜你看完原文的第一部份了！</p><p>第二部分之後閱讀完在和大家分享~</p>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[論文速速讀]ReZero is All You Need: Fast Convergence at Large Depth</title>
      <link href="/posts/98f60d1d/"/>
      <url>/posts/98f60d1d/</url>
      
        <content type="html"><![CDATA[<p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p><h2 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h2><p>論文網址: <a href="https://arxiv.org/pdf/2003.04887v1.pdf?fbclid=IwAR2Zo1-fPYCy-S1LONlQmL3bIwsas8qx05hgLx9fhhw163JNiR2vInPoBC4" target="_blank" rel="noopener">ReZero is All You Need: Fast Convergence at Large Depth</a></p><p>20200310發在arXiv的論文，主要是提出了一種Residual blocks的變形，使得在深層模型的時候也能夠有效地進行back propagation，而盡量降低gradient vanish或gradient exploding的影響，並且能夠加速收斂的速度。簡單，卻很有效。<br><a id="more"></a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Deep networks have enabled significant performance gains across domains, but they often suffer from vanishing/exploding gradients. This is especially true for Transformer architectures where depth beyond 12 layers is difficult to train without large datasets and computational budgets. <strong>In general, we find that inefficient signal propagation impedes learning in deep networks. In Transformers, multi-head self-attention is the main cause of this poor signal propagation. To facilitate deep signal propagation</strong>, we propose ReZero, a simple change to the architecture that initializes an arbitrary layer as the identity map, using a single additional learned parameter per layer. We apply this technique to language modeling and find that we can easily train ReZero-Transformer networks over a hundred layers. When applied to 12 layer Transformers, ReZero converges 56% faster on enwiki8. ReZero applies beyond Transformers to other residual networks, enabling 1,500% faster convergence for deep fully connected networks and 32% faster convergence for a ResNet-56 trained on CIFAR 10.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>有看過ResNet的就知道(想更了解ResNet可以參閱<a href="https://meetonfriday.com/posts/18a141c2/">[DL]淺談CNN在Object Classification上的各種架構</a> 和 <a href="https://meetonfriday.com/posts/fb19d450/">[Python]逐步解釋ResNet34程式碼(Pytorch)</a>，ResNet透過提出了Residual block做了一個identify operation，對於input $x_i$, 一層neuron layer $F(\cdot)$，把上一層的資料直接加到經過layer的output，也就是</p><p>$x_{i+1} = x_i + F(x_i)$</p><p>而ReZero就是residual with zero initialization，對於每一層layer，加上一個可以訓練的參數(trainable variable)$\alpha$，zero initialization的意思就是一開始訓練的時候讓每一層的$\alpha=0$。隨著訓練次數增加，$\alpha$會逐漸調整，公式如下:</p><p>$x_{i+1} = x_i + \alpha_i{F(x_i)}$ 架構圖如下，可以比對一下ResNet就蠻清楚的了。   </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/03/wcel1wg.png" alt="WCeL1wG"> </p><p>論文中提到ReZero的兩個優點:</p><ul><li><strong>Deeper learning</strong>: Signals effectively propagate through deep networks, which allows for learning in otherwise untrainable networks. ReZero successfully trains 10,000 layers of fully-connected networks, and we are the first to train Transformers over 100 layers without learning rate warm-up or LayerNorm. In contrast to [11] we find that to get good results at this depth, it is not necessary to add auxiliary losses.</li><li><strong>Faster convergence</strong>: We observe significantly accelerated convergence in ReZero networks compared to regular residual networks with normalization. When ReZero is applied to Transformers, we converge 56% faster than the vanilla Transformer to reach 1.2 BPB on the enwiki8 language modeling benchmark. When applied to ResNets, we obtain 32% speed up to reach 85% accuracy on CIFAR</li></ul><p>最後，論文中也提到了和其他normalization and residual connections變形的比較，如下圖: </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/03/wq7tyrq.png" alt="wQ7Tyrq"> </p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>看起來是一種新的block架構，有了它就可以在沒有Normalization的情況下達到相同甚至更好的收斂速度和結果，而且也可以插在任意的NN中。不過作者放在paper的code掛掉了，所以暫時看不到作者的source code(?)</p>]]></content>
      
      
      <categories>
          
          <category> 【論文速速讀】 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[OSDI]Qemu Emulator on Raspi3</title>
      <link href="/posts/f4aa4a6a/"/>
      <url>/posts/f4aa4a6a/</url>
      
        <content type="html"><![CDATA[<p>最近在搞如何在Qemu上模擬Raspi 3，搜尋了一下發現在Qemu2.12後的內建支援raspi3，而之前的版本則需要另外設定一些東西(參考<a href="https://github.com/bztsrc/qemu-raspi3" target="_blank" rel="noopener">QEMU Raspberry Pi 3 support</a>)。<br><a id="more"></a><br>我打算用2.12版本的Qemu來免除額外設定的麻煩，但是明明Qemu版本都已經3.x了，當我用apt-get install安裝的Qemu版本卻只有2.11，上網找了很久後發現<strong>原來我的作業系統是Ubuntu 18.04 TLS，綁定的Qemu是2.11版本，如果想要更高的版本就需要自己build或是用別人的PPA</strong>，參照<a href="https://mathiashueber.com/manually-update-qemu-on-ubuntu-18-04/" target="_blank" rel="noopener">How to update QEMU on Ubuntu 18.04</a>的說明，加入別人已經做好的PPA然後upgrade即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:jacob/virtualisation </span><br><span class="line">sudo apt-get update </span><br><span class="line">sudo apt-get upgrade</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> osdi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2020微軟一日實習生(RDI)</title>
      <link href="/posts/3cef1e25/"/>
      <url>/posts/3cef1e25/</url>
      
        <content type="html"><![CDATA[<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/03/87179746_3387713074578253_2576386941968711680_n.jpg" alt="87179746_3387713074578253_2576386941968711680_n"></p><p>因為一些機緣報名了2020的一日微軟實習生，是一個免費體驗的活動，可以更加了解微軟實習生的資訊以及參觀微軟總部。當初是在<a href="https://www.facebook.com/Microsoft.Student.Program/" target="_blank" rel="noopener">粉絲專頁</a>看到相關的報名資訊的，如果有其他資訊好像也都是在這裡公告。<br><a id="more"></a><br>報名前一個禮拜收到了主辦方的來信，由於我是報名RDI(Research Development Intern，研發助理)，所以他們事前寄了一封信要大家先去寫幾題題目(不確定其他類型的實習生是不是也要寫)，寫程式是用他們自己的online judge。題目有三題，不算太難，一題你會hello world就會寫；一題是背包問題的DP；一題是模擬題，可是這題我卡了蠻久…因為寫起來有點小麻煩。不過總之後來三題都寫完了。</p><p>在活動舉辦前幾天就收到了錄取的通知信，主要就是告知當天的活動流程，以及因應武漢肺炎所做的一些防疫措施。這一屆的流程只有大約半天不到，之前上網看到一些資訊原本以為是一整天的體驗工作，不過這次的流程就是實習生經驗分享，然後是組隊coding…最後是可以跟到時候招募實習的公司進行洽談。</p><p>雖然都去微軟體驗實習生活了結果是要寫寫程式競賽有點那個啥…不過想說有機會可以去微軟總部逛逛還蠻不錯的，所以就還是去參加了，微軟的總部就在捷運市政府站旁邊，附近就是一堆百貨公司，在這裡上班的話是不是每天下班都可以去逛街啊(?)。然後總部內部也很漂亮，有著大大的logo。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/03/88360887_3387712931244934_236769632789200896_n.jpg" alt="88360887_3387712931244934_236769632789200896_n"></p><p>據說一日實習生的活動策劃都是由上一屆的實習生舉辦，所以可能每年都不一樣也說不定。當天的流程一開始就是會有上一屆的實習生來分享他們在實習生活裡面都做了什麼事情，然後讓大家問問題。接下來就是開始寫程式，三個人一組，五題寫一個半小時，和程式競賽一樣答對一題就會丟一顆氣球給你，最後前三名會頒發小獎勵這樣。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/03/88205465_3387712821244945_1622279645292396544_n.jpg" alt="88205465_3387712821244945_1622279645292396544_n"></p><p>一個半小時的時間內，我寫了一題AC，然後一個組員把一題的部分分數拿到了，原本看時間快結束了我們只有一題半的分數，連排行榜前三都進不去我就打算東西收一收回家了。結果組員在截止前一分鐘繳交了一題AC，然後我們就超過二三名(都是對兩題)直接變成第一名了。在公布第一名的組別的時候我還愣在那邊想說是哪一組這麼強，原來就是我們啊xD 感謝組員凱瑞。最後拿到了一個小小的獎品，雖然還沒打開不知道是什麼就是了，不過我猜是usb或是之類的。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/03/88301388_3387712684578292_940174143595216896_n.jpg" alt="88301388_3387712684578292_940174143595216896_n">   </p><p>總之，如果是對於微軟實習有興趣的人，還蠻推薦來體驗看看的，除了能和實習生直接對話之外，或許也有機會更加了解應聘的一些小技巧以及面試的流程關卡等資訊。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> intern </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux Kernel慢慢學]Bit fields介紹</title>
      <link href="/posts/5d55ce7a/"/>
      <url>/posts/5d55ce7a/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前在<a href="https://meetonfriday.com/posts/58e72281/">上一篇</a>介紹linux kernel中build_on_zero的時候有使用到了bit fields技巧，然後我很懶的丟了一個超連結就代過了，後來越想越覺得自己對這個概念還沒是很清楚，所以痛定思痛來寫一篇介紹文順便把它搞懂。</p><h2 id="Bit-fields"><a href="#Bit-fields" class="headerlink" title="Bit fields"></a>Bit fields</h2><p>bit fields是什麼? </p><p>簡單來說是可以在struct內以bit為單位來指定變數，考慮我們有一個需要紀錄4個flag的struct，如果都用bool(p.s. 題外話，c99後才在stdbool.h定義了bool這個type，我自首，我以前都沒注意…)來儲存的話則需要4*1byte = 4bytes。</p><p><strong>但是其實每個flag只有true/false，也就是說只要一個bit來紀錄就可以了</strong>，所以我們可以用一個byte的4個bits來記錄這些flag，總共只佔了1byte。</p><p>用法如下，在struct中以冒號來指定bit field </p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">bit_field_name</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">  type member_name : <span class="built_in">width</span>; </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><a id="more"></a><p>注意在<a href="https://en.cppreference.com/w/c/language/bit_field" target="_blank" rel="noopener">document</a>有提到，bit field的type只能是以下這幾種:</p><ul><li>unsigned int (Ex: unsigned int b:3 ，代表b的範圍介於0..7)</li><li>signed int(Ex: unsigned int b:3 ，代表b的範圍介於-4..3)</li><li>int，注意這裡的int是implement-defined，也就是說它是有號無號的取決於compiler</li><li>_Bool</li></ul><p>下面直接舉例子一邊介紹比較相關的概念比較快。</p><h3 id="Example1-使用方式"><a href="#Example1-使用方式" class="headerlink" title="Example1 - 使用方式"></a>Example1 - 使用方式</h3><p>取自<a href="https://www.tutorialspoint.com/cprogramming/c_bit_fields.htm" target="_blank" rel="noopener">C - Bit Fields</a>，當如果有兩個flag要設置的時候，使用bit field和不使用的差別。</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* define simple structure */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">int</span> widthValidated;</span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">int</span> heightValidated;</span><br><span class="line">&#125; status1;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/* define a structure with bit fields */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">int</span> widthValidated : <span class="number">1</span>;</span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">int</span> heightValidated : <span class="number">1</span>;</span><br><span class="line">&#125; status2;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">( )</span> </span>&#123;</span><br><span class="line">   <span class="built_in">printf</span>( <span class="string">"Memory size occupied by status1 : %d\n"</span>, <span class="keyword">sizeof</span>(status1));</span><br><span class="line">   <span class="built_in">printf</span>( <span class="string">"Memory size occupied by status2 : %d\n"</span>, <span class="keyword">sizeof</span>(status2));</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由於status1裡面有兩個unsigned int，所以大小是 2 * 4(unsigned int) = 8；而如果使用bit field則只有占用一個unsigned int，也就是4byte的空間。</p><h3 id="Example2-bit-fields的對齊"><a href="#Example2-bit-fields的對齊" class="headerlink" title="Example2 - bit fields的對齊"></a>Example2 - bit fields的對齊</h3><p><strong>bit fields的對齊可以使用unamed bit field來使得下一個bit field 對齊到下一個 unit 的 boundary，unamed bit field可以指定要pad的數量或是直接指定0對齊下一個unit，並且zero-width bit field 宣告不會使用到任何空間。</strong></p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">S</span> &#123;</span></span><br><span class="line">    <span class="comment">// will usually occupy 8 bytes:</span></span><br><span class="line">    <span class="comment">// 5 bits: value of b1</span></span><br><span class="line">    <span class="comment">// 27 bits: unused</span></span><br><span class="line">    <span class="comment">// 6 bits: value of b2</span></span><br><span class="line">    <span class="comment">// 15 bits: value of b3</span></span><br><span class="line">    <span class="comment">// 11 bits: unused</span></span><br><span class="line">    <span class="keyword">unsigned</span> b1 : <span class="number">5</span>;</span><br><span class="line">    <span class="keyword">unsigned</span> :<span class="number">27</span>; <span class="comment">// start a new unsigned int</span></span><br><span class="line">    <span class="keyword">unsigned</span> b2 : <span class="number">6</span>;</span><br><span class="line">    <span class="keyword">unsigned</span> b3 : <span class="number">15</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%zu\n"</span>, <span class="keyword">sizeof</span>(struct S)); <span class="comment">// usually prints 8</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或是<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">S</span> &#123;</span></span><br><span class="line">    <span class="comment">// will usually occupy 8 bytes:</span></span><br><span class="line">    <span class="comment">// 5 bits: value of b1</span></span><br><span class="line">    <span class="comment">// 27 bits: unused</span></span><br><span class="line">    <span class="comment">// 6 bits: value of b2</span></span><br><span class="line">    <span class="comment">// 15 bits: value of b3</span></span><br><span class="line">    <span class="comment">// 11 bits: unused</span></span><br><span class="line">    <span class="keyword">unsigned</span> b1 : <span class="number">5</span>;</span><br><span class="line">    <span class="keyword">unsigned</span> :<span class="number">0</span>; <span class="comment">// start a new unsigned int</span></span><br><span class="line">    <span class="keyword">unsigned</span> b2 : <span class="number">6</span>;</span><br><span class="line">    <span class="keyword">unsigned</span> b3 : <span class="number">15</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%zu\n"</span>, <span class="keyword">sizeof</span>(struct S)); <span class="comment">// usually prints 8</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="Example3-指定struct值"><a href="#Example3-指定struct值" class="headerlink" title="Example3 - 指定struct值"></a>Example3 - 指定struct值</h3><p>參照 <a href="http://www.yuan-ji.me/C-C-%E4%BD%8D%E5%9F%9F-Bit-fields-%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/" target="_blank" rel="noopener">C/C++ 位域 Bit fields 学习心得</a>，bit fields struct的值除了可以一個一個變數指定外，也可以透過re-mapping的方式來達成。</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span>* p = (<span class="keyword">int</span> *) &amp;b1; <span class="comment">// 将 "位域结构体的地址" 映射至 "整形（int*) 的地址" </span></span><br><span class="line">*p = <span class="number">0</span>; <span class="comment">// 清除 s1，将各成员归零</span></span><br></pre></td></tr></table></figure><p>也可以透過union來指定內容，透過在union內宣告一個和struct一樣大小的variable，透過指定該變數來初始化struct的記憶體空間。</p><h3 id="Example4-指定struct值的注意事項"><a href="#Example4-指定struct值的注意事項" class="headerlink" title="Example4 - 指定struct值的注意事項"></a>Example4 - 指定struct值的注意事項</h3><p>參照<a href="https://stackoverflow.com/questions/13802728/what-is-zero-width-bit-field" target="_blank" rel="noopener">stackoverflow的這篇</a>，考慮以下程式碼:</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">foo</span> &#123;</span></span><br><span class="line">    <span class="keyword">int</span> a : <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">int</span> b : <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">int</span> : <span class="number">0</span>; <span class="comment">/* Force alignment to next boundary */</span></span><br><span class="line">    <span class="keyword">int</span> c : <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">int</span> d : <span class="number">3</span>;</span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0xFFFF</span>;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">foo</span> *<span class="title">f</span> = (<span class="title">struct</span> <span class="title">foo</span> *) &amp;<span class="title">i</span>;</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"a=%d\nb=%d\nc=%d\nd=%d\n"</span>, f-&gt;a, f-&gt;b, f-&gt;c, f-&gt;d);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>在指定值的時候由於alignment，c跟d都已經取到了超出0xFFFF的記憶體空間了，所以可能會取得不正確的值。</strong></p><h3 id="Example5-應用實例"><a href="#Example5-應用實例" class="headerlink" title="Example5 - 應用實例"></a>Example5 - 應用實例</h3><p>參閱<a href="https://stackoverflow.com/questions/4297095/practical-use-of-zero-length-bitfields" target="_blank" rel="noopener">Practical Use of Zero-Length Bitfields</a>這篇的回答，當兩個平台所使用的規格不同的時候，有時為了相容，會進行alignment，這時候就會用到bit fields的技巧。</p><h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>最後，關於bit fields有幾點要注意的:</p><ul><li>一個struct只有zero-width bit field是未定義行為</li><li>無法透過pointer去操作(Because bit fields do not necessarily begin at the beginning of a byte, address of a bit field cannot be taken)</li><li>不能用sizeof取得大小(但可以對整個struct做)</li><li>Endianness影響了bit fields的順序(The order of bit fields within an allocation unit (on some platforms, bit fields are packed left-to-right, on others right-to-left))，這取決於compiler-dependent，參閱<a href="http://mjfrazer.org/mjfrazer/bitfields/" target="_blank" rel="noopener">How Endianness Effects Bitfield Packing</a></li><li>雖然上面規定了使用的type，但其實也有人用char或unsigned char，參閱<a href="https://stackoverflow.com/questions/3971085/how-does-a-bit-field-work-with-character-types" target="_blank" rel="noopener">How does a bit field work with character types?</a></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://en.cppreference.com/w/c/language/bit_field" target="_blank" rel="noopener">document</a></li><li><a href="https://www.tutorialspoint.com/cprogramming/c_bit_fields.htm" target="_blank" rel="noopener">C - Bit Fields</a></li><li><a href="http://www.yuan-ji.me/C-C-%E4%BD%8D%E5%9F%9F-Bit-fields-%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/" target="_blank" rel="noopener">C/C++ 位域 Bit fields 学习心得</a></li><li><a href="http://mjfrazer.org/mjfrazer/bitfields/" target="_blank" rel="noopener">How Endianness Effects Bitfield Packing</a></li><li><a href="https://stackoverflow.com/questions/3971085/how-does-a-bit-field-work-with-character-types" target="_blank" rel="noopener">How does a bit field work with character types?</a></li><li><a href="https://stackoverflow.com/questions/4297095/practical-use-of-zero-length-bitfields" target="_blank" rel="noopener">Practical Use of Zero-Length Bitfields</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> linux kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux Kernel慢慢學]ARRAY_SIZE macro in linux kernel</title>
      <link href="/posts/58e72281/"/>
      <url>/posts/58e72281/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>為什麼c語言使用function在傳參數的時候，如果傳入一個array會無法正確得到array大小呢?</p><p>這其實跟array decay有關，在linux kernel中為了防呆會加上一些code來防止產生這樣的問題(透過在compile time就產生error)，接下來從array size該如何取得，到linux kernel的macro來簡單介紹。<br><a id="more"></a></p><h2 id="array-size-amp-sizeof"><a href="#array-size-amp-sizeof" class="headerlink" title="array size &amp; sizeof()"></a>array size &amp; sizeof()</h2><p>在寫c的時候，如果想知道一個array的大小可以這樣寫</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> arr[<span class="number">3</span>] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%zu\n"</span>, <span class="keyword">sizeof</span>(arr)/<span class="keyword">sizeof</span>((arr)[<span class="number">0</span>]));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>可是當arr不是一個array，而是一個指向該array的pointer時(pointer to array)，就會有問題。由於pointer的大小是4 or 8 bytes(取決於OS是32bit或64bit)，所以除以一個array(arr[0])的話就會得到不一樣的結果，例如下面就會得到2而不是3。</strong> </p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> arr[<span class="number">3</span>] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> *ptr = arr;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%zu\n"</span>, <span class="keyword">sizeof</span>(ptr)/<span class="keyword">sizeof</span>((ptr)[<span class="number">0</span>]));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>這個問題如果平常沒注意很容易發生，比方說傳了一個pointer進去給function當作參數，在呼叫function時，實際上function的會先創造一個指標變數再去紀錄pointer的內容，所以就會因為上面的理由得到不正確的陣列大小。</p><p>在linux kernel內為了有如下的macro(<a href="https://elixir.bootlin.com/linux/v4.19-rc2/source/include/linux/kernel.h#L72" target="_blank" rel="noopener">include/linux/kernel.h</a>)，定義了如何取得array size:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))</span></span><br></pre></td></tr></table></figure><p>可以注意到他還有多一個<code>_must_be_array(arr)</code>，這是用來確保傳入的arr一定是個array而不是pointer to array，<code>__must_be_array</code>的定義如下:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* &amp;a[0] degrades to a pointer: a different type from an array */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __must_be_array(a)    BUILD_BUG_ON_ZERO(__same_type((a), &amp;(a)[0]))</span></span><br></pre></td></tr></table></figure><p>先來看_same_type在幹嘛再回來理解這一句:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __same_type(a, b) __builtin_types_compatible_p(typeof(a), typeof(b))</span></span><br></pre></td></tr></table></figure><p><code>__builtin_types_compatible_p</code>是gcc的extension，有興趣的請再自己往下追，總之它會比較兩個variable type，如果相同回傳1，不同則回傳0。</p><p>所以<code>__must_be_array</code>就是比較a和&amp;(a)[0]的type是不是相同:</p><ul><li><strong>如果a是array，&amp;(a)[0]會被degrade成pointer，_same_type會回傳1</strong></li><li><strong>如果a是pointer，&amp;(a)[0]仍然是一個pointer，_same_type會回傳0</strong></li></ul><p>接下來來看BUILD_BUG_ON_ZERO在幹嘛:<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUILD_BUG_ON_ZERO(e) (sizeof(struct &#123; int:-!!(e); &#125;))</span></span><br></pre></td></tr></table></figure></p><ol><li>對e做兩次not，確保e一定會是0或1</li><li>然後乘上-1，確保e一定會是0或-1</li><li>透過<a href="https://frankchang0125.blogspot.com/2012/10/linux-kernel-buildbugonzero.html?fbclid=IwAR2RdwEIXe4tY2s7ii71r48SPIdVo6_pUI3kLsItNAzPbwl20fnPq2SXZTA" target="_blank" rel="noopener">bit-field</a>的技巧宣告一個包含int的struct，詳細可以參考超連結</li></ol><p>當e不是0的時候，這一個macro就會產生一個包含-1個bit的struct，不過沒有這種東西，所以在compile time就會產生錯誤。</p><p>好了，追了那麼多code，回頭重看一次，在linux kernel的<code>ARRAY_SIZE macro</code>為了預防傳入的變數是pointer而不是真正的array，背後其實花了很多心力來防範這件事，使得如果發生問題就會在編譯時期就失敗，避免了因為誤用而在run-time才發生問題的狀況。</p><p>最近開始慢慢接觸linux code，真的覺得越來越不懂c了呢。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://blog.louie.lu/2018/09/07/til-array-size-in-kernel/?fbclid=IwAR0gBRFAhO2MeV3MEey__EbEVO75qObyu9GETk60s3ylbpLeKlfltqyEeI8" target="_blank" rel="noopener">TIL: ARRAY_SIZE in Linux kernel</a></li><li><a href="https://frankchang0125.blogspot.com/2012/10/linux-kernel-buildbugonzero.html?fbclid=IwAR2RdwEIXe4tY2s7ii71r48SPIdVo6_pUI3kLsItNAzPbwl20fnPq2SXZTA" target="_blank" rel="noopener">Linux Kernel: BUILD_BUG_ON_ZERO() / BUILD_BUG_ON_NULL()</a></li><li><a href="https://frankchang0125.blogspot.com/2012/10/linux-kernel-arraysize.html?fbclid=IwAR27tvOfGasW0RQQJEtc5RHI_CiU52YxK2yeyGPPgxL-BDmsR7qLEG2yBj0" target="_blank" rel="noopener">Linux Kernel: ARRAY_SIZE()</a></li><li><a href="http://wucodingroad.blogspot.com/2017/09/Cplusplus-array-decay.html" target="_blank" rel="noopener">C++ - array decay (C++軟體開發 - 陣列衰變 概念與實例)</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> linux kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux Kernel慢慢學]Flexible Array Member</title>
      <link href="/posts/f995ead2/"/>
      <url>/posts/f995ead2/</url>
      
        <content type="html"><![CDATA[<p>在C中，有一種struct是在最後一個variable加上一個不指定長度的陣列，用法如下:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">student</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">   <span class="keyword">int</span> stud_id; </span><br><span class="line">   <span class="keyword">int</span> name_len; </span><br><span class="line">   <span class="keyword">int</span> struct_size; </span><br><span class="line">   <span class="keyword">char</span> stud_name[]; </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><a id="more"></a><p>沒有大小的陣列稱之flexible array member，在<strong>規格書中他是一個incomplete type，所以會被sizeof忽略，也就是說當宣告一個struct的變數的時候，他的大小不會被考慮進去</strong>。所以這樣子的話就可以根據需求產生不同長度的空間，比方說最後一個stud_name每個名字長度不同，那就可以根據string length去malloc變數後在指派給stud_name。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">student</span> *<span class="title">s</span> = <span class="title">malloc</span>( <span class="title">sizeof</span>(*<span class="title">s</span>) + <span class="title">sizeof</span>(<span class="title">char</span> [<span class="title">strlen</span>(<span class="title">stud_name</span>)])  );</span></span><br></pre></td></tr></table></figure><p>更多的用法可以參閱<a href="https://www.geeksforgeeks.org/flexible-array-members-structure-c/" target="_blank" rel="noopener">Flexible Array Members in a structure in C</a></p><p>此外，Flexible array member也可以用Zero-length array來實現，也就是</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">student</span> </span></span><br><span class="line"><span class="class">&#123;</span> </span><br><span class="line">   <span class="keyword">int</span> stud_id; </span><br><span class="line">   <span class="keyword">int</span> name_len; </span><br><span class="line">   <span class="keyword">int</span> struct_size; </span><br><span class="line">   <span class="keyword">char</span> stud_name[<span class="number">0</span>]; </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>不過zero-length array在C90以前是不被允許的，當時是gcc extension。而在C99後flexible array出來了，由於是C standard，所以使用起來比較不用顧慮相容性的問題，所以建議用後者的寫法。</p><p>不過這樣跟原本的寫法(在struct中宣告一個char*指標，然後把指標指到需要的位置上)有什麼差?</p><ul><li><strong>由於flexible array member是跟struct一起被malloc的，所以記憶體區段會是連續的，在struct常被存取的時候可以增加cache hit的次數</strong>(由於Spatial locality，也就是位置相近的比較常被存取的特性，例如迴圈。而如果用pointer to char array的寫法，記憶體空間並不會是連續的。</li></ul><p>不過這種用法有一些要注意的地方:</p><ul><li>由於產生的記憶體空間是連續的，所以可能會非法存取到其他變數的記憶體空間。如果compiler又不會檢查那就可能會沒有注意到而產生問題。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="http://frankchang0125.blogspot.com/2013/01/c-struct-hack-structure-with-variable.html" target="_blank" rel="noopener">C Struct Hack - Structure with variable length array</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> linux kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>台東走春行程DAY-2 &amp; DAY-3</title>
      <link href="/posts/54fd4849/"/>
      <url>/posts/54fd4849/</url>
      
        <content type="html"><![CDATA[<p>關於三天的行程、以及<strong>DAY1</strong>都去那兒玩請看上一篇文章：<a href="https://meetonfriday.com/posts/b97515db/">台東走春行程DAY-1</a></p><h2 id="DAY-2"><a href="#DAY-2" class="headerlink" title="DAY-2"></a>DAY-2</h2><h3 id="池上車站-–-家鄉池上飯包"><a href="#池上車站-–-家鄉池上飯包" class="headerlink" title="池上車站 – 家鄉池上飯包"></a>池上車站 – 家鄉池上飯包</h3><ul><li>【地址】958台東縣池上鄉中正路4號</li><li>【電話】08 986 3521</li></ul><p>既然都在池上了，怎麼可以不吃池上飯包！</p><p>不過先自首一下，原本我對池上飯包沒有很大的興趣，因為平常在家附近也有池上便當店，然後我吃起來都覺得…好像還好。不過親自來吃了之後覺得真的很好吃，池上飯包的米粒真的很Q彈飽滿，讓我覺得我家附近開的池上便當是不是都在詐欺。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/83427931_3303235713025990_1695452062516510720_n.jpg?w=300" alt="83427931_3303235713025990_1695452062516510720_n"><br><a id="more"></a></p><hr><h3 id="伯朗大道-–-自行車行程"><a href="#伯朗大道-–-自行車行程" class="headerlink" title="伯朗大道 – 自行車行程"></a>伯朗大道 – 自行車行程</h3><ul><li>【地址】958台東縣池上鄉</li></ul><p>吃完之後，我們就去附近的伯朗大道騎腳踏車走走，伯朗大道因為<a href="https://www.youtube.com/watch?v=v0Kn7xAid3k" target="_blank" rel="noopener">金城武桑在這裡拍廣告</a>而聞名，所以那棵金城武樹旁邊就有一堆人等著拍照(不過我們沒有特地去排隊拍那棵)。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/84465130_3303236116359283_1826912060735225856_n.jpg?w=300" alt="84465130_3303236116359283_1826912060735225856_n"></p><p>當天早上在下雨，一度還很擔心這樣下午要怎麼騎自行車，不過下午之後天氣就很好，出大太陽還有藍天，所以拍了很多張照片。</p><p>伯朗大道的路口處有一個很漂亮的相框可以拍照，不過基本上在春節期間是不可能拍到空景的，因為相框和木頭後面有著滿滿的人龍在排隊阿…這張照片是很努力的在某個神奇的角度，然後抓準路人拍完離開相框的那瞬間才捕捉到這張照片的QQ</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/84549228_3303235966359298_7643491574269083648_n.jpg?w=300" alt="84549228_3303235966359298_7643491574269083648_n"> 雖然我對金城武桑不熟，不過這裡天氣好的時候也是一幅很美的風景。 [gallery ids=”3149,3147” type=”rectangular”]  </p><hr><h3 id="王子麵店"><a href="#王子麵店" class="headerlink" title="王子麵店"></a>王子麵店</h3><ul><li>【地址】950台東縣台東市和平街131號</li><li>【電話】08 932 6578</li></ul><p>晚餐的部分我們則是回市區去吃。原本我們打算去吃台東觀光夜市，不過跟民宿老闆聊天的時候老闆不知為啥撿到槍說：”我們的夜市是全台評價倒數第三，可以去看看喔”，聽到都不禁笑了出來，所以就跟老闆問了一家在地人才會去吃的王子麵店然後出發。</p><p>可惜的是晚餐的部份我們沒有拍照，不過這家還算不錯吃，價錢也不會太貴，推個。</p><p>此外我們也跑去買了藍蜻蜓炸雞當消夜…人真的多到很誇張…排了一個小時才買到…</p><h2 id="DAY-3"><a href="#DAY-3" class="headerlink" title="DAY-3"></a>DAY-3</h2><h3 id="台東森林公園-自行車行程"><a href="#台東森林公園-自行車行程" class="headerlink" title="台東森林公園 - 自行車行程"></a>台東森林公園 - 自行車行程</h3><ul><li>【地址】950台東縣台東市華泰路300號</li><li>【電話】08 936 2025</li></ul><p>第三天的行程是去台東森林公園走走，然後騎自行車。(其實一開始沒有打算騎自行車，是到了之後發現這個公園比想像的大，不騎車好像會累死…)</p><p>森林公園有一個很漂亮的琵琶湖，儘管第三天因為寒流天氣沒那麼好，不過沒有下雨還是很感恩。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/84605755_3303278719688356_6415283540686012416_n.jpg?w=300" alt="84605755_3303278719688356_6415283540686012416_n"></p><p>不過最讓我驚豔的是在琵琶湖的附近有一條很美的小徑，有種歐洲莊園的感覺(?)，地上的警示線用了很漂亮的藍綠色，搭配兩側的樹叢整個就很好看，在這裡我們拍照拍了很久xD</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/84563735_3303278653021696_6011567549848748032_n.jpg?w=300" alt="84563735_3303278653021696_6011567549848748032_n"></p><p>最後我們沒有繞完整個森林公園，因為腳踏車只有兩小時的時間，然後我們為了拍照停留的時間太久了，不過還是玩得很開心～</p><h3 id="東鼎牛肉麵"><a href="#東鼎牛肉麵" class="headerlink" title="東鼎牛肉麵"></a>東鼎牛肉麵</h3><ul><li>【地址】950台東縣台東市光明路107號</li><li>【電話】08 935 0321</li></ul><p>原本要去吃卑南豬血湯…可是沒有開，然後老東台米苔目的人又爆炸多，騎完腳踏車的我們實在沒什麼耐心等待下去，就隨便找了一間看起來還不錯的店吃午餐了。進來之後才發現這間牛肉麵店其實再台東也小有名氣，然後牛肉又很鮮嫩，入口即化的口感讓大家吃的津津有味。</p><p>最後我們吃完午餐後，大家休息了一下就準備各自回家。由於我們要去台東車站搭車回高雄，怕我們叫計程車不方便而且價格會更貴，民宿主人也很熱心地收取很便宜的費用幫我們接送的服務，真的很貼心。</p><h2 id="後記"><a href="#後記" class="headerlink" title="後記"></a>後記</h2><p>三天兩夜的台東行很順利地結束了，儘管那幾天氣象預報說有冷氣團下來可能會下雨，東部就是一個很大自然的路線，天氣不好的話可能什麼都玩不起來。不過很幸運的我們沒有被壞天氣打擾的結束了這三天的行程。</p><p>家人們這次也都玩得很開心(終於不用待在膩到爆的高雄想說到底要去哪裡了)，說不定以後的過年還可以去其他地方走走。</p><p>唯一可惜的是原本的行程有一項鹿野高台玩飛行傘，不過後來因為敢玩的人太少 &amp; 天氣不好而取消了，未來有機會再來台東挑戰一次。</p><p>(是說在寫這篇遊記的時候才發現，阿…我的食物照片呢QQ我好像都顧著吃了…所以都沒有幾張美美的食物照可以放上來，以後會多注意不要顧著吃的。)</p><p>最後，如果你也想要在過年期間來台東，以下有一些小提醒！</p><ol><li>如果是要用火車當交通工具，記得確認春節火車訂票時間，然後記得去搶票</li><li>提早訂好住宿才不會露宿街頭</li><li>過年期間不是什麼店都有開，記得打電話確認才不會去了發現沒東西吃</li><li>到台東後還是有輛車趴趴走才會比較方便</li></ol>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> travel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>台東走春行程DAY-1</title>
      <link href="/posts/b97515db/"/>
      <url>/posts/b97515db/</url>
      
        <content type="html"><![CDATA[<p>關於<strong>DAY2, DAY3</strong>的行程請看下一篇文章：<a href="https://meetonfriday.com/posts/54fd4849/">台東走春行程DAY-2 &amp; DAY-3</a></p><p>由於每年過節總是待在高雄，然後姊姊們回來也不知道要去哪裡玩，所以今年早早就決定去台東玩個三天兩夜，從初一(1/25)開始玩到初三(1/27)。大約在12月的時候就開始在做功課排行程，然後開始預約民宿、車票、租車…blabla</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/83600540_3303142236368671_8611001363637207040_n.jpg?w=300" alt="83600540_3303142236368671_8611001363637207040_n"></p><p>一開始聽說過年去台東會很難玩，因為大家都會想趁過年跑出來走走，所以應該會塞車。不過好險那三天其實沒塞多久，而且天氣也都蠻好的所以玩得很開心。</p><p>下面是我們三天兩夜的行程，只會在這裡打一次，第二篇blog就不會再放了：<br><a id="more"></a><br>[DAY - 1]</p><ul><li>太麻里車站 - 櫻木花道平交道</li><li>知本老爺酒店 - 天幕風呂 &amp; 船歌餐廳(晚餐)</li></ul><p>[DAY - 2]</p><ul><li>池上車站 - 家鄉池上飯包</li><li>伯朗大道 - 自行車行程</li><li>王子麵店(晚餐)</li></ul><p>[DAY - 3]</p><ul><li>台東森林公園 - 自行車行程</li><li><p>東鼎牛肉麵(午餐)</p><p>這篇只會紀錄DAY1的行程～因為還有圖片要放怕三天都放完篇幅會太長～</p></li></ul><h2 id="DAY-1"><a href="#DAY-1" class="headerlink" title="DAY-1"></a>DAY-1</h2><h3 id="太麻里車站-櫻木花道平交道"><a href="#太麻里車站-櫻木花道平交道" class="headerlink" title="太麻里車站 - 櫻木花道平交道"></a>太麻里車站 - 櫻木花道平交道</h3><ul><li>【地址】963台東縣太麻里鄉站前路2號</li></ul><p>(春節花東的火車必須要提早去訂，所以要先去查好訂票日期才不會到時候沒有票買)</p><p>我們早上9點多從高雄車站搭自強號，姊姊們則是從台北下來，大約在中午左右到達太麻里車站。這一站最想來的其實是我，身為一個灌籃高手迷，一定知道OP的櫻木花道和平交道對面的晴子揮手那一幕…如果你不知道的話給我回去重看一遍<a href="https://www.youtube.com/watch?v=dcLi2akfPFw" target="_blank" rel="noopener">灌籃高手主題曲</a>(0:55的地方)！</p><p>太麻里車站其實蠻好看的，不過也就只有好看…車站空空的什麼都沒有，所以我們午餐是在火車上買好了火車便當然後在車站吃完後再出發。</p><div class="three-imgs">  <div class="vertical-to-square"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/2020-01-25-02.42.19-1.jpg?w=1000&h=&crop=1" /></div>  <div class="vertical-to-square"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/2020-01-25-01.58.29-1.jpg?w=1000&h=&crop=1" /></div></div><p>一走出車站就是日昇路，這條路筆直的大道搭配背景的天空，是個蠻漂亮的景色。</p><p>不過因為知本車站也沒辦法寄行李…我們就拖著所有家當前往櫻木花道平交道QQ(從車站出發還要走一下下)，順便拍張照裝個文青。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/2020-01-25-02.18.59-2.jpg?w=300" alt="Processed with VSCO with s1 preset"></p><p>當天天氣雖然沒有下雨，不過也沒有很美的藍天有點可惜。然後春節期間人很多，如果想拍空景的話還是平日來吧～</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/2020-01-25-01.41.56-1.jpg?w=300" alt="Processed with VSCO with s1 preset"></p><hr><h3 id="知本老爺酒店-天幕風呂-amp-船歌餐廳-晚餐"><a href="#知本老爺酒店-天幕風呂-amp-船歌餐廳-晚餐" class="headerlink" title="知本老爺酒店 - 天幕風呂 &amp; 船歌餐廳(晚餐)"></a>知本老爺酒店 - 天幕風呂 &amp; 船歌餐廳(晚餐)</h3><ul><li>【地址】954台東縣卑南鄉龍泉路113巷23號</li><li>【電話】08 951 0666</li></ul><p>去完平交道後我們下一站是去知本老爺酒店泡湯，要注意網路上的泡湯卷春節期間不適用，所以要現場買。我們搭火車到知本車站，然後透過飯店的接駁車接送，蠻便宜的一個人130而已。(很糗的是官網說要前一天預約接駁車，但我們忘了預約了…當天才很不好意思的再打電話去問能不能當日預約。)</p><p>老爺酒店的溫泉是碳酸氫鈉泉，所以看起來清澈透明也不會有硫磺味，不過泡完皮膚還是會咕嚕咕嚕的。除了溫泉外，也有露天游泳池可以游泳。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/83836248_3303142086368686_6698873986358444032_n.jpg?w=300" alt="83836248_3303142086368686_6698873986358444032_n"></p><p>晚餐則是直接在飯店裡吃船歌餐廳的美食，春節期間要先打電話去預約。</p><p>船歌餐廳是一主餐(牛/豬 + 龍蝦)加上buffet，主餐的部分非常美味，buffet也都還不錯吃，整體來說可以吃得非常飽。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/2020-01-25-05.45.26-3.jpg?w=300" alt="Processed with VSCO with f2 preset"></p><hr><h3 id="羅密歐檜木民宿"><a href="#羅密歐檜木民宿" class="headerlink" title="羅密歐檜木民宿"></a>羅密歐檜木民宿</h3><ul><li>【地址】950台東縣台東市中正路251巷1號</li><li>【電話】08 934 5359</li></ul><p>之後則是搭乘老爺飯店的接駁車到市區去我們訂的民宿，這邊幫忙工商一下(?)，因為覺得住的這間品質很好，裡面的家具都是檜木製的，所以可以聞到檜木的香味。房間也都非常大，老闆人也很好，也有附早餐(可以選擇要吃什麼)。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/2020-01-25-09.11.06-2.jpg?w=300" alt="Processed with VSCO with a1 preset"> </p><p>我們住的雙人房型長這樣： </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/02/2020-01-27-10.30.15-1.jpg?w=1024" alt="Processed with VSCO with a1 preset">   </p><p>然後我們的DAY1行程就結束了～！ DAY2, DAY3的行程看這裡~GOGO -&gt;<a href="https://meetonfriday.com/posts/54fd4849/">[20200125-20200127]台東走春行程DAY-2 &amp; DAY-3</a></p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> travel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Pytorch]當DataParallel碰上RNN的那些坑</title>
      <link href="/posts/d9cbeda0/"/>
      <url>/posts/d9cbeda0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在使用Pytorch寫RNN相關的模型，然後因為實驗室有兩張GPU可以用，所以我就想把model放到兩張GPU平行處理，參考<a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" target="_blank" rel="noopener">Pytorch document</a>就知道其實用法很簡單，只要使<code>nn.DataParallel(model)</code>就可以了，不過在使用RNN相關模型的時候有一些問題要注意，既然最近遇到了就順便記錄一下，以免以後又遇到重複的問題。<br><a id="more"></a></p><h2 id="坑-1"><a href="#坑-1" class="headerlink" title="坑-1"></a>坑-1</h2><p>當使用RNN相關的model搭配<code>nn.DataParallel()</code>時會出現下列warning:</p><blockquote><p>RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().</p></blockquote><p>參照<a href="https://discuss.pytorch.org/t/why-do-we-need-flatten-parameters-when-using-rnn-with-dataparallel/46506/2" target="_blank" rel="noopener">網路上</a>的說法，由於<strong>將model的參數放到gpu上的時候不保證放置的memory位置一定是連續的，所以可能會有fragmentation現象，這會造成效能的降低，透過<code>flatten_parameters()</code>可以使得model的參數在gpu memory上的位置是連續的</strong>。</p><p>而這樣的情況也同樣地出現在使用多張gpu訓練，也就是呼叫<code>nn.DataParallel()</code>的情況下，由於<code>nn.DataParallel()</code>做的事情其實就是把model放到多張gpu上一起訓練，所以單張gpu上面會遇到fragmentation的問題在多張上也會遇到，所以同樣的需要<code>flatten_parameters()</code>。</p><p>好，大概知道warning產生的原因後，那<code>flatten_parameters()</code>到底要怎麼用、加在哪裡呢？</p><p>建議是<strong>加在model的<code>forward()</code>的第一行，如此一來當model被放在多張gpu上訓練時，因為每個gpu上的model都會呼叫<code>forward()</code>，所以也就都會呼叫到<code>flatten_parameters()</code>這個function</strong>。</p><p>用法大概如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, output_size)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.rnn = nn.RNN(input_size, output_size)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        self.rnn.flatten_parameters()</span><br><span class="line"> </span><br><span class="line">        ....</span><br></pre></td></tr></table></figure><h2 id="坑-2"><a href="#坑-2" class="headerlink" title="坑-2"></a>坑-2</h2><p>接下來要講的坑是當使用RNN系列搭配<code>pad_packed_sequence()</code>時會遇到的問題，這邊沒有打算要介紹<code>pack_padded_sequence()</code>和<code>pad_packed_sequence()</code>，以後有時間再說(隨然通常這樣說最後都不會寫)(20200720更新: 哼!我有寫了! 看這: <a href="https://meetonfriday.com/posts/4d6a906a">[Pytorch]Pack the data to train variable length sequences</a>)，不過如果你有在用這兩個function搭配RNN和DataParallel那就可能會遇到下面的Error：</p><blockquote><p>RuntimeError: Gather got an input of invalid size: (blalblabla..反正就是size對不上)</p></blockquote><p>一開始百思不得其解，不斷檢查自己的model的shape發現都沒問題，沒使用nn.DataParallel()時單獨在一張gpu上跑也沒問題，去google一開始也不知道下什麼關鍵字，不過後來查了一陣子終於找到問題：原來是<code>pad_packed_sequence()</code>的坑。</p><p>這個問題主要是因為<strong>將data放到不同的gpu上跑時，由於使用了<code>pack_padded_sequence()和pad_packed_sequence()</code>，每個batch的長度都是不固定的，在每張gpu上執行<code>pad_packed_sequence()</code>時，會取它當下batch的最大長度來對其他句子進行padding，這時因為每個gpu上data不同導致當下的最大長度都會不同，在gather的時候就會產生維度不匹配的問題</strong>。</p><p>所以解決方法是，<strong>在使用<code>pad_packed_sequence()</code>時要額外帶入一個參數告訴當下最長的長度是多少</strong>，大概像這樣寫：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># ... __init__, other methods, etc.</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># padded_input is of shape [B x T x *] (batch_first mode) and contains</span></span><br><span class="line">    <span class="comment"># the sequences sorted by lengths</span></span><br><span class="line">    <span class="comment">#   B is the batch size</span></span><br><span class="line">    <span class="comment">#   T is max sequence length</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, padded_input, input_lengths)</span>:</span></span><br><span class="line">        total_length = padded_input.size(<span class="number">1</span>)  <span class="comment"># get the max sequence length</span></span><br><span class="line">        packed_input = pack_padded_sequence(padded_input, input_lengths,</span><br><span class="line">                                            batch_first=<span class="literal">True</span>)</span><br><span class="line">        packed_output, _ = self.my_lstm(packed_input)</span><br><span class="line">        output, _ = pad_packed_sequence(packed_output, batch_first=<span class="literal">True</span>,</span><br><span class="line">                                        total_length=total_length)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"> </span><br><span class="line">m = MyModule().cuda()</span><br><span class="line">dp_m = nn.DataParallel(m)</span><br></pre></td></tr></table></figure><p>咦，你說這code怎麼有點眼熟？其實這是pytorch document上的，不過遇到問題當下不知道用什麼keyword去查因此費了不少力氣，現在把它記錄下來以後才不會重道覆轍。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" target="_blank" rel="noopener">Pytorch document</a></li><li><a href="https://discuss.pytorch.org/t/why-do-we-need-flatten-parameters-when-using-rnn-with-dataparallel/46506" target="_blank" rel="noopener">Why do we need “flatten_parameters” when using RNN with DataParallel</a></li><li><a href="https://blog.csdn.net/ccbrid/article/details/95754946" target="_blank" rel="noopener">RuntimeError】Gather got an input of invalid size【DataParallel问题】</a></li><li><a href="https://pytorch.org/docs/stable/notes/faq.html#my-recurrent-network-doesn-t-work-with-data-parallelism" target="_blank" rel="noopener">My recurrent network doesn’t work with data parallelism</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> machine learning </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Python]json.load() v.s. json.loads()</title>
      <link href="/posts/1e9914bf/"/>
      <url>/posts/1e9914bf/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在用別人的open source時因為json檔案讀取的問題卡一段時間，去trace code下去發現原來是別人的專案裡面有用到json.loads()，它的用處跟我平常在用的json.load()不太一樣，所以在這裡紀錄一下。<br><a id="more"></a></p><h2 id="問題描述"><a href="#問題描述" class="headerlink" title="問題描述"></a>問題描述</h2><p>首先，json.load()是用來讀取整個檔案的，也就是說通常會先用open()去打開json檔案，然後把檔案讀進來，大概像下面這樣：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(fileneme, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">     result=json.load(f)</span><br></pre></td></tr></table></figure><p>不過如果是用json.loads()的時候，他會搭配檔案一次讀一行的動作，把該行字串當成json讀進來，用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        result=json.loads(line)</span><br></pre></td></tr></table></figure><p>不過<strong>使用json.loads()的時候檔案格式就必須一行是一個json structure</strong>，如果妳的json file長得像這樣就會讀取失敗：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;dataset&quot;:&#123;</span><br><span class="line">    &quot;train&quot;: &#123;&quot;type&quot;: &quot;mnist&quot;, &quot;data_set&quot;: &quot;train&quot;, &quot;layout_x&quot;: &quot;tensor&quot;&#125;,</span><br><span class="line">    &quot;test&quot;: &#123;&quot;type&quot;: &quot;mnist&quot;, &quot;data_set&quot;: &quot;test&quot;, &quot;layout_x&quot;: &quot;tensor&quot;&#125;</span><br><span class="line">&#125;,</span><br><span class="line">&quot;train&quot;:&#123;</span><br><span class="line">    &quot;keep_model_in_mem&quot;:0,</span><br><span class="line">    &quot;random_state&quot;:0,</span><br><span class="line">    &quot;data_cache&quot;:&#123;</span><br><span class="line">        &quot;cache_in_disk&quot;:&#123;</span><br><span class="line">            &quot;default&quot;:1</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;keep_in_mem&quot;:&#123;</span><br><span class="line">            &quot;default&quot;:0</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;cache_dir&quot;:&quot;&#x2F;mnt&#x2F;raid&#x2F;fengji&#x2F;gcforest&#x2F;mnist&#x2F;fg-tree500-depth100-3folds&#x2F;datas&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>我原本的json檔案就如上面這個格式一樣，不過在使用別人的tool時發現有問題，trace code下去才發現他是使用json.loads()去一行一行處理，與其去改別人的code倒不如把檔案格式改成符合要求的比較快速，在上網查詢後找到了解決方法如下：<strong>將上述的格式轉成只有一行的json structure即可</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(filename, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    cleaned = <span class="string">''</span>.join([item.strip() <span class="keyword">if</span> item.strip() <span class="keyword">is</span> <span class="keyword">not</span> <span class="string">''</span> <span class="keyword">else</span> <span class="string">'-split_here-'</span> <span class="keyword">for</span> item <span class="keyword">in</span> f.readlines()]).split(<span class="string">'-split_here-'</span>)</span><br></pre></td></tr></table></figure><p>完畢，就john！  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://stackoverflow.com/questions/54446387/parse-json-structures-in-a-txt-file-containing-json-and-text-structures?fbclid=IwAR2Atr0l2DfNGOoVCHeApvoQnOUBOJ5Rl92rnjqGRpeycufpsc7KSFsyt7U" target="_blank" rel="noopener">Parse JSON structures in a txt file containing JSON and text structures</a></li><li><a href="https://blog.csdn.net/xiongchengluo1129/article/details/78779418" target="_blank" rel="noopener">Pythonh中用json.load() json.loads()加载json数据的方法</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019 SecBuzzer AI UP!人工智慧資安挑戰賽參賽心得</title>
      <link href="/posts/33e680bb/"/>
      <url>/posts/33e680bb/</url>
      
        <content type="html"><![CDATA[<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/e693b7e58f961.png" alt="擷取1.PNG"></p><p>在去年11月底的時候，因緣際會下有位學長通知我去參加這個由資策會資安所舉辦的<a href="https://competition.secbuzzer.co/competition/1" target="_blank" rel="noopener">惡意程式分類競賽</a>，這是一個為期一個月的競賽，以競賽時程來說算是相對小品的競賽。以下擷取活動網站的競賽內容：</p><blockquote><p>本次競賽中的每一筆惡意程式 (malware) 均有一組可獨立辨識的 md5 編碼，競賽者須透過人工智慧訓練、推論技術，對每一筆惡意程式進行類別 (class) 可能性的歸屬判定。</p></blockquote><p>總之就是要透過ML/DL的方式來進行惡意程式的分類，期間只有一個月，而<strong>資料的大小又蠻大的(train + test ~ 60G)</strong>所以其實沒有什麼太多時間做無謂的嘗試，整個時程大約被我分成三個階段，就我自己來說，我覺得所有資料科學競賽的規劃其實都是大同小異的：</p><ol><li><strong>資料前處理，做一些基本的特徵萃取和建立base line</strong></li><li><strong>基於baseline開始嘗試一些額外的不同方式</strong></li><li><strong>大約最後一個禮拜，開始就現有最好的模型版本進行進一步的調參</strong></li></ol><p>下面會簡述整個比賽的規劃、心路歷程以及一些分享：<br><a id="more"></a></p><h2 id="建立base-line"><a href="#建立base-line" class="headerlink" title="建立base line"></a>建立base line</h2><p><strong>一開始建立base line是很重要的一件事，這可以讓你的其他模型有了比較的依據</strong>，如何建立base line?有很多方法，最簡單的就是亂猜：</p><ul><li>全部都猜某一類別</li><li>按照類別數量猜機率</li></ul><p>如果你的模型分數比上述亂猜得還爛，那就知道一定哪裡有問題，可以立即轉換其他策略。其他建立baseline的方法如透過簡單的ML model(SVM, Regression, RandomForest…)快速得到一個結果都是不錯的方式。</p><h2 id="特徵萃取"><a href="#特徵萃取" class="headerlink" title="特徵萃取"></a>特徵萃取</h2><p>接下來就是資料前處理，可以上網去看有沒有類似的資訊可以做為萃取特徵的參考。其實找一下就可以發現這次競賽和2016年的微軟惡意程式競賽類型非常相似。於是在此次的競賽我使用了基本的language model - N-gram來做特徵萃取，在資料量龐大N又很大的情況下這一步驟其實花了我不少的時間，不過其實有還不錯的結果。最後使用了N = 1, 3, 4, 7, 10的N-gram模型作為特徵。此外，檔案的壓縮率和檔案大小也有被我納入作為特徵。</p><h2 id="進一步的嘗試"><a href="#進一步的嘗試" class="headerlink" title="進一步的嘗試"></a>進一步的嘗試</h2><p>透過了基本的特徵取得了一定程度的結果後，中間大約一個禮拜開始嘗試一些比較不同的方式看看是否能取得出乎意料的結果，像是：DL-based的CNN/RNN、其他的feature…不過這個部分最後並沒有提升分數，所以沒有被採用。儘管如此，<strong>就競賽的規劃來說這個階段還是必要的，有時候可以發現意想不到的特徵而一舉贏過其他參賽者</strong>。</p><h2 id="模型調參"><a href="#模型調參" class="headerlink" title="模型調參"></a>模型調參</h2><p>在競賽結束大約剩下一個禮拜的時刻，算算時間差不多該停手了，<strong>這時候要做的不應該事繼續探索未知的新特徵，而是在自己的模型下如何讓分數變得更好，有參加過競賽的都知道，分數差個0.01都會影響排名，這時候模型參數的好壞就顯得很重要</strong>。最後一個禮拜都花grid search在調整xgboost和lightgbm的參數(其實時間來說還非常不夠用，並沒有將模型調到最好)，並將兩個模型的結果進行ensemble。</p><h2 id="進階處理"><a href="#進階處理" class="headerlink" title="進階處理"></a>進階處理</h2><p>這我把它獨立出來說，因為我覺得這個部分其實蠻tricky的，根據不同的競賽、資料集會有不同的應對策略。 比方說這次競賽是以<strong>logloss作為評分依據(logloss就是cross entropy)，所以這裡就要知道你用accuracy來做的話會很慘</strong>(自己就中槍過一次QQ)。 此外log有個特性就是你猜到狠準的時候(越接近1)其實分數就不會影響太多；<strong>相對來說猜很不準的時候(很接近0)分數的變動幅度會很可怕，所以就可以想辦法去做probability clip</strong>來避免這個問題。 再來，<strong>資料有imbalanced的問題時，一定程度會影響模型的預測結果</strong>，所以這時候就有兩種方式可以來應對：over-sampling 和 down-sampling，不同方式適用不同的場景。 接下來，模型預測的結果可以<strong>透過confusion matrix來看到底分類得狀況如何，如果有某幾類怎麼分都分不好，也可以試著單獨針對那幾類做一些特別的處理</strong>。  </p><hr><p>整個一個月的時間雖然看似很長，但其實非常的短，每個步驟都花了相對大的時間和精力，訓練模型訓練到懷疑人生、調整參數調到懷疑人生、每天12點還要上去看排名看到懷疑人生…</p><p>最後，我的public score /private score都在第三名的排名，也有這個榮幸在頒獎典禮上和其他參賽者進行交流和經驗分享，感謝資策會資安所舉辦了這個活動，讓我們可以對資安有更進一步的認識外，也可以認識台灣其他的Data scientist。下面就放個幾張頒獎典禮上的照片當作這篇文章的結尾～</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/82117127_2482054501905859_1557123917670252544_o.jpg" alt="82117127_2482054501905859_1557123917670252544_o.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> competition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LINE TAIWAN TECHPULSE 2019參加心得</title>
      <link href="/posts/224202e2/"/>
      <url>/posts/224202e2/</url>
      
        <content type="html"><![CDATA[<p>這是第一次參加LINE TAIWAN TECHPULSE，會得知這個訊息的原因是由於FB上某個社團友人分享了這則訊息。雖然經驗尚淺，不過以前也製作過一兩個chatbot，也因此一直都有在關注Line的相關技術發展，看到這訊息後就毅然決定報名參加。</p><p><strong>活動的地點舉辦在台北和平籃球館，參加費用是免費，不過報名的時候他們有提到會進行篩選，所以有可能不是人人都有票</strong>，總之後來我有拿到票就是了。<br><a id="more"></a><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/img_20191204_095219.jpg?w=1024" alt="img_20191204_095219"></p><p>從台北和平籃球館進去覺得Line真的很厲害，可以把一個技術發表會搞得很像演唱會一樣，很有氣氛跟格局，一踏進去那瞬間我真的有傻眼到哈哈。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/img_20191204_105036.jpg?w=1024" alt="img_20191204_105036"></p><p>整個活動是一整天的，有一連串的精采議程(詳情可以參考<a href="https://techpulse.line.me/" target="_blank" rel="noopener">活動網站</a>)，不過聽說去年其實只有半天而已，雖然是聽眾，一整天下來其實還是挺累的…</p><p>議程包含的範圍很多很廣，<strong>從LINE整體的布局、到開發者最在意的LINE DEVELOPER API的改動、LINE Clova、Security、Recommender System…到一些基於Line而產生的新創公司介紹</strong>，都可以在大會裡聽到很豐富的內容。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/img_20191204_095934.jpg?w=700&amp;h=" alt=""></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/img_20191204_103344.jpg?w=1000&amp;h=&amp;crop=1" alt=""></p><p>LINE也有一個計畫，專門再協助想要透過LINE建立自己產品的開發者建立自己的新創公司，再當日的議程中，有一個時段是特別讓這些公司來推廣介紹他們各自的產品特色，以及他們使用到了哪一些LINE上面的技術。現場<strong>比較有印象的是記帳雞，因為之前在Dcard上看過，而且記帳雞的UI/UX設計非常可愛，是個會讓人想要乖乖記帳的存在呢</strong>。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/img_20191204_135850.jpg?w=463&amp;h=463&amp;crop=1" alt=""></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/img_20191204_144910.jpg?w=696&amp;h=696&amp;crop=1" alt=""></p><p>除了議程，在場外也有很多區域，分別許多LINE的專家和工作人員可以交流相關的議題和技術。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/img_20191204_132411.jpg?w=346&amp;h=346&amp;crop=1" alt=""></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/img_20191204_132527.jpg?w=346&amp;h=346&amp;crop=1" alt=""></p><p><strong>中場休息的時候除了有buffet可以吃，還提供了啤酒給大家喝</strong>，第一次參加活動居然是提供啤酒，覺得很酷。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/img_20191204_161537.jpg" alt="IMG_20191204_161537.jpg"></p><p>最後的贈品是一個小包包以及Tshirt，對於一個免費參加的活動覺得收穫滿滿，有得吃有得喝還有東西可以拿，唯一可惜的是時程有點偏長，聽到後面其實有點精神疲憊了。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2020/01/1204-11.jpg" alt="1204-11.jpg"></p><p>最後附上當時紀錄的筆記，不過其實細節的部分新聞稿或是相關社群有很多人記得很詳細，歡迎去看看他們的內容唷！</p><ul><li><a href="https://hackmd.io/@sXG2cRDpRbONCsrtz8jfqg/rkukuqNar" target="_blank" rel="noopener">[筆記]LINE TECHPLUSE 2019</a></li><li><a href="https://techpulse.line.me/" target="_blank" rel="noopener">活動網站</a></li><li><a href="https://engineering.linecorp.com/zh-hant/blog/line-taiwan-techpulse-2019-arrangement/" target="_blank" rel="noopener">LINE TAIWAN TECHPULSE 2019 活動安排幕後秘辛</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> conference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[DL]運行Keras時遇到CUDNN_STATUS_INTERNAL_ERROR問題</title>
      <link href="/posts/303c1c64/"/>
      <url>/posts/303c1c64/</url>
      
        <content type="html"><![CDATA[<h2 id="問題描述"><a href="#問題描述" class="headerlink" title="問題描述"></a>問題描述</h2><p>最近在執行Keras程式碼時，遇到了以下的問題：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">UnknownError: 2 root error(s) found.</span><br><span class="line">  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.</span><br><span class="line">     [[&#123; &#123;node conv2d_3&#x2F;convolution&#125;&#125;]]</span><br><span class="line">     [[loss_1&#x2F;mul&#x2F;_201]]</span><br><span class="line">  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.</span><br><span class="line">     [[&#123; &#123;node conv2d_3&#x2F;convolution&#125;&#125;]]</span><br><span class="line">0 successful operations.</span><br><span class="line">0 derived errors ignored.</span><br></pre></td></tr></table></figure><br><a id="more"></a></p><h2 id="環境描述"><a href="#環境描述" class="headerlink" title="環境描述"></a>環境描述</h2><ul><li>當前的tensorflow版本: 1.14.0</li><li>當前的keras版本: 2.2.4</li><li>當前的cuda版本: cuda 10</li></ul><h2 id="解決辦法"><a href="#解決辦法" class="headerlink" title="解決辦法"></a>解決辦法</h2><p>一開始以為是cuda版本或是gpu驅動不相容的問題，不過後來研究了一下找到了solution，不用重裝cuda和驅動就可以解決，解法是<a href="https://github.com/tensorflow/tensorflow/issues/24496#issuecomment-464909727" target="_blank" rel="noopener">參照github的issue</a>加入這段code： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.compat.v1 <span class="keyword">import</span> ConfigProto</span><br><span class="line"><span class="keyword">from</span> tensorflow.compat.v1 <span class="keyword">import</span> InteractiveSession</span><br><span class="line"> </span><br><span class="line">config = ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">session = InteractiveSession(config=config)</span><br></pre></td></tr></table></figure><p>之後就可以正常執行Keras。</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> python </tag>
            
            <tag> keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Pytorch]cuda out of memory when increase loop iteration</title>
      <link href="/posts/95ebc771/"/>
      <url>/posts/95ebc771/</url>
      
        <content type="html"><![CDATA[<p>今天發現了這陣子一直沒有解決的一個bug，蠢到都想挖一個洞把自己埋起來，還是記錄一下以免日後又犯蠢。</p><p>不太清楚要怎麼用英文下這個標題，主要的問題敘述是：在使用pytorch訓練模型的時候，為了紀錄模型對於testing set的效果，每隔幾個epoch我就會將acc / loss append到一個list中，然後我的模型常常會發生一個很神奇的狀況：</p><p><strong>剛開始跑的時候GPU mem還塞得下，但隨著loop次數增加，mem居然也相對的增加了，導致幾個loop後就會發生”cuda out of memory”的問題。</strong></p><a id="more"></a><p>一開始我還以為是模型太大，可是後來想想，不對啊，如果前幾個loop是可以跑完的，那就代表gpu memoey應該是可以塞下整個模型的，後來去檢查我的code和上網搜尋相關資源後才發現我犯了一個很蠢的事情，先給大家看一下我的code大概是怎麼寫的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epoch):</span><br><span class="line">    <span class="comment"># training</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (batch_X, batch_y) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        batch_X = batch_X.to(device)</span><br><span class="line">        batch_y = batch_y.to(device)</span><br><span class="line">        output, _ = model(batch_X)</span><br><span class="line">        loss = criterion(output, batch_y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># testing</span></span><br><span class="line">    <span class="keyword">if</span> e % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        model.eval()</span><br><span class="line">        X_test = X_test.to(device)</span><br><span class="line">        y_test = y_test.to(device)</span><br><span class="line">        output, _ = model(X_test)</span><br><span class="line">        <span class="comment"># get the class of prediction</span></span><br><span class="line">        prediction = torch.max(output, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        loss = criterion(output, y_test)</span><br><span class="line">        acc = torch.sum(prediction == y_test).cpu().numpy() / X_test.shape[<span class="number">0</span>]</span><br><span class="line">        acc_history.append(acc)</span><br><span class="line">        loss_history.append(loss)</span><br><span class="line">        print(<span class="string">'Epoch: &#123;epoch&#125; | loss: &#123;loss&#125; | acc:&#123;acc&#125;'</span>.format(epoch=e, loss=loss, acc=acc))</span><br><span class="line">        torch.save(model.state_dict(), model_weight_filepath)</span><br></pre></td></tr></table></figure><p>問題主要出在22~24行，我會把每次model對testing set的acc和loss記錄下來，可是我<strong>沒有先將這些tensor variable進行detach()，所以這些變數其實還包含了computation graph的資訊，造成每次append到list的時候都會重複佔用memory。</strong></p><p>解決方法就是將不需要backward的variables都detatch()，也就是將22~24行的code改成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">acc = torch.sum(prediction == y_test).detach().cpu().numpy() / X_test.shape[<span class="number">0</span>]</span><br><span class="line">acc_history.append(acc)</span><br><span class="line">loss_history.append(loss.detach())</span><br></pre></td></tr></table></figure><p>如此一來，迴圈越跑越多memory也不會越來越大了！   網路上也有人遇到一樣的問題：可以看看原文<a href="https://discuss.pytorch.org/t/i-run-out-of-memory-after-a-certain-amount-of-batches-when-training-a-resnet18/1911" target="_blank" rel="noopener">I run out of memory after a certain amount of batches when training a resnet18</a></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> python </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Python]Utility function of calculate convolution output shape</title>
      <link href="/posts/a418962a/"/>
      <url>/posts/a418962a/</url>
      
        <content type="html"><![CDATA[<p>心情不好就來發發<del>廢文</del>技術文，然後完蛋了要過年了距離我的目標100篇還差5篇。</p><p>在使用pytorch時比較麻煩的一點是，convolution的shape要計算好，才不會給錯conv的參數，所以這時候<strong>使用者就要對於Convolution到底怎麼運作的(kernel size/padding/stride/dilation)有很清楚的瞭解才行，不然連shape都推不出來是要怎麼給模型參數？</strong></p><a id="more"></a><p>在<a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">pytorch的doc</a>內對於各種Conv(Conv1d, Conv2d)都給了一個公式讓大家可以很方便的推出input shape 和 output shape之間的對應關係，可是我又很懶不想每次都慢慢去看公式來算，所以參考網路上的資源就寫了兩個function，<strong>分別來計算給定input shape和你想要的Conv設定(kernel size/ padding…這些)時，output shape會長怎樣</strong>，順便拿這個來發廢文，廢話不多說，直接發代碼啦。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_2d_output_shape</span><span class="params">(h_w, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, pad=<span class="number">0</span>, dilation=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Utility function for computing output shape of 2d-convolutions</span></span><br><span class="line"><span class="string">    input:</span></span><br><span class="line"><span class="string">        (h,w)(tuple): (height, width) of input</span></span><br><span class="line"><span class="string">        kernel_size: kernel size of kernel</span></span><br><span class="line"><span class="string">        stride: stride of convolution</span></span><br><span class="line"><span class="string">        pad: padding of convolution</span></span><br><span class="line"><span class="string">        dilation: dilation of convolution</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    output:</span></span><br><span class="line"><span class="string">       (h,w)(tuple): (height, width) of output</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> type(kernel_size) <span class="keyword">is</span> <span class="keyword">not</span> tuple:</span><br><span class="line">        kernel_size = (kernel_size, kernel_size)</span><br><span class="line">    h = math.floor( ((h_w[<span class="number">0</span>] + (<span class="number">2</span> * pad) - ( dilation * (kernel_size[<span class="number">0</span>] - <span class="number">1</span>) ) - <span class="number">1</span> )/ stride) + <span class="number">1</span>)</span><br><span class="line">    w = math.floor( ((h_w[<span class="number">1</span>] + (<span class="number">2</span> * pad) - ( dilation * (kernel_size[<span class="number">1</span>] - <span class="number">1</span>) ) - <span class="number">1</span> )/ stride) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> h, w</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_1d_output_shape</span><span class="params">(l, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, pad=<span class="number">0</span>, dilation=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Utility function for computing output shape of 1d-convolutions</span></span><br><span class="line"><span class="string">    input:</span></span><br><span class="line"><span class="string">        l: length of input</span></span><br><span class="line"><span class="string">        kernel_size: kernel size of kernel</span></span><br><span class="line"><span class="string">        stride: stride of convolution</span></span><br><span class="line"><span class="string">        pad: padding of convolution</span></span><br><span class="line"><span class="string">        dilation: dilation of convolution</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    output:</span></span><br><span class="line"><span class="string">        l: lenght of output</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> type(kernel_size) <span class="keyword">is</span> <span class="keyword">not</span> tuple:</span><br><span class="line">        kernel_size = (kernel_size, kernel_size)</span><br><span class="line">    l = math.floor( ((l + (<span class="number">2</span> * pad) - ( dilation * (kernel_size[<span class="number">0</span>] - <span class="number">1</span>) ) - <span class="number">1</span> )/ stride) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> l</span><br></pre></td></tr></table></figure><p>對了，如果還是想了解這個公式在算什麼的，很久以前有寫了一篇<a href="https://meetonfriday.com/posts/4647b68d/">[ML]Calculate Parameter Numbers of MLP &amp; CNN</a>，裡面主要是在手把手的介紹2d的Conv參數怎麼算，是用keras，不過shape的介紹那部分或許也可以參考看看。</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019新北耶誕城</title>
      <link href="/posts/e8dfc58f/"/>
      <url>/posts/e8dfc58f/</url>
      
        <content type="html"><![CDATA[<p>這個假日因為一些因緣際會，就上去台北參加一個課程，結果發現那堂課根本沒有要講我想要的東西，講師也對這領域沒有到非常專精，所以根本沒有什麼收穫，(<del>我覺得我來講說不定都比講師好，主辦方要不要聘我去當講師</del>)。</p><p>不過既然都來台北了，就想去耶誕城晃晃，雖然那邊就是一個聚集了各種光害(各種意義上)的地方，不過今年的藝術裝置很吸引我，想了想還是想去拍幾張照片，於是就一個人出發了。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/12/img_20191130_190823.jpg?w=7296" alt="img_20191130_190823"></p><a id="more"></a><p>礙於時間關係所以沒有每個點都去，不過有去了我最想要看的「森林沐浴」就滿足了。</p><p>照片是拿華為Nova2拍的(原本想換pixel後再來)，然後雖然現場很漂亮，不過不是專業的也沒辦法拍出很漂亮的照片出來xD</p><hr><p>從板橋捷運站還沒出來就已經有滿滿的燈飾點綴著天畫板了。話說前往耶誕城的路上有個小插曲：因為太久沒來了我以為地點在市政府捷運站，結果捷運坐到市政府發現來錯地方了又跑回去坐捷運來板橋。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/12/img_20191130_174547.jpg" alt="img_20191130_174547"></p><p>還沒十二月的人潮就已經這樣了，越接近聖誕節應該會越可怕，是說每年都有的超大型聖誕樹跟大型投影秀其實沒有特別吸引我。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/12/img_20191130_190850.jpg?w=1024" alt="img_20191130_190850"></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/12/img_20191130_182944.jpg" alt="img_20191130_182944"></p><p>旁邊的人行道也都有很漂亮的燈飾，很好拍，只是人很多，想要等到背景沒有人再拍…辦不到。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/12/img_20191130_194845.jpg?w=1024" alt="img_20191130_194845"></p><p>耶誕城就是一個很好拍照的地方，如果是喜歡拍照的人在這裡花一個晚上其實都不是問題。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/12/img_20191130_195929.jpg" alt="img_20191130_195929"></p><p>很漂亮的粉紅星橋，這裡真的人很多…工作人員會疏導說要拍照的請靠邊，儘管如此光是要走過去還是要塞個十分鐘。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/12/img_20191130_195725.jpg" alt="img_20191130_195725"></p><p>從粉紅星橋上拍的華為聖誕樹，都拿華為手機了就幫忙捧場一下，雖然之後我要換手機了。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/12/img_20191130_185259.jpg" alt="img_20191130_185259"></p><p>最後是今年會吸引我來的裝置藝術，就是這個「森林沐浴」，之前在新聞上看到就覺得真的很漂亮，現場看到又有不一樣的感覺，不過相對的會來這個點拍照的人也很多…這張已經是等待了好久終於沒有人在我面前經過才拍到的照片了。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/12/img_20191130_181955.jpg" alt="img_20191130_181955"></p><hr><p>好啦，難得的日常文就到這裡，如果沒被騙上來台北還沒這個機會發這篇文。</p><p>什麼，你說為什麼這篇的字有這麼多繽紛的顏色？</p><p>因為平常看技術文的人類們才不會Care這個，而且每篇都用不同顏色很累的好嗎。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> travel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Python]Use Capstone to disassemble code</title>
      <link href="/posts/7a4bb453/"/>
      <url>/posts/7a4bb453/</url>
      
        <content type="html"><![CDATA[<p>最近因為研究需要，開始學習如何透過Python將一個PE file的程式碼部分給反組譯成組合語言，以下很快地紀錄整個過程，由於對於資安逆向工程還沒有很熟悉，如果有錯誤還請指正。</p><p>PE file，全名又稱The Portable Executable，是一種可以在Windows作業系統下被執行的檔案格式(順帶一提Linux下似乎是ELF)，檔案結構大概長下面這個樣子(<a href="https://nagareshwar.securityxploded.com/2013/10/15/overview-and-internals-of-pe-file/" target="_blank" rel="noopener">圖片來源</a>)：</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/11/pe-architecture.jpg" alt="PE-architecture.jpg"></p><a id="more"></a><p>整個檔案除了header以外，還有許多的sections，而其中程式碼的部分會被存放在.text section裡面。</p><p>如果是使用Python，要取出.text section的話可以使用<a href="https://pypi.org/project/pefile/" target="_blank" rel="noopener">PEfile</a>這個套件，怎麼取出的不是這篇想講的東西，總之成功後你可以從.text section拿到一大堆的bytes code。</p><p>然後這些bytes code可以透過<a href="https://www.capstone-engine.org/lang_python.html" target="_blank" rel="noopener">Capstone</a>來disassemble：</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/11/e89ea2e5b995e5bfabe785a7-2019-11-27-e4b88be58d883.50.15.png?w=600" alt="螢幕快照 2019-11-27 下午3.50.15"></p><p>官方網站的教學真是簡潔易懂，令人痛哭流涕，比方說有一段bytes code要反組譯的話只要：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test1.py</span></span><br><span class="line"><span class="keyword">from</span> capstone <span class="keyword">import</span> *</span><br><span class="line"> </span><br><span class="line">CODE = <span class="string">b"\x55\x48\x8b\x05\xb8\x13\x00\x00"</span></span><br><span class="line"> </span><br><span class="line">md = Cs(CS_ARCH_X86, CS_MODE_64)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> md.disasm(CODE, <span class="number">0x1000</span>):</span><br><span class="line">    print(<span class="string">"0x%x:\t%s\t%s"</span> %(i.address, i.mnemonic, i.op_str))</span><br></pre></td></tr></table></figure><ol><li>給一段bytes string</li><li>定義系統架構和MODE</li><li>給予一個起始位址，開始反組譯</li></ol><p>然後就可以得到如下的結果：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ python test1.py</span><br><span class="line"> </span><br><span class="line">0x1000: push    rbp</span><br><span class="line">0x1001: mov rax, qword ptr [rip + 0x13b8]</span><br></pre></td></tr></table></figure><p>於是當我很開心的這樣做的時候，卻發現有時候會沒有辦法完整的輸出所有的組合語言指令，原來是.text section有時候並不會只放code，而會放一些data等資源(不是很懂為什麼要這樣設計)，而capstone又預設如果讀到不懂的指令就會停下來：</p><blockquote><p>By default, Capstone stops disassembling when it encounters a broken instruction. Most of the time, the reason is that this is data mixed inside the input, and it is understandable that Capstone does not understand this “weird” code.</p></blockquote><p>所以這個時候，棒棒噠capstone提供了<a href="https://www.capstone-engine.org/skipdata.html" target="_blank" rel="noopener">skip mode</a>這個模式，如果看不懂的咱們就跳過唄！用python的話只要加上一行即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">md = Cs(CS_ARCH_X86, CS_MODE_32) </span><br><span class="line"> </span><br><span class="line"><span class="comment"># By default, SKIPDATA mode is OFF. Now let's turn it ON</span></span><br><span class="line">md.skipdata = <span class="literal">True</span></span><br><span class="line"><span class="comment"># From here onwards, Capstone skips data until it finds a legitimate instruction </span></span><br><span class="line"><span class="comment"># ..... </span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Turn off SKIPDATA mode, so we are back to the default mode </span></span><br><span class="line">md.skipdata = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>然後又可以繼續快樂的反組譯了！</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[DL]Attention Mechanism學習筆記</title>
      <link href="/posts/e26b7840/"/>
      <url>/posts/e26b7840/</url>
      
        <content type="html"><![CDATA[<p>這篇是之前自己在學習NLP的attention筆記，整理了網路上不少覺得不錯的資源分享給大家，會從Seq2seq開始，講述attention基本的概念，並如何利用attention融合在seq2seq，最後在講述NLP無人不知無人不曉的”Attention is all you need”這篇的概念(Transfromer)。<br><a id="more"></a></p><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/EN4377S.png" alt=""></p><ul><li>基於encoder-decoder的RNN架構</li><li>透過encoder產生context vector，然後透過decoder將context vector和前一個輸出結合來預測下一個時間點的輸出</li><li>目標函數: <script type="math/tex">P(y_1,...y_{T'}|x_1,...x_T)=\prod_{t=1}^{T'}p(y_t|v,y_1,...y_{t-1})</script></li><li>缺點: 把任意長度的input encode成一個global context vector，<strong>當input長的時候很難囊括所有info</strong></li></ul><h2 id="Seq2Seq-with-attention"><a href="#Seq2Seq-with-attention" class="headerlink" title="Seq2Seq with attention"></a>Seq2Seq with attention</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/esuoAhv.jpg" alt=""></p><p><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></p><ul><li>注意力機制: 透過attention weight產生動態的context vector<ul><li>每一個decoder的hidden state都會有不同的context vector</li></ul></li></ul><h3 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h3><ul><li>輸入: <script type="math/tex">x=(x_1,...x_{T_x})</script></li><li>輸出: <script type="math/tex">y=(y_1,...y_{T_y})</script></li><li>encoder的hidden state <script type="math/tex">h_s=RNN_{enc}(x_t, h_{s-1})</script></li><li>decoder的hidden state <script type="math/tex">h_t=RNN_{dec}(y_t,h_{t-1})</script></li></ul><ol><li><strong>attention weights</strong>: 每次都用當下decoder的hidden state$h_t$去對所有encoder的hidden state進行<strong>score function</strong>，得出$h_t$對每個$h_s$的重要程度<script type="math/tex; mode=display">\alpha_{ts}=\frac{exp(score(h_t, h_s))}{\sum_{s'=1}^{S}exp(score(h_t, h_s'))}</script></li><li><strong>context vector</strong>: 透過attention weight和原本的$h_s$做weight averaging<script type="math/tex; mode=display">c_t=\sum_{s}\alpha_{ts}h_s</script></li><li><strong>attention vector</strong>: 將context vector和decoder的hidden state做concat並做一個nonlinear-transformation<script type="math/tex; mode=display">\alpha'=f(c_t, h_t) = tanh(W_c[c_t;h_t])</script></li></ol><h3 id="討論"><a href="#討論" class="headerlink" title="討論"></a>討論</h3><ul><li>這裏的attention是關注decoder的output對於encoder的input重要程度，不同於Transformer的self-attention是指關注同一個句子中其他位置的token的重要程度(後面會介紹)</li><li>整體的架構仍然是基於RNN的Seq2Seq，RNN很大一個缺點是不能parallelization</li></ul><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a></p><ul><li>RNN的優點是可以引入長時間的資訊，但缺點是很難平行化</li><li>可以透過深層的CNN使得包含相同的資訊量，但缺點是CNN要疊很多層</li><li>self-attention可以包含長期的資訊，並且可以平行化</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/fZOxHUm.png" alt=""></p><p>完整的transformer架構:</p><h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/fJ9wPhc.jpg" alt=""></p><ol><li>對input $x$做embedding<ul><li>$X’ = WX$</li></ul></li><li>將相同的$X$分成三份，每一份乘上不同的矩陣得到$Q,K,V$<ul><li>$X’ \times W_Q = Q$</li><li>$X’ \times W_K = K$</li><li>$X’ \times W_V = V$</li></ul></li><li>透過score和$Q, K$計算attention weights<ul><li>$softmax(\frac{Q \times K^T}{\sqrt{d}})$</li><li>$d$是$Q$和$K$的維度，因為$Q$和$K$會隨著dimension變大使得variance越大，所以做一點scaled</li></ul></li><li>透過attentnion weights和$V$做weighting average<ul><li>$Z = softmax(\frac{Q \times K^T}{\sqrt{d}})\cdot V$</li></ul></li></ol><ul><li>這些矩陣運算可以透過GPU來加速</li></ul><h3 id="multi-head-attention"><a href="#multi-head-attention" class="headerlink" title="multi-head attention"></a>multi-head attention</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/t5Uy3la.jpg" alt=""></p><ul><li>$Q, K, V$可以有多個，使得不同的attention關注不同的地方</li><li>最後會有多個$Z$，可以把他們concat起來，也可以做一個線性轉換進行降維$Z\cdot W_Z=Z’$</li></ul><h3 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h3><ul><li>對於self-attention來說，input的token位置是無關的，因為每個token都會跟前後的所有token做attention</li><li>所以必須加入position information，直觀的想法是想辦法讓被加入位置編碼的 word embedding 在$d_{model}$維度的空間裡頭不只會因為語義相近而靠近，也會因為位置靠近而在該空間裡頭靠近。</li></ul><p>公式為:</p><ul><li>$PE(pos, 2i)=sin(\frac{pos}{10000^{2i/d_{model}}})$</li><li>$PE(pos, 2i+1)=cos(\frac{pos}{10000^{2i/d_{model}}})$</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/dOd7a2l.png" alt=""></p><p>畫出來的圖長這樣:</p><blockquote><p>論文裡頭提到他們之所以這樣設計位置編碼（Positional Encoding, PE）是因為這個函數有個很好的特性：給定任一位置 pos 的位置編碼 PE(pos)，跟它距離 k 個單位的位置 pos + k 的位置編碼 PE(pos + k) 可以表示為 PE(pos) 的一個線性函數（linear function）。</p><p>因此透過在 word embedding 裡加入這樣的資訊，作者們認為可以幫助 Transformer 學會 model 序列中的子詞的相對位置關係。</p></blockquote><h3 id="Implementation-detail"><a href="#Implementation-detail" class="headerlink" title="Implementation detail"></a>Implementation detail</h3><p>實作上，還有下列的一些需要注意的地方:</p><ul><li>position encoding要用add / concat?<ul><li><a href="https://meetonfriday.com/posts/45f8d851/">[DL]Difference between add/concat operation in Neural Network</a></li></ul></li><li>兩個會被用到的mask<ul><li>padding mask使得attention不會注意到padding的地方</li><li>look ahead mask使得decoder在做attention時不會關注到decoder產生的詞(只會往前看)</li></ul></li><li>架構中其實還用到了skip connection的概念</li><li>可以透過attention weights來關注模型注重的部分，並且在decoder的第二個multi-attention的$Q,K$是來自encoder的output，所以這一個block不是self-attention架構，也因此可以透過這一個block來視覺化input和output之間對應的重要性</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.youtube.com/watch?v=ugWDIIOHtPA" target="_blank" rel="noopener">Transformer</a></li><li><a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html" target="_blank" rel="noopener">淺談神經機器翻譯 &amp; 用 Transformer 與 TensorFlow 2 英翻中</a></li><li><a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener">详解Transformer （Attention Is All You Need）</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[DL]Difference between add/concat operation in Neural Network</title>
      <link href="/posts/45f8d851/"/>
      <url>/posts/45f8d851/</url>
      
        <content type="html"><![CDATA[<p>最近在看學習transformer的相關介紹，再transformer中，對於input($x$)的embedding($a$)會再加上一個position encoding($e$)，目的是為了使model可以學習到句子的位置資訊，也就是：<br><a id="more"></a><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/NKPt2Oe.png" alt=""></p><p>而在<a href="">Attention is all you need</a>這篇paper中，使用的是add將這兩個資訊融合再一起，不過是使用add而不是concat感覺是不是會怪怪的？<br><a href="https://youtu.be/ugWDIIOHtPA?t=1816">李宏毅老師的Transformer教學</a>中，從線性代數的角度出發給予了下面的解釋：</p><blockquote><p>對於原本的輸入$X$，會和矩陣$W$做矩陣乘法得到$A$，也就是：</p><script type="math/tex; mode=display">\left[\begin{matrix}W^i\end{matrix}\right]\left[\begin{matrix}X^i\end{matrix}\right]=\left[\begin{matrix}A^i\end{matrix}\right]</script><p>如果加入一個代表position information的one-hot vector$P$，對$X$進行concat後則式子會變成</p><script type="math/tex; mode=display">\left[\begin{matrix}W^i\\W^p\end{matrix}\right]\left[\begin{matrix}X^i\\P^i\end{matrix}\right]=\left[\begin{matrix}W^i\end{matrix}\right]\left[\begin{matrix}X^i\end{matrix}\right]+\left[\begin{matrix}W^p\end{matrix}\right] \left[\begin{matrix}P^i\end{matrix}\right]</script><p>而多出來的那一項$\left[\begin{matrix} W^p \end{matrix}\right] \left[\begin{matrix} P^i \end{matrix}\right]$就是原本圖中的$E$了，所以實際上兩者的運算可以達到一樣的效果。</p></blockquote><h2 id="Aspect-of-Computer-Vision"><a href="#Aspect-of-Computer-Vision" class="headerlink" title="Aspect of Computer Vision"></a>Aspect of Computer Vision</h2><p>看到上面的介紹後，感覺add和concat似乎能達到一樣的效果？下面試著從CV上來探討兩者之間的差異。</p><p>CV的model也最常使用到上述兩者的技巧，經典的例子如<strong>ResNet中是使用add來進行skip connection，而GoogleNet是使用concat來完成inception block</strong>。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>假設對於input $X$具有$X_1, X_2, X_3…X_c$，而kernel是$K$，$*$是convolution operation，則convolution的式子如下：</p><script type="math/tex; mode=display">Z=\sum^c_{i=1}X_i*K_i</script><p>如果將$X$與一個新的feature vector $Y_1, Y_2, Y_3…Y_c$進行concat再做convolution，則式子變成</p><script type="math/tex; mode=display">Z=\sum^c_{i=1}X_i*K_i+ \sum^c_{i=1}Y_i\ast K'_i</script><p>如果將$X$與一個新的feature vector $Y_1, Y_2, Y_3…Y_c$進行add再做convolution，則式子變成</p><script type="math/tex; mode=display">Z=\sum^c_{i=1} (X_i+Y_i)*K_i=\sum^c_{i=1}X_i\ast K_i+\sum^c_{i=1}Y_i\ast K_i</script><h3 id="Observation"><a href="#Observation" class="headerlink" title="Observation"></a>Observation</h3><ol><li>可以發現差異只差在對於concat時，多出來的channel會使用不同的kernel($K’$)來進行運算，也因此使用<strong>concat</strong>會導致parameter size變多</li><li>對於兩個不同的input feature vector，如果對應的input channel具有類似的語意性質(比如說$X_1$和$Y_1$代表的意義類似、$X_2$和$Y_2$代表的意義類似…)，則可以用add來代替concat，可以更節省參數和計算量</li></ol><h2 id="Conculsion"><a href="#Conculsion" class="headerlink" title="Conculsion"></a>Conculsion</h2><p>從訊息量的角度來看的話</p><ul><li><strong>add後的維度不變，原本的資訊意義會被改變，但也因此單一維度包含了更多的資訊量，更容易訓練</strong></li><li><strong>concat後的維度變了，可以保留原始的資訊，也使得模型能有更豐富的表達能力(因為維度增加)，但相對的更不容易訓練</strong></li></ul><p>但沒有說上述哪個到底誰比較好，引用一句網友講的話：</p><blockquote><p>NN牛叉就牛叉在这里，说得清的话，早就用特征工程搞定了，还用什么NN。</p></blockquote><p>如果還有人有任何的看法或觀點，歡迎分享討論。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.youtube.com/watch?v=ugWDIIOHtPA" target="_blank" rel="noopener">Transformer</a></li><li><a href="https://www.zhihu.com/question/306213462" target="_blank" rel="noopener">如何理解神经网络中通过add的方式融合特征？</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>聯發科[實習/預聘]面試分享</title>
      <link href="/posts/46f0456a/"/>
      <url>/posts/46f0456a/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在研究所生涯中，實習和預聘都面試過聯發科，聯發科是我最想去的公司前三名，但面試過程也是所有面試過的公司內最讓我疲憊的(?)。在前些日子，長途的面試終於畫下休止符，所以也打算針對聯發科專門寫一篇文章記錄自己在實習/預聘的申請過程。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/11/72738750_3106018126081084_2147407210467033088_o.jpg" alt="72738750_3106018126081084_2147407210467033088_o"></p><a id="more"></a><p>關於面試經驗和題目其實網路上很多大大們都分享的非常詳細了，所以不打算在很詳細的介紹到底會被問什麼跟考什麼(齁～如果你說你不知道代表你沒有認真做功課)，<strong>這篇主要會著墨在記錄自己的心路歷程和準備時程</strong>，關於時程的部分，網路上這部分的資訊比較少，而且我的時程其實都蠻長的(不論是實習/預聘申請)，在等待的時候其實都很讓人糾結，因為網路上大家好像都很快，所以想跟大家分享等比較久的也不一定沒有機會。</p><h2 id="暑期實習"><a href="#暑期實習" class="headerlink" title="暑期實習"></a>暑期實習</h2><p>為了提早讓自己更加了解業界和學界的落差，也為了累積業界的經歷，所以在碩一升碩二的暑假本來就有規劃去企業進行實習，在三月學校的就業博覽會(<a href="https://meetonfriday.com/posts/a1b74789/">[3/10]交大就職博覽會心得&amp;參加攻略</a>)後就開始投遞各家履歷，並於大約那時候開始針對一些面試可能會被問的領域和技術進行加強，也就是英語、LeetCode、計組、作業系統、網路概論……這些部分，這段時間其實非常崩潰，在一個交作業大學內有著不小的課業壓力下又要花很多時間心力去複習以前的知識(好險以前有認真上課複習起來不算太難)。</p><p>然後面試的心得其實實習完後我有很懶的打了一篇(<a href="https://meetonfriday.com/posts/83ee3dab/">[9/4]暑期實習申請心得</a>)，大致上的心得那篇都有講了，總之就是<strong>考程式的編譯器如果無法編譯的話是可以跟HR或主管反映以保障自己得權益的</strong>。</p><p>(題外話，其實當下面試並不是每場都很順利，有幾個部門也被面試官電到牆壁上過，不過很驚訝的是那個被電到牆壁上的部門居然後來進二面了。)</p><p>接下來是時程的部分，我自己的申請時程大約如下(以下時程有加上亂數種子作保護xD)：</p><ul><li>投履歷 ：D</li><li>一面：D+7</li><li>二面：D+33</li><li>其他部門一面：D+47</li><li>二面：D+55</li><li>HR通知：D+63</li></ul><p>一開始一面蠻多個部門的主管的，後來進二面只有一個部門，不過很可惜的沒有上(二面完過一陣子有打電話跟HR確認結果)。就當我以為沒機會的時候又有其他部門希望找我再去面談，而這次也很榮幸的在二面完一個禮拜後由HR通知得到了offer。</p><h2 id="預聘申請"><a href="#預聘申請" class="headerlink" title="預聘申請"></a>預聘申請</h2><p>預聘在碩二上的9~10月開始開放申請，這次的準備時間相對實習來說很趕，因為我是快十月的時候才知道預聘的申請時間(事前準備不足QQ)，好險三月的時候複習的資料都還有留下來，所以準備起來快很多。</p><p>預聘和實習的申請流程以及會被問的問題其實也差不多，唯一比較值得提的是<strong>當初收到預聘面談邀約的時候，HR又重新幫我安排程式設計的上機考以及英文測驗</strong>，當下一度很崩潰，不過有個認識的學姐說她有參加暑期實習然後預聘就沒有再被重新測驗了，所以我就致電詢問HR，得到的回覆是<strong>一年內如果有做過測驗的話就不用再重新測驗，然後HR就幫我取消了</strong>，感謝學姊提醒：）</p><p>在來時程的部分：</p><ul><li>投履歷 ：D</li><li>一面：D+7</li><li>二面：D+24</li><li>詢問HR：D+30</li><li>二度詢問HR：D+38</li><li>HR通知：D+42</li></ul><p>在二面完後等了將近兩個禮拜多才收到通知，這段時間每天都在擔心是不是沒有上(因為網路上大部分好像二面完兩個禮拜內就會收到通知)，所以前後打了兩通電話去和HR詢問，後來終於收到了HR的offer通知，覺得痛哭流涕，也是想分享並不是等超過兩個禮拜就沒機會了。</p><hr><p>總之，孤獨且漫長的找工作之旅大概告了一個段落，在準備工作這件事上真的很費時費力又讓人心累，把這段經歷分享給有需要的人，也希望大家都能找到自己喜歡的工作。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> interview </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>軟體面試問題(一)</title>
      <link href="/posts/8ce0687/"/>
      <url>/posts/8ce0687/</url>
      
        <content type="html"><![CDATA[<p>最近都在準備一些公司的預聘面試，但老實說準備的還不夠充分，所以常常被面試官電在牆上，然後電完後在被拖出來繼續電。</p><p>身為一個從小看獵人的人，也希望自己像奇犽一樣，被電久了之後也可以學會特殊的能力(?)</p><p>所以還是決定每次被電後去面對問題，把不會或不清楚的地方的搞懂。所以這篇就會分享幾題當初被問到的問題，以及事後做完功課後自己會怎麼去回答。其實有想過把所有被問過的題目整理起來變成一篇分享，不過目前沒有那個時間，所以會先以一篇一篇的形式，每篇紀錄一些內容這樣。</p><h2 id="logical-operator-和-bitwise-operator的差別"><a href="#logical-operator-和-bitwise-operator的差別" class="headerlink" title="logical operator 和 bitwise operator的差別?"></a>logical operator 和 bitwise operator的差別?</h2><p>這題一開始我有簡單的回答出來，當時我的回答是這樣的：</p><ul><li>bitwise operator如|、&amp;會以bit的角度去看兩側的operand，針對每一個bit去做運算</li><li>而logical operator如||、&amp;&amp;則是以boolean的角度去看兩側的operand，在C語言中，只要不是0的值會被視為true，0則會被視為false。基於此概念下去做運算<a id="more"></a></li></ul><p>不過顯然這樣的回答還不夠充分，後來面試官問說除此之外還有什麼差異我就回答不出了，回來想了一下後想針對原本的回答再進一步補充：</p><p>使用bitwise operator和logical operator的差異在於，今天如果有個敘述”if( 4 __ 2) {…}”，考慮使用 &amp;&amp; 和 &amp; 的運算就會得到不一樣的結果</p><ul><li>4 &amp;&amp; 2 的時候由於兩個數都不為0，所以會變成true &amp;&amp; true = true</li><li>4 &amp; 2 的時候會以bit的角度去看，也就是(0100) &amp; (0010) = 0，所以答案會是false</li></ul><p>此外，logical operator還具有另一個特性，也就是<strong>short-circuit evaluation</strong>，在某些情況下他不會去看後面的條件：</p><ul><li>對於&amp;&amp;來說，如果前面的條件是false，則不管後面條件是什麼結果都會是false，此時就不會去看後面的條件了</li><li>同理，對於||來說，如果前面是true則不會去看後面的條件</li></ul><p>這樣會造成什麼結果呢? 考慮以下程式碼：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>( j &gt; <span class="number">0</span>  &amp;&amp; arr[j<span class="number">-1</span>] &gt;= <span class="number">0</span> )&#123; ... &#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>( arr[j<span class="number">-1</span>] &gt;= <span class="number">0</span> &amp;&amp; j &gt; <span class="number">0</span>)&#123; ... &#125;</span><br></pre></td></tr></table></figure><p>後者的程式碼可能會有問題，例如今天當j = 0時，while的第一個判斷就會出現問題了(arr[-1])，而<strong>如果先判斷j&gt;0這個條件的話，因為short-circuit的特性他並不會去判斷第二個條件，此時就不會取到負的index</strong>。</p><hr><h2 id="給予兩個bit-sequence-s1-s2-，請把s1的-1-3個bit-從第0個bit開始計算-指定給s2"><a href="#給予兩個bit-sequence-s1-s2-，請把s1的-1-3個bit-從第0個bit開始計算-指定給s2" class="headerlink" title="給予兩個bit sequence (s1, s2)，請把s1的 1~3個bit (從第0個bit開始計算)指定給s2"></a>給予兩個bit sequence (s1, s2)，請把s1的 1~3個bit (從第0個bit開始計算)指定給s2</h2><p>這題後來回去就想出來了，可是當下腦袋就是打結怎麼想就差一點點..</p><p>假設一下s1 s2會比較好去思考，由於是白板題，所以我覺得一邊寫一邊把思考過程講給對方聽還不錯，假設：s1: 10101, s2: 11000</p><p>下面提供兩個後來想到的解法，但是我沒有實際去測試過，如果有錯誤還請提出~</p><p>[A1]</p><p>要取得s1的1~3bit相對簡單，只要把對應的位置與1做&amp;，其他位置與0做&amp;即可，也就是s1&amp;14(二進位的01110)</p><p>接下來就要把s2的0~3bit變成0，然後加上s1的1~3bit和s2的第0個bit，也就是：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s2 = ((s2 &gt;&gt; <span class="number">4</span>) &lt;&lt; <span class="number">4</span>) + (s1 &amp; <span class="number">14</span>) + (s2 &amp; <span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>((s2 &gt;&gt; 4) &lt;&lt; 4)先把值向右shift 4個bit在向左shift回來，會把那些部分都變成0</li><li>(s1 &amp; 14)是s1的1~3bit</li><li>(s2 &amp; 1)是s2的第0個bit，由於剛剛shift的時候被清空了，所以這裡要加回來</li></ul><p>[A2]</p><p>和[A1]類似的想法，都是先把s2的1~3bit變成0</p><p>1. 先取得s1的1~3bit: s1 &amp; 14</p><p>2. 把s2的1~3bit清空，這裡想到了另一種方式，也就是s2 &amp; (1111…10001) = s2 &amp; ~(000…01110) = s2 &amp; ~14</p><p>3. 這邊用到了XOR的特性，XOR 0會是原本的數值</p><ul><li>(s1 &amp; 14)除了1~3bit以外的值都是0，所以對s2 XOR後1~3bit都會是s2原本的數值</li><li>s2的1~3bit都是0，所以與(s1 &amp; 14)XOR之後的1~3bit都會是s1原本的數值</li></ul><p>所以[A2]的過程總共是:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> mask = s1 &amp; <span class="number">14</span>;</span><br><span class="line"> </span><br><span class="line">s2 = s2 &amp; (~<span class="number">14</span>);</span><br><span class="line"> </span><br><span class="line">s2 = s2 ^ mask;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> interview </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[DL]Deep Learning的Batch Normalization為什麼有用</title>
      <link href="/posts/d34f9238/"/>
      <url>/posts/d34f9238/</url>
      
        <content type="html"><![CDATA[<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><h3 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h3><p>BN在2015年由Google提出(<a href="https://arxiv.org/abs/1502.03167?context=cs" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>)，主要來解決上面描述的問題，幫助模型加速收斂。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/DxbRG5f.jpg" alt=""></p><a id="more"></a><p>先來看一下paper中提到的BN algorithm，再來試著從兩個不同的角度來看待為何要使用BN:</p><p>其實單看algo的話並不難理解，不過為何要這樣做?下面會試著針對幾個角度來探討：</p><ol><li><strong>Internal Covariate Shift</strong></li><li><strong>Gradient Vanishing/Exploding</strong></li><li><strong>Feature Scaling</strong></li></ol><h3 id="從Internal-Covariate-Shift的角度看BN"><a href="#從Internal-Covariate-Shift的角度看BN" class="headerlink" title="從Internal Covariate Shift的角度看BN"></a>從Internal Covariate Shift的角度看BN</h3><h4 id="Internal-Covariate-Shift"><a href="#Internal-Covariate-Shift" class="headerlink" title="Internal Covariate Shift"></a>Internal Covariate Shift</h4><p>在訓練Deep learning model時，每一層的forward propagation都會造成參數分布的變化，進而後面的layer都要重新去適應這個變動，造成<strong>Internal Covariate Shift(ICS)</strong>。</p><ul><li>實際上，BN之所以有用，可能和減少ICS無關，關於細節可參考<a href="./#延伸討論-BN真的跟ICS有關係">延伸討論: BN真的跟ICS有關係?</a>，不過這裡的介紹仍然會先以2015年這篇的觀點來敘述。</li></ul><p>簡單介紹一下什麼是<a href="https://blog.csdn.net/mao_xiao_feng/article/details/54317852" target="_blank" rel="noopener">Covariate Shift</a>：假設$q_1$是testing set中某個樣本點的機率密度，$q_0$是training set中一個樣本點的機率密度。ML/DL的任務就是找到夠小的$loss(\theta)$使得$P(y|x, \theta)$最大。</p><p>當我們在$q_0(x)$上找到一組最好的參數$\theta’$時，是否能夠保證$q_1(x)$上也會是最好呢?</p><ul><li>傳統ML問題會假設訓練集和測試集是$i.i.d$的，不過現實上往往這個假設不成立，這個testing set分布和training set分布不同的現象就稱之Covariate Shift</li><li>而在deep learning task中，由於這個現象會發生在每一層內，所以稱之Internal Covariate Shift(ICS)</li></ul><h4 id="如何解決Internal-Covariate-Shift-ICS"><a href="#如何解決Internal-Covariate-Shift-ICS" class="headerlink" title="如何解決Internal Covariate Shift(ICS)?"></a>如何解決Internal Covariate Shift(ICS)?</h4><p>要減少ICS的問題，就得先理解他產生的原因：<strong>由於weights更新導致模型中每一層的input distribution改變</strong></p><p>既然如此，我們就可以透過固定每一層間的分布來減少ICS的問題。</p><p>傳統上ICS的問題，可以透過<a href="https://blog.csdn.net/hjimce/article/details/50864602" target="_blank" rel="noopener">PCA Whitening</a>(或ZCA weighting)來針對每一層的input進行變換(關於PCA/ZCA whitening的細節這裡就不討論，只是讓大家知道有這種方法)，使得輸入的特徵之間具有相同的mean和variance，並去除特徵之間的相關性，但是PCA whitening有著一些缺點:</p><ul><li>對於DL的task，每個epoch都要對每一層做一次PCA whitening，cost太大</li><li>PCA whitening改變了每一層參數之間的分布，可能改變了layer中數據的表達能力，造成喪失一些資訊</li></ul><p>既然cost太大，那就試著來簡化一下:</p><ul><li>只<strong>單獨對每個feature來進行normalization</strong>，使得每個feature的mean為0，variance為1</li><li><strong>加個線性變換</strong>，讓數據盡可能地恢復本身的表達能力</li><li>深度學習的task通常是以mini-batch為單位，所以做的時候也是<strong>基於mini-batch的基礎上進行計算</strong></li></ul><p>好的，以上三點合起來就是Batch Normalization，對照一下應該不難理解一開始提供的algo。而其中的$\gamma$、$\beta$，就是為了讓分布盡可能地回復本身的表達能力而做的線性變換</p><h4 id="延伸討論-BN真的跟ICS有關係"><a href="#延伸討論-BN真的跟ICS有關係" class="headerlink" title="延伸討論: BN真的跟ICS有關係?"></a>延伸討論: BN真的跟ICS有關係?</h4><p>其實前面講了這麼多，關於BN減少了ICS的看法在2018年的<a href="https://arxiv.org/abs/1805.11604" target="_blank" rel="noopener">How Does Batch Normalization Help Optimization?</a>這篇就被被打臉了，這篇文章的觀點認為<strong>BN之所以有用是因為Normalization而不是減少ICS</strong>。</p><blockquote><p>论文中提到并没有发现BN跟internal covariate shift有半毛钱关系，实验方法是加入了一个“noise batch normalization”的对照实验，在batch normalization中加入噪声，使得z发生协变量偏移，但是这样的noise batch normalization的效果也远优于未经BN的网络，由此反驳了ICS的观点。</p><p>论文中认为BN使得优化问题更加平滑，由此梯度的预测性更强，所以可以允许使用更大学习率，并且加快网络收敛。在使用BN后loss和gradient的Lipschitzness都提升了(也就是更平滑了）。</p></blockquote><h3 id="從Gradient-Vanishing-Exploding的角度看BN"><a href="#從Gradient-Vanishing-Exploding的角度看BN" class="headerlink" title="從Gradient Vanishing/Exploding的角度看BN"></a>從Gradient Vanishing/Exploding的角度看BN</h3><h4 id="Saturating-Non-linearity-amp-Non-Saturating-Nonlinearitiy"><a href="#Saturating-Non-linearity-amp-Non-Saturating-Nonlinearitiy" class="headerlink" title="Saturating Non-linearity &amp; Non-Saturating Nonlinearitiy"></a>Saturating Non-linearity &amp; Non-Saturating Nonlinearitiy</h4><p>根據2012年的一篇paper: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a>將DL模型內常被使用的activation function分成了<strong>非飽和非線性(non-saturating nonlinearities)</strong> 和 <strong>飽和非線性(saturating nonlinearities)</strong> 兩種，定義如下:</p><ul><li>f is non-staurating iff <script type="math/tex">(|lim_{z\rightarrow-\infty}f(z)|=+\infty)\wedge(|lim_{z\rightarrow+\infty}f(z)|=+\infty)</script></li><li>f is staurating iff f is non-staurating</li></ul><p>簡單來說，飽和函數會壓縮input的value，所以DL常用的activation function裡:</p><ul><li>ReLU是非飽和非線性函數</li><li>Sigmoid、Tanh是飽和非線性函數</li></ul><h4 id="Saturating-Non-linearity如何影響Training"><a href="#Saturating-Non-linearity如何影響Training" class="headerlink" title="Saturating Non-linearity如何影響Training?"></a>Saturating Non-linearity如何影響Training?</h4><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/tjJDup5.png" alt=""></p><p>這邊參考<a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/5-04-A-batch-normalization/" target="_blank" rel="noopener">莫煩的Normalization介紹</a>，考慮activation function是tanh的情況</p><p>上圖是activation function的圖，中間那張是數值沒有經過normalization的分布，最下面那張則是數值經過normalization後的分布。此時會有什麼影響?</p><ul><li>如果沒有做normalization，大部分的值經過tanh後就會趨近於+1或-1，也就是進入了所謂的<strong>飽和區域</strong>，造成neuron對於輸入的變化不再敏感</li><li>進入飽和區也就代表Gradient Vanishing(因為沒有斜率)，降低訓練速度</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/quLAZHK.png" alt=""></p><p>如果做了Normalization，就可以大幅地避免值都進入飽和區的狀況，如下圖:</p><ul><li>上兩張是進入tanh前有/沒有Normalization的數據分布；而下兩張是進入tanh後有/沒有Normalization的數據分布，可以看到有Normalization的數據能有更好的分布</li></ul><h4 id="從數學公式來看呢"><a href="#從數學公式來看呢" class="headerlink" title="從數學公式來看呢?"></a>從數學公式來看呢?</h4><p>用數學式子來解釋一下Gradient Vanishing/Exploding，實際上，<strong>在back propagation時，該層的gradient是要乘上該層的weights的</strong>，這會造成什麼問題?<br>假設第l層的forward propagation為:</p><script type="math/tex; mode=display">h_l=w_l^Th_{l-1}</script><p>那麼該層的backward將會是</p><script type="math/tex; mode=display">\frac{\delta{l}}{\delta{h_{l-1}}}=\frac{\delta{l}}{\delta{h_l}}\cdot\frac{\delta{h_l}}{\delta{h_{l-1}}}=\frac{\delta{l}}{\delta{h_{l}}}w_t</script><p>延伸一下，考慮從l層到k層的情況，backward propagation就會是</p><script type="math/tex; mode=display">\frac{\delta{l}}{\delta{h_k}}=\frac{\delta{l}}{\delta{h_{l}}}\prod_{i=k+1}^l{w_i}</script><p>針對$\prod_{i=k+1}^l{w_i}$來討論一下:</p><ul><li>如果$w_i$大部分很小，那整體的值就會變成0(Ex: $0.9^{100}$)</li><li>如果$w_i$大部分很大，那整體的值就會爆炸(Ex: $1.1^{100}$)</li></ul><p>那如果加上了BN呢?</p><script type="math/tex; mode=display">h_l=BN(\alpha w_l^Th_{l-1})</script><p>此時backward就會是</p><script type="math/tex; mode=display">\frac{\delta{h_l}}{\delta{h_{l-1}}}=\frac{1}{\alpha}\cdot\frac{\delta{BN(\alpha w_l^Th_{l-1})}}{\delta{w_l}}</script><p>透過BN，各層的輸出和backward的值都會先做了縮放，減少了Gradient Vanishing/Exploding的狀況</p><h3 id="從Feature-Scaling的角度看BN"><a href="#從Feature-Scaling的角度看BN" class="headerlink" title="從Feature Scaling的角度看BN"></a>從Feature Scaling的角度看BN</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/IwYGCd2.jpg" alt=""></p><p>如果不同的feature之間的值域相差很大時，數值大的feature的參數會對結果的影響更大，導致訓練過程中的結果往往被數值大的feature影響。要解決這個問題就必須在訓練過程中，為不同的方向設置不同的學習率，可是這樣很不方便，此時就會做feature scaling。</p><p>上圖取自<a href="https://www.youtube.com/watch?v=BZh1ltr5Rkg&amp;feature=share" target="_blank" rel="noopener">李宏毅老師的BN介紹</a>，其中w1和w2是兩種不同的feature，如果直接拿來訓練會因為數值本身的range造成loss cruve圖呈現橢圓狀(左圖)。透過BN做feature scaling後，可期望loss curve呈現圓形狀(右圖)，這樣會有助於幫助模型更快收斂。</p><h3 id="延伸討論-BN在實務上的設計"><a href="#延伸討論-BN在實務上的設計" class="headerlink" title="延伸討論: BN在實務上的設計"></a>延伸討論: BN在實務上的設計</h3><p>在實務上，training時會以batch為單位去算平均跟標準差，但在testing時資料是一筆一筆進來的，沒辦法這樣做，所以通常會<strong>採用training時的平均跟變異數</strong>。</p><p>所以，在pytorch中提供了2種mode來區別這件事: <code>train()</code>以及<code>eval()</code></p><ul><li>當有使用BN的模型在training時前面就會加上<code>train()</code></li><li>而在testing時，透過在前面加上<code>eval()</code>告訴模型不要再重新去取平均跟變異數，而是使用training好的值<ul><li>如果沒有加上的話，一旦batch_size過小，可能就會造成data產生大量的誤差</li></ul></li></ul><h3 id="延伸討論-BN到底要加在activation-function前還是後面"><a href="#延伸討論-BN到底要加在activation-function前還是後面" class="headerlink" title="延伸討論: BN到底要加在activation function前還是後面?"></a>延伸討論: BN到底要加在activation function前還是後面?</h3><p><strong>實際上有放在前面效果比較好的，也有放在後面效果比較好的</strong>。<br>這是個萬年不解謎題，大家都在吵吵吵，各自提出各自的看法，到現在我也還不知道正確答案。</p><p>總之，我採用了<a href="https://www.zhihu.com/question/283715823" target="_blank" rel="noopener">知乎</a>上面某位網友的回答並稍微整理了一下內容給大家參考(比較令我信服的回答):</p><blockquote><p>在BN的原始论文中，BN是放在非线性激活层前面的（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1502.03167v3">arXiv:1502.03167v3</a>，第5页）</p><p>> We add the BN transform immediately before the nonlinearity（注意：before的黑体是我加的，为了突出重点）</p><p>但是，François Chollet<a href="https://link.zhihu.com/?target=https%3A//github.com/keras-team/keras/issues/1802%23issuecomment-187966878">爆料</a>说BN论文的作者之一Christian把BN放在ReLU后面</p><p>> I can guarantee that recent code written by Christian applies relu before BN.</p><p>另外，Jeremy Howard直接<a href="https://forums.fast.ai/t/questions-about-batch-normalization/230/2" target="_blank" rel="noopener">主张</a>把BN放在非线性激活后面</p><p>> You want the batchnorm after the non-linearity, and before the dropout.</p><p>“应该”放在前面还是后面？这个“应该”其实有两种解释：</p><ol><li>放在前面还是后面比较好？</li><li>为什么要放在前面还是后面？</li></ol><p>对于第一问，目前在实践上，倾向于把BN放在ReLU后面。也有评测表明BN放ReLU后面效果更好。</p><p>对于第二问，实际上，我们目前对BN的机制仍然不是特别清楚，这里只能尝试做些（玄学）解释，不一定正确。</p><p>考虑一些飽和激活函数(tanh、sigmoid)。函数图像的两端，相对于x的变化，y的变化都很小。也就是说，容易出现梯度衰减的问题。那么，如果在tanh或sigmoid之前，进行一些normalization处理，就可以缓解梯度衰减的问题。我想这可能也是最初的BN论文选择把BN层放在非线性激活之前的原因。</p><p>但是ReLU和它们完全不一样啊(因為是非飽和激活函数，比較沒有上述梯度衰减問題)。</p><p>实际上，最初的BN论文虽然也在使用ReLU的Inception上进行了试验，但首先研究的是sigmoid激活。因此，试验ReLU的，我猜想作者可能就顺便延续了之前把BN放前面的配置，而没有单独针对ReLU进行处理。</p><p><strong>总结一下，BN层的作用机制也许是通过平滑隐藏层输入的分布，帮助随机梯度下降的进行，缓解随机梯度下降权重更新对后续层的负面影响。因此，实际上，无论是放非线性激活之前，还是之后，也许都能发挥这个作用。只不过，取决于具体激活函数的不同，效果也许有一点差别（比如，对sigmoid和tanh而言，放非线性激活之前，也许顺便还能缓解sigmoid/tanh的梯度衰减问题，而对ReLU而言，这个平滑作用经ReLU“扭曲”之后也许有所衰弱）。</strong></p></blockquote><h3 id="延伸討論-BN的batch-size應該要多大-多小"><a href="#延伸討論-BN的batch-size應該要多大-多小" class="headerlink" title="延伸討論: BN的batch size應該要多大/多小?"></a>延伸討論: BN的batch size應該要多大/多小?</h3><p>這個問題本質上和<strong>batch size對於training的影響</strong>是一樣的問題，所以直接從探討batch size大小對於訓練的影響這個角度來回答:</p><ul><li>batch size太小，訓練時的震盪會很嚴重，有機會跳出local min，但不利於收斂；而對於BN來說，找到的平均和變異則不能代表全局平均和變異，因此效果會不好</li><li>batch size太大，震盪會比較小，但容易接近全局(這裡的全局是指training set而不是數據母體)的結果，導致不同batch的梯度方向没有任何變化，容易陷入local min</li></ul><p>所以結論就是，只探討batch_size的話，答案是不能太大也不能太小(廢話)。</p><p>但如果一併探討BN的話，過小會造成效果不好，大一點其實不影響BN，但會有local min問題。</p><h3 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h3><p>綜上所述，BN之所以有用是因為:</p><ul><li>減緩了梯度消失/爆炸的問題</li><li>改善了非飽和模型不易訓練的問題</li><li>減少weight initialization的影響(因為假設初始化的不好，可以透過後面的BN來進行改善)</li><li>起到對資料做normalization的作用</li></ul><p>(這裡並沒有提到”減緩ICS”這項特性是因為根據2018年提出的paper提到，實際上BN之所以有用可能和減緩ICS無關)</p><h2 id="Normalization的各種變形"><a href="#Normalization的各種變形" class="headerlink" title="Normalization的各種變形"></a>Normalization的各種變形</h2><p>常用的Normalization方法除了上面講了這麼多的Batch Normalization外，還有</p><ul><li>Layer Normalization（LN，2016年）</li><li>Instance Normalization（IN，2017年）</li><li>Group Normalization（GN，2018年）</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/Bz6hXO7.jpg" alt=""></p><p>簡單用一張圖來理解他們之間的差異:</p><p>現在假設feature map的維度為: $(N, C, H, W)$</p><ul><li>N: batch size</li><li>C: channel</li><li>H, W: height &amp; width</li></ul><ol><li>BN是在batch方向上，对N、H、W做normalization，保留C的维度。<ul><li>對於較小的batch_size效果較差(因為數量不夠代表平均跟變異)</li><li>BN適用於固定深度的Network，Ex:CNN；不適用於RNN</li></ul></li><li>LN在channel方向上，对C、H、W做normalization，主要用在RNN</li><li>IN在image pixel上，对H、W做normalization，用在style transfer</li><li>GN先將channel分组，然後再做normalization。</li></ol><p>打到這邊也累了所以提個大概就好，所以下面只介紹LN，其餘的有興趣的在自己去看xD</p><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>根據上面BN介紹了那麼一大坨，可以知道BN的缺點在於: <strong>當batch_size過小時，batch的平均和變異並不能代表全局的平均和變異，此時BN的效果就會很差</strong>。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://i.imgur.com/yAVptoY.jpg" alt=""></p><p>這個情形在RNN上尤其明顯，由於RNN是的input長度是動態的，所以考慮以下的情形:</p><ul><li>當N過小的時候，對於T&gt;4的數據只有一筆(淺藍色)，無法代表全局資訊</li><li>當在testing時遇到比training set裡面任何一筆都還要長的input時，此時並沒有任何一個平均和變異可以拿來做BN</li></ul><p>所以，LN的主要想法是<strong>以單一的input為單位，去計算平均和變異</strong>(BN是以batch為單位)，假設$H$是一層中隱藏節點的數量，$l$是Model的layer數，$\alpha$是每一個neuron:</p><script type="math/tex; mode=display">u^l=\frac{1}{H}\sum^H_{i=1}\alpha_i^l, \space\space \sigma^l=\sqrt{\frac{1}{H}\sum^H_{i=1}(\alpha^l_i-u^l)^2}</script><p>然後就可以做Normaluization，注意到這裡的計算和batch_size完全無關，所以避開了BN在batch_size過小時會遇到的問題。<br>此外，LN裡面也提供了類似BN的兩個trainable parameters: gain(g)和bias(b)來盡可能地還原被破壞的訊息。</p><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>這篇文章主要從不同的角度來探討為何BN對於Training有幫助，以及延伸探討實務上應該如何使用BN，最後也簡介了BN的各種變形及應用。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">Batch Normalization原理与实战</a></li><li><a href="https://blog.csdn.net/mao_xiao_feng/article/details/54317852" target="_blank" rel="noopener">【机器学习】covariate shift现象的解释</a></li><li><a href="https://blog.csdn.net/hjimce/article/details/50864602" target="_blank" rel="noopener">PCA Whitening</a></li><li><a href="https://zhuanlan.zhihu.com/p/54073204" target="_blank" rel="noopener">Batch Normalization的通俗解释</a></li><li><a href="https://arxiv.org/abs/1805.11604" target="_blank" rel="noopener">How Does Batch Normalization Help Optimization?</a></li><li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></li><li><a href="https://www.youtube.com/watch?v=BZh1ltr5Rkg&amp;feature=share" target="_blank" rel="noopener">李宏毅老師的BN介紹</a></li><li><a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/5-04-A-batch-normalization/" target="_blank" rel="noopener">莫煩的Normalization介紹</a></li><li><a href="https://www.zhihu.com/question/283715823" target="_blank" rel="noopener">Batch-normalized 应该放在非线性激活层的前面还是后面？</a></li><li><a href="https://zhuanlan.zhihu.com/p/72589565" target="_blank" rel="noopener">常用的 Normalization 方法：BN、LN、IN、GN</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[ML]Sklearn的scoring parameters</title>
      <link href="/posts/882f9d11/"/>
      <url>/posts/882f9d11/</url>
      
        <content type="html"><![CDATA[<p>對於Machine Learning / Deep Learning，常會根據Task的不同而使用不同的Evaluation Metrics，例如MAE、MSE、RMSE…等等，在sklearn中當然也提供了function讓user能夠快速地進行計算，比方說如果想要算MAE的話可以這樣寫：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line">y_true = [<span class="number">3</span>, <span class="number">-0.5</span>, <span class="number">2</span>, <span class="number">7</span>]</span><br><span class="line">y_pred = [<span class="number">2.5</span>, <span class="number">0.0</span>, <span class="number">2</span>, <span class="number">8</span>]</span><br><span class="line">mean_absolute_error(y_true, y_pred)</span><br><span class="line"> </span><br><span class="line">y_true = [[<span class="number">0.5</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">1</span>], [<span class="number">7</span>, <span class="number">-6</span>]]</span><br><span class="line">y_pred = [[<span class="number">0</span>, <span class="number">2</span>], [<span class="number">-1</span>, <span class="number">2</span>], [<span class="number">8</span>, <span class="number">-5</span>]]</span><br><span class="line">mean_absolute_error(y_true, y_pred)</span><br><span class="line"> </span><br><span class="line">mean_absolute_error(y_true, y_pred, multioutput=<span class="string">'raw_values'</span>)</span><br><span class="line"> </span><br><span class="line">mean_absolute_error(y_true, y_pred, multioutput=[<span class="number">0.3</span>, <span class="number">0.7</span>])</span><br></pre></td></tr></table></figure><p>透過呼叫sklearn.metrics下的mean_absolute_error()來進行計算，但如果仔細看sklearn的document的話會發現在<a href="https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules" target="_blank" rel="noopener">scoring parameter</a>下有一些奇怪的名詞，例如：</p><ul><li>neg_mean_absolute_error</li><li>neg_mean_squared_error</li><li>neg_mean_squared_log_error</li><li>neg_median_absolute_error</li></ul><p>不難猜測前綴的neg_是指negative，所以這些<strong>值實際上應該是原本的值加上一個負號</strong>。</p><p>不過為什麼要將這些值加上一個負號呢？看了一下scoring這章的說明有提到：</p><blockquote><p>All scorer objects follow the convention that <strong>higher return values are better than lower return values</strong>.</p></blockquote><p>也就是說，這裡的<strong>所有scorer obj都被定義為分數越高越好</strong>，所以對於一些分數越低越好的評估標準(MAE、MSE…)就要加上一個負號反過來。</p><p>那為什麼要這樣做呢？在<a href="https://stackoverflow.com/questions/43081251/sklearn-metrics-log-loss-is-positive-vs-scoring-neg-log-loss-is-negative" target="_blank" rel="noopener">stackoverflow</a>中找到了這篇解釋，覺得解釋得還不錯：</p><blockquote><p>The sklearn.metrics.log_loss is an implementation of the error metric as typically defined, and which is as most error metrics a positive number. In this case, it is a metric which is generally minimized (e.g. as mean squared error for regression), in contrast to metrics such as accuracy which is maximized. The ‘neg_log_loss’ is a hence a technicality to make create a utility value, which allows optimizing functions and classes of sklearn to maximize this utility without having to change the function’s behavior for each metric (such include for instance named cross_val_score, GridSearchCV, RandomizedSearchCV, and others).</p></blockquote><p>簡而言之，一般在做cross vaildation的時候會透過迴圈讓他自己去跑固定value range內的參數，然後去比較究竟哪一個參數的loss(或是score)越大越好，注意到在sklearn的相關方式實作都是以越大越好做為比較值，所以這裡就呼應到上述為什麼需要把一些metrics的正負翻轉過來了，例如下面的例子，在cross_val_score中指定”neg_mean_absolute_error”的scoring：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm, datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line">clf = svm.SVC(gamma=<span class="string">'scale'</span>, random_state=<span class="number">0</span>)</span><br><span class="line">cross_val_score(clf, X, y, scoring=<span class="string">'recall_macro'</span>,</span><br><span class="line">                cv=<span class="number">5</span>)  </span><br><span class="line"> </span><br><span class="line">model = svm.SVC()</span><br><span class="line">cross_val_score(model, X, y, cv=<span class="number">5</span>, scoring=<span class="string">'neg_mean_absolute_error'</span>)</span><br></pre></td></tr></table></figure><p>而最後要注意的是，<strong>雖然scoring object是negative的，但在sklearn.metric下的function仍然是正常的定義</strong>，不要搞混了！</p>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[ML/DL]從計算機編碼的角度看Entropy, Cross Entropy和KL-Divergence</title>
      <link href="/posts/216f7e20/"/>
      <url>/posts/216f7e20/</url>
      
        <content type="html"><![CDATA[<h2 id="Shannon-Entropy"><a href="#Shannon-Entropy" class="headerlink" title="Shannon Entropy"></a>Shannon Entropy</h2><h3 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h3><p>Entropy概念最早被用於熱力學，在1948年由Shannon將此概念引入<a href="https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E8%AE%BA" target="_blank" rel="noopener">information theory</a>中，被用來<strong>計算根據訊息的機率分布對訊息編碼所需要的平均編碼長度</strong>，也因此Entropy也被稱之Shannon Entropy。</p><a id="more"></a><h3 id="情境思考"><a href="#情境思考" class="headerlink" title="情境思考"></a>情境思考</h3><p>在早期計算機設備昂貴的年代，如果需要傳遞一段由26個英文字母組成的英文文本，按照<a href="https://zh.wikipedia.org/wiki/ASCII" target="_blank" rel="noopener">ASCII code的編碼</a>每一個字元共需要8個bit進行編碼，但實際上並不是每個字母出現的頻率(或機率)都是相等的，因此，<strong>如果可以讓越常出現的字元使用較少的bit數，而較不常出現的字元使用較多的bit數，就有機會整體傳送的bits數較少。</strong></p><ul><li>你說節省傳送的bit數量有什麼好處? cost is money阿孩子</li><li>根據鴿籠原理，任何無損壓縮技術不可能縮短任何訊息，如果有一些訊息變短，則至少有一條訊息變長。</li><li>在實際使用中，由於我們通常只關注於壓縮特定的某一類訊息，一些編碼技術恰好使用了這特性。<ul><li>對於壓縮編碼的技術，可以參考<a href="https://zh.wikipedia.org/wiki/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81" target="_blank" rel="noopener">霍夫曼編碼</a></li></ul></li></ul><p>對於上述的敘述，實際上，英文文本的平均編碼長度大約在4.7bits，也就是說，給予一段長度為10的英文文本:</p><ul><li>不使用任何編碼壓縮技術，直接使用ASCII傳送的話需要 $8\times10 = 80$ bits</li><li>使用Entropy的編碼技術來傳送的話平均只要$4.7\times10 = 47$ bits</li></ul><p>所以要怎麼知道對於某些訊息，應該要用多少bit來編碼呢? Entropy可以幫助你</p><h3 id="Entropy-formula"><a href="#Entropy-formula" class="headerlink" title="Entropy formula"></a>Entropy formula</h3><p>先完整看一次Entropy的公式</p><p>$Entropy = -\sum_ip_ilog_b(p_i)$</p><ul><li>注意這裡log的底沒有限定是什麼，不同的b對應到不同單位，不過為了從information theory的角度出發，下面會用b=2來講解，此時的單位為<strong>bit</strong></li></ul><h3 id="理解公式"><a href="#理解公式" class="headerlink" title="理解公式"></a>理解公式</h3><p>這鬼東西跟編碼長度到底有什麼鬼關係?</p><p>簡化一下剛剛提到的情境來思考這個問題，現在假設需要傳遞一個由$S={S_1, S_2, S_3, S_4}$組成的字串，使用ASCII的話每個字元需要8 bits來進行傳遞，但實際上只有四種可能，也就是說我們只需要</p><p>$log_2(4)$</p><p>2個bit就可以來進行四個答案的編碼了: {00, 01, 10, 11}</p><p>而我們又可以將上述的式子改寫一下</p><p>$log_2(4) = log_2(\frac{1}{\frac{1}{4}}) = -log_2(\frac{1}{4})$</p><p>這裡的1/4可以看成是一個隨機變數的機率，並且1/4其實對應到了上面這個例子沒有提到的一個假設: <strong>4個字元出現的機率是相等的</strong>，基於機率相等的假設下，$S_1, S_2, S_3, S_4$都可以透過上述的公式算出需要2個bit的編碼。</p><p>不過並不是每個case都這麼是機率相等的，<strong>為了可以套用在機率不同的情況下，需要引入期望值的概念</strong>，也就是說式子變成了</p><p>$-\sum_ip_ilog_2(p_i)$</p><ul><li>實際上，將 $p_1 = p_2 = p_3 = p_4 = 1/4$ 帶入Entropy的公式也會得到2</li><li>在information theory中，$-log_2(\frac{1}{p})$也稱之為資訊本體(self-information)，也就是說，<strong>越不常出現的訊息往往帶著越大的資訊含量</strong>。</li></ul><p>所以到這裡我們可以說，<strong>Entropy在算的是對於每個編碼，所需要長度的期望值</strong>。</p><hr><p>接下來繼續考慮上述問題，但是這次$S={S_1, S_2, S_3, S_4}$的機率並不相等了，假設:</p><ul><li>$P(S_1)=1/4$</li><li>$P(S_2)=1/8$</li><li>$P(S_3)=1/2$</li><li>$P(S_4)=1/8$</li></ul><p>在這種情況下，各別需要的編碼數為:</p><ul><li>$S_1$需要2 bits編碼</li><li>$S_2$需要3 bits編碼</li><li>$S_3$需要1 bits編碼</li><li>$S_4$需要3 bits編碼</li></ul><p>我們可以隨便編，例如給$S_1$:10、$S_2$:011、$S_3$:0、$S_4$:111之類的，只要沒有衝突到就好了，然後算一下編碼長度的期望值</p><p>$-\sum_ip_ilog_b(p_i) = 1/4\times2+1/8\times3+1/2\times1+1/8\times3 = 1.75$</p><p>所以平均編碼長度總共是1.75個bit。</p><p>你說小數是什麼鬼?阿不都說了是平均==，如果還覺得哪裡怪怪的話，那接下來來個實際情境想一下。</p><hr><p>接續上述問題，假設由$S={S_1, S_2, S_3, S_4}$組成的字串長度為200個字元，並且字元出現的次數恰好完全符合上述的機率分布，也就是說</p><ul><li>$S_1$出現了50次</li><li>$S_2$出現了25次</li><li>$S_3$出現了100次</li><li>$S_4$出現了25次</li></ul><p>所以傳遞這個字串總共需要$50\times2+25\times3+100\times1+50\times2=375$個bit，平均每個字元只用了375/200 = 1.75個bit。</p><h3 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h3><p>簡介了從Information Theory的Encoding角度去看待Entropy這件事，並透過簡單的例子回顧公式的意義。</p><p>重申一次，所以Entropy在Information Theory中的用途是<strong>計算根據訊息的機率分布對訊息編碼所需要的平均編碼長度</strong>。</p><h3 id="延伸討論-Entropy必不能為負"><a href="#延伸討論-Entropy必不能為負" class="headerlink" title="延伸討論: Entropy必不能為負?"></a>延伸討論: Entropy必不能為負?</h3><ol><li>從數學的角度講，因為$0=0$</li><li>從資訊意義的角度，取自<a href="https://www.zhihu.com/question/263824964" target="_blank" rel="noopener">为什么信息量不能为负值？</a></li></ol><blockquote><p>因为，信息值为负的已经不能叫做信息，而被叫做干扰。<br>香农的理论是很简朴而典型的，A想要把某个信息a传递给B，但传播信息的信道存在种种异常，比如，A站在山顶喊话“我喜欢你”，结果因为风太大，B只听到“我喜欢”，虽然信息量丢失，但仍有一部分信息传达了，那就是“我（A）”和“喜欢（L）”。</p><p>第二次，A还对B说，“我喜欢你”，但因为风太大，B听成了，“我喜欢隔壁老王”。那么B所得到的信息是什么呢？“我(A)”和“喜欢(L)”这两个信息的置信度提高了，而“你(B)”的置信度降低了。</p><p>但是当A说，“我喜欢你”，B可能会因为风太大而听成“老李讨厌隔壁老王”么？可能性很低。</p></blockquote><hr><h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h2><p>知道Entropy在information theory的意義後，後面就很好介紹了。</p><p>先說結論，Cross Entropy可以這樣理解: <strong>使用了估計出來的編碼後所得到的平均編碼長度</strong>。</p><h3 id="Cross-Entropy-formula"><a href="#Cross-Entropy-formula" class="headerlink" title="Cross Entropy formula"></a>Cross Entropy formula</h3><p>先完整看一次Entropy的公式$Cross Entropy = -\sum_ip_ilog_b(q_i)$</p><ul><li>注意儘管公式很像Shannon Entropy，但不同的地方在於log裡面用的是q(x)</li></ul><h3 id="理解公式-1"><a href="#理解公式-1" class="headerlink" title="理解公式"></a>理解公式</h3><p>對於上述公式我們可以這麼理解:<br>對於某個需要被傳遞的資料，$p$是真正的機率分布，但這個機率分布我們事實上不知道，所以我們透過了神奇的Machine Learning / Deep Learning等黑技術去學習到了一個我們以為的機率分布$q$。</p><p>所以接下來我們使用$-\sum_ilog_b(q_i)$來進行編碼，但事實上資料真正的機率分布是$p$，所以我們得到的平均編碼長度就變成了$-\sum_ip_ilog_b(q_i)$。</p><p>也就是說，當$q$估計的越準(越接近q)，平均編碼長度才會是最短的(接近Shannon Entropy)。</p><h3 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h3><ul><li>實際上從ML/DL的角度來看，Cross Entropy Loss的概念就是，今天有一個資料真正的機率分布以及透過model學習出來的分布，我們希望這兩個機率分布越接近越好。</li></ul><h3 id="延伸討論-Cross-Entropy遇到-log0"><a href="#延伸討論-Cross-Entropy遇到-log0" class="headerlink" title="延伸討論: Cross Entropy遇到$log0$?"></a>延伸討論: Cross Entropy遇到$log0$?</h3><ul><li>這在training中常常會遇到，implement上通常會採用$log(x+ \epsilon)$來handle這個狀況。</li></ul><h2 id="KL-Divergence"><a href="#KL-Divergence" class="headerlink" title="KL-Divergence"></a>KL-Divergence</h2><p>接下來來介紹KL-Divergence，一樣先說結論: KL-Divergence實際上是<strong>當估出來的編碼方式和理想上的編碼有差時，而導致平均編碼差度的誤差值</strong>。</p><ul><li>實際上，這很常發生，因為我們不知道理想上的編碼方式應該是如何(不知道機率分布)，所以我們使用model估出來的編碼方式可能會產生誤差。</li></ul><h3 id="KL-Divergence-formula"><a href="#KL-Divergence-formula" class="headerlink" title="KL-Divergence formula"></a>KL-Divergence formula</h3><p>公式如下:</p><p>$KL-Divergence = \sum_ip_ilog_b(\frac{p_i}{q_i})$</p><h3 id="理解公式-2"><a href="#理解公式-2" class="headerlink" title="理解公式"></a>理解公式</h3><p>再看一遍Cross Entropy的公式和它的定義: 使用了估計出來的編碼後所得到的平均編碼長度</p><p>$Cross Entropy = -\sum_ip_ilog_b(q_i)$</p><p>那這個估計出來的編碼長度到底理想上的編碼長度差了多少?我們可以來算算看</p><script type="math/tex; mode=display">Cross Entropy-Shannon Entropy =-\sum_ip_ilog_b(q_i) -(-\sum_ip_ilog_b(p_i)) \\=\sum_ip_ilog_b(p_i)-\sum_ip_ilog_b(q_i) \\=\sum_ip_ilog_b(\frac{p_i}{q_i})</script><p>驚!這不就是KL-Divergency的公式嗎，再仔細看…<br>又驚!這不就是KL-Divergency的定義嗎</p><h3 id="結論-1"><a href="#結論-1" class="headerlink" title="結論"></a>結論</h3><ul><li>實際上KL-Divergence算的就是兩個機率分布的距離。</li></ul><h3 id="延伸討論-為何DL常使用Cross-Entropy而不是KL"><a href="#延伸討論-為何DL常使用Cross-Entropy而不是KL" class="headerlink" title="延伸討論: 為何DL常使用Cross Entropy而不是KL?"></a>延伸討論: 為何DL常使用Cross Entropy而不是KL?</h3><p>簡單來說，在ML的task裡面，KL和CE是等價的，只是算KL的話還必須多算一個Entropy，所以大家都只算CE。</p><p>詳細的解釋可以看下面這篇，取自<a href="https://ai.stackexchange.com/questions/3065/why-has-cross-entropy-become-the-classification-standard-loss-function-and-not-k" target="_blank" rel="noopener">Why has cross entropy become the classification standard loss function and not Kullbeck Leibler divergence?</a></p><blockquote><p>When it comes to classification problem in machine learning, the cross entropy and KL divergence are equal. As already stated in the question, the general formula is this:$H(p,q)=H(p)+DKL(p||q)$<br>Where p a “true” distribution and q is an estimated distribution, H(p,q) is the cross-entropy, H(p) is the entropy and D is the Kullback-Leibler divergence.<br>Note that in machine learning, p is a one-hot representation of the ground-truth class, i.e., $p=[0,…,1,…,0]$<br>which is basically a delta-function distribution. But the entropy of the delta function is zero, hence KL divergence simply equals the cross-entropy.<br>In fact, even if H(p) wasn’t 0 (e.g., soft labels), it is fixed and has no contribution to the gradient. In terms of optimization, it’s safe to simply remove it and optimize the Kullback-Leibler divergence.</p></blockquote><h3 id="延伸討論-為何DL常使用CE而不是LSE"><a href="#延伸討論-為何DL常使用CE而不是LSE" class="headerlink" title="延伸討論: 為何DL常使用CE而不是LSE?"></a>延伸討論: 為何DL常使用CE而不是LSE?</h3><p>在gradient更新上速度的不同，有機會下次再說。</p><h2 id="總結-1"><a href="#總結-1" class="headerlink" title="總結"></a>總結</h2><p>從計算機編碼的角度來介紹Shannon Entropy的意義，並延伸介紹了Cross Entropy以及KL-Divergence，最後在總結三者之間的關係</p><p>$Cross Entropy = Shannon Entropy + KL-Divergence$</p><p>我們常常在做的Min Cross Entropy實際上就是是在Min KL-Divergence，當KL-Divergence = 0時，這代表這兩個機率分布是沒有差異的。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://zh.wikipedia.org/wiki/ASCII" target="_blank" rel="noopener">ASCII</a></li><li><a href="https://en.wiktionary.org/wiki/Shannon_entropy" target="_blank" rel="noopener">Shannon entropy(weki)</a></li><li><a href="https://zh.wikipedia.org/wiki/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81" target="_blank" rel="noopener">霍夫曼編碼</a></li><li><a href="https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA" target="_blank" rel="noopener">熵(wiki)</a>)</li><li><a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E4%BF%A1%E6%81%AF" target="_blank" rel="noopener">self-information</a></li><li><a href="https://www.cnblogs.com/frombeijingwithlove/p/5931750.html" target="_blank" rel="noopener">熵和编码长度</a></li><li><a href="https://www.zhihu.com/question/263824964" target="_blank" rel="noopener">为什么信息量不能为负值？</a></li><li><a href="https://www.cnblogs.com/ljy2013/p/6432269.html" target="_blank" rel="noopener">交叉熵(Cross Entropy)</a></li><li><a href="https://www.zhihu.com/question/41252833" target="_blank" rel="noopener">如何通俗的解释交叉熵与相对熵?</a></li><li><a href="https://ai.stackexchange.com/questions/3065/why-has-cross-entropy-become-the-classification-standard-loss-function-and-not-k" target="_blank" rel="noopener">Why has cross entropy become the classification standard loss function and not Kullbeck Leibler divergence?</a></li><li><a href="https://www.zhihu.com/question/336677048/answer/760789081" target="_blank" rel="noopener">分类问题中损失函数为什么交叉熵用的多，而不是KL?</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[LeetCode]各種Max subarray問題-2</title>
      <link href="/posts/6400f047/"/>
      <url>/posts/6400f047/</url>
      
        <content type="html"><![CDATA[<p>接續上一篇Leetcode探討max subarray文章，進一步探討相關的題型</p><ul><li>上一篇看這裡：<a href="https://meetonfriday.com/posts/fd09a1fd/">[LeetCode]各種Max subarray問題-1</a></li></ul><h2 id="1191-K-Concatenation-Maximum-Sum"><a href="#1191-K-Concatenation-Maximum-Sum" class="headerlink" title="1191. K-Concatenation Maximum Sum"></a>1191. K-Concatenation Maximum Sum</h2><ul><li><a href="https://leetcode.com/problems/k-concatenation-maximum-sum/" target="_blank" rel="noopener">leetcode</a></li><li><a href="https://github.com/john850512/LeetCode_Solved/blob/master/1191.%20K-Concatenation%20Maximum%20Sum.md" target="_blank" rel="noopener">code(github)</a></li></ul><p>題意: 將一個array重複拼接K次後，找一個subarray，使得該subarray有最大的和。</p><a id="more"></a><p><strong>Input:</strong> arr = [1,2], k = 3</p><p><strong>Output:</strong> 9</p><p>1. 把所有array重複k次後做kadane’s algo，但會TLE，因為資料(10^5) *k(10^5)太大了</p><ol><li>考慮以下幾種case:</li></ol><ul><li>k == 1的時候答案就是kadane’s</li><li>k&gt;=2且array total sum&gt;0，此時越多的array有助於增加長度，答案會是<ul><li>[第一個array的最大後綴和] + (k-2)*total_sum + [最後一個array的最大前綴和]</li></ul></li><li><p>k&gt;=2且array total sum &lt;= 0，此時array越多也沒有幫助，就直接拼接兩次做kadane’s algo</p><ul><li>之所以拼接兩次是要顧慮到如果答案是一端的尾巴 + 一端的頭的情況</li></ul><p>這次的code弄了好久還是貼不上來，只能說wordpress支援code的功能真的爛到爆，所以有需要的去github上看吧。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Python]Add legend on cluster visualization use Matplotlib</title>
      <link href="/posts/eaa05bd3/"/>
      <url>/posts/eaa05bd3/</url>
      
        <content type="html"><![CDATA[<p><strong>這篇主要在介紹如何正確在Matplotlib上為每種cluster加上圖例(legend)。</strong> </p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>資料視覺化在不管是機器學習/深度學習都是很重要的一個環節，有了資料視覺化能夠很快速的讓其他人了解你的資料/模型。 但是在呈現dataset的時候，資料往往維度不是2維，無法透過x, y軸呈現出來。這時候通常會透過降維的方法來把高維度的資料壓縮到低維度呈現，常見的降維方法有PCA和t-SNE。 所以，你以為這篇要來介紹這兩個方法背後的原理嗎? </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/09/5efa825b04849f9665c723e29793ac46.png?w=300" alt="5efa825b04849f9665c723e29793ac46">  </p><a id="more"></a><p>詳細的資料網路上都有很多，這邊丟幾篇給有興趣的自己去看：</p><ul><li><a href="https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E7%B5%B1%E8%A8%88%E5%AD%B8%E7%BF%92-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-principle-component-analysis-pca-58229cd26e71" target="_blank" rel="noopener">機器/統計學習:主成分分析(Principal Component Analysis, PCA)</a></li><li><a href="https://mropengate.blogspot.com/2019/06/t-sne.html" target="_blank" rel="noopener">資料降維與視覺化：t-SNE 理論與應用</a></li></ul><p>而在其中t-SNE又因為某些特性，使得他<strong>在低維度的時候能比PCA有更好的視覺化效果</strong>，使用方始也很簡單，<a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" target="_blank" rel="noopener">sklearn</a>上就有支援，所以這篇也沒有打算教怎麼使用。 [透過Matplotlib呈現cluster結果] 當把資料降到低維度(2維或3維)後，就可以透過Matplotlib來呈現cluster。下面介紹兩種呈現的方式：</p><h2 id="1"><a href="#1" class="headerlink" title="1."></a>1.</h2><p>相信有在學深度學習相關知識的人對下面這張圖都不陌生：</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/09/0001.png" alt="0001"> 對，這張圖是<a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/4-01-CNN/" target="_blank" rel="noopener">莫煩CNN教程中提供的t-SNE visualization code</a>，如果去看code就會知道他是寫了一個迴圈，<strong>針對每一個point去做plt.text()，並同時計算該點的color</strong>。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(lowDWeights, labels)</span>:</span></span><br><span class="line">    plt.cla()</span><br><span class="line">    X, Y = lowDWeights[:, <span class="number">0</span>], lowDWeights[:, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> x, y, s <span class="keyword">in</span> zip(X, Y, labels):</span><br><span class="line">        c = cm.rainbow(int(<span class="number">255</span> * s / <span class="number">9</span>)); plt.text(x, y, s, backgroundcolor=c, fontsize=<span class="number">9</span>)</span><br><span class="line">    plt.xlim(X.min(), X.max());</span><br><span class="line">    plt.ylim(Y.min(), Y.max());</span><br><span class="line">    plt.title(<span class="string">'Visualize last layer'</span>);</span><br><span class="line">    plt.show();</span><br><span class="line">    plt.pause(<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><hr><h2 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h2><p>如果不想以文字呈現的話，也可以用<strong>散布圖(plt.scatter())</strong>來呈現，這時候用一行解決，把整個x, y array餵進去，而不用寫個迴圈去做： </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, Y = lowDWeights[:, <span class="number">0</span>], lowDWeights[:, <span class="number">1</span>]</span><br><span class="line">plt.scatter(X, Y, c=labels, marker=marker, cmap=cmap)</span><br></pre></td></tr></table></figure><p>常用matplotlib的人就會知道，以往要在圖片上加上圖例，都會在每次的繪圖function指定lebel，最後呼叫plt.legend()： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(......, label = <span class="string">'圖例1'</span>)</span><br><span class="line">plt.plot(......, label = <span class="string">'圖例2'</span>)</span><br><span class="line">...</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure><p>不過對於cluster而言，我們想要的應該是每一個cluster一個color，但又不想用迴圈一個一個點慢慢加，這時候可以使用<strong><a href="https://matplotlib.org/3.1.0/users/whats_new.html#legend-for-scatter" target="_blank" rel="noopener">matplotlib version</a></strong> <a href="https://matplotlib.org/3.1.0/users/whats_new.html#legend-for-scatter" target="_blank" rel="noopener">(</a><strong><a href="https://matplotlib.org/3.1.0/users/whats_new.html#legend-for-scatter" target="_blank" rel="noopener">3.1.0</a></strong><a href="https://matplotlib.org/3.1.0/users/whats_new.html#legend-for-scatter" target="_blank" rel="noopener">)</a>後提供的new method：</p><blockquote><p>Now, PathCollection provides a method <strong>legend_elements()</strong> to obtain the handles and labels for a scatter plot in an automated way. This makes creating a legend for a scatter plot as easy as</p></blockquote><p>使用方法如下：</p><ol><li>把原本的plt.scatter用個變數接好</li><li>透過呼叫該變數的function legend_elements()取得總共有哪些labels數(matplotlib會自動算好)    </li><li>legend_elements()會回傳一個list，把list拆開餵到plt.legend()裡面</li></ol><p>上面步驟看不懂沒關係，咱們直接看code，兩行而已： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot</span></span><br><span class="line">scatter = plt.scatter(x_arr, y_arr, c=labels, marker=marker, cmap=cmap)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># produce a legend with the unique colors from the scatter</span></span><br><span class="line">plt.legend(*scatter.legend_elements())</span><br></pre></td></tr></table></figure><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/09/000.png" alt="000.PNG"> 就john，結束！</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[LeetCode]各種Max subarray問題-1</title>
      <link href="/posts/fd09a1fd/"/>
      <url>/posts/fd09a1fd/</url>
      
        <content type="html"><![CDATA[<p>我覺得我和這個題型特別有緣阿，之前實習面試還被問到，然後這兩週的week competition都有出到，下面就從最原始的問題開始，然後慢慢講解其他有的沒的變形題目。</p><p>第二篇看這裡：<a href="https://meetonfriday.com/posts/6400f047/">[LeetCode]各種Max subarray問題-2</a>  </p><a id="more"></a><h2 id="LeetCode-53-Maximum-Subarray"><a href="#LeetCode-53-Maximum-Subarray" class="headerlink" title="LeetCode 53. Maximum Subarray"></a>LeetCode 53. Maximum Subarray</h2><ul><li><a href="https://leetcode.com/problems/maximum-subarray/" target="_blank" rel="noopener">leetcode</a></li><li><a href="https://github.com/john850512/LeetCode_Solved/blob/master/0053.%20Maximum%20Subarray.md" target="_blank" rel="noopener">code(github)</a></li></ul><p>題意: 從array裡面找一個subarray，使得該subarray有最大的和。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Input: [-2,1,-3,4,-1,2,1,-5,4],</span><br><span class="line">Output: 6</span><br><span class="line">Explanation: [4,-1,2,1] has the largest sum &#x3D; 6.</span><br></pre></td></tr></table></figure><p>1. 暴力掃過: 枚舉所有可能的區段，找出最大值</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(nums.empty()) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> ans = nums[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; nums.<span class="built_in">size</span>() ; i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> temp = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = i ; j &lt; nums.<span class="built_in">size</span>() ; j++)&#123;</span><br><span class="line">                temp = temp + nums[j];</span><br><span class="line">                ans = <span class="built_in">max</span>(ans, temp);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>2. kadane’s algo</p><p>DP方法，DP[i]記錄了在i-th當下的max subarray，DP[i]只有兩種可能:</p><ul><li>DP[i] = arr[i]</li><li>DP[i] = DP[i-1] + arr[i]</li></ul><p>最後掃過一次DP取得最大的值即可。實際上，可以一邊掃一邊存最大值，如此就不用另外開一個DP array。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(nums.empty()) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> temp_max = nums[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> max_so_far = nums[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span> ; i &lt; nums.<span class="built_in">size</span>() ; i++)&#123;</span><br><span class="line">            temp_max = <span class="built_in">max</span>(nums[i], temp_max+nums[i]);</span><br><span class="line">            max_so_far = <span class="built_in">max</span>(temp_max, max_so_far);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> max_so_far;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><hr><h2 id="LeetCode-918-Maximum-Sum-Circular-Subarray"><a href="#LeetCode-918-Maximum-Sum-Circular-Subarray" class="headerlink" title="LeetCode 918. Maximum Sum Circular Subarray"></a>LeetCode 918. Maximum Sum Circular Subarray</h2><ul><li><a href="https://leetcode.com/problems/maximum-sum-circular-subarray/" target="_blank" rel="noopener">leetcode</a></li><li><a href="https://github.com/john850512/LeetCode_Solved/blob/master/0918.%20Maximum%20Sum%20Circular%20Subarray.md" target="_blank" rel="noopener">code(github)</a></li></ul><p>題意: 從array裡面找一個subarray，使得該subarray有最大的和，但這個array是一個circular array。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Input: [5,-3,5]</span><br><span class="line">Output: 10</span><br><span class="line">Explanation: Subarray [5,5] has maximum sum 5 + 5 &#x3D; 10</span><br></pre></td></tr></table></figure><p>1. kadane’s algo</p><p>有兩種case:</p><ul><li>max_sum_subarray在array中間，這時候直接用kadane’s algo</li><li>max_sum_subarray在array的頭跟尾，這時要換個想法，把問題轉成total_sum - min_sum_subarray(也是用kadane’s algo去計算)</li></ul><p>答案在這兩個case取最大值即可 ，但要注意如果array全部都是負數，此時total_sum - min_sum_array = 0，會得到錯誤的結果，這個case要特別處理(直接回傳max_sum_subarray)</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxSubarraySumCircular</span><span class="params">(<span class="built_in">vector</span>&amp; A)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> temp_max = A[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> max_so_far = A[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> temp_min = A[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> min_so_far = A[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> total_sum = A[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span> ; i &lt; A.<span class="built_in">size</span>() ; i++)&#123;</span><br><span class="line">            temp_max = <span class="built_in">max</span>(A[i], temp_max + A[i]);</span><br><span class="line">            max_so_far = <span class="built_in">max</span>(max_so_far, temp_max);</span><br><span class="line"></span><br><span class="line">            temp_min = <span class="built_in">min</span>(A[i], temp_min + A[i]);</span><br><span class="line">            min_so_far = <span class="built_in">min</span>(min_so_far, temp_min);</span><br><span class="line"></span><br><span class="line">            total_sum += A[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(min_so_far == total_sum) <span class="keyword">return</span> max_so_far;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(max_so_far, total_sum-min_so_far);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>暑期實習申請心得</title>
      <link href="/posts/83ee3dab/"/>
      <url>/posts/83ee3dab/</url>
      
        <content type="html"><![CDATA[<p>這個人怎麼回事…都實習結束了才在發申請的心得文。</p><p>當初因為一些事情所以很多事情都亂了步調，連原本預計要打的文章都還沒打…</p><p>總之現在就是來還債的，這篇文章會分享我在今年三月申請各家的實習過程和心得，以下內容全憑三個月前的微薄記憶，如果有內容錯了就算了吧。</p><p>在當初<a href="https://meetonfriday.com/posts/a1b74789/">參加就職博覽會</a>之前我就開始準備各項實習的申請了，然後在三月初的時候開始履歷轟炸各家有在招募實習的公司(<strong>有些公司的招募時程會比較不同，所以要特別注意，例如Google</strong>)，大部分要準備的東西如下：</p><ul><li>歷屆成績單</li><li>中英文CV</li></ul><a id="more"></a><p>我自己的CV是類似國外CV的風格，把背景、經歷、專長跟作品條列式列出來，輔以一些簡單的敘述這樣。也沒有說其他的呈現方式不好，各有各的喜好。</p><p>然後要怎麼知道有哪些可以投呢？我當初找的管道有以下來源：</p><ul><li>104/1111的實習專區</li><li>學校就業博覽會的時候花了大把時間確認了哪些有在招聘實習，順便工商一下快去看我那篇就博的文章</li><li>各家公司的粉絲專頁(如果知道某些公司每年都會固定招聘的就可以先去按讚follow了，例如台積聯發…)</li><li>各種奇奇怪怪的資訊社群，臉書社群加多了有時候會意外的收到一些蠻不錯的資訊</li></ul><p>然後接下來就是瘋狂轟炸他們，以下是相關應徵的經歷分享，直接被刷掉的就沒分享了QQ</p><h2 id="趨勢"><a href="#趨勢" class="headerlink" title="趨勢"></a>趨勢</h2><p>趨勢再就博跟104上都有相關招募資訊，我記得透過哪個管道應徵都可以，然後他會寄一個線上測驗的網址給你考coding，有兩題，難度我覺得不難(leetcode easy?)，寫完後交出去就等後續了。不過我到這邊就沒後續了xD</p><h2 id="ASUS"><a href="#ASUS" class="headerlink" title="ASUS"></a>ASUS</h2><p>那時申請的是一個新成立的部門，主要在做AI相關的應用和專案，當初就博的時候聽他們的人一直說他們的面試很硬，就抱著戰戰兢兢的心態去申請了。印象中申請的時候要去他們的網站填相關資訊，之後會有人來安排進行一個小時左右的skype interview，過程主要是簡單的自我介紹，然後介紹自己做過了什麼，最後會有一個coding測驗。</p><p>如果第一階段過了的話，會有第二階段，這時候會邀請你去部門那邊進行三小時左右的面試。</p><p>還記得當天我去台北的那天遇上大地震，結果火車誤點然後台北捷運又短暫的全面停駛造成遲到半小時(是多不想讓我去這家..)。然後到現場面完第一位之後才發現當初HR沒有跟我說清楚，我原本只以為只需要一次的coding測驗，然後剩下的時間可能都在面談之類的，結果到現場我才知道有三關，每關都是考coding……還記得那天走出來的時候真的快累死了。</p><p>歐對了，這段期間還會有一個大約半小時的HR電訪，會問一些關於你的個人問題，我猜主要是了解你的個性、人格特質之類的吧。</p><p>以上的關卡如果都順利通過了，那HR會很快地告知你請你錄取通知(我那時是一週內就被通知了)，這點我覺得還不錯，效率很好，這是我最快收到結果的一家公司，大約在四月底就收到確認了。</p><h2 id="聯發"><a href="#聯發" class="headerlink" title="聯發"></a>聯發</h2><p>這家是我面試最久的公司，主要是上公司的網站去填資料還有要應徵的職位。然後如果有某個部門的主管對你有興趣就會透過HR邀請你去面試，HR會請你在他們網站上填寫你可以的時段，再依據你給的資訊發面試邀請給你。</p><p>面試的內容其實網路上都找得到很多寫得很好的分享？所以我就不寫太多了xD</p><p>簡單來說，如果是第一次去面試的話就會被抓去進行一個程式測驗還有英文考試(因為我沒有附英文檢定成績)，各一個小時。程式測驗的話會有選擇題跟2題上機題，所以要自己控制好時間，難度的話我覺得沒有到特別難;英文測驗的話我覺得比多益稍稍簡單一點(也有可能是因為前陣子瘋狂抱佛腳有用)，不過最後成績只有面試的人看得到就是了。</p><p>然後就是面試～每個人面試的方式都不太一樣，不過大致上的流程就是請你自我介紹(建議可以準備投影片)，然後問你一些問題，可能是從你剛才的自我介紹，也有可能是問你一些OS、計組、或是白板題……就看他們想問什麼。最後問完他們會介紹他們自己是什麼部門，主要在做什麼事情，希望招募intern幫助他們協助或訓練什麼，我覺得最後這部分的資訊量蠻大的，可以拿個紙筆來記錄一下才不會一出來放鬆就忘光光了。</p><p>一面結束如果通過的話，會有更高階的主管來進行二面，面試流程一樣是自我介紹然後主管會再問你一些問題，最後二面通過的話過一兩個禮拜HR就會通知你錄取～</p><p>最後我一定要吐嘲一下，當初第一次在上機考的時候是在竹北，然後當時使用的那台電腦完全不能編譯，所以根本沒辦法debug，我那時還跑去問人資說請問這樣是正常的嗎，人資跟我說應該是</p><p>…好，我就還是硬著頭皮寫完交出去了，後來和面試官面試的差不多之後我還是忍不住詢問說請問上機考不能編譯除錯是希望測驗面試者一次寫對程式碼的能力嗎？然後當時他們就回我說會再去確認一下…</p><p>然後下次再收到面試邀約的時候我就被抓去重新測驗了:|</p><p>而且這次去竹科總部的設備是可以正常編譯、確認測資是否正確的，所以上次應該真的是設備的問題…</p><hr><p>整個過程大概這樣，從三月底開始申請、陸續面試到最後確定要去哪裡的過程其實花了比想像還久的時間，幾乎後半學期都投進去了。在這裡把這段過程記錄下來，很感謝這段時間一起陪我奮鬥的實驗室學長，給予我機會的每個公司以及花時間面試我的每一位，在每一場面試中，都是不同的經歷和體驗，讓我知道自己還有什麼不足以及應該努力的方向。</p><p>最後去哪裡、覺得如何有時間的話會再另外寫一篇來介紹～</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> intern </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[ML]Explainable ML介紹(CAM、Grad-CAM)</title>
      <link href="/posts/df7592be/"/>
      <url>/posts/df7592be/</url>
      
        <content type="html"><![CDATA[<h2 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h2><p>前陣子花了一點時間在看<a href="https://www.youtube.com/watch?v=lnjrn3bF9lA" target="_blank" rel="noopener">李宏毅老師的Explainable ML</a>的部分，裡面有講到explainable ML的部分主要可以分成：</p><ul><li><strong>global explanation(Ex: 機器覺得一隻貓”長什麼樣子”)</strong></li><li><strong>local explanation(Ex: 機器是”看到了什麼”而覺得這是一隻貓)</strong></li></ul><p>而在local explanation的技術中有CAM、Grad-CAM、Grad-CAM++，這篇文章會簡單介紹這三個技術(因為這是一系列的)(好啦只有兩個，最後一個我沒看)</p><p>下一篇會逐步介紹grad-cam的pytorch程式碼是怎麼寫出來的(參考一篇github的source code)。</p> <a id="more"></a><h2 id="什麼是NIN"><a href="#什麼是NIN" class="headerlink" title="什麼是NIN?"></a>什麼是NIN?</h2><p>（更新: 關於NIN可以參考我新出的文章: <a href="https://meetonfriday.com/posts/a151bfa2/">[論文速速讀]Network In Network</a>)<br>不是說好從CAM開始介紹嗎？怎麼又跑出了一個NIN，搞毛啊？</p><p>讓我娓娓道來，首先，<strong>講到CAM的核心技術就不能不提到NIN(Network in Network)，在NIN這篇論文章中提出了GAP(Global Average Pooling)的想法。</strong></p><p>這想法特牛逼啊，想想看在以往的時候CNN最後必須要接上Fully connected layer，礙於neuron數量必須事先指定，所以CNN模型的input size也被固定住了。但是透過GAP，我可以不用去顧慮最後一層的shape長什麼樣子而直接去接FC。</p><p>Ex: Conv層結束後的shape是(7, 7, 64)-&gt;透過GAP後每一個kernel只會有一個value，所以最後的shape會是(1, 64)去接FC(也就是管你Conv最後的shape是7x7還是128x128還是什麼的，都會變成1)。</p><p>也就是說，<strong>有了GAP就再也不用限定input size一定要是多大。此外，GAP也可以增加model的robustness。</strong></p><h2 id="什麼是CAM"><a href="#什麼是CAM" class="headerlink" title="什麼是CAM?"></a>什麼是CAM?</h2><p>CAM，全名class activation map，不過這不是他的論文名稱，有興趣請去看2016的CVPR《Learning Deep Features for Discriminative Localization》。</p><p>講完GAP就可以來講CAM了，那咱們就好奇啦，最後一層的Conv完後，model到底是根據了什麼去做了classifiy呢？我們知道Conv是用來取出圖片的特徵，所以最後一層Conv出來的feature map應該會包含最多的資訊，<strong>那有沒有辦法去看對於某一個類別，到底是哪些重要的feature map導致了model將該圖片分類至該類別呢？</strong></p><p>CAM的想法就是透過GAP，我把最後一層Conv接上GAP後再接FC，這樣最後一層的每個feature map就可以用FC layer的一個neuron來代表，那就可以<strong>把FC的權重就可以想成是對於預測某個類別，每一個feature map的重要性。</strong></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/07/v2-02cffea9152847e7aea65a3d60e72986_hd.jpg" alt="v2-02cffea9152847e7aea65a3d60e72986_hd"></p><p>這樣做之後，只要跑一次forward取得最後一層Conv的feature map，將feature map乘上對應的權重然後reshape成原本圖形的大小，就可以用類似熱力圖的形式呈現出來了。</p><h2 id="什麼是Grad-CAM"><a href="#什麼是Grad-CAM" class="headerlink" title="什麼是Grad-CAM?"></a>什麼是Grad-CAM?</h2><p>好，看過CAM我們知道我們需要兩個東西：<strong>最後一層的feature map和對應的權重。</strong></p><p><strong>CAM有一個缺點，就是模型的架構最後一定要接GAP，不是這個架構的model就不能這樣做</strong>(Ex: VGG)，因為得不出每張feature map的權重(對於feature map還是可以拿得到的)。</p><p>所以Grad-CAM就誕生了，我們可以不用GAP，而是<strong>透過back propagation去計算對於某一個類別，最後一層feature map上每個pixel的權重，然後對這些pixel算一個加權平均就可以得出該feature map的權重。</strong></p><p>聽說(?)經過嚴格的推倒，CAM和Grad_CAM實際上是等價的。 公式如下，小心服用： <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/07/v2-d5277f8d3bbdd715f0754a0c35e0ec46_hd.jpg?w=600" alt="v2-d5277f8d3bbdd715f0754a0c35e0ec46_hd"> <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/07/v2-a0f902e9524aaf247a8f6ecac1791c7e_hd.jpg?w=600" alt="v2-a0f902e9524aaf247a8f6ecac1791c7e_hd">   [Grad-CAM++?] 顧名思義，他就是Grad_CAM的進階版，好，結束。 好啦自首，這個我沒有仔細讀QQ，所以鞭小力一點… 看懂Grad_CAM++就知道，其實還是透過每個pixel做加權平均得到每張feature map的權重，不過如果不做平均的話呢？就是直接針對每個pixel去看他的影響程度…是這樣嗎…..可能吧(?) 放棄…有興趣還請去看論文或其他人的介紹吧xD  </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://bindog.github.io/blog/2018/02/10/model-explanation/" target="_blank" rel="noopener">凭什么相信你，我的CNN模型？（篇一：CAM和Grad-CAM)</a></li><li><a href="https://zhuanlan.zhihu.com/p/39822145" target="_blank" rel="noopener">利用可视化方法直观理解CNN</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>碩一生活總結</title>
      <link href="/posts/79132bb1/"/>
      <url>/posts/79132bb1/</url>
      
        <content type="html"><![CDATA[<p>這是一篇日常文。</p><p>不要懷疑，這才是正常的，這個部落格從來就不是技術部落格啊xD</p><p>其實最初在寫網誌的時候並不是只想分享純技術相關的事情，也想當個吃吃喝喝到處玩樂的部落客，不過不知道為啥總是只發一些奇奇怪怪的文章。</p><p>碩士生活偶爾會覺得很漫長，但突然停下腳步回顧時卻會覺得時間其實過得很快，就用一篇文章來回顧一下自己這一年都在如何耍廢吧xD</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/06/64661456_2806766249339608_3157311664236265472_n-e1561111469847.jpg?w=600" alt="64661456_2806766249339608_3157311664236265472_n"></p><p>(2019新竹藝術燈節的『蓋亞』)</p><p> (碩二的悲慘續集出爐摟，想了解的可以看: <a href="https://meetonfriday.com/posts/2c756e7/">碩二生活總結</a>)</p><a id="more"></a><h2 id="八點過後找不到晚餐吃"><a href="#八點過後找不到晚餐吃" class="headerlink" title="八點過後找不到晚餐吃"></a>八點過後找不到晚餐吃</h2><p>在上學期剛上新竹時，對於新環境其實還不是很適應，記得當初最令我印象深刻的事情：</p><p>那時我剛上來一個禮拜還沒有機車，然後住在博愛校區，晚上八點想說去學校旁邊的一條街買個晚餐好了(這條街有一些吃的店家)，然後我發現沒有一家是開的。</p><p>在晚上八點。</p><p>這是我對於新竹的第一個印象。</p><h2 id="新竹風靠北大"><a href="#新竹風靠北大" class="headerlink" title="新竹風靠北大"></a>新竹風靠北大</h2><p>這原本就知道了，不過還是很誇張，最誇張的幾次是我騎在馬路上看著前面一台50cc從機車道被風吹的飄移到了汽車道去。</p><p>還有一次是我在高鐵站準備搭高鐵，然後突然一陣風來，不誇張，當時我真的有覺得自己快要飛起來了。</p><h2 id="目前的新竹美食前三名"><a href="#目前的新竹美食前三名" class="headerlink" title="目前的新竹美食前三名"></a>目前的新竹美食前三名</h2><ol><li>小七 </li><li>麥當勞 </li><li>學校的一間燒臘店</li></ol><p>據說學校內的小七曾拿過全國的營業額冠軍呢。</p><p>然後燒臘真的是我們學校的良心啊，每次吃飯時間那個人都會從二樓排到樓梯去，甚至有排到一樓過的。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/06/64529631_2806795282670038_4409151234947153920_n.jpg?w=600" alt="64529631_2806795282670038_4409151234947153920_n"></p><p>(學校內很好吃的燒臘飯)</p><p>然後講個小故事，上學期時這家燒臘店會在每個禮拜二四推出限量的脆皮燒肉飯，在某個禮拜二的因緣際會下吃了一次，不吃還好，一吃發現不得了（單押），真是驚為天人啊！原來新竹也是有美食的！！毅然決定以後二四有機會就來吃這個。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/06/64705232_2806847775998122_656188788564819968_o.jpg?w=600" alt="64705232_2806847775998122_656188788564819968_o"></p><p>(驚為天人的脆皮燒肉)</p><p>但是無奈的是我二四都有課到中午，每次去的時候早就賣完了。</p><p>每個禮拜都是這樣的情形讓我很憤怒，大家都知道對食物的怨念是很可怕的，於是我在某個禮拜二翹了第三節的課，11點下課就衝去準備買脆皮燒肉……</p><p>但還是賣光了。</p><p>到底？？？？？？</p><p>結果，我上學期只吃了大概兩次的脆皮燒肉。</p><p>什麼，你覺得我下學期沒課就可以吃了嗎？他下學期就沒賣了。</p><p>於是它就這樣成為了我記憶中的新竹美食之一，也是我在新竹的小小遺憾。</p><p>以上大概是生活篇的感想，接下來講點學業跟工作上的：</p><h2 id="北部活動真的比南部多"><a href="#北部活動真的比南部多" class="headerlink" title="北部活動真的比南部多"></a>北部活動真的比南部多</h2><p>這其實蠻有感的，北部一堆有的沒的活動，動不動就跳出來讓你報名，報到你不想報了還報不完。</p><p>還記得剛來學校的時候，學校的社團博覽會超級大，還請了我最喜歡的欸欸福由～ <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/06/64902087_2806850412664525_7100874528602980352_o.jpg?w=600" alt="64902087_2806850412664525_7100874528602980352_o"></p><p>(Afu本人&lt;3)</p><p>或是<a href="https://meetonfriday.com/posts/c119e85f/">[3/24] SITCON2019</a>這篇，有興趣的可以去看看～</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/06/64936833_2806799206002979_6057121377444429824_o.jpg?w=600" alt="64936833_2806799206002979_6057121377444429824_o"></p><p>(SITCON 2019)</p><h2 id="上學期都在修課地獄"><a href="#上學期都在修課地獄" class="headerlink" title="上學期都在修課地獄"></a>上學期都在修課地獄</h2><p>碩士兩年要修滿24學分才能畢業。</p><p>原本想說大學動不動就修個20幾學分，碩一上學期只修了13學分應該還好吧，結果被實驗室學姊問的時候，學姊聽到很驚訝的說：”你怎麼修這麼多喔”</p><p>當下還沒有什麼感覺，不過寫作業還有期中期末的時候就深刻體會到了。</p><p>尤其是碩士學校的作業跟大學學校的loading差很多，原本我以為上課認真聽回去複習一下就寫得出來，但是我發現我錯了，老師為了要讓我們最大限度學到東西，上課的部分跟作業的部分往往是互斥的啊QQ</p><p>印象中最崩潰的一次是某天在lab研究一個影像課程的作業怎麼寫，研究到半夜兩點多還是沒有頭緒，當下巨崩潰，人生中第一次浮現我是不是要退選比較好的念頭(雖然後來還是很崩潰的完成了)。</p><h2 id="下學期則是在面試地獄"><a href="#下學期則是在面試地獄" class="headerlink" title="下學期則是在面試地獄"></a>下學期則是在面試地獄</h2><p>當時猶豫了一陣子之後，在教授許可後開始準備一些實習的面試，大概從三月初開始準備到六月初整個過程才結束，真的也是巨崩潰。</p><p>打從確認要準備申請實習開始，就認真的開始準備自己的簡歷、複習大學的一些科目、狂刷leetcode找回寫題目的手感，甚至還跟lab要準備投正職的學長組團開讀書會一起準備。每天就是重複這些事情搞得自己很心累，尤其是當你還沒有任何一家給你確定的offer時，那種忐忑不安的心情很煎熬。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/06/64633557_2806825252667041_2308198970574241792_n.jpg?w=600" alt="64633557_2806825252667041_2308198970574241792_n"></p><p>(和學長每週開個讀書會討論各種問題)</p><p>然後有些公司又要一直來來去去好多次，保安！可以這樣面了又面面了又面嗎！(這句沒有抱怨的意思，我只是想講出那句台詞而已xD)</p><p>幸好最後努力有成功換到結果，算是比較令人欣慰的，不然我真的會覺得自己這學期浪費了。</p><p>關於實習的申請跟準備過程過段日子我會寫大概一到兩篇來分享如何準備、可能會被問到的問題跟最後取捨要去哪一家的心路歷程，如果我暑假下班後還有體力的話啦xD</p><p>歐對了，在組員們的carry下這學期我終於把學分都修完啦！！，下學期開始就要來努力做研究了…希望研究也能順順利利</p><hr><p>除了上面這些其實還有很多想說，可是似乎又說不出什麼所以然來。</p><p>其實，這一年可以說是自己最茫然也最撞牆的一年，好像有朝著目標前進，但問自己目標是什麼卻又說不出來，希望碩二開始能夠重新振作～</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SITCON 2019</title>
      <link href="/posts/c119e85f/"/>
      <url>/posts/c119e85f/</url>
      
        <content type="html"><![CDATA[<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/main-logo.805726b.png" alt="main-logo.805726b.png"></p><p>想當初上一次參加SITCON是…兩年前吧…在高雄的時候實在是太遠了也不是想來就來，不過現在在新竹所以沒關係:|</p><p>地點一樣舉辦在中研院，從南港展覽館捷運站(不是南港)出來後走到對面搭公車就可以到中研院站，然後再走進去就是了。有很多班公車都會到中研院站所以不用太擔心。</p><p>如果真的不知道怎麼走的話，其實當天捷運站出來看到一堆學生們就跟著他們走就對了，應該都可以順利到達目的地(我當天中研院裡面就是這樣走的，連導航都沒開xD)。</p><a id="more"></a><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_084428.jpg" alt="dav"></p><p>報到的時候使用一個APP就可以避免大家都在排隊等報到的情況，很方便。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/screenshot_20190324-084616.jpg" alt="Screenshot_20190324-084616.jpg"></p><p>SITCON今年的主題是”Algorithm in a Box”，也就是開箱演算法。我蠻喜歡這屆的風格設計的，簡潔風，不過仍然很好看。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_150702.jpg" alt="IMG_20190324_150702.jpg"></p><p>很喜歡這種年齡相仿的學生們為了資訊活動一起聚起來的感覺，跟在學校比較不同的是，會來的幾乎都是真的具有興趣或熱忱的朋友，彼此可以一起交流經驗跟討論問題，這種感覺會讓人覺得很棒。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_085357.jpg" alt="sdr"></p><p>大體上活動都跟我當初參加差不多，不過提問方式改成可以在網路上匿名發問了(不知道什麼時候開始的)，我覺得這是個還不錯的方式，不過有時候會有一些蠻有趣的狀況…像是下面這樣：</p><p>“媽！我上新聞啦”</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_095942.jpg" alt="IMG_20190324_095942.jpg"></p><p>前兩個時段的議程是共通的，其他演講廳會同步直播，有經驗的就會知道其實會在第二場共通議程快結束前先離開(或是直接在第二場時去其他地方看直播)，這樣才能在之後想聽的小議程搶到位置。由於其他場地相較R0小很多，如果太晚去你就會看到如下畫面：</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_125131.jpg" alt="IMG_20190324_125131.jpg"></p><p>此外，在外頭走廊的白板上面都會有一些很有趣的題目讓大家自由發揮回答，比方說…</p><p>“你都用什麼CPU?”</p><p>“6.YAMAHA”</p><p>…….?????</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_100601.jpg" alt="dav"></p><p>這張圖我想是在評論前陣子的萬物皆O(1)的討論(想知道的請去臉書python taiwan社團爬文)xD</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_100730.jpg" alt="dav"></p><p>然後來參加年會就是要吃東西，吃也是很重要的一環怎麼可以不介紹！！！</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_141345.jpg" alt="sdr"></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_152031-1.jpg" alt="IMG_20190324_152031.jpg"></p><p>最後今年有真實境矩陣的活動，集滿廠商的章就可以換大禮包，可以換到手提袋(用買的要200元)。</p><p>我很喜歡SITCON今年的手提袋，覺得真的好看～還有衣服我也很喜歡，所以我也買了一件，紙膠帶覺得有點小貴，不過還是買了，支持一下。</p><p>(之後再補圖片…如果沒忘記的話)</p><hr><h2 id="議程分享"><a href="#議程分享" class="headerlink" title="議程分享"></a>議程分享</h2><p>首先小抱怨一下，一整天下來網路有點不給力阿QQ 想打共筆也是打得很辛苦，一直斷斷續續的，後來果斷連中研院自己的網路而不用SITCON提供的站點，才終於順暢了一點。</p><p>接下來就分享一下我聽了哪些的議程，以及一些心得，以下都只是簡述議程內容跟心得，如果想知道更多資訊歡迎到最下面看共筆唷！</p><h3 id="議程分享-人工智慧的過去現在與未來"><a href="#議程分享-人工智慧的過去現在與未來" class="headerlink" title="議程分享 - 人工智慧的過去現在與未來"></a>議程分享 - 人工智慧的過去現在與未來</h3><p>摁…這是SITCON的第一場議程，主要是在講人工智慧的一些科普背景，因為我本來就是走相關領域的，所以聽得有些無聊xD</p><p>不過我還是很認真地幫忙做共筆了(然後一邊被網路搞得很崩潰)。</p><h3 id="議程分享-商管、演算法、商管"><a href="#議程分享-商管、演算法、商管" class="headerlink" title="議程分享 - 商管、演算法、商管"></a>議程分享 - 商管、演算法、商管</h3><p>這場的議程就相對有趣多了，講者透過一些例子來分享商管策略跟演算法之間的一些連結關係，講者也分享了許多實際的例子：</p><ol><li>自動化的(競爭標價)系統(為什麼網路上有一本書的訂價永遠是別人的1.27倍？)、</li><li>套利(arbitrage)(無套利條件、高頻交易、新聞交易…)、</li><li>決策支援(航空公司人員排班:1000名地勤人員在桃園機場服務，如何排班?)</li></ol><p>最後也給了大家一些結論跟建議：</p><ol><li>不是每個問題都一定要用很複雜的演算法來解決，搞不好老闆只是想要最簡單的解決方法。</li><li>同時不要瞧不起老闆，”不要因為自己比較會寫程式就覺得老闆是廢物，老闆常常是廢物、也常常不是”…這句話是講者說的，不是我說的，老闆止步。</li></ol><h3 id="議程分享-Rex-算法初探"><a href="#議程分享-Rex-算法初探" class="headerlink" title="議程分享 - Rex 算法初探"></a>議程分享 - Rex 算法初探</h3><p>我以為我可以越級打巴洛古，直到進了副本才發現這是炎魔團…</p><p>初心者是沒辦法發表什麼心得的…留給高手們吧QQ</p><h3 id="議程分享-手把手玩-PE-Injection-lt-3"><a href="#議程分享-手把手玩-PE-Injection-lt-3" class="headerlink" title="議程分享 - 手把手玩 PE Injection &lt;3"></a>議程分享 - 手把手玩 PE Injection &lt;3</h3><p>這場我因為記錯吃下午茶時間跟都在逛樓上廠商攤位……所以晚了一點才進去…明明只有十分鐘的演講我只聽了一半左右吧QQ</p><p>主要是在介紹PE file的結構，然後如何透過修改PE file去改寫程式，雖然時間很短但是我覺得講的很清楚很棒～</p><p>這好像也是少數我有看到有梗圖的簡報(簡報還是要有梗圖才好玩嘛xD)，而且這場其實也是我當初列為最想聽的清單之一(但是遲到了QQQQQ)。</p><h3 id="議程分享-AI-for-the-Art-in-a-Box"><a href="#議程分享-AI-for-the-Art-in-a-Box" class="headerlink" title="議程分享 - AI for the Art in a Box"></a>議程分享 - AI for the Art in a Box</h3><p>這場也是10分鐘小議程，主要是講說GAN是什麼(有人念GAN，也有人念GAN，不過我都念GAN)：就是一個警察一個壞人，兩個在那邊互相進化，最後兩個都超進化的故事。</p><p>以及怎麼透過GAN怎麼做到一些藝術上的應用(如何產生表情微奇怪的二次元妹子)，因為時間關係沒辦法講太細節的部份，不過對於還沒接觸過GAN的朋友們應該會覺得很酷～</p><h3 id="議程分享-TMML真相靠自己─You-are-Fake-News！"><a href="#議程分享-TMML真相靠自己─You-are-Fake-News！" class="headerlink" title="議程分享 - TMML真相靠自己─You are Fake News！"></a>議程分享 - TMML真相靠自己─You are Fake News！</h3><p>這場也是我當初最想聽的議程之一，不過不得不說講者真的很厲害…各種意義上……</p><p>主要是透過三種open dataset，包含了8種不同類別的文章去做一些語意分析(資料視覺化)、特徵萃取(Chi-Square, Log likehood, EMI, TF-IDF)，資料前處理，然後透過Dilated CNN + GRU去判斷一篇文章是不是假新聞(對於假新聞的ground truth，似乎是事前透過了supervised learning訓練出來…這部分我沒聽的很懂)。</p><p>雖然40分鐘還是沒辦法把所有東西都講得很細，不過還是收穫很多。</p><p>有趣的是，最後提問的時候蠻多有趣的問題，像是”到底是嗑了什麼才能做出這種東西”、”中天新聞表示不喜歡這個主題”之類的，讓全場都哄堂大笑。</p><h3 id="議程分享-從學校到業界，工程師作為職業的現實"><a href="#議程分享-從學校到業界，工程師作為職業的現實" class="headerlink" title="議程分享 - 從學校到業界，工程師作為職業的現實"></a>議程分享 - 從學校到業界，工程師作為職業的現實</h3><p>這場議程主要是在介紹業界跟學界的一些差別，作為一個即將要出社會的人覺得心有戚戚焉QQ</p><p>可惜的是整場聽下來有點小悶…</p><hr><h2 id="light-show"><a href="#light-show" class="headerlink" title="light show"></a>light show</h2><p>這應該是整天的重點活動了，每人3分鐘的時間去講自己想要講的東西，沒有講完的話就會被Timeout，為此大家的講話速度都自動倍速了起來。不過儘管如此，整場只有一個人被Timeout(主辦表示難過，做了這麼辛苦的圖只用到一次)。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_173706.jpg" alt="IMG_20190324_173706.jpg"></p><p>此外大家好像都為投影而苦惱不少，有些人光是在喬投影問題就花了半分鐘了…最後講不完怎麼辦呢？</p><p>直接跳到最後一頁就對了。</p><p>“這個故事告訴我們，投影片不用做太多，反正最後也用不到”…這也是主持人講的xD</p><h2 id="閉幕"><a href="#閉幕" class="headerlink" title="閉幕"></a>閉幕</h2><p>用兩張照片來總結今年的SITCON：</p><p>…10G那位同學是怎樣 開直播嗎xD</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_175015.jpg" alt="ozdfov"></p><p>下面這張是大家填寫最有代表性的關鍵字，不得不說冰淇淋那個： 奧義科技真的是佛心來的，那天的冰淇淋真的是幹你娘發爆，只要有人過去就是發一個冰淇淋，讓大家吃好吃滿，聽說他們公司的願景就是讓每個資訊年會都有冰淇淋吃xD</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/img_20190324_175131.jpg" alt="IMG_20190324_175131.jpg"></p><h2 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h2><p>覺得能定期參加這些活動除了可以認識更多同年齡層的朋友外，在這個氣氛下能夠提升自己對於開發程式的熱情(不然在學校都快被消磨光了)，當心很累的時候還是可以來補個血的。不過幫SITCON呼籲一下，票是免費的真的夠佛心，也希望大家不要占著票不來，這樣很母湯～</p><p>今年聽完也給了自己一個小小目標，如果有機會希望自己明年也可以投個10分鐘小議程試試看(?)，在學生的最後一年給自己一個小任務這樣。</p><p>最後附上我當初做的筆記(部分是來自共筆的整理，會比較多紀錄我有興趣的部分)跟共筆、官網連結，有興趣的就上去看看瞜～</p><ul><li><a href="https://hackmd.io/FOG2p2x1RIiZTr_Bpb_RYQ" target="_blank" rel="noopener">SITCON 2019 R0 共筆</a></li><li><a href="https://hackmd.io/wT-LCYjMRcGB283k4imuUw" target="_blank" rel="noopener">SITCON 2019</a></li><li><a href="https://sitcon.org/2019/" target="_blank" rel="noopener">官方網站</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> conference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>交大就職博覽會心得&amp;參加攻略</title>
      <link href="/posts/a1b74789/"/>
      <url>/posts/a1b74789/</url>
      
        <content type="html"><![CDATA[<p>好久沒有寫非技術類的文章了喔…上次寫心得文不知道是幾年前的事情了…感動QQ</p><p>這次要分享的是交大的就職博覽會，大學校的就博真的很值得去，每年都會有超多企業會來學校徵人，甚至會有現場媒合面試的活動，可以快速地和主管談話了解公司，也因此會有很多外校的人也來一同參與。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/01.jpg?w=2048" alt="¦C¦L"></p><p>既然碩士都來唸交大了一定得參加的，所以雖然還是碩一我還是去了～</p><p>這篇文章會分享我的參加心得，然後講一下我覺得事前應該要準備什麼比較好(因為我是第一次參加，當初不知道大概長怎樣或是要準備什麼東西還上網查了一下…不過好像沒看到什麼人在分享)。</p><p>想看需要哪些事前準備&amp;攻略的就直接滑到最下面～</p><a id="more"></a><h2 id="交大就博-校園徵才說明會"><a href="#交大就博-校園徵才說明會" class="headerlink" title="交大就博 - 校園徵才說明會"></a>交大就博 - 校園徵才說明會</h2><p>交大就博其實不只有一天而已，而是一連串的活動，為期大約一個月的時間，這段時間主辦單位幾乎每天都會和一些企業合作舉辦說明會給學生參與，先放張活動時程給大家瞧瞧兒。許多公司們都不會放棄這個能向高材生們(?)宣傳自己公司的機會，而學生也會想要知道自己到底適不適合這間公司而來，還有為了吃…</p><p>對你沒看錯，就是為了吃，每場說明會都會有一些東西提供給前幾位報到的領取，還記得我那幾個禮拜吃得很開心，尤其是那個泰式奶茶跟泰式炒板條(流口水)，有些還會送洗髮乳跟乳液跟…衛生棉，我還記得我參加第一場說明會的時候那個服務同學問了我</p><p>“摁…贈品有乳液跟衛生棉……你要乳液嗎？”</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/54220594_2628309827185252_8639832023101341696_n.jpg" alt="54220594_2628309827185252_8639832023101341696_n.jpg"></p><h2 id="交大就博-就業博覽會"><a href="#交大就博-就業博覽會" class="headerlink" title="交大就博 - 就業博覽會"></a>交大就博 - 就業博覽會</h2><p>不過對大部分的人來說，最盛大的活動還是在就業博覽會當天，只有一天的時間，各家企業會來校園內擺攤搶人才們，下面就看圖片講話摟～</p><p>我一大早就起床去了，然後先去服務台領取一些資訊，像是地圖(含集點卡)，集點卡的功用是你去廠商攤位聊過後就可以請他們幫你貼貼紙，然後集滿幾點就可以換扭蛋抽抽樂卷跟摸彩卷，不要小看這集點卡，摸彩跟抽抽樂的獎品都很好的(Iphone、冰箱還有…一大箱的衛生棉……)。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/53229486_2628310133851888_9201814977520861184_n.jpg" alt="53229486_2628310133851888_9201814977520861184_n"></p><p>然後接下來找廠商的時候因為我蠻認真在聽的就忘記拍照了，不過大概就是你經過他們的時候，通常都會有他們的人來發DM給你，或是你也可以主動去詢問他們關於他們公司的相關資訊，如果不知道問什麼的話…起手式不外乎就以下幾種：</p><p>“那個我想了解你們公司”、”請問你們公司有在招募<em>_</em>的缺嗎?”(像我就是問暑期實習)、”可以請你介紹一下你們公司嗎?”……之類的，只要開個頭之後剩下的就交給他們，他就會很熱情的幫你接話，會來攤位的都是話題高手，所以完全不用擔心會尷尬xD</p><p>歐對了，剛剛說到大家會一直塞東西給你，塞塞塞然後就stackoverflow了…不是，那是不是要自己帶個包包呢？</p><p>不用，你帶了你包包也塞不下，除非你帶的是行李箱。</p><p>所以攻略方法就是先去找到能幫你解決這個問題的好企業xDDD，有些企業會提供你超大袋子讓你裝東西，接下來就可以開心拿著大袋子一邊逛一邊把其他家企業的東西都塞進去摟！</p><p>這張圖是最後我逛完得到的所有物品們，有些其實還不錯，像我就從Mixerbox拿到了一個木質的小隨充、環保飲料提袋、忘記哪一家的敲敲棒以及…某家的吉祥物(?)</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/53553177_2628310327185202_182788864610402304_n.jpg" alt="53553177_2628310327185202_182788864610402304_n"></p><p>在找攤位的過程中，如果對該企業有興趣就可以把資料留給他們，也因為這樣我其實事前有準備了10份紙本履歷，不過最後其實只交出去了一份，其他都是現場填寫聯絡方式。我覺得部分的原因可能是因為就博企業主要找的都是應屆畢業生(也會宣傳暑期實習，不過大部分都是告知你，請你上網登記)，另外很大的原因就是他們都改成電子化了，所以會請你直接用現場的平板填他們的電子履歷，然後他們會在事後透過你的聯絡方式聯絡你，像是請你補上完整的履歷、成績單之類的……。</p><p>那天我問過其中一家的HR說”所以你們現在其實都不太收紙本履歷摟？”，然後他是說因為收到的紙本格式也跟他們家的履歷格式不合，而且他們收了之後還是要手動把資料key in進去，所以還是會請大家都現場填。</p><p>最後我在當天活動快結束前(活動只到4點)總共搜集了30家的企業貼紙(我很認真的聽完後才跟他們領取貼紙的)，然後領了兩張扭蛋抽獎卷跟3張摸彩卷的樣子。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/53349961_2628310010518567_5155184369907793920_n.jpg" alt="53349961_2628310010518567_5155184369907793920_n"></p><p>然後匆匆忙忙投完摸彩箱，跑去扭蛋排隊，排呀排排呀排…</p><p>結果到我的前兩個的時候扭蛋都被扭光了QQ只能拍一張當作紀念，下次真的得早一點來領獎。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/53516187_2628310347185200_2233023277524058112_n.jpg" alt="53516187_2628310347185200_2233023277524058112_n"></p><p>事後整理完的各家DM，我覺得這些資訊都很寶貴，如果是一個真的在找正職或實習的人，一天下來你可以拿到一堆企業的DM，整理一下就可以知道各家投遞履歷方式、薪資待遇以及企業介紹……等資訊。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/54197785_2628309897185245_421002297900793856_n.jpg" alt="54197785_2628309897185245_421002297900793856_n"></p><p>然後這是4點的時候，在摸彩的情況，滿滿的人都在期待大獎iphone是他的(我站的位置還不是最後面)。</p><p>然後我沒抽中。一個都沒有。所以這裡沒有什麼好說的。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/03/54236892_2628310527185182_1235179104806895616_n.jpg" alt="54236892_2628310527185182_1235179104806895616_n"></p><h2 id="交大就博-攻略方式"><a href="#交大就博-攻略方式" class="headerlink" title="交大就博 - 攻略方式"></a>交大就博 - 攻略方式</h2><p>最後提供我今年參加完後，覺得以後如果要參加的話可以怎麼樣比較好：</p><ol><li>早點來&amp;控制好時間：這真的很重要，我早上10點就去了，連午餐都沒吃(可惡當天其實有攤販來賣吃的…沒吃到好可惜QQ)待到下午4點結束前一刻才逛完所有廠商，而且最後一個小時我還是驚覺時間不夠才用很趕的速度逛，只去聽我比較有興趣的廠商。並且要好好控制時間，因為這麼多廠商一定有你比較中意的，不要因為想一家一家逛而導致真正想要的卻沒什麼逛到。</li><li>紙本履歷還是可以準備：但不用太多份，如同我上面說的，現在很多都用電子化履歷，這樣他們也比較不會麻煩。不過還是有會收紙本的企業，所以還是建議準備幾份以備不時之需。</li><li>一開始先去服務台&amp;找有大袋子的廠商：服務台可以問一下今年相關的活動細節，找個大袋子讓你一路逛爽爽。</li><li>對於真正有興趣的企業，事前上網看一下他們是否有發佈相關訊息：舉例來說好了，今年的聯詠就是採事前預約制，預約之後就會有相關部門的主管來跟你聊，我覺得這樣的好處是你能夠更加深入的理解他們公司，不過如果事前沒有預約的話就沒辦法去他們攤位聊天。又或是有幾家公司有現場面試的，他們就會上網公告請你事前先去完成某些檢測並登記現場面試。這些資訊都要自己事前留意一下，才不會錯過這些機會。</li><li>抽獎跟扭蛋早點去：不然會像我一樣最後沒得抽QQ而且我很努力逛了才集了30個，我發現有人把那個集點卡上全部集滿的…有夠傻眼哈哈哈</li><li><p>回去後把資料整理一下：在逛的時候基本上是收到什麼就塞進去，如果回去後不整理，那他們就會變成一疊不會再被拿出來看的雜物們，好好整理一下把有興趣的公司挑出來，對未來找工作都會很有幫助的。</p><p>最後，最近在找暑期實習，實驗室學長姐們也面臨找正職的壓力，我知道大家也都很辛苦或壓力很大，在這裏祝大家找工作順利～都進到一個好公司工作～～</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nctu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Pytorch]逐步解釋ResNet34程式碼</title>
      <link href="/posts/fb19d450/"/>
      <url>/posts/fb19d450/</url>
      
        <content type="html"><![CDATA[<h2 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h2><p>今天想來介紹如何用Pytorch寫出一個ResNet34，網路上有很多類似的文章，不過我一開始看的時候還沒辦法很理解為什麼Module的參數要這樣設?這裡的shortcut又是什麼意思?……於是後來終於理解後，想要寫一篇來介紹ResNet code，順便確認自己是有理解的。</p><a id="more"></a><h2 id="ResNet簡介"><a href="#ResNet簡介" class="headerlink" title="ResNet簡介"></a>ResNet簡介</h2><p>他是2015年ILSVRC的冠軍，<strong>特色就是提出了一個可以到處插進別人家module的residual block，該block的設計有效的解決了當網路層過深的時候資訊無法有效地往下傳遞的問題(因為一直不斷地被壓縮)</strong>，想知道細節的話就來看<a href="https://meetonfriday.com/posts/18a141c2/">[DL]淺談CNN在Object Classification上的各種架構</a> 這篇吧，我把過去每一年有代表性的CNN架構都做了粗淺的介紹。</p><p>所以這裡就不細說了，我就只把ResNet34跟Residual Block的架構提出來，到時候可以跟code相互對照。你問為什麼叫ResNet34?因為他只有34層阿孩子(當初論文提出了許多不同層數的ResNet)。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/02/resnet1.png" alt="resnet1"></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/02/residual.png" alt="residual"></p><h2 id="Code解析"><a href="#Code解析" class="headerlink" title="Code解析"></a>Code解析</h2><p>source code的部份可以參照torchvision裡面內建的code來研究，不過在這裡我推薦這本書<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">深度学习框架PyTorch：入门与实践</a>，作者人很好的將書本內容跟code都放上去了，想好好研讀pytorch的可以去好好看一看。 我也參照了這個書籍作者在第四章的最後面練習寫了ResNet34，並加上一些註釋希望可以更加清楚的理解，code放在<a href="https://github.com/john850512/Machine_Learning/blob/master/Pytorch/Pytorch_Practice_ResNet34.ipynb" target="_blank" rel="noopener">github</a>上，下面也會擷取部分的code來進行講解。 首先想先講講最近練習Pytorch的過程中很大的感觸是：Pytorch和Keras很大的不同點在於CNN的寫法，<strong>寫Pytorch的人必須要很夠很清楚的知道Input/Output Channel、Kernel、Padding、Stride彼此之間的關係。</strong> </p><p>不過其實有個公式可以幫助你去推出這些參數該用什麼： </p><script type="math/tex; mode=display">OutSize = \frac{floor((InSize + 2 * padding - dilation*(kernel -1)-1)}{stride + 1}</script><p>不管是Convolution或是Maxpool都可以套用這個公式來得到shape的關係，至於channel的關係則是透過out_channel的資訊就可以得到。 好，接下來要開始一段一段的介紹code在幹嘛了： </p><h3 id="Code解析-ResidualBlock"><a href="#Code解析-ResidualBlock" class="headerlink" title="Code解析 - ResidualBlock"></a>Code解析 - ResidualBlock</h3><p>觀察上面的架構圖會發現其實ResidualBlock都是兩層的Conv組成，所以為了方便我們可以先把這兩層合併成一個Module，方便後面呼叫。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channel, out_channel, stride=<span class="number">1</span>, shortcut=None)</span>:</span>         </span><br><span class="line">    super(ResidualBlock, self).__init__() </span><br><span class="line">    self.left = nn.Sequential(   </span><br><span class="line">      nn.Conv2d(in_channel, out_channel, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="literal">False</span>), <span class="comment"># bias=False是因為bias再BN中已經有了，如果stride=2則shape會變成一半 </span></span><br><span class="line">      nn.BatchNorm2d(out_channel), nn.ReLU(), </span><br><span class="line">      nn.Conv2d(out_channel, out_channel, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">False</span>), <span class="comment"># shape前後仍然相同 </span></span><br><span class="line">      nn.BatchNorm2d(out_channel), </span><br><span class="line">    ) </span><br><span class="line">    self.right = shortcut <span class="comment"># 根據情況是否做出增維或是縮小shape </span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> </span><br><span class="line">    out = self.left(x) </span><br><span class="line">    residual = x <span class="keyword">if</span> self.right <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> self.right(x) </span><br><span class="line">    out = out + residual </span><br><span class="line">    out = F.relu(out) </span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>從第一個Conv開始講起: <code>nn.Conv2d(in_channel, out_channel, 3, stride, 1, bias=False)</code></p><ul><li>首先，為什麼這裡(包含之後)的Conv bias=False呢? <ul><li><strong>因為BatchNorm中就提供了Bias的效果，所以這裡就不需要了</strong></li></ul></li><li>第三個參數是kernel size，設置成3是因為作者的架構設定，不過為什麼stride要使用變數和padding=1?<ul><li>請先看一下架構圖，會發現第一個Residual Block沒有【/2】這個符號，但後面有一些Block有，【/2】代表要將圖片的shape縮小一半，所以為了應付這兩種不同情況，才把stride弄成變數。 不妨可以自己推推看上面的公式，在InSize=x、kernel=3和padding=1(dilation=1，這個先不用管，這裡不會用到)的情況下，stride=1和stride=2的差別會是什麼(答案是stride=1時shape不變，stride=2時shape會縮小一半)。</li></ul></li></ul><p>官方的sourcecode上是寫<code>nn.ReLU(inplace=True)</code>，和<code>nn.ReLU()</code>差別在哪?  </p><ul><li>inplace會直接改變原本tensor內的值，節省記憶體的存取，不過在某些情況(?)可能會有問題，實際上有沒有加並不會影響結果</li></ul><p>裡面的shortcut是什麼? </p><ul><li>裡面有 <code>self.right = shortcut</code> 和 <code>residual = x if self.right is None else self.right(x)</code> 這兩行，不過並沒有定義shortcut(他是一個參數)，那shortcut到底是什麼? </li><li>剛剛也說了，在架構圖中，有時候會對圖片做縮小shape或是改變圖片維度的操作，但<strong>為了能夠把residual block左右邊加起來，必須確保兩邊的資料維度和大小是相同的，shortcut會在另一段被定義，不過說白了他其實就是1d的cnn，用來改變維度或形狀用的Conv</strong>。 如果呼叫ResidualBlock呼叫時沒有從外面傳shortcut參數，代表該次block不需要對資料作維度的改變，那就會用原本的資料(<code>residual = x if self.right is None...</code>)。</li></ul><h3 id="Code解析-ResNet的init"><a href="#Code解析-ResNet的init" class="headerlink" title="Code解析 - ResNet的init()"></a>Code解析 - ResNet的<strong>init</strong>()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">1000</span>)</span>:</span> </span><br><span class="line">    super(ResNet, self).__init__() </span><br><span class="line">    self.pre_layer = nn.Sequential( </span><br><span class="line">        nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>, bias=<span class="literal">False</span>), <span class="comment">#為了使shape變一半，stride必須是2，在固定kernel=7下由公式推得padding=3 </span></span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>), </span><br><span class="line">        nn.ReLU(), </span><br><span class="line">        nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>) , <span class="comment">#為了使shape變一半，stride必須是2，在固定kernel=3下由公式推得padding=1 </span></span><br><span class="line">      ) </span><br><span class="line">      self.layer1 = self._make_layer(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>) </span><br><span class="line">      self.layer2 = self._make_layer(<span class="number">64</span>, <span class="number">128</span>, <span class="number">4</span>, stride=<span class="number">2</span>) <span class="comment"># 對照架構圖，第二段後每次都會將shape再度縮小一半 </span></span><br><span class="line">      self.layer3 = self._make_layer(<span class="number">128</span>, <span class="number">256</span>, <span class="number">6</span>, stride=<span class="number">2</span>) </span><br><span class="line">      self.layer4 = self._make_layer(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>pre_layer的Conv參數(3, 64, 7, 2, 3)怎麼來的?</p><ul><li>3是input，這是第一個Conv，所以input的維度就是圖片的RGB channel=3，64(output)跟7(kernel)是架構設定好的，stride=2是因為這邊要將圖片的shape縮小一半(注意到架構圖中這一層有【/2】)，那這時候padding要用多少呢?套一下公式(OutSize=x/2、InSize=x、padding=y)就可以求出padding=3。</li><li>同理，pre_layer的MaxPool2d也是一樣，kernel=3是預設，為了縮小圖片stride=2，套入公式得出padding=1。</li></ul><p>關於self._make_layer()這個function</p><ul><li>觀看架構圖可以發現Residual Block一直在重複，所以後面會用一個function包起來，只要傳數字就可以製造出指定數量的Block。並且有些Block開頭需要縮小圖片shape，所以stride也是一個參數，當stride=2時代表需要縮小圖片大小至一半。</li></ul><h3 id="Code解析-ResNet的-make-layer"><a href="#Code解析-ResNet的-make-layer" class="headerlink" title="Code解析 - ResNet的_make_layer()"></a>Code解析 - ResNet的_make_layer()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">  ... </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self, in_channel, out_channel, block_num, stride=<span class="number">1</span>)</span>:</span> </span><br><span class="line">    <span class="comment"># shortcut的部份必須和該block最後一層維度相同，所以這裡做1d conv增加維度 </span></span><br><span class="line">    <span class="comment"># 並且根據有沒有縮小shape(stride=2)做相同的動作 </span></span><br><span class="line">    shortcut = nn.Sequential( </span><br><span class="line">      nn.Conv2d(in_channel, out_channel, <span class="number">1</span>, stride, bias=<span class="literal">False</span>), </span><br><span class="line">      nn.BatchNorm2d(out_channel), </span><br><span class="line">    ) </span><br><span class="line">    layers = [] <span class="comment"># 第一次的ResidualBlock可能會縮小shape(根據stride)，所以要獨立出來做 </span></span><br><span class="line">    layers.append(ResidualBlock(in_channel, out_channel, stride, shortcut)) <span class="comment">#注意這邊都是第二次以後的ResidualBlock，所以不會有維度或大小不同的問題，參數跟shortcut都不用做 </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, block_num): </span><br><span class="line">      layers.append(ResidualBlock(out_channel, out_channel)) </span><br><span class="line">      <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><p>shortcut? </p><ul><li>這邊定義了上面提到的shortcut，說白了就是1d的cnn(因為kernel=1)，把維度變成跟out_channel一樣之後才能被相加。 </li><li>stride可以在圖片要被縮小的時候進行相同的操作，才不會有Residual Block左邊的圖片大小已經縮小了(還記得Residual Block可以指定stride參數嗎?)，右邊卻沒跟著縮小的情況發生。</li></ul><p>後面的layer好像都在做一樣的事情(將Residual Block加入list中)，不過為什麼第一次要獨立出來寫? </p><ul><li>跟上面一樣的問題，為了應付有時候shape必須被縮小的時候可以根據stride產生不同的Residual Block，你會發現後面迴圈內的Residual Block都沒有stride這個參數，因為都是用default=1的值(接在後面的層都不需要縮小)。</li></ul><p>後面的部分就剩下forward()了，這邊相較簡單多了，我有把資料經過每一層後的維度都印出來，方便理解，所以就不細講了～</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> python </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[DL]關於莫煩【RNN 循环神经网络 (回归)】裡面沒有講到的事情</title>
      <link href="/posts/ae465c9e/"/>
      <url>/posts/ae465c9e/</url>
      
        <content type="html"><![CDATA[<p>今天在看莫煩的Pytorch教學，在練習<a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/4-03-RNN-regression/" target="_blank" rel="noopener">RNN 循环神经网络 (回归)</a>這篇時發現了一些問題想要記錄下來，下面我就直接拿他在<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py" target="_blank" rel="noopener">github上的code</a>來說比較快：</p><h2 id="關於增加資料維度的方法"><a href="#關於增加資料維度的方法" class="headerlink" title="關於增加資料維度的方法"></a>關於增加資料維度的方法</h2><p>第一個要講的在code第83行的地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis])</span><br><span class="line">y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])</span><br></pre></td></tr></table></figure><p>這兩行是為了將ndarray增加維度後轉成tensor，一開始看到<code>x_np[np.newaxis, :, np.newaxis]</code>的時候其實有點不知道這在幹啥，不過其實印出來就知道了，它是一種增加資料維度的形式， 如果原本x_np的shape是(1)，<code>x_np[np.newaxis, :, np.newaxis]</code>後的shape會變成（1, 1, 1)。</p><p>也可以透過<code>np.expand(x_np, dim=0)</code>達到相同的效果，不過似乎一次只能增加一個維度。</p><p>或是，也可以先轉成tensor在透過view()來任意改變你想要的維度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.from_numpy(x_np).view(<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>)               </span><br><span class="line">y = torch.from_numpy(y_np).view(<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="關於backward-的小細節"><a href="#關於backward-的小細節" class="headerlink" title="關於backward()的小細節"></a>關於backward()的小細節</h2><p>第二個要講的在code第86行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">prediction, h_state = rnn(x, h_state) <span class="comment"># rnn output</span></span><br><span class="line"><span class="comment"># !! next step is important !!</span></span><br><span class="line">h_state = h_state.data</span><br><span class="line"> </span><br><span class="line">loss = loss_func(prediction, y)         <span class="comment"># calculate loss</span></span><br><span class="line">optimizer.zero_grad()                   <span class="comment"># clear gradients for this training step</span></span><br><span class="line">loss.backward()                         <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">optimizer.step()                        <span class="comment"># apply gradients</span></span><br></pre></td></tr></table></figure><p>莫煩上課說這一步很重要，要用.data把h_state的值取出來，不然會出錯，不過好像沒有詳細講到為什麼，所以我想來提一下，不過我也不是很清楚，有錯誤還請糾正。</p><p>我練習的時候還真的不信邪，就是不打那一行，然後就出錯了，到底加這一行的原因是什麼呢？</p><p><strong>大家都知道在Pytorch中是動態建立計算圖的，之所以要把ndarray轉成tensor(注意，在0.4.0後variable和tensor合併了，不然以前還是要將tensor要轉成variable的）就是為了要建立這個計算圖，不過，tensor不只儲存了資料，他還儲存了梯度的資訊。</strong></p><p>另外一點，<strong>為了節省記憶體，pytorch每次執行backward()的時候會將計算圖清空。</strong></p><p>所以基於這兩點，其實h_state是計算圖的一部分，如果不信可以把h_state印出來會看到類似的訊息：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor(..., device&#x3D;&#39;cuda:0&#39;, grad_fn&#x3D;)</span><br></pre></td></tr></table></figure><p><strong>如果該tensor是一個單純的計算結果，grad_fn會返回None，否則他會返回與該函數相關的對象</strong>。在這一個練習中，<strong>rnn每次都會將h_state的值餵入下一次的訓練，但只是需要h_state的值而已，所以要透過.data將值取出來</strong>，如果沒有.data，則下一次餵入的時候由於計算圖在上一次已經被清空了，屆時h_state就會產生錯誤。</p><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><p><a href="https://zhuanlan.zhihu.com/p/29904755" target="_blank" rel="noopener">Autograd:PyTorch中的梯度计算</a></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Python]在自己的Server上整合Dialogflow + Line Messaging API</title>
      <link href="/posts/dcfaf0e2/"/>
      <url>/posts/dcfaf0e2/</url>
      
        <content type="html"><![CDATA[<p>新年快樂，我寫故我在，為了證明我還活得好好的所以來寫文章（？）</p><p>今天要來講的是當初寫chatbot遇到的一個問題，首先先來簡單科普一下兩個名詞：Diaglogflow和Line Messaging API。</p><p>在製作Line的聊天機器人時可以透過網站後端的平台在完全不用寫程式的情況下設定一些關鍵字回覆，但這只限於簡單的關鍵字，如果你打的是同義字但你卻沒有新增這個關鍵字的對應回應，那他就不會觸發。此外，如果需要一些額外的功能，比方說我輸入今日天氣，就回傳我今日天氣的話，就必須寫程式了（你必須有個server收到這指令之後去爬蟲今日天氣，並回傳給client)，Line Messaging API就是用來處理訊息收送的API。</p><a id="more"></a><p>Dialogflow是google開發的AI工具，透過一些簡單的設置就可以完成簡單的對話流程。比方說你可以設定飲料和他的關鍵字（果汁、汽水、喝的……），當使用者輸入的句子中包含了這些關鍵字，dialogflow就會回傳你設定好的答案回去。</p><p>關於這兩個的細節就不詳述了，網路上隨便找都有一堆很好的教學和介紹文，今天是來講自己開發chatbot時遇到的問題：如果我想要把這兩個工具整合在一起使用該怎麼辦呢？</p><p>如果只是單純要整合Line + Dialogflow，並沒有要自己架Server的話，那只要在Dialogflow的Integrations設定即可，只要填上Channel ID、Secret和Access token，這些都可以在Line後端的網站上取得。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/02/e89ea2e5b995e5bfabe785a7-2019-02-17-e4b88ae58d8811.02.42.png" alt="螢幕快照 2019-02-17 上午11.02.42"></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/02/e89ea2e5b995e5bfabe785a7-2019-02-17-e4b88ae58d8811.03.00.png" alt="螢幕快照 2019-02-17 上午11.03.00"></p><p>不過有些情境就需要自己來架Server處理訊息對應的動作，舉個應用的情境，我希望在Line收到訊息後，透過Dialogflow判斷語意，然後在Server上執行對應的爬蟲動作再回傳資料，這時候又該如何串接這兩個平台呢？</p><p>問題在於，<strong>這兩個平台都會需要使用到webhook</strong>，webhook就像是一個郵箱，告訴Line和Dialogflow收到訊息之後要把對應的資訊寄到哪裡去，可是同時使用的話似乎會<a href="https://ithelp.ithome.com.tw/articles/10196144" target="_blank" rel="noopener">引發衝突</a>，所以這邊提供我後來解決的方法：</p><p>解決方法就是<strong>把webhook給Line用就好</strong>，所以大致上的Server架構還是跟Line架設Server是一樣的。至於Dialogflow那邊其實他們有提供API(pip install apiai)，只要提供Dialogflow帳號的CLIENT_ACCESS_TOKEN就可以透過API查詢該帳號下設置好的intent、action……了，sample code如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> apiai</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    sys.path.append(</span><br><span class="line">        os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">import</span> apiai</span><br><span class="line"> </span><br><span class="line">CLIENT_ACCESS_TOKEN = <span class="string">''</span> <span class="comment"># put your CLIENT_ACCESS_TOKEN here</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dialog_detect</span><span class="params">(say)</span>:</span></span><br><span class="line">    ai = apiai.ApiAI(CLIENT_ACCESS_TOKEN)</span><br><span class="line"> </span><br><span class="line">    request = ai.text_request()</span><br><span class="line"> </span><br><span class="line">    request.lang = <span class="string">'tw'</span>  <span class="comment"># optional, default value equal 'en'</span></span><br><span class="line"> </span><br><span class="line">    request.query = say</span><br><span class="line">    response = request.getresponse().read().decode()</span><br><span class="line">    result=json.loads(response)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> api </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Python]Line客製化回覆訊息</title>
      <link href="/posts/65f8bdcc/"/>
      <url>/posts/65f8bdcc/</url>
      
        <content type="html"><![CDATA[<p>距離上次發文已經間隔兩個月了，其實一直都有東西可以寫可是卻總是沒有時間寫出來（找藉口），所以趁過年前來多多少少還一些技術債…</p><p>在12月底某一天，莫名的被通知我們進了『第三屆華南金控金融科技創新競賽』複賽（350組取40組），然後距離deadline只剩一個禮拜。沒錯，就是那個只有7天，也就是不吃飯不睡覺不上課也只有168小時的一個禮拜……</p><p>橫豎都是一刀，咱們就來硬著頭皮做吧…所以我們的作品就在168小時內『哩聽無某-華南聲控助理』誕生了，這是一個使用dialogflow + google speech recognition的line chatbot，可以透過語音訊息和chatbot取得你所需要的相關資訊……不過今天不是要來講這個，詳細的內容之後會整理完放在github。</p><a id="more"></a><hr><p><strong>今天要來分享的是透過如何利用line message api製作客製化，製作類似表格的回覆訊息。</strong></p><p>Line可以回覆不同種的訊息給使用者，像是文字訊息、貼圖訊息、語音訊息……，又或是一般使用者無法傳送的樣板格式（Template），樣板的部分Line也提供了幾種格式，分別是： Confirm、Buttons、Carousel、Image carousel……（想知道更多關於line的訊息種類我推薦看這一篇：<a href="https://ithelp.ithome.com.tw/articles/10195640" target="_blank" rel="noopener">Day15[Line ChatBot]Messaging types下集</a>，介紹得非常清楚。），此外，<strong>Line也提供Flex Messages讓使用者可以根據自己的需求客製化出不同的訊息格式</strong>，如下圖取自<a href="https://developers.line.biz/en/docs/messaging-api/using-flex-messages/" target="_blank" rel="noopener">Line的document</a>：</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2019/02/bubblesample-77d825e6.png" alt="bubbleSample-77d825e6.png"></p><p><strong>其實，Flex Messages還可以做成表格狀的形式</strong>，下圖取自中信chatbot和我們這次參加華南競賽所製作的chatbot，每一個都是一個按鈕，按下後可以選擇發送文字訊息或是開啟超連結：</p><p>[gallery ids=”2643,2644” type=”rectangular”]</p><p>當初我也不知道可以做成這樣，是因為我本身有在使用中信的chatbot，發現有這種種類的回覆訊息格式後才去研究的。</p><p>設計概念其實跟Android UI的XML設計概念很類似，你<strong>必須一層一層的由外到內定義排列方式和對應的物件個數</strong>，比方說，我想要設計一個2x2的回覆訊息，</p><ol><li>從最外面看的話你會有兩列，所以你最外層的排列方式就要是vertical，並且裡面包含了兩個Component</li><li><p>接下來每一列都會有兩個按鈕，所以每一個Component內都要定義排列方式為</p><p>horizontal，並且裡面包含了2個Component</p></li></ol><p><strong>Component是什麼呢？line的官方文檔提供了很多種Component類型，如BoxComponent、SpacerComponent、ImageComponent、TextComponent</strong>，看名字就知道是箱型的物件（我是把它想成你訊息格式的骨架）、可以顯示空白（用來對齊、排版）的物件、可以放貼圖和放文字的物件…。</p><p>最後想分享一下，line message api的部分在demo code真的做得很好，其實只要看一下官方範例就能很容易的改出屬於自己想要的東西，有興趣的人真的很推薦去研究一下line的github: <a href="https://github.com/line/line-bot-sdk-python" target="_blank" rel="noopener">line-bot-sdk-python</a></p><p>我也將這次華南的custom template放在自己的github上，我是參照demo code去改的，並沒有改得很好，不過如果對於想製作類似回應訊息格式的人或許可以作為一些的參考：<a href="https://github.com/john850512/line_costumed_reply" target="_blank" rel="noopener">line_costumed_reply</a></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> api </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[課程體驗]區塊鏈與智能合約-以太坊實作入門</title>
      <link href="/posts/b4ba3bf5/"/>
      <url>/posts/b4ba3bf5/</url>
      
        <content type="html"><![CDATA[<p>前陣子在網路上無意間看到了由北科大舉辦的以太坊實作workshop，對於區塊鏈總是懵懵懂懂的我就去報名了……後來雖然沒有得到正取的資格，不過很開心的是今年提供了直播班讓沒有正取的人也可以旁聽。</p><p>原本心裡有點猶豫，因為對我來說在假日早起有點辛苦(?)，不過兩天下來，只能說真的非常值得！！</p><p><strong>在兩天將近20小時的workshop中，最後我在以太坊發行了自己虛擬貨幣(MHC)，以及得到了一張很精緻的（據說也是全台灣第一批）區塊鏈證書，真心覺得讚：）</strong></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/12/46898063_2066749513386206_3576392866215231488_o.jpg" alt="46898063_2066749513386206_3576392866215231488_o.jpg"></p><a id="more"></a><p>(p.s.上圖看到的是當時的宣傳海報，老實說當初會報名有部分原因是被講者吸引了，想說是個很漂亮的女孩，結果去看了一下她的經歷…覺得非常厲害xD)</p><h2 id="Day1"><a href="#Day1" class="headerlink" title="Day1"></a>Day1</h2><p>第一天講者們用很生活化的語言介紹了區塊鏈的基本概念，區塊鏈的概念我自己曾在大三暑假時在工研院聽別的講者講過一次，不過那時講的偏向技術層面，像是區塊和區塊之間是如何用Hash function聯繫起來的，如何調整Hash function的難易度……當時聽得有點不清不楚。</p><p>不過這次聽起來完全不會有聽不懂的感覺，反而覺得豁然開朗。在講課的過程中，講者們用許多不同的角度來和大家一起探討區塊鏈，這是令我覺得堂workshop中最新奇的部分：<strong>不是由程式的角度來講解，而是由生活面、市場經濟、人民彼此的共識…多方向來介紹為什麼區塊鏈要這樣設計。</strong>聽起來彷彿自己是一個文明的開創者，在思考要怎麼樣讓一個尚未成熟的經濟體系活絡起來的感覺(可能聽起來誇大了點xD 不過最近剛好在看類似的小說所以特別有感覺)。</p><p>對於兩天下來的上課內容我有做成筆記，不過這篇主要是來分享參加這次workshop的心得，所以就不打算再內容介紹區塊鏈到底在幹嘛了xD</p><p>關於上課的筆記，有興趣的可以在文章最後連結到我的Github去看。</p><p>在介紹完區塊鏈的基本概念後，第一天後半段的課程就開始介紹以太坊和智能合約。懂了區塊鏈的原理後對於智能合約有了更加清楚的了解，並且<strong>在講者的教學下，我們陸續在Ropsten Test Net上實作了數個智能合約，雖然功能小小的，但是看到自己寫的程式真的在區塊鏈上可以被執行真的很有成就感。</strong></p><h2 id="Day2"><a href="#Day2" class="headerlink" title="Day2"></a>Day2</h2><p>第二天開始教大家如何設計一個發布貨幣的智能合約，最後讓大家<strong>基於ERC20開發了屬於自己的虛擬貨幣，然後部署在Ropsten Net上</strong>，最後講者甚至發給每人一美金的以太幣讓大家把自己的虛擬貨幣部署到以太坊的主鏈上。</p><p>於是….屬於我自己的虛擬貨幣 - MHC就誕生啦xDDD</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/12/e89ea2e5b995e5bfabe785a7-2018-12-02-e4b88be58d883-35-06.png?w=356" alt="螢幕快照 2018-12-02 下午3.35.06"></p><p>身為一個貨幣發起人，數量當然要多才不會面臨貨幣發行完畢的問題xDDD聽說中本聰的比特幣總量是21億，所以我就使用了2倍，也就是42億的發行總量(突然變得好有錢的感覺)。</p><p>p.s.如果想要取得MHC歡迎聯絡我，我可以進行大灑幣xD</p><p>很有趣的一件事是當大家在創造屬於自己虛擬貨幣的時候，因為發行總量是固定的(ERC20)，所以出現了每個人都很苦惱到底要發行多少總量的虛擬貨幣，導致遲遲沒有人去發布自己的虛擬貨幣的情形，在網路直播上看到這情境還蠻有趣的。</p><p>最後，直播班的同學也能拿到參加證書，<strong>據說這是台灣第一批使用區塊鏈的證書</strong>，而且非常的精緻～兩天下來真的覺得收穫非常的多～～</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/12/certificate.png?w=1106" alt="certificate"></p><p>希望下次有機會能到現場去參與相關活動，也謝謝舉辦這次活動的講師、教授、還有其他工作人員！</p><p>如果想了解上課內容教了什麼，歡迎到我的github參觀上課筆記還有實作過的智能合約：<a href="https://github.com/john850512/smart_contract_workshop" target="_blank" rel="noopener">gihub</a></p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> workshop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Python]Call matlab function from python(matlab engine)</title>
      <link href="/posts/7d0fa0f6/"/>
      <url>/posts/7d0fa0f6/</url>
      
        <content type="html"><![CDATA[<p>這幾天我在研究一個matlab的程式碼，原本想看懂內容在用python自己實作一次，因為自己還是對python環境比較熟……但是我看不懂。</p><p>對。不是數學系出生的我怎麼看就是看不懂我需要的那段matlab的code再寫啥，崩潰。</p><p>於是念頭一轉，我有沒有辦法直接在python裡面呼叫matlab的function呢？嘿嘿，還真的有這個東西：<a href="https://www.mathworks.com/help/matlab/matlab-engine-for-python.html" target="_blank" rel="noopener">MATLAB API for Python</a>，所以我今天就來介紹它啦～</p><a id="more"></a><h2 id="matlab-engine安裝"><a href="#matlab-engine安裝" class="headerlink" title="matlab engine安裝"></a>matlab engine安裝</h2><p>首先必須先安裝<a href="http://ww2.mathworks.cn/help/matlab/matlab_external/install-the-matlab-engine-for-python.html?ue" target="_blank" rel="noopener">MATLAB Engine API for python</a>，方法如下：</p><ol><li>使用cmd進到matlab的安裝目錄</li><li>進extern/engines/pythonpython</li><li>setup.py install</li></ol><p>安裝完後，就可以來使用matlab engine了！很簡單吧！</p><p>啊…先說我的matlab是2018b版本…</p><h2 id="matlab-engine-for-python範例"><a href="#matlab-engine-for-python範例" class="headerlink" title="matlab engine for python範例"></a>matlab engine for python範例</h2><p>拿幾個<a href="http://ww2.mathworks.cn/help/matlab/matlab_external/call-user-script-and-function-from-python.html" target="_blank" rel="noopener">官方範例</a>先來一些簡單的介紹：</p><h3 id="單純呼叫matlab檔案內的程式碼（沒有function）"><a href="#單純呼叫matlab檔案內的程式碼（沒有function）" class="headerlink" title="單純呼叫matlab檔案內的程式碼（沒有function）"></a>單純呼叫matlab檔案內的程式碼（沒有function）</h3><p>先用matlab建立一個名稱為demo.m的檔案，輸入以下內容</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">b &#x3D; 5;</span><br><span class="line">h &#x3D; 3;</span><br><span class="line">a &#x3D; 0.5*(b.* h)</span><br></pre></td></tr></table></figure><p>接下來在同一個目錄下建立一個python檔案，輸入以下程式碼</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matlab.engine</span><br><span class="line">eng = matlab.engine.start_matlab()</span><br><span class="line">eng.demo(nargout=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>簡單來說，就是我先import相關的package，然後啟動matlab engine，要怎麼引入寫好的matlab檔案呢？<strong>就是eng.demo，demo為matlab檔案的檔名，所以記得要放在同一個目錄下，而參數nargout=0代表沒有輸出的參數</strong>，這個之後會再講詳細一點。</p><h3 id="呼叫matlab內的function"><a href="#呼叫matlab內的function" class="headerlink" title="呼叫matlab內的function"></a>呼叫matlab內的function</h3><p>如果要呼叫matlab的function該怎麼辦呢？</p><p>首先，<strong>matlab要被呼叫的function名稱必須和檔名相同</strong>（這裡不是很確定，也有可能是matlab本來的規定），所以如果matlab內的程式碼長這樣（檔名為demo_function.m)：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">out</span> = <span class="title">demo_function</span><span class="params">()</span></span></span><br><span class="line">    out = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>那在python內呼叫他基本上跟第一個範例相同，不過因為function有一個回傳值，我們就可以用個變數去接住它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matlab.engine</span><br><span class="line">eng = matlab.engine.start_matlab()</span><br><span class="line">out = eng.demo_function(nargout=<span class="number">1</span>)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure><p>那，如果我說，<del>愛我沒有如果</del>…不是，如果function有多個參數呢？</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[out1, out2]</span> = <span class="title">demo_function</span><span class="params">()</span></span></span><br><span class="line">    out1 = <span class="number">10</span>;</span><br><span class="line">    out2 = <span class="number">20</span>;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>python那邊只要改一行就好了，分別是nargout的值跟接收回傳的變數，也就是說，回傳值有幾個變數nargout就要是多少。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[out1,out2] = eng.demo_function(nargout=<span class="number">2</span>)</span><br><span class="line">print(out1, out2)</span><br></pre></td></tr></table></figure><p>換個寫法，<strong>當我有多個輸出的時候我也可以用matlab的struct包起來一起回傳，對應到python就會是dict型態的回傳值</strong>，你問我為什麼知道他是dict？matlab官方文擋（<a href="https://www.mathworks.com/help/matlab/matlab_external/handle-data-returned-from-matlab-to-python.html" target="_blank" rel="noopener">Handle Data Returned from MATLAB to Python</a>）給了一個對照表讓我們可以了解語言之間型態的對應為何。</p><p>[sourcecode language=”matlab”] function out_struct = demo_function() out1 = 10; out2 = 20; out_struct = struct(out1, out2) end [/sourcecode]  </p><h3 id="呼叫matlab內的function，並傳入矩陣進行運算"><a href="#呼叫matlab內的function，並傳入矩陣進行運算" class="headerlink" title="呼叫matlab內的function，並傳入矩陣進行運算"></a>呼叫matlab內的function，並傳入矩陣進行運算</h3><p>我這次要做的事情是希望透過python傳入矩陣給matlab算完後再回傳結果給我就好，問題來了，matlab的矩陣在python是相同的型態嗎？顯然不是，<strong>而且python也沒matrix，傳ndarray也是不行的（我幫各位試過了），那到底要傳什麼進去matlab才會是矩陣呢？</strong></p><p>官方文擋這篇有提到（<a href="https://www.mathworks.com/help/matlab/matlab_external/matlab-arrays-as-python-variables.html" target="_blank" rel="noopener">MATLAB Arrays as Python Variables</a>），<strong>我們可以用list來構構造矩陣結構，當成參數傳到matlab時對應到的就會是matrix，但只用list還是不夠的，還需要先透過他們的api進行轉換</strong>，如下方的python code：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">2</span>]]</span><br><span class="line">A = matlab.double(A)</span><br><span class="line">engine.matlab_function_name(A)</span><br></pre></td></tr></table></figure><p>上方的code中我將list透過matlab的api轉換成了double的matrix，不過這是對matlab來說，如果在python把他印出來仍然是一個list（型態都變成double了）。</p><p>好啦，今日的教學就到這裡了，如果有講錯的地方還請告訴我，謝謝～</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[DL]淺談CNN在Object Classification上的各種架構</title>
      <link href="/posts/18a141c2/"/>
      <url>/posts/18a141c2/</url>
      
        <content type="html"><![CDATA[<p>這學期修了一門課程叫做視訊串流與追蹤，主要在介紹深度學習在圖像、影像上的各種應用，從學期初至今被摧殘了不少，但也學到了不少，於是想來紀錄一下目前學習到的一些內容。</p><a id="more"></a><h2 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h2><p>首先，深度學習在圖形的相關應用，目前CNN再圖像、影像上的應用上取得了非常突出的成果，基本上有以下這幾種(當然還有更多其他的有趣應用…太多了說不完…不過我不要寫成表格)：</p><ul><li>Object Classification：分類一張圖片內的物體是哪一種類別</li><li>Object Detection &amp; Localization：檢測出圖片中的物體類別，還要知道他的位置(Bounding box)</li><li>Object Segmentation(Semantic / Instance)：除了知道位置外，更進階的是找出圖片中哪些pixel是屬於這個類別</li><li>Object Identification：確認這個物體是哪個個體</li><li>Object Tracking：在影片中追蹤物體的位置、形變</li></ul><p>知乎上有找到一則我覺得很不錯的回答，循序漸進的帶我們了解了深度學習再圖像上的各種應用，推薦大家前去看看(<a href="https://www.zhihu.com/question/36500536/answer/281943900" target="_blank" rel="noopener">用深度学习玩图像的七重关卡</a>)</p><p>今天要來介紹的是在Object Classification上取得了好成績的各種神奇架構，以及它們各自的特色(都只是點出重點，細節部分有興趣請去看原始論文)。</p><p>下面會簡介這幾種網路架構，以及他們的特色：</p><ol><li>LeNet</li><li>AlexNet</li><li>ZFNet</li><li>VGG</li><li>GoogLeNet</li><li>ResNet</li></ol><hr><h3 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/11/20150903161702138.png" alt="20150903161702138"></p><p>CNN的經典架構，兩層Conv(Convolution Layer)接兩層的FC(Fully Connected Layer)，主要被用於手寫辨識上，好像沒啥好介紹的xD</p><hr><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/11/vz2vvvq.png" alt="vz2vvVQ"></p><p>2012年的ILSVRC上，AlexNet憑著更深的CNN打敗了其他的對手，對於使用ML調參調的半死的人想必恨的牙癢癢的，它的特色如下：</p><ul><li>使用兩張GPU分開訓練(在當時，GPU記憶體還沒有那麼大)</li><li>使用8層架構</li><li>第一個使用<strong>ReLU</strong>作為activation function</li><li>使用Norm layers: Local Response Norm(LRN)</li><li><strong>Data augmentation</strong>: 將圖片平移、旋轉，增加training set的多樣性</li></ul><hr><h3 id="ZFNet"><a href="#ZFNet" class="headerlink" title="ZFNet"></a>ZFNet</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/11/zfnet.png" alt="ZFnet"></p><p>ZFNet將AlexNet的架構拿來修改了一下，然後得到了2013年的冠軍！恭喜恭喜</p><p>主要修改的部分是縮小filter size和增加filter number</p><hr><h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p>這應該是大家比較熟悉的一個網路架構，嫌AlexNet 8層不夠深？那我們就來邁向更深網路吧</p><p>VGG是ILSVRC 2014年的第二名，第一名則是下面介紹的的GoogLeNet</p><ul><li><strong>只使用3x3的Conv和2x2的Maxpool</strong>：目的是用多層的較小的filter達到一個大的filter包含的資訊。舉例來說2個3x3的Conv可以和1個5x5的Conv涵蓋一樣的資訊量，但參數量卻比較少(3*3*2 &lt; 5*5*2)。</li><li>相較於以前一個Conv後面就接一個Pooling，<strong>VGG做了很多次Conv才接Pooling，因為這樣可以透過activation function使data有更多non-linear的變化。</strong></li></ul><hr><h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/11/googlenet.png" alt="GoogLeNet"></p><p>然後這才是正宮，使用了22層達到了ILSVRC 2014年的第一名！恭喜恭喜，那他做了什麼事情呢？</p><ul><li><strong>不用FC層</strong>：有看過<a href="https://john850512.wordpress.com/2018/09/30/mlparameter-number-calculate-of-neural-networknn/" target="_blank" rel="noopener">[ML]Calculate Parameter Numbers of MLP &amp; CNN</a>這一篇(對，偷偷工商一下)的人就會知道FC由於是全連接會產生大量的weights參數，不使用FC使得參數量大大的降低(~5M，遠小於VGG的138M)。</li><li><strong>提出了inception model</strong>：我們在自己寫CNN時常常會想，Conv的filtersize到底要多大比較好呢？要先做Pooling嗎？…這些問題真的很困難，<strong>那何不讓網路幫我們自己決定就好呢？</strong>所以就inception model就出現啦</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/11/inception-model.png" alt="inception model"></p><ul><li><strong>1x1 Conv</strong>：看一下inception model就會發現GoogLeNet使用了1x1 Conv來做降維的動作，1x1 Conv的output size會跟原圖一樣，如果filter number減少，就是降維了。</li></ul><hr><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>接下來讓我們介紹2015的冠軍，總共使用了152層…</p><ul><li>並不是越深越好，該篇論文有指出，<strong>僅僅是增加深度不會使得效果比較好，甚至會使得誤差增大。</strong></li><li><strong>residual block</strong>：於是他們提出了residual block這個架構，在越深的layer資訊越難往下傳遞，所以透過這個架構使得上層的資訊能夠比較容易被保留</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/11/res-block.png" alt="res block"></p><hr><h3 id="Deformable-Convolutional-Network"><a href="#Deformable-Convolutional-Network" class="headerlink" title="Deformable Convolutional Network"></a>Deformable Convolutional Network</h3><p>這是2017年提出的，不確定有沒有得到獎，不過想法蠻有意思的，想拿出來說一下：</p><p>我們以往在做Convolution的時候都是一個正方形的運算，也就是拿一個filter去對image對應位置做運算。但是這篇提出來的想法是，我<strong>不一定要對應到原本的位置，我也可以利用不同的對應去模擬一些情況，比方說我可以模擬縮放跟旋轉</strong>，看圖片會比較明白。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/11/e4b88be8bc89.jpg" alt="下載"></p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/11/1_6lbz5rm1fexa_n_vttnfxw.png" alt="1_6lBZ5rM1fExa_N_VTtNfXw"></p>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Pytorch]The `requires_grad` attribute of Tensor</title>
      <link href="/posts/b6b2a7ff/"/>
      <url>/posts/b6b2a7ff/</url>
      
        <content type="html"><![CDATA[<p>前陣子死沉keras海裡面……最近好不容易稍微浮上來了結果因為作業需要好像又得去學Pytorch…(再度倒地)</p><p>在剛學的時候難免會遇到一些奇奇怪怪的問題，把學習中遇到的問題紀錄一下好讓未來的我看看自己以前有多蠢順便證明我還在新竹活得好好的，最近都沒什麼動態絕對不是因為沒東西吃餓死了。</p><p>好的開始講故事：萌新如我，最近透過莫煩大大大概知道pytorch搭建一個神經網絡的模式了，首先</p><ol><li>資料必須是Tensor的型式(注意注意在pytorch 0.4.0 ver之前資料必須是Variable的，不過0.4.0之後Variable和Tensor合併再一起啦～)</li><li>透過class搭建神經網路模型，好久沒碰類別和繼承了，這花了我不少的時間去熟悉。</li><li>重載(?)torch.nn.Module這個class的__init__()和forward()兩個function，建立網絡模型</li><li>建立optimizer, loss function，然後手刻迴圈進行training(keras中這裡都幫你寫好了，只要跟他說epochs要幾次就好了…懷念QQ)</li></ol><a id="more"></a><p>因為是神經網路模型，我們需要用神奇的back propagation(BP)去反向傳遞gradient，達到優化loss的目的，為了達到這個目的我們資料(Tensor)必須是可以求倒數的。</p><p>然後我最近練習的時候自己創了一個資料如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)</span><br><span class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.2</span> * torch.rand(x.size())</span><br></pre></td></tr></table></figure><p>問題來了，<strong>如果去查文件會發現Tensor預設是不能求導數的(Tensor 預設requires_grad是False，再度重申在0.4.0後Tensor和Variable合併了，requires_grad原本是Variable的屬性。)</strong>，一開始沒注意到這個問題我就傻傻地把資料丟到回歸模型去訓練了，訓練結果也很正常。</p><p>然後突然覺得哪裡怪怪的…不對啊？Tensor不是預設不能求導數媽？我沒有更改預設的requires_grad=False屬性為啥還能在模型中正常的做BP？？？？</p><p>然後百思不得其解的我就去看了一下我搭建的模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_feature, n_hidden, n_output)</span>:</span></span><br><span class="line">        <span class="comment"># 定義網路架構</span></span><br><span class="line">        super(Net, self).__init__() <span class="comment">#初始化父類別</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        print(x.requires_grad)</span><br><span class="line">        print((self.hidden(x)).requires_grad)</span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line"> </span><br><span class="line">        x = self.predict(x) <span class="comment"># in regression problem, we dont need activation function at output layer</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>看起來很正常Ｒ，可是理論上應該是要有問題的，黑人問號？？？</p><p>我想說一定有某個地方會自動幫我轉requires_grad的屬性，於是我又去一步一步看source code，看到底哪步我的Tensor默默的被轉屬性了，最後我發現在<a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html" target="_blank" rel="noopener">torch.nn.Linear</a>中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features, bias=True)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()</span><br><span class="line">        self.in_features = in_features</span><br><span class="line">        self.out_features = out_features</span><br><span class="line">        self.weight = Parameter(torch.Tensor(out_features, in_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = Parameter(torch.Tensor(out_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">'bias'</span>, <span class="literal">None</span>)</span><br><span class="line">        self.reset_parameters()</span><br></pre></td></tr></table></figure><p>用到了<a href="https://pytorch.org/docs/stable/_modules/torch/nn/parameter.html" target="_blank" rel="noopener">torch.nn.Parameter</a>這個，我們在追下去看他是怎麼寫的 ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parameter</span><span class="params">(torch.Tensor)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, data=None, requires_grad=True)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> data <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            data = torch.Tensor()</span><br><span class="line">        <span class="keyword">return</span> torch.Tensor._make_subclass(cls, data, requires_grad)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Parameter containing:\n'</span> + super(Parameter, self).__repr__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__reduce_ex__</span><span class="params">(self, proto)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Parameter, (super(Parameter, self), self.requires_grad)</span><br></pre></td></tr></table></figure><p>有沒有看到<code>def __new__(cls, data=None, requires_grad=True)</code>了！！<strong>哎呀呀原來是在將Linear Model轉成Parameters的時候自動幫我們將資料屬性轉成requires_grad=True了呢，所以後續的才能正常地進行BP求導。</strong></p><p>好啦結論是解決了為啥我忘記對資料的Tensor加上requires_grad=True也能正常的訓練模型呢？因為在Linear模型中某一個步驟(torch.nn.Paramenter)自動幫我們將資料的屬性轉換了。</p><p>好啦這篇只是寫給自己看的xD應該也沒人想知道這個吧。</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> python </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[DL]Calculate Parameter Numbers of MLP &amp; CNN</title>
      <link href="/posts/4647b68d/"/>
      <url>/posts/4647b68d/</url>
      
        <content type="html"><![CDATA[<p>往最近在上CNN的課程，對於<strong>如何計算Layers之間要訓練的參數數量</strong>有更加清楚的理解，於是想發一篇文章將這些數量到底如何推出的記錄下來，順便證明我還活著只是久久沒發文:-(</p><a id="more"></a><h2 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h2><p>透過Keras來搭建簡易的NN模型，並透過code和執行結果解釋層和層之間的訓練參數數量如何得來。</p><h2 id="程式碼分析-Fully-Connection-Layer"><a href="#程式碼分析-Fully-Connection-Layer" class="headerlink" title="程式碼分析 - Fully Connection Layer"></a>程式碼分析 - Fully Connection Layer</h2><p>首先會先利用幾個Case來分析全連接層的參數數量到底怎麼計算的，全連接層的意思就是每個neural彼此之間都有一條線(weight)相連，聽不懂的話下面case有簡潔的醜陋的圖形可以看看。</p><p>首先import待會需要的東西(可能不會全部用到)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, MaxPool2D, Flatten, Dropout, Dense, Activation,GlobalMaxPooling2D</span><br></pre></td></tr></table></figure><h3 id="Case-1"><a href="#Case-1" class="headerlink" title="Case 1"></a>Case 1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(units=<span class="number">10</span>, input_shape = (<span class="number">1</span>,)))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p>會得到:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">dense_4 (Dense)              (None, 10)                20        </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 20</span><br><span class="line">Trainable params: 20</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure><p>這個架構簡單看就是長這樣(畫得很醜):</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/09/img_20180930_011605.jpg" alt="IMG_20180930_011605.jpg"></p><p>輸入是一個數字，所以只有一個neural，輸出層有10個layer(第一層我們稱為input layer，最後一層被稱為output layer)，而第二層neural的值來自於第一層，公式為</p><p>$y = wx + b$</p><p>x是輸入，w和b是我們要學習的參數，所以有10個neural就有10個w和b，10 + 10 = 20。</p><hr><h3 id="Case-2"><a href="#Case-2" class="headerlink" title="Case 2."></a>Case 2.</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(units=<span class="number">1024</span>, input_shape = (<span class="number">10</span>,)))</span><br><span class="line">model.add(Dense(units=<span class="number">1</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">dense_5 (Dense)              (None, 1024)              11264     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_6 (Dense)              (None, 1)                 1025      </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 12,289</span><br><span class="line">Trainable params: 12,289</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure><p>圖形架構是:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/09/img_20180930_013224.jpg" alt="IMG_20180930_013224"></p><p>input layer有10個neurals，中間有一層1024個neurals，由於是全連接層，所以輸入層和中間層之間總共會有10 * 1024 + 1024(bias) = 11264個參數要學。</p><p>同樣地，中間層和輸出層的數量計算為 1024 * 1 + 1(bias) = 1025個參數。</p><p>而Total params就是11264 + 1025 = 12289</p><hr><p>到這邊相信聰明的大家已經發現計算的方式，對於Layer A 和 Layer B之間的參數數量計算公式為:</p><p>(# of unitA) * (# of unit B) + (# of unit B)</p><p>所以對於Full Connection Layer，不管多深的網路，我們都可以簡單算出要訓練的參數了，參數的數量大大的影響著訓練一個模型的時間。</p><h2 id="程式碼分析-Convolution-Layer"><a href="#程式碼分析-Convolution-Layer" class="headerlink" title="程式碼分析 - Convolution Layer"></a>程式碼分析 - Convolution Layer</h2><p>了解原理後其實FC Layer的計算就很簡單了，再來看比較複雜一點的，在做CNN時用到的Convoluation Layer參數又是怎麼計算的呢?</p><h3 id="Case-3"><a href="#Case-3" class="headerlink" title="Case 3"></a>Case 3</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Conv2D(filters=<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>))</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">conv2d_2 (Conv2D)            (None, 32, 32, 32)        320       </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 320</span><br><span class="line">Trainable params: 320</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure><p>解釋一下Conv2D的參數，我有一張32*32的黑白圖(因為只有一個channel，也就是32*32*1的1)</p><p>filters=32和kernel_size=(3, 3)代表我使用32組3*3的filter來做convolution，至於convolution怎麼做不是這裡的重點，請自行Google。</p><p>怎麼理解Param #的320?</p><p>想一下做CNN的時候我們要訓練的是什麼，我們沒有FC的neurals，我們要訓練的是filter的值。<strong>在影像處理中，已經有許多知名的filter可以達到不同的效果，例如找出圖片邊緣，模糊圖片，去除雜訊……，但在神經網路裡我們希望讓機器自動學習我們應該使用什麼樣的filter，也就是說，每個filter的值就是要被訓練的參數。</strong></p><p>這個case有32組filter，每張大小為3*3，所以32 * (3*3) = 288…好像不太對???</p><p>別忘了還有bias，每個filter我們都給予一個bias，所以32 <em> (3</em>3) + 32 = 320。</p><hr><h3 id="Case-4"><a href="#Case-4" class="headerlink" title="Case 4"></a>Case 4</h3><p>最後來個大魔王，結合更多不同的Layer:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Conv2D(filters=<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>))</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>))</span><br><span class="line">model.add(Conv2D(filters=<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">conv2d_15 (Conv2D)           (None, 32, 32, 32)        896       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_6 (Dropout)          (None, 16, 16, 32)        0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">conv2d_16 (Conv2D)           (None, 16, 16, 32)        9248      </span><br><span class="line">_________________________________________________________________</span><br><span class="line">flatten_1 (Flatten)          (None, 8192)              0         </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 10,144</span><br><span class="line">Trainable params: 10,144</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br></pre></td></tr></table></figure><p>這次的輸入是彩色32*32的圖，為甚麼呢?因為在實際上，彩色的圖會被分層紅色(R)的一張，綠色的(G)一張，藍色的(B)一張，所以輸入才會是(32, 32, 3)。</p><p>既然圖片被分層RGB每個顏色各有一張32*32的圖，那我們又要怎麼做convolution呢?如果敵人開影分身，我們也使用影分身就好了，所以我們一組filter數量實際上是3*3大小的filter有3張，分別對R、G、B去做convolution，當初我一直卡在這裡是因為被kernel_size=(3, 3)的意思給誤解了。</p><p>好所以現在一組filter有三層(RGB)，每層的大小是3*3，也就是說一組filter的參數有3*3*3=27個，而我們有32組(filters=32)，加上每組有一個bias，所以總共是(3*3*3) * 32 + 32 = 896。</p><p>並且因為我有32組，做完之後我會得到32個32*32的data(因為有做padding，這裡不多贅述)，所以可以看到第一個convolution layer的output shape是(32, 32, 32)。</p><p>再來看下一個Convolution Layer，其他的先跳過，注意在第二個Convolution Layer前一層的output shape為(None, 16, 16, 32)，也就是說我們有16*16的圖片，有32層(這邊已經有點難理解實際上圖片呈現會長怎樣了)。</p><p>按照剛剛說過的，我們有3*3的filter，但是對方影分身變成32層了，所以我們就跟著影分身吧！！所以現在3*3的filter有的32個(層)，(3*3) * 32這樣稱為一組filter，我們有32組，所以總共有(3*3) * 32 * 32 + 32 = 9248個要訓練的參數。</p><p>我們可以發現Maxpooling()和Flatten()並不會有要訓練的參數，因為pooling就只是從data中取區域最大值，不需要訓練；flatten則是將weights鋪平(reshape)，也不會更動到參數的數量。</p><hr><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>Full Connection Layer 和 Convolution Layer的參數計算就到這邊，總結一下結論：</p><ul><li>Full Connection Layer的參數數量<strong>與兩層之間的neural個數有關</strong>，neural數量越多，要訓練的參數數量越多。</li><li>Convolution Layer的參數數量<strong>與filter大小、數量，以及data的channel(有幾張)有關，與data的大小(shape)無關</strong>。由於Convolution Layer參數相較FC少很多，所以不太需要加Dropout(在進入下一層Layer之前隨機捨棄一定比例的weights，常用在FC，用來防止overfitting)，也有人提出使用全部都是Convolution Layer的神經網路模型架構來減少訓練的負擔。</li></ul>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[DL]BinaryClassification with Sigmoid / Softmax</title>
      <link href="/posts/abdb3171/"/>
      <url>/posts/abdb3171/</url>
      
        <content type="html"><![CDATA[<p>這篇會用我自己的理解簡介一下在做二元分類(Binary Classification or Logistic Classification)上時使用Sigmoid和Softmax的差異，以及他們應該對應哪一個crossentropy(不然你會得到錯誤訊息或是怎麼train都train不好…QQ)。</p><a id="more"></a><h2 id="簡介Activation-function"><a href="#簡介Activation-function" class="headerlink" title="簡介Activation function"></a>簡介Activation function</h2><p>在訓練Neural Network的時候，會針對每層Layer加上Activation function，使得每層的輸入(inputs)乘上權重(weights)後會先轉換成另一個數值再傳到下一層。</p><p>比方說對於Hidden Layer我們通常會加上sigmoid, relu…作為activation function，以神經網路的意義來看就是今天當輸入給予的能量超過一個閥值(threshold)時便會刺激神經元發出訊息傳導，但如果沒有超出該閥值則訊息不會傳導。</p><p>而在分類問題上，最後一層的activation function通常會不一樣。我們常使用softmax在多元分類的問題(multi-classification)，而使在二元分類(binary/ lodistic classification)的問題上則使用sigmoid。</p><h2 id="公式介紹"><a href="#公式介紹" class="headerlink" title="公式介紹"></a>公式介紹</h2><p>公式來自這<a href="https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier" target="_blank" rel="noopener">網址</a>，我擷取部分的資料出來簡述。</p><p>Sigmoid的公式:將一個數值轉成介於[0, 1]之間的值，也可以想成二元分類時的機率</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/09/01.jpg" alt="01"></p><p>Softmax的公式: 可以想成多類別的時候某個類別的機率</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/09/022.jpg" alt="022"></p><p>然後想提個問題讓大家想想，<strong>softmax和sigmoid都可以表示機率，只是差在類別數量，那是否都能用在二元分類呢？</strong></p><p>來做個推導一下：</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/09/032.jpg" alt="032"></p><p>結論是：<strong>在二元分類時，其實softmax和sigmoid是等價的，也就是說我們可以說softmax是sigmoid的一個特例(在二元分類時)。</strong></p><h2 id="討論"><a href="#討論" class="headerlink" title="討論"></a>討論</h2><p>再重複一次想討論的問題：既然兩個activation function在二元分類的時候是等價的，那實際上在做二元分類(只有0和1)時，是否使用softmax和sigmoid都可以呢？</p><p>答案是<strong>可以的，但是必須要注意搭配的Loss function</strong></p><ul><li><strong>softmax + categorical_crossentropy</strong>: 並且在Keras裡面對於categorical_crossentropy的實作有點不一樣(詳情看<a href="https://stackoverflow.com/questions/36927025/how-to-use-keras-multi-layer-perceptron-for-multi-class-classification" target="_blank" rel="noopener">這篇</a>): <strong>Keras對於categorical_crossentropy是使用one-hot的形式去計算的，所以即便是二元分類問題(output只有{0, 1})，最後一層也必須要是2顆Neuron，並且y必須是one-hot encoding後的array。</strong></li><li><strong>sigmoid + binary_crossentropy</strong>: 這是最不會有問題的方式，並且最後一層只需要1顆Neuron即可。</li></ul><h2 id="程式碼範例"><a href="#程式碼範例" class="headerlink" title="程式碼範例"></a>程式碼範例</h2><p>以下給了使用VGG16作為example，講說對於二元分類問題使用softmax + categorical_crossentropy和sigmoid+binary_crossentropy的寫法差異，主要看最後一層Layer和Lossfunction、y的樣子的不同之處即可。</p><h3 id="sigmoid-binary-crossentropy"><a href="#sigmoid-binary-crossentropy" class="headerlink" title="sigmoid+binary_crossentropy"></a>sigmoid+binary_crossentropy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#VGG 16</span></span><br><span class="line"> </span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Conv2D(filters=<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(Conv2D(filters=<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(Conv2D(filters=<span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"> </span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(units=<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"> </span><br><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># X:(2000,150,150,3)</span></span><br><span class="line"><span class="comment"># y:(2000,)</span></span><br><span class="line">model.fit(X, y)</span><br></pre></td></tr></table></figure><h3 id="softmax-categorical-crossentropy"><a href="#softmax-categorical-crossentropy" class="headerlink" title="softmax + categorical_crossentropy"></a>softmax + categorical_crossentropy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#VGG 16</span></span><br><span class="line"> </span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Conv2D(filters=<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(Conv2D(filters=<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(Conv2D(filters=<span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"> </span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(units=<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">2</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"> </span><br><span class="line"><span class="comment"># X:(2000,150,150,3)</span></span><br><span class="line"><span class="comment"># y:need to ont-hot form -&gt;(2000,2)</span></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</span><br><span class="line">model.fit(X, to_categorical(y))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[ML]LabelEncoder will order with alphabet(Sklearn)</title>
      <link href="/posts/fe4b699a/"/>
      <url>/posts/fe4b699a/</url>
      
        <content type="html"><![CDATA[<p>最近沉在kaggle浮不上來，快要被淹死了，哀…</p><p>扯遠了，今天要講的是一個在練習的時候遇到的坑(其實遇到很多坑，之後再慢慢補吧，因為我很懶)。</p><p>在做Machine Learning時，大家都知道對於Categorical features要做preprocess，將分類的資料轉成數值型的資料才能丟到model去train。基本上有兩種轉換的方式，One-hot Encoding和Label Encoding，先簡單簡介下兩者的差異：</p><h2 id="差異簡介"><a href="#差異簡介" class="headerlink" title="差異簡介"></a>差異簡介</h2><ul><li><strong>One-hot Encoding: 如果資料有N個類別，就會轉換成N維度空間，使得每個類別對應到特定的維度。例如{‘Apple’, ‘Banana’, ‘CClemon’} -&gt; {(1, 0, 0), (0, 1, 0), (0, 0, 1)}。</strong></li><li><strong>Label Encoding: 將N個類別用數值代替，維度只有一維。例如{‘Apple’, ‘Banana’, ‘CClemon’} -&gt; {1, 2, 3}。</strong></li></ul><a id="more"></a><p>One-hot Encoding，在還是Dataframe的時候可以直接用pd.get_dummies()，或是可以使用sklearn的OneHotEncoder()，又或者可以使用keras.util的to_categorical()；Label Encoding則是可以自己寫或是透過sklearn的LabelEncoder()進行轉換。</p><h2 id="使用時機"><a href="#使用時機" class="headerlink" title="使用時機"></a>使用時機</h2><p>同樣都是Categorical卻有不同的做法，這兩個差在哪裡呢？這要取決於feature的性質：</p><ul><li><strong>如果Categorical資料是Nominal類型的，也就是說類別之間並沒有一個次序性的關係，像是{‘貓’, ‘狗’}，只是不同的種類，但狗在貓後面並不代表狗比較厲害還是怎樣的，這時候就會比較適合用One-hot encoding。</strong></li><li><strong>如果Categorical資料是Ordinal類型的，類別之間有一定的順序性或是關聯，例如{‘小’, ‘中’, ‘大’}，那就比較適合用Label encoding，因為轉換後變成{1, 2, 3}，我們仍然可以透過數字的關係(3&gt;2, 2&gt;1)來取得原本類別之間的資訊。</strong></li></ul><h2 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h2><p>好啦嚴格說起來也不是坑，就是我在用sklearn的LabelEncoder做Encoding的時候，卻發現出來的次序都不是我想要的關係，例如我原始的資料是</p><p>{‘first’, ‘second’, ‘third’, ‘fourth’}</p><p>我想要得到得到的對應數字是{1, 2, 3, 4}，可是我做出來的結果卻是</p><p>{1, 4, 2, 3} #{‘first’, ‘fourth’, ‘second’, ‘third’}</p><p>???</p><p>這一定哪裡有問題</p><p>當時花了不少時間再找問題，然後再<a href="https://stackoverflow.com/questions/38749305/labelencoder-order-of-fit-for-a-pandas-df" target="_blank" rel="noopener">stackoverflow</a>上找到有人有跟我一樣的問題，我用中文簡述一下原因：</p><p>以下片段是sklearn的LabelEncoder()的<a href="https://github.com/scikit-learn/scikit-learn/blob/f0ab589f/sklearn/preprocessing/label.py#L39" target="_blank" rel="noopener">sourcecode</a>片段(在transform()這個function裡面)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"> classes = np.unique(y)</span><br><span class="line"> <span class="keyword">if</span> len(np.intersect1d(classes, self.classes_)) &amp;amp;amp;lt; len(classes):</span><br><span class="line">  diff = np.setdiff1d(classes, self.classes_)</span><br><span class="line">  <span class="keyword">raise</span> ValueError(<span class="string">"y contains new labels: %s"</span> % str(diff))</span><br><span class="line"><span class="keyword">return</span> np.searchsorted(self.classes_, y)</span><br></pre></td></tr></table></figure><p>裡面實際上是怎麼運作的就不闡述了(其實我也沒有仔細看)，不過我們可以看到裡面用到了setdiff1d()這個函式去比較兩個class的差異，<strong>問題就在這個setdiff1d()return的是sorted list，也就是說如果我們的資料是英文字串，回傳的list就會是按照英文字母(alphabet)排序後的list。</strong></p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>所以說，如果今天想要對資料進行Label Encoding的時候，如果我們想避免上述問題，也就是轉換後仍然保留類別之間的次序性，我們可能要用其他的方式來轉換，例如pd.map()，以下給個簡單的例子。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset_df[feature] = dataset_df[feature].map(&#123;<span class="string">'Ex'</span>:<span class="number">5</span>, <span class="string">'Gd'</span>:<span class="number">4</span>, <span class="string">'TA'</span>:<span class="number">3</span>, <span class="string">'Fa'</span>:<span class="number">2</span>, <span class="string">'Po'</span>:<span class="number">1</span>, <span class="string">'None'</span>:<span class="number">0</span>&#125;)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Unity]Maplesotry文字冒險遊戲-MapleHistory(未完成)</title>
      <link href="/posts/6bdfee4d/"/>
      <url>/posts/6bdfee4d/</url>
      
        <content type="html"><![CDATA[<p>附上一首音樂，讀的時候會更有感覺xD: <a href="https://www.youtube.com/watch?v=VtOqQ5kOvk8" target="_blank" rel="noopener">[Youtube]maplestory login music</a></p><p>楓之谷 / MapleStory，是許多七八年級的共同回憶。記得國小的時候遊戲還到我們的國小發水世界的序號，為了多拿一點我還跟朋友一直在門口來來回回，甚至還思考要怎麼變裝。拿完一疊後一群人跑去圖書館用那邊的電腦下載楓之谷兌換序號，還因此被管理員罵…</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/images.jpg" alt="images"></p><a id="more"></a><p>好啦，扯遠了。總之我把我國小五六年級、甚至國中三年的休閒娛樂都奉獻給了它，雖然現在沒有玩了(找不回當初那種感覺…相信很多人都有這種想法)，但是對於當初那種瘋狂還是很懷念…題外話(再度扯遠)，大學的時候楓之谷在三創舉辦十周年慶，我也跑去參加了，還領養了一隻姑姑寶貝回來，下面讓曬幾張當初拍的照片(我發現當時居然沒有打這篇文章，可是都過好久了現在發也很奇怪QQ)。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/imag0351.jpg" alt="IMAG0351"> </p><p>斷掉手的炎魔，還記得當初我第一頂炎虧還是用買的…!</p><p><a href="https://john850512.files.wordpress.com/2018/07/imag0377.jpg" target="_blank" rel="noopener">IMAG0377</a> </p><p>一些楓之谷的小知識 </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/imag0391.jpg" alt="IMAG0391">  </p><hr><p>…回到正題，在國中時也因為楓之谷開始接觸程式設計(外掛，這又是另一個故事了)。然後就也想試試看做個小遊戲，還記得當時打算嘗試做迷你版的楓之谷(真正可以打怪練功的那種)，不過我沒辦法做出那種攻擊動畫、特效(我是美術白癡)就放棄了。</p><p>後來想說，如果<strong>做不出來會動的，那我做個不會動的總行吧</strong>，於是第二次想過開發類似卡牌的單機遊戲(發想來自遊戲王，給予各種怪物攻擊力防禦力之類的)，不過後來也不了了之，原因年代久遠，已不可考。</p><p>大學讀了資工後，在大二的時候也成功跟朋友一起開發了一個手機小遊戲出來(<a href="https://meetonfriday.com/posts/231cc914/">[專題競賽]2016波克城市數位遊戲設計-參賽心得</a>)。不過關於小時候的興趣一直沒有接下去做(部分原因是我真的很希望有一些會美術的可以一起幫忙，我這個美術智障QQ，而且<strong>好的美術做出來的作品真的令人很有成就感</strong>)，直到前陣子有時間(推甄結束)，才終於開頭做了一些東西出來。</p><p>這次想做的是<strong>文字冒險類的小遊戲</strong>(如果不清楚這是什麼可以參閱:<a href="https://www.youtube.com/watch?v=TP-95frO3E0" target="_blank" rel="noopener">[Youtube]【桓哥】《Lucy－她所期望的一切－》</a>，想做出的感覺是讓人在一邊觀看小說時，能有相關的背景音樂和情境圖片，試著讓人有身歷其中的感覺，小說方面我還沒問過原作者，不過當初我在巴哈上看到一部讓我看到淚崩的小說，如果未來有機會我很想以它作為雛型去開發(<a href="https://forum.gamer.com.tw/G1.php?bsn=07650&amp;parent=4682" target="_blank" rel="noopener">是妳，改變了我，在這MapleStory。</a>)。素材的部份我很辛勞的再拆楓之谷的wz檔然後在各種PS(PS到頭很疼)，所以目前圖片跟音樂、特效都是從那裏面挖出來的xD</p><p>目前進度緩慢，只做了最前面的UI介面(Demo: 請至Github上觀看)，還有到伺服器選擇的部分(到時候想拿來當作小說章節選擇的功能)。</p><p><strong>我也不知道我什麼時候會完成它，不過就是當作對於小時後一個夢想和憧憬的實踐，想到就做一點想到就做一點這樣</strong>。發這篇文主要是想說，我同時也把我的所有專案都放上Github了，如果有人對於目前做的東西有興趣想下載來改成自己想做的東西我也沒意見，只希望到時候能附註出處就好了xD</p><p>Github: <a href="https://github.com/john850512/MapleHistory" target="_blank" rel="noopener">MapleHistory(喜歡或支持的話請給我星星&gt;&lt;)</a></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c# </tag>
            
            <tag> unity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[ML]Colab介紹&amp;如何在Colab上使用Tensorboard</title>
      <link href="/posts/74000ffe/"/>
      <url>/posts/74000ffe/</url>
      
        <content type="html"><![CDATA[<p>以往在撰寫Python程式碼時，我都是使用<strong>Jupyter-notebook</strong>做為開發環境，互動式介面這個特色真的很吸引人，能夠區塊式的執行程式碼，並可以搭配Markdown語法將程式碼編輯成類似筆記的形式。</p><p>但開始接觸深度學習後，開始遇上效能的問題了，比方說使用CNN訓練CIFAR-10在我的電腦上一個epoch需要將近兩分鐘，整個訓練下來花上了不少時間……，於是我最近開始改用Google的Colab做為開發環境，<strong>Colab一大特色就是提供了GPU(</strong>Nvidia Tesla K80 GPU<strong>)讓使用者可以在訓練的時候使用他們的GPU進行加速！！光是這點就值得我來使用Colab了，實測後使用相同的code訓練CIFAR-10一個epoch只需要9秒</strong></p><a id="more"></a><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/unnamed-file.png" alt="unnamed-file"></p><hr><h2 id="colab介紹"><a href="#colab介紹" class="headerlink" title="colab介紹"></a>colab介紹</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/1_g_x1-5iyrn-smdvucceiww.png" alt="1_g_x1-5iyrn-smdvucceiww">colab網址: <a href="https://colab.research.google.com" target="_blank" rel="noopener">https://colab.research.google.com</a></p><p>使用時必須登入Google帳號，之後撰寫的檔案會存在你的雲端硬碟裡面。介面很像Jupyter-notebook，所以使用過Jupyter的人可以很快速上手，並且檔案最後也可以轉成(.ipython)，非常方便。不過第一次使用時覺得較困擾的是快捷鍵的部分則不太一樣，比方說:</p><ul><li>Ctrl + M + M: 將該區塊變成Markdown模式</li><li>Ctrl + M + D: 刪除該區塊</li><li>Ctrl + M + A: 在上方新增一區塊</li><li>Ctrl + M + B: 在下方新增一區塊</li></ul><p>以上是我自己常用的幾個快捷鍵，執行則沒有差異，Ctrl + Enter是執行該區塊的程式碼，Shift + Enter也是執行該區塊的程式碼，並將目前框框跳到下一區塊。</p><p>詳細的使用教學我就不贅述了，畢竟網路上有很多介紹的比我清楚的人xD</p><h2 id="TensorBoard簡介"><a href="#TensorBoard簡介" class="headerlink" title="TensorBoard簡介"></a>TensorBoard簡介</h2><p>Tensorboard是一個能夠視覺化呈現訓練過程中的一些相關數據(Accuracy、Loss、Gradient…)的套件，當使用Tensorflow或Keras在進行模型訓練的時候，可以透過這個套件去查看訓練中的一些數值變化，也可以查看tensorflow diagram。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/21.png" alt="21"><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/31.png" alt="31"><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/41.png" alt="41"></p><p>一般在自己裝置上進行模型訓練的，只要在command下開啟tensorboard並指定訓練時的Logdir，就可以進入localhost:6060去查看tensorboard了。不過使用Colab的話則有一個問題:</p><p><strong>Colab是Google提供我們一個虛擬環境讓我們執行程式碼的地方，如果再Colab上架設tensorboard，會因為我們不是localhost而無法進入該service</strong>，此時不是在自己裝置上執行的我們要怎麼使用tensorboard呢?</p><hr><h2 id="在Colab上使用Tensorboard"><a href="#在Colab上使用Tensorboard" class="headerlink" title="在Colab上使用Tensorboard"></a>在Colab上使用Tensorboard</h2><p>網路上找到一個英文版的解決方法，先附上參考來源: <a href="https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/" target="_blank" rel="noopener">Quick guide to run TensorBoard in Google Colab</a>，接下來我會用中文簡單介紹一下文章的內容，參照著該文章做，很快就可以在Colab上架設tensorboard。</p><p>首先，關鍵是我們要使用一個套件: <strong>Ngrok，這個套件可以把你原本在localhost上的service轉成一個網址，當外部有人連上這個網址的時候Ngrok就會把外部的請求轉送給你localhost上的某個port。</strong>架過網站的都知道，如果apache server架好了，但網路的IP不是public IP，外面的人怎麼連都還是連不進來，這時候如果用了Ngrok就會產生一個新的網址，外面的人只要連到這個網址就會被轉進來我們在localhost上某個port的服務。</p><p>Ngrok免費版的有限制，每天好像只能轉特定的數量，不過如果只是想使用Tensorboard來觀察模型訓練過程的很夠用了。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/51.png" alt="51">這個網站利用的也是同樣的原理，我們在google的virtual machine上架ngrok和</p><p>tensorboard，得到一組網址後，就可以從外部透過這個網址進入virtual machine上的tensorboard了！</p><p>以下是擷取自該網站的步驟，我把他翻成中文並用自己的理解重新介紹: </p><p>Step1. 在Colab下下載Ngrok，在前面加上!就可以使用linux下的command </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!wget https:&#x2F;&#x2F;bin.equinox.io&#x2F;c&#x2F;4VmDzA7iaHb&#x2F;ngrok-stable-linux-amd64.zip</span><br><span class="line">!unzip ngrok-stable-linux-amd64.zip</span><br></pre></td></tr></table></figure><p>Step2.指定訓練過程的紀錄檔(logdir)要存放在virtual machine下的哪個位置，並開啟tensorboard service(port 6006) </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LOG_DIR &#x3D; &#39;.&#x2F;log&#39;</span><br><span class="line">get_ipython().system_raw(</span><br><span class="line">&#39;tensorboard --logdir &#123;&#125; --host 0.0.0.0 --port 6006 &amp;&#39;</span><br><span class="line">.format(LOG_DIR)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Step3.開啟ngrok service，並綁定port 6006(tensorboard) </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_ipython().system_raw(&#39;.&#x2F;ngrok http 6006 &amp;&#39;)</span><br></pre></td></tr></table></figure><p>Step4.產生一組網址，這組接下來只要從外部連到這組網址，就會導向virtual machine的port 6006，也就是colab裝置內的tensorboard了！ </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">! curl -s http:&#x2F;&#x2F;localhost:4040&#x2F;api&#x2F;tunnels | python3 -c \</span><br><span class="line">&quot;import sys, json; print(json.load(sys.stdin)[&#39;tunnels&#39;][0][&#39;public_url&#39;])&quot;</span><br></pre></td></tr></table></figure><p>最後，網址裡面也有附上colab的檔案，可以下載下來對照就知道怎麼做了。</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> machine learning </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高大資工程式加強班-教學心路歷程</title>
      <link href="/posts/a3a36bf7/"/>
      <url>/posts/a3a36bf7/</url>
      
        <content type="html"><![CDATA[<p>這篇拿來記錄一下我的大學助教生涯(2017~2018)，並在文章最後提供了我上課時給學弟妹用的加強班講義。</p><p>我從大三開始，開始擔任高大資工的程式課程助教(大一必修課程)，系上這門課有一個特色就是，<strong>助教會利用晚上的時間開設程式加強班(非強制參加)，加強修課同學的程式實作能力</strong>然後我的加強班的講義都是自己製作(為了<del>好玩</del>不要讓內容太死板，每學期的風格我都弄得會不一樣xD)</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/1.png?w=1400&amp;h=" alt=""></p><a id="more"></a><h2 id="關於加強班"><a href="#關於加強班" class="headerlink" title="關於加強班"></a>關於加強班</h2><p>在109級(我是107級)開始，這門課被分為上下學習(當初我修的時候只有一學期)，上學期主要在教導同學C語言的基本概念(變數、流程控制、迴圈、函式……)，下學期則是訓練同學的程式能力，旨在通過系上的畢業門檻(CPE，一次兩題或累積四題)。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/9.jpg?w=1000&amp;h=" alt=""></p><p>在加強班開設的規劃上，上學期我習慣配合白天教授的上課進度；下學期的部分則比較隨興，我會依據自己的喜好來決定要教什麼內容給他們(<del>隨便教</del>)，不過雖然是這樣說，這兩年我下學期大部分都是在教Linked list、大數運算、stack &amp; queue、backtracking、DFS…這些稍微有點難度的資料結構和演算法。</p><p>[gallery ids=”2387,2388,2389,2390” type=”rectangular”]  </p><p>在教加強班最慘的是大三那年，由於教授公務繁忙，有時候會叫助教去代他白天的正課，很不幸的三年級我的必修課衝到了大一的程式必修課，跟系上求助的結論是已經定了無法更改…，造成有時候必修我就必須跟老師請假去大一那邊上課，這真的很慘…為了推甄必須穩住自己的課業，所以得花上不少時間自己想辦法補回來沒聽到的部份。下學期老師則是幾乎白天的課都要我去，可是大三下的必修仍然跟大一課程衝到，無奈之下只好跟學長求助，變成兩位助教一起帶當時的大一學弟妹(真的很感謝當時那位學長願意幫忙，不然我真的要撞牆了)。後來升上大四就沒有這個問題了。</p><hr><h2 id="關於競程"><a href="#關於競程" class="headerlink" title="關於競程"></a>關於競程</h2><p>我是大一升大二的暑假開始接觸競技程式的，那時暑假跑去參加的交大的NCTU PCCA，了解了何謂程式競賽，也學了一點資料結構和演算法回來。但<strong>由於那時候系上的競程風氣很差，系上根本沒幾個人會想要出去比程式競賽，我連想組個隊出去都有不小的難度。大二時也有跟教大一程式程式設計的教授談過這個話題，希望系上是否能幫忙帶動一些程式競賽的風氣，不過教授當時似乎對此沒有很有興趣…(題外話，後來在學弟妹的努力下，系上陸續有些結果出來，此時教授才開始重視這一塊…想想真令人心寒)</strong></p><p>後來有些有興趣的學弟陸續進來高大資工，所以想說也是時候了，便成立了一個社團給有興趣的學弟妹加入，順便利用助教的權限在大一上課時推推系上競程的風氣。大一下學期的課程我和一些人自掏腰包辦一個小組賽給大一學弟妹體會一下程式競賽大概是怎麼一回事，前幾名請他們吃東西當作獎勵(雖然兩年都是肯德基)。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/10.jpg?w=1400&amp;h=" alt=""><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/15.png?w=1200&amp;h=" alt=""></p><h2 id="關於成果"><a href="#關於成果" class="headerlink" title="關於成果"></a>關於成果</h2><p>後來成立了社團後，系上參加對外競賽的隊伍明顯變多了，<strong>每個月的ITSA也越來越多學弟妹主動參加，也有熱心的學弟願意幫大家借教室讓大家可以聚在一起考，這是我最感動的一件事。</strong>對外競賽也得到了兩次桂冠盃闖關組的佳作(一年是我跟學長一起參賽拿的，另一年是我和學弟組隊拿下的)。</p><p>[gallery ids=”2392,2393” type=”rectangular”]</p><p>系上最重視的CPE部分，這兩年在學弟妹很爭氣地努力練習下，<strong>連續兩年大一就有半數同學通過系上的畢業門檻。</strong> 這讓教這門課的教授很是開心，還叫我做了一個數據圖表給他去對外演講如何提升學生的程式能力，由於圖表是我做的所以偷偷放一下。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/07/13.png" alt="13"></p><hr><p>最後，很慶幸自己有有這個運氣可以當上程式加強班助教，這兩年利用加強班增進了自己的表達和報告能力(還是覺得我講得很爛的學弟妹就抱歉了xD)，也因為助教的關係跟學弟妹比較熟(不然大四基本上沒有什麼機會認識大一的人)，這一年有很多加強班都沒開了(大二的資料結構和OOP)，但仍希望程式設計的加強班能夠一直延續下去，好好幫助剛進入大學的學弟妹培養好程式設計的基礎。</p><p>所有的加強班講義我放在我的github上，給任何有需要的人參考，只求使用時附註來源: <a href="https://github.com/john850512/Notebook/tree/master/%E5%A4%A7%E4%B8%80%E7%A8%8B%E5%BC%8F%E5%8A%A0%E5%BC%B7%E7%8F%AD" target="_blank" rel="noopener">高大資工程式加強班講義(109~110級)</a></p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>華為手機系統更新後不能使用Google服務(play store、youtube)</title>
      <link href="/posts/8f046e7b/"/>
      <url>/posts/8f046e7b/</url>
      
        <content type="html"><![CDATA[<p>在今年第一次使用了華為的手機，由於是使用二手機，所以拿到的時候別人已經幫我弄好相關設定了(可使用Google Play Store下載相關App)。</p><p>在前幾天手機跳出通知要我進行系統更新，當時沒想太多就按下更新了，結果這一更新問題來了：”我無法開啟google play store，會不斷閃退；對於Youtube則是開啟後沒有畫面”，當下一直不知道發生什麼事情，以為是系統更新後產生的BUG</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/06/screenshot_20180627-023212.jpg" alt="Screenshot_20180627-023212"></p><a id="more"></a><p>後來查了很久才發現，因為Google全面退出大陸市場，所以有些大陸品牌的手機沒有內建Google Framework，<strong>Google Framework是一個Google的框架，有了他可以使用Google下的其他APP，並可以讓他們彼此進行資訊交流、溝通…，沒有Google Framework就無法使用Google的相關APP。</strong></p><hr><p>解決方法:</p><p><strong>下載GMSInstaller這個APP(可以先從電腦下載好後把APK檔移到手機進行安裝)</strong>，他會一鍵安裝Google Framework以及一些相關的應用，安裝完就好了。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/06/screenshot_20180627-022953.jpg" alt="Screenshot_20180627-022953"></p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>[ML]Logistic Regression(SGD version)</title>
      <link href="/posts/83d3de3e/"/>
      <url>/posts/83d3de3e/</url>
      
        <content type="html"><![CDATA[<p>這次課堂的Assignment是要使用106個features來預測一個人的年收入是否有達到50K，使用Logistic Regression(SGD version)，Logistic Regression與Linear Regression的想法蠻相近的，這裡就以Linear Regression為基礎來簡述一下我的理解中何謂Logistic Regression，以及SGD(Stochastic Gradient Descent)是什麼。</p><h2 id="作業說明"><a href="#作業說明" class="headerlink" title="作業說明"></a>作業說明</h2><p>你可以直接在我的<a href="https://github.com/john850512/Intelligent_System/tree/master/Assignment3" target="_blank" rel="noopener">github</a>看到英文版的簡介，這裡就不贅述了。</p><a id="more"></a><h2 id="Logistic-Regression簡介"><a href="#Logistic-Regression簡介" class="headerlink" title="Logistic Regression簡介"></a>Logistic Regression簡介</h2><h3 id="Logistic-Regression是什麼-跟Linear-Regressin差在哪裡"><a href="#Logistic-Regression是什麼-跟Linear-Regressin差在哪裡" class="headerlink" title="Logistic Regression是什麼?跟Linear Regressin差在哪裡?"></a>Logistic Regression是什麼?跟Linear Regressin差在哪裡?</h3><p>複習一下上次<a href="https://meetonfriday.com/posts/848f9596/">[Python]Predict PM2.5 Use Linear Regression – 1</a>介紹到的，Linear Regression的公式為：</p><p>$y = wx + b$</p><p>我們想透過學習的方式找到一組w,b使得x上的點對於真正的y差距越小越好。</p><p>不過這個y值畢竟是連續的，如果我今天想做的預測只有兩類(binary classification)，例如我想知道明天會不會下雨(會-&gt;1 ; 不會-&gt;0)?</p><p>這時候我們可以透過將Linear Regression的結果加上一個sigmoid函式進行轉換，sigmoid畫出來的結果如下：</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/04/1200px-logistic-curve-svg1.png" alt="1200px-Logistic-curve.svg"></p><p>從圖中可以發現，<strong>sigmoid轉換出來的結果必定介於[0,1]之間，如果原本的y值越大，那sigmoid = z(y)的值就會越接近1，反之z(y)的值就會趨近於0，我們就可以把這個值視成一個機率來處理上述的binary classification問題</strong>，這就是Logistic Regression的原理。</p><h3 id="如何評估找出來的模型是最佳的-Loss-Function"><a href="#如何評估找出來的模型是最佳的-Loss-Function" class="headerlink" title="如何評估找出來的模型是最佳的(Loss Function)?"></a>如何評估找出來的模型是最佳的(Loss Function)?</h3><p>Linear Regression使用的Loss function實際上自MSE(Minmum Square Error)，也就是最小平方法。<strong>Logistic Regression使用Cross Entropy(亂度)來評估”目前學習到的模型與問題理想的模型存在的差異大小”</strong></p><script type="math/tex; mode=display">ln(L(w,b)) = -sigma( y' * ln(f(x)) + (1-y')ln(1-f(x))  )</script><p>y’是真正的label，f(x)是我們預測的label(經過sigmoid後的)，如果我們預測的f(w)和y實際上差異很小的話，代表我們的模型離真正的理想模型差異很小，那Cross Entropy的值就會相對的小。所以在訓練中我們要去不斷的minimun這個Loss function。</p><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>每一次訓練中，我們都想使ln(L(w,b))的值越來越小，所以就會使用到Gradient Descent，把這個式子分別對於w和b做偏微會得到：</p><script type="math/tex; mode=display">wi \leftarrow wi - learning rate * ( sigma( f(x)-y' ) * xi )</script><script type="math/tex; mode=display">bi \leftarrow bi - learning rate * ( sigma( f(x) - y' ) )</script><p>(這個偏微的推導中使用到了chain rule，這邊我還不夠熟練囧…)</p><p>這個式子實際上跟Linear Regression的式子一樣，所以之後再實作其實不會有太大的差別。</p><hr><h2 id="Stochastic-Gradient-Descent-SGD"><a href="#Stochastic-Gradient-Descent-SGD" class="headerlink" title="Stochastic Gradient Descent(SGD)"></a>Stochastic Gradient Descent(SGD)</h2><p>假設你有100筆的資料，每筆都有106個features，則你的x會是(100, 106)的矩陣，y是(100, 1)，而你的w會是(106, 1)，b則是(1, 1)。</p><p>那做一個epoch就會牽涉到一個100*106(x)的大矩陣運算，如果今天資料很大，這個x矩陣也會變得很大使得運算上效率降低。</p><p>SGD的方式是使用了batchs(批次)的想法，每一次只針對一筆(或者，多筆)來進行訓練，雖然訓練的次數變多了，但整理的運算量會比較好一點。<strong>以gradient descent的角度來看，原本的方式是看過所有資料後走一大步，而SGD則是每次都看一個(或者，多個)資料，然後走一小小步。</strong></p><p>所以像這個例子，如果使用SGD，我的x可能就變成(1, 106)的矩陣、y是(1, 1)、w是(106, 1)和b是(1, 1)。不過SGD的batchs並不一定只能一筆，我也可以一次25筆25筆做…</p><p>借個老師上課時的PPT來介紹一下原本的方法跟SGD對於在做Gradient Descent時的差別：</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/04/sgd.png" alt="SGD.PNG"></p><p>SGD在訓練過程中Loss function會有起伏的狀況，因為你走的一小步可能並不是對於真正的問題比較好的一步，但是當走的步數夠多的時候，還是可能會走到理想的終點(全域最佳解)。</p><hr><h2 id="Normalization-amp-Standardization"><a href="#Normalization-amp-Standardization" class="headerlink" title="Normalization &amp; Standardization"></a>Normalization &amp; Standardization</h2><p>在這次的Assignment中可以發現有些屬性的數值非常大，所以該屬性只要有一點變動就很容易影響全局的變動。我們可以透過Normalization(或Standardization)將屬性的離散程度進行轉換，如此一來有機會增加training的速度和正確率。</p><p>Normalization(正規化):將資料的分布變成介於0和1之間的數值，公式為:</p><p>$(x - min) / (max - min)$</p><p>Standardization(標準化):將資料的分布變成平均值為0，標準差為1的分布，公式為:</p><p>$(x - mean(x) ) / std(x)$</p><p>最後在Standardization下，我的training set 有著~85%的Accurancy。</p>]]></content>
      
      
      <categories>
          
          <category> study </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[大學推甄]我適不適合讀資工？</title>
      <link href="/posts/3850b79a/"/>
      <url>/posts/3850b79a/</url>
      
        <content type="html"><![CDATA[<p><strong>懶人包：這篇是寫給要準備升大學想填資工系的高中生們，簡介一下資工系到底在幹嘛。</strong></p><p>又到了這個高中生要面臨大學面試的時期，這幾天主任找我討論面試當天的流程的時候才突然想到我居然已經要畢業了，四年的時光一下子就過去了阿，大一剛進來的生活我還歷歷在目呢…(感嘆)</p><p>好啦這篇不是要講廢話的，這幾年資訊相關科系越來越夯，就連教育部都把程式設計納入課綱裡面了，相信未來會有越來越多人想往這方面就讀(不要再來跟我搶飯碗了)。不過我一直覺的行行出狀元，有些人天生就是念財經的料，有些人對於設計、美術就很有底子……，希望大家能夠想清楚自己到底想不想往這個領域邁進，不要只是一昧的覺得這個很夯就往這裡走(沒錯，真的別來跟我搶飯碗)，或是父母覺得你該讀資工就選填資工系。</p><p><strong>“如果你進資工系只是因為分數到就填了，那你有很大的機率會遇到這個網站所說的情況(<a href="https://www.slideshare.net/ccckmit/ss-57048659" target="_blank" rel="noopener">用十分鐘決定要不要念大學《資訊工程系》</a>)”</strong></p><p>因為擔任系上程式助教的關係，我看過不少學弟妹因為進來後發現自己不適合而萌生轉系或轉學考的念頭了，如果傻楞楞的什麼都不知道就進來了或許你也會有這種情形，所以打算寫一篇來介紹一下資工系都在幹嘛，希望可以幫助還在困惑的小高三xD</p><a id="more"></a><h2 id="讀資工需要具備什麼特質"><a href="#讀資工需要具備什麼特質" class="headerlink" title="讀資工需要具備什麼特質"></a>讀資工需要具備什麼特質</h2><p>首先來談談我認為讀資工需要具備什麼特質呢？</p><ul><li>具有耐心</li><li>不怕挫折</li><li>不懼怕英文</li><li>能夠接受長時間待在電腦</li><li>有自律性(能夠主動學習)</li></ul><h3 id="具有耐心、不怕挫折"><a href="#具有耐心、不怕挫折" class="headerlink" title="具有耐心、不怕挫折"></a>具有耐心、不怕挫折</h3><p>進資工後，隨著你在寫得程式規模越大，你越會遇到一些無從解決的問題產生，這些問題常常令你豆頁痛，甚至令你想要走出房間去便利商店買一包綠色乖乖回來放在電腦前，祈禱問體從此不再發生……，我要說的是這種情況在資工很常發生，但<strong>如果你沒有辦法沉住氣去面對問題而是很容易放棄，那麼或許你不太適合這門科系。</strong></p><h3 id="不懼怕英文"><a href="#不懼怕英文" class="headerlink" title="不懼怕英文"></a>不懼怕英文</h3><p>資工系的書都是原文書，雖然大部分教授都是中文授課，但是你還是要有基本的英文閱讀能力，並且你不能畏懼讀英文(像我以前如果看到滿滿的一篇英文文章我就會懶得看，這是不對的!)，因為未來在Google(後面會說到)的時候你的解答都會從英文網站上得到。</p><p>大家都說英文要好，到底要到什麼程度呢？來個例子吧！請<strong>Google “Stackoverflow”，進去裡面隨便點一個問題，能大致理解問題的意思就足夠了</strong>(程式碼不懂沒關係)，這個網站是資工系會很常用到的網站，大家對於程式有任何的問題都會在這上面發問，如果有問題很多都可以在這上面找到解答。</p><p>P.s 如果你點進去發現問題完全看不懂有兩個情況:</p><ol><li>可以趁還沒升大學前把握時間稍微加強一下英文，對於未來會很有幫助的</li><li>你點到的問題太難了QQ請換一篇點點看吧</li></ol><h3 id="能夠接受長時間待在電腦前"><a href="#能夠接受長時間待在電腦前" class="headerlink" title="能夠接受長時間待在電腦前"></a>能夠接受長時間待在電腦前</h3><p>來來來想一下進入資工後妳的大學生活：你有很多的系上課程都會使用電腦去完成作業(畢竟要寫程式嘛)，資工系有很多課程期末會要求所謂的”期末專題”，舉個例子：如果你修了一堂”java程式語言設計”，老師說期末請用java程式語言撰寫一個程式來發表，然後你可能期末前就會花很多時間在做專題，甚至做完發現窗外天亮了也是常有的事情。</p><p>(在這裡順便幫資工系平反一下～常常有人說資工系很宅，不過就像設計系很辛苦常常要在工作室做作品那樣，我們很大的時間都必須使用電腦來完成事情，所以有時候不是我們不想出門，而是真的沒時間出門阿xD)</p><p><strong>寫程式是一個需要長時間保持集中力的事情，所以你可能會花很多時間坐在電腦前面執行這件事，更別提當你的休閒娛樂是建立在使用電腦上(玩遊戲、看韓劇、滑臉書、聊賴……)，你一天24小時除了睡覺時間都在使用電腦也不是沒有可能</strong>，所以如果你很抗拒長時間使用電腦這件事，或許可以考慮考慮別的科系。</p><h3 id="有自律性-能夠主動學習"><a href="#有自律性-能夠主動學習" class="headerlink" title="有自律性(能夠主動學習)"></a>有自律性(能夠主動學習)</h3><p><strong>講個很現實的，這或許是最重要也是最基本的條件，如果你辦不到那還是不要讀資工吧。</strong></p><p>來講個生活層面的例子：今天安C教練教了櫻木花道打籃球的基本技巧(運球、投球姿勢……)，好那學完後櫻木變成高中第一的籃球員了嗎？根本不會。他得運用這些基礎做出進一步的技術(胯下過人、卡位搶籃板……)，花道的籃球技術才會不斷進步。</p><p>回到資工的領域，學校教授教的都只是基本的東西，當你學完這些東西後呢？你得要有自學的能力去學習課外的知識，比方說老師教的內容我可以如何去應用。資訊不斷在推陳出新，大學教授一堂課只有18週*3小時，他連一本原文書的1/3能不能上完都是個問題了，如果你沒有辦法自主學習吸收新知，那你很快就會落後其他人一大截。</p><p>想一下小明每天比你多了一小時的自主學習(也就是你打一場LOL的時候他在看書)，四年後你們的差距到底會多大？大概有大象那麼大吧。</p><hr><h2 id="資工系四年都在幹嘛"><a href="#資工系四年都在幹嘛" class="headerlink" title="資工系四年都在幹嘛"></a>資工系四年都在幹嘛</h2><p>接下來談談資工系四年都在幹嘛？</p><ul><li>學習計算機概論、程式語言</li><li>學習網路架構、作業系統架構</li><li>學習資料結構、演算法</li><li>學習應用層面</li></ul><h3 id="學習計算機概論、程式語言"><a href="#學習計算機概論、程式語言" class="headerlink" title="學習計算機概論、程式語言"></a>學習計算機概論、程式語言</h3><p>這是大一大二會接觸到的課程，讓你對資訊方面的知識有更深的了解(Ex:二進位、十六進位…)，通常學校會教的程式語言不會很多，不過當你學透一種語言後便轉換到另一種語言上時會輕鬆很多。</p><h3 id="學習網路架構、作業系統架構"><a href="#學習網路架構、作業系統架構" class="headerlink" title="學習網路架構、作業系統架構"></a>學習網路架構、作業系統架構</h3><p>網路大家都在用、電腦大家都在用，但到底是什麼樣的原理能夠讓你的裝置上網?什麼樣的流程能夠讓你執行電腦中的程式?在資工系會讓你理解這些原理，而不是像其他人只是單純的能用就好。了解這些原理之後有助於你去對相關的應用進行開發(例如現在網路架構學得好的話或許可以朝5G的開發有進一步的深造)。</p><h3 id="學習資料結構、演算法"><a href="#學習資料結構、演算法" class="headerlink" title="學習資料結構、演算法"></a>學習資料結構、演算法</h3><p>這跟程式語言是有關係的，當你會寫程式後，如何讓你的程式有更好的執行效能就是藉著資料結構和演算法來改善。舉個白話的例子同樣從高雄到台北，你可以選擇翻山越嶺，但你也可以選擇做飛機，這兩種方式所造成時間的差別可以想成兩種不同效率的程式造成的差別。</p><h3 id="學習應用層面"><a href="#學習應用層面" class="headerlink" title="學習應用層面"></a>學習應用層面</h3><p>這是我覺得最有意義的事情，學了知識總是要拿出來用。系上會有些課教你如何利用你所學的知識去做出進一步的應用，例如修完影像處理你可以使用程式寫出類似修圖軟體的功能；修完網路程式設計可以寫出線上聊天室；修了網頁程式設計你可以自己寫出一個自己的網站…… 如果想讀某間學校的資工系或許可以去看看他們系上的課程地圖，你可以看到未來會學到什麼課程。</p><hr><h2 id="如果你已經決定要念資工系了，你可以做什麼"><a href="#如果你已經決定要念資工系了，你可以做什麼" class="headerlink" title="如果你已經決定要念資工系了，你可以做什麼"></a>如果你已經決定要念資工系了，你可以做什麼</h2><p>最後，在高三下到大學的這段期間，如果你已經決定要念資工系了，你可以做什麼事情呢？(不要騙我很忙了，我那時根本無聊到每天都在教室滑神魔看漫畫)</p><ul><li>自學程式語言</li><li>學Google</li><li>練英文</li></ul><h3 id="自學一點程式語言"><a href="#自學一點程式語言" class="headerlink" title="自學一點程式語言"></a>自學一點程式語言</h3><p>建議可以自己上網先學C或Java，這兩個語言的中文資源很多，可以找到很多學習的資訊，如果不知道如何學習，最快速的方法就是去買一本書把他K完，因為書本會循序漸進地告訴你要學哪些東西。</p><p>如果你在進大學之前學過一些程式語言了，代表你具備了一定的邏輯思維，(通常)這會讓你在大一的時候比其他同學輕鬆很多。如果沒有也沒關係，但是相對的在剛接觸大學相關程式語言課程的時候，你或許會有一定程度的挫折感，因為剛開始你沒辦法把一些想法轉換成程式碼，不過這是可以克服的，<strong>你只是需要比其他同學更多的耐心跟毅力，當你發現你寫的程式有錯誤(BUG)的時候不能放棄，而是不斷的去尋找解決問題的方法。</strong></p><h3 id="學會、學好Google"><a href="#學會、學好Google" class="headerlink" title="學會、學好Google"></a>學會、學好Google</h3><p>我每次教學弟妹的時候第一堂課我總是跟他們說<strong>“資工的第一必備技能不是程式設計，而是學會如何Google”</strong>，至今我仍然這麼覺得。Google看似簡單，但要如何使用正確的關鍵字(keyword)去表達你的問題？在資工系未來你會遇到無數的問題，有些問題甚至沒有人可以問，在只能問Google的狀況下你有辦法找到解答嗎？</p><p>舉個現實的例子，如果有在玩遊戲的在社群中看到有網友發文問一些很基本的遊戲問題，底下就會嗆說請先爬文，然後你就會看到發問的人說他爬過了可是根本找不到資料，問題出在哪裡呢？他根本找錯了關鍵字，所以如何正確的利用關鍵字去找出你想要的解答是很重要的技能。</p><p>網路上有一篇文章詳細介紹了學習資源多寡對於學習的影響(<a href="https://www.inside.com.tw/2015/03/27/why-learning-to-code-is-so-damn-hard" target="_blank" rel="noopener">為什麼成為一名工程師這麼難 —— 從程式新手到準工程師的必經之路)</a>，如這篇所述，如果你的google技能不好，到了中期階段學習資源急速下降的時候，你根本無法找到正確的資訊。</p><h3 id="學好英文"><a href="#學好英文" class="headerlink" title="學好英文"></a>學好英文</h3><p>前面提到了，英文很重要的，大學很多學校也都有英文多益畢業門檻，你如果進大學前分數夠甚至可以直接抵免大一的英文課程，好處多多啊。</p><p>打了這麼多，希望或多或少可以幫助到一些還在對你的未來充滿困惑的人～～</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[ML]DataConversionWarning with K-NN(Sklearn)</title>
      <link href="/posts/c27a2dc7/"/>
      <url>/posts/c27a2dc7/</url>
      
        <content type="html"><![CDATA[<p>上次在練習sklearn的KNN分類時，遇到了一些問題，了解後覺得對於資料的維度能夠理解的更加清楚，所以在這裡記錄下來。 首先先簡述一下我在做KNN的流程： </p><ol><li>使用的資料集是sklearn內建的資料集進行練習 </li><li>用<code>sklearn.model_selection</code>的<code>train_test_split()</code>把資料切成training set / testing set </li><li>使用sklearn的KNeighborsClassifier進行分類、預測</li></ol><a id="more"></a><p>在處理過程中我遇到了兩個DataConversionWarning，分別是不同的原因造成的，以下來講一下：</p><ol><li><strong>DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().</strong></li></ol><p>在處理output(label)的array的時候我的label原本是一個(569,1)，但它顯示了warning說不能用二維的array，所以要透過np.ravel()這個function將資料轉成一維的array，也就是(569,)，處理完就沒warning了。</p><ol><li><strong>DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.DeprecationWarning)</strong></li></ol><p>這個則是在predict的時候，我的input(test_feature)原本是一維的資料，所以我就用一維的array代進去了，Ex: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction_data = np.array([<span class="number">20</span>,<span class="number">4.3</span>,<span class="number">5.5</span>])</span><br></pre></td></tr></table></figure><p>然後就產生這個warning告訴你說data input不能是一維的，要透過np.reshape(1,-1)把資料轉成二維的，轉成後會變成[[20,4.3,5.5]]。 </p><p>補充一下reshape的-1代表自動計算轉換前的array個數，reshape(1,-1)是變成一條橫的array，reshape(-1,1)是變成一條直的array。 </p><hr><p>在知乎上看到一則網友對此的說明，覺得比較好理解位什麼要這樣處理input / output，我直接引用他的話： </p><blockquote><p>“我觉得你对机器学习算法还是不了解的，其实是这样的： 你的输入的数据X应该是数据的特征向量，y是特征向量对应的标签。每一个样本都有一个特征向量，这样你输入的X一定是二维数组才对，y如果是单标签就为一维数组，若为多标签或者像神经元网络那样的标签就为二维数组。 你的x=[[6],[8],[10],[14],[18]]，算法可以理解为只有一个特征项，但是如果为x=[6,8,10,14,18]，你让算法怎么理解” </p></blockquote><p>附上原網址：<a href="https://segmentfault.com/q/1010000010743191" target="_blank" rel="noopener">https://segmentfault.com/q/1010000010743191</a></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[ML]Predict PM2.5 Use Linear Regression-2</title>
      <link href="/posts/1d86c42c/"/>
      <url>/posts/1d86c42c/</url>
      
        <content type="html"><![CDATA[<p>上一篇連結：<a href="https://meetonfriday.com/posts/848f9596/">[Python]Predict PM2.5 Use Linear Regression-1</a></p><p>上篇用自己的想法簡單了闡述Linear Regression、原理和實作過程，但畢竟還是理論，面對實際資料的時候又是另一回事了，這篇要來講處理實際data的過程以及遇到的問題還有解決方法，做一些紀錄：</p><a id="more"></a><p>1. 打開dataset先觀察一下，總要先知道資料長什麼樣子才有辦法做處理</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/03/31.png" alt="31.png"></p><p>這份dataset每小時紀錄一次18種不同的空氣資料，一天紀錄24小時，且一個月紀錄20天，總共12個月。</p><p>=&gt;資料總共有 1(index row) + 18<em>20</em>12 = 4320列(rows)，3(text) + 24 = 27行(columns)</p><p>(第一次真正的動手處理資料，由於之後的動作都是牽涉到矩陣乘法，必須第一個矩陣的列數和第二個矩陣的行數相同才能乘，所以我很重視每個矩陣的row和col數量，才不會到時候有這方面的BUG產生。)</p><ol><li>Feature Extracting：把我們需要的資料取出來</li></ol><p>觀察一下資料，前三行的資訊都是我們不需要的(我們只需要連續9小時，每小時18個不同空氣的數值，不用管是哪一天的)</p><ul><li>為了方便之後擷取data，我<strong>把4320<em>27的資料轉換成一個18</em>5760(12月<em>20天</em>1天24小時)的矩陣</strong>，也就是從1/1開始，把1/2的資料拉上來接再1/1的後面，之後把1/3拉上來接再1/1後面…，這樣一來我就可以用迴圈去從這個18*5760的矩陣連續取出資料(如果不做這一步，會很難從1/1晚上取到1/2早上，因為要處理跨天的問題)。</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/03/32.png" alt="32.png"></p><ul><li><strong>以每個月(20天*24小時 = 480筆)為單位，每9個小時的資料存成矩陣x的一列(162)，並在第163個位置存上1這個數字</strong>，這個1是用來對y = xw + b的b做乘法的，之後我們同樣的也在w矩陣的最後一個位置多存上b這個數字，如此一來，<strong>原本要更新w和b兩個變數，就可以變成更新一個w矩陣即可。</strong>我覺得這個很難講解，看下面的圖說故事吧。</li><li>此外，每個第10小時的PM2.5值則存到一個矩陣y的一列中。</li><li>在做資料萃取的時候我沒有跨月做(其實也可以做)，所以一個月總共有480- 10 + 1 = 471筆資料，乘上12個月=5652筆。</li></ul><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/03/33.png" alt="33.png"><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/03/34.png" alt="34.png"></p><p>所以做完上述兩件事後，會有y= xw 這個式子中的x和y矩陣(b被合併了)，x的size會是5652(12個月<em>一個月可以取471筆)</em>163(18個資料<em>9小時 + b)，y會是5652</em>1。</p><p><strong>到這裡來檢查一下，y=xw的式子中，y是5652<em>1，x是5652</em>163，矩陣乘法要成立的話w就必須是163*1的矩陣才行，實際上也沒錯，實際上這個式子真正的樣子是：</strong></p><p>$y = w0<em>x0 + w1</em>x1 + w2<em>x2 + w3</em>x3 + w4<em>x4 + … + w162</em>x162 + b$，w和b剛好有163個，而資料{x0,x1…,x162}共有5652組。</p><p>那麼w的163個數字應該要是什麼呢?亂數設定即可(當然也可以設0，因為0也是亂數的一種可能嘛xD)</p><ol><li>更新Weights(Gradient Descent)</li></ol><p>到這裡就輕鬆多了，我深深覺得在這個Assignment中最難的就是資料處理的部分…</p><p>現在每做一次y = xw，就等於是predict 5652筆data的PM2.5值，但一定不準，所以要用上次講到的梯度下降法去更新我們的權重，也就是w這個矩陣，式子就不說了上次有提到。</p><p>上次有說到我們要控制兩個參數：learning rate以及epochs，接下來就是<strong>不斷的調整，使得妳的Loss function越小越好。</strong>一開始我完全沒概念要如何給定這兩個值，結果發現Loss function爆炸了(指數級上升)，後來先把learning rate調小，然後逐漸增加epochs兩三次來觀察L有無下降的趨勢，如果沒有代表這個rate可能不太適合，再重新設定learning rate，然後再用不同的epochs來觀察…</p><p>最後我發現我的L降低到一個程度後就不會再降低了，所以我的model就train到這裡，雖然用這個model預測PM2.5還是會有誤差， 但誤差都介於0~20以內，所以我覺得或許是個還不錯的結果…吧。</p><ol><li>Adagrad 調整learning rate是需要一直嘗試的，但透過Adagrad可以幫助妳不用太care這個參數，它的原理大概是：<strong>learning rate會隨著妳的epochs增加而越來越小，進而達到model的收斂。就像妳打高爾夫，第一次揮桿很大力，但隨著揮桿次數越多，快到洞的時候妳的揮桿就會越來越小力，因為妳只想讓球移動一點點距離。</strong>我最後是用了這個方法來training model的，做出來的效果比不用adagrad好一點，不過也沒差太多(但是要嘗試很多次就是了)。 好啦，說了這麼多，完整的程式碼放在<a href="https://github.com/john850512/Linear_Regression_Assignment" target="_blank" rel="noopener">github</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[ML]Predict PM2.5 Use Linear Regression-1</title>
      <link href="/posts/848f9596/"/>
      <url>/posts/848f9596/</url>
      
        <content type="html"><![CDATA[<p>第二篇出爐摟：<a href="https://meetonfriday.com/posts/1d86c42c/">[Python]Predict PM2.5 Use Linear Regression-2</a></p><p>這是課堂的一次Assignment，利用OPENDATA歷史資料去predict未來的PM2.5值，使用線性回歸的方法，覺得很有很有意思，將做的過程記錄下來。</p><p>這篇會稍微介紹一下Machine Learning中Linear Regression的原理，以及如何實作，下一篇再來講述在實際面對數據時所遇到的挑戰和解決方法。</p><a id="more"></a><h2 id="作業說明"><a href="#作業說明" class="headerlink" title="作業說明"></a>作業說明</h2><p>本次作業的資料是從中央氣象局網站下載的真實觀測資料,希望大家利用linear regression或其他方法預測PM2.5的數值。本次作業使用豐原站的觀測記錄,分成train set跟test set,train set是豐原站每個月的前20天所有資料。test set則是從豐原站剩下的資料中取樣出來。</p><p>train.csv:每個月前20天的完整資料。</p><p>test.csv:從剩下的資料當中取樣出連續的10小時為一筆,前九小時的所有觀測數據當作feature,第十小時的PM2.5當作answer。一共取出240筆不重複的test data,請根據feauure預測這240筆的PM2.5。</p><h2 id="Linear-Regression簡介"><a href="#Linear-Regression簡介" class="headerlink" title="Linear Regression簡介"></a>Linear Regression簡介</h2><p>太詳細的介紹就不說了，網路上都查的到，這邊講一下自己的理解，有些可能沒有那麼正確，如果講錯還請更正：</p><p>線性回歸的基本性質就是在你手上有一些資料(data)的時候，想要找出一個模型(方程式)使得所有的data(你現在手上的，以及，未來出現的)離這條線的差距都盡可能小。</p><p>這條線如果是直線，那他的方程式就是一維的線性方程式$y = ax + b$</p><p>如果是曲線那方程式就會是二維以上，ex: $y = ax + bx^2 + cx^3 +…. + z$</p><p><strong>我們要做的就是利用手上的data(在方程式中便是x)去找出最佳的參數(a,b,c…z)。</strong></p><p>接下來會有一些問題：</p><ol><li>為什麼Linear Regression可以進行Predict?</li></ol><p>想一下回歸要做的事情，就是找到一組參數使得所有data對這條線的誤差越小越好，那麼如果我真的找到了這一條線，我就可以用一個訓練時不曾出現過的data x’代到這個方程式中得到y’，這個y’就會是我們預測的結果(因為x’和你在訓練的data都是來自於一個相同的資料分布)。</p><p>以本次作業預測PM2.5為例，OPENDATA中每小時記錄了CO、CH4、NO、NO2…各種氣體的觀察值，那麼我好奇：這些氣體的值是否一定程度上會影響到PM2.5呢?(例如一氧化碳越高，PM2.5會不會也提高呢)，假設PM2.5的值 = 2<em>CO + 3</em>CH4，那之後我只要知道CO跟CH4的值，我就可以去預測PM2.5的值了。</p><p>在上述例子中：</p><p>CO、CH4…這些值就是data，也就是x</p><p>CO的係數2和CH4的係數3則是我們要找的weight，也就是a,b,c,….z</p><ol><li>如何評估找出的線是最佳的?</li></ol><p>我們可以透過Loss Function來估算我們的參數好壞，Loss Function的公式為$\sum( (y - y’)^2)$</p><p>y是真正的data</p><p>y’是透過我們的方程式預估出來的值</p><p>平方是因為誤差有正有負，做sigma則是要將所有data誤差進行加總</p><p><strong>所以透過Loss Function，我們可以發現值越大代表資料離那條線越遠，則這條線較差。</strong></p><ol><li>如何找出最佳的線(weights)?</li></ol><p>我們剛剛說Loss Function的式子是$\sum( (y - y’)^2)$ ，將y’展開成y’ = wx + b(假設是一維的線性方程式，如果是多維的問題一樣是這個式子，把y,w,x,b都想像成矩陣即可)</p><p>變成: $\sum( (y - (wx + b) )^2)$，其中w,b是我們要求的</p><p>不難看出: <strong>要使這個Loss Function越小越好可以將問題轉換成=&gt;找出最佳的w和b</strong></p><p>先考慮w的部份，還記得微積分找一個多項式的最小/最大值怎麼做嗎?找出一階微分=0的點</p><p>就物理的意義來說，一階微分也可以被視作斜率，所以我對於一個Loss Function(L)做一階微分會有三種結果，y軸是Loss值，x軸是w，這邊搭配一張老師上課的PPT比較好理解:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/03/unnamed-file.png" alt=".png"></p><ol><li>斜率=0，那我們找到了最佳(或局部最佳)的w</li><li>斜率&gt;0，代表現在線是往右上的趨勢，那我們應該要往反方向(左下)走才會走到平地(斜率=0)</li><li>斜率&lt;0，代表現在線是往左上的趨勢，那我們應該要往反方向(右下)走才會走到平地(斜率=0)</li></ol><p>首先我們對L進行w的偏微分(因為要最小的只是w)，會得到</p><script type="math/tex; mode=display">2 * \sum ( (y - (wx + b) ) * (-x))</script><p>將原本的w減掉這個2 <em>sigma( (y - (wx + b) ) </em> (-x))，就會使w依照當前的斜率去進行更新，這就是Gradient Descent(梯度下降法)。同理參數b，所以總共有兩個式子要更新:</p><script type="math/tex; mode=display">w1 \leftarrow w0 - 2 * \sum( (y - (wx + b) ) * (-x)) * n</script><script type="math/tex; mode=display">b1 \leftarrow b0 - 2 * \sum( (y - (wx + b) ) * (-x)) * n</script><p>n是learning rate，實際上在做的時候這個變數會決定了你更改的幅度(就像你決定要往右下走，但要走多遠就是由n來控制的)，並且，你有沒有可能走過頭?所以你必須要不斷地去重複做上述的事情(做幾次在Machine Learning稱為epochs)，值到整個函數達到收斂。</p><p><strong>SO，you will determine two parameters: learning rate(n) and training times(eopchs),and then you can start your training!!</strong></p><p>現在我們知道了何謂Linear Regression、如何用它來預測、實作的步驟，下一篇將會講我在實作這個Assignment中面臨到的問題。</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Python]將字串轉變成正確的型態</title>
      <link href="/posts/9c8e3713/"/>
      <url>/posts/9c8e3713/</url>
      
        <content type="html"><![CDATA[<p>最近在實作某個作業的時候遇到這樣的問題，我用<code>read_csv()</code>將資料讀進來了，但是因為讀進來後都會變成str的型態，我想要把他們自動轉成正確的資料型態，例如，有一筆list如下:</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&quot;hello&quot;,&quot;100&quot;,&quot;3.14&quot;]</span><br></pre></td></tr></table></figure><p>我想要把它轉換成:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[&quot;hello&quot;,100,3.14]</span><br></pre></td></tr></table></figure></p><p>並且它的type分別是str int float</p><p>可是我要如何知道list內的元素到底是哪種型態，並且轉換它呢?</p><p>上網查了之後發現有兩個方式:</p><ol><li>使用<code>eval()</code>這個function，它的功用原本是將作為參數的字串當作指令執行，不過也可以透過它達成自動轉換型態。</li></ol><p>使用方式如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="string">'7'</span></span><br><span class="line">a = eval(x)</span><br><span class="line">print(type(a)) <span class="comment">#int</span></span><br></pre></td></tr></table></figure><p>詳細使用方式可以參考這篇:</p><p><a href="http://www.runoob.com/python/python-func-eval.html" target="_blank" rel="noopener">http://www.runoob.com/python/python-func-eval.html</a></p><p>2.ast.liter_eval()，這個也可以達到上面的結果，和<code>eval()</code>的差別在於<strong>“<code>eval()</code>如果無法進行轉換會出現例外訊息，而<code>ast.liter_eval()</code>會先檢查，如果無法進行轉換則不會轉換”</strong>，使用前須import  ast</p><p>最終我採用了第二種方式，並且寫成function的形式:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ast</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">type_converter</span><span class="params">(string_list)</span>:</span> <span class="comment">#change str to correct type</span></span><br><span class="line">    coverted_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> string_list:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            coverted_list.append(ast.literal_eval(i))</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            coverted_list.append(i) <span class="comment">#some still with str type (ex:date、chinese)</span></span><br><span class="line">    <span class="keyword">return</span> coverted_list</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Python]用Histogram呈現文章單詞出現數量</title>
      <link href="/posts/d64554f8/"/>
      <url>/posts/d64554f8/</url>
      
        <content type="html"><![CDATA[<p>這次的練習是統計一篇英文文章出現的單字數量，並將頻率最大的前十個單詞透過直方圖(histogram)表示出來。</p><p>一些比較有問題的點在於：</p><ul><li>拿到的文章中有著一些標點符號，ex:, . ‘ “ \ - …這種，在統計單字數量的時候如果沒有處理掉這些符號就會有不同的結果，比方說：”woman”和”woman.”會是不同的單詞。</li><li>第二個問題是在處理文本的時候發現裡面夾雜著utf編碼的內容，所以在開檔的時候要指定encoding = “utf8”</li></ul><p>繪圖的部份使用matplotlib這個lib，如果沒有安裝過的話要先行安裝，在cmd下輸入<code>pip install matplotlib</code></p><a id="more"></a><hr><p>1.</p><p>為了統計每個單詞出現的次數，我的作法是先一行一行讀取文章，針對每行先把不必要的符號切掉(<code>replace()</code>)，切完後會只剩下英文單字和空格夾雜。</p><p>之後再透過<code>split()</code>用空格把一行文字切割成一個個單詞組成的list，並透過<code>dict()</code>來統計每個單詞出現的次數：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count[i] = count.get(i,<span class="number">0</span>) + <span class="number">1</span> <span class="comment">#如果i這個index出現過就取得他的value+1，不存在則用0代替</span></span><br></pre></td></tr></table></figure><p>2.</p><p>到這邊就取得了所有單詞的出現次數，接下來要進行排序，透過lambda去設定key是dict的value，並且要由大到小排序，所以reverse = True:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">key = <span class="keyword">lambda</span> d:d[<span class="number">1</span>] <span class="comment"># d[0]是dict()的key d[1]是dict的value</span></span><br></pre></td></tr></table></figure><p>3.</p><p>把前十個印出來，sort完會是一個tuple的list，所以用個loop把前十個的key和value取出即可。</p><p>提一下一些畫圖的funcion：</p><ul><li><code>plt.title() #設置圖表的標題</code></li><li><code>plt.xlabel() / plt.ylabel() #設置圖表的x / y軸文字</code></li><li><code>plt.bar() #設置圖表的值</code></li><li><code>plt.show()顯示圖表</code></li></ul><p>然後matplotlib目前好像有個bug，印出來的順序會按照字母排序，而不是給定的list的順序，相關資訊可以看這篇：</p><p><a href="https://stackoverflow.com/questions/47373762/pyplot-sorting-y-values-automatically" target="_blank" rel="noopener">https://stackoverflow.com/questions/47373762/pyplot-sorting-y-values-automatically</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#make a string contain marks we can ingore</span></span><br><span class="line">mark_set = <span class="string">"\"\',.?!*[]#:-"</span> <span class="comment">#without space</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    count = dict()</span><br><span class="line">    document_space = []</span><br><span class="line">    filename = input(<span class="string">"please input the filename(must in the same workspace):"</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> open(filename+<span class="string">".txt"</span>,<span class="string">"r"</span>,encoding=<span class="string">"utf8"</span>) <span class="keyword">as</span> file:</span><br><span class="line">            <span class="keyword">for</span> each_row <span class="keyword">in</span> file:</span><br><span class="line">                <span class="keyword">for</span> iterator <span class="keyword">in</span> mark_set:</span><br><span class="line">                    each_row = each_row.replace(iterator,<span class="string">''</span>)</span><br><span class="line">                each_row = each_row.replace(<span class="string">'\n'</span>,<span class="string">''</span>) <span class="comment">#replace space</span></span><br><span class="line">                each_row = each_row.lower() <span class="comment"># tolower</span></span><br><span class="line">                <span class="comment">#print(i)</span></span><br><span class="line"> </span><br><span class="line">                document_space = each_row.split() <span class="comment">#split word</span></span><br><span class="line">                <span class="comment">#print(document_space)</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> document_space:</span><br><span class="line">                    count[i] = count.get(i,<span class="number">0</span>) + <span class="number">1</span> <span class="comment">#cal frequency of every word in each row</span></span><br><span class="line">            <span class="comment">#print(count)</span></span><br><span class="line"> </span><br><span class="line">            <span class="comment">#sort dict by value</span></span><br><span class="line">            sorted_count = sorted(count.items(),key = <span class="keyword">lambda</span> d:d[<span class="number">1</span>],reverse = <span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">                print(sorted_count[i][<span class="number">0</span>],sorted_count[i][<span class="number">1</span>])</span><br><span class="line">            <span class="comment">#plot</span></span><br><span class="line">            plt.title(<span class="string">"Word Count"</span>,color=<span class="string">"r"</span>)</span><br><span class="line">            plt.xlabel(<span class="string">"Word"</span>,color=<span class="string">"r"</span>)</span><br><span class="line">            plt.ylabel(<span class="string">"Count"</span>,color=<span class="string">"r"</span>)</span><br><span class="line">            <span class="comment">#in matplotlib,the output will be sorted by category, hence alphabetically...this is a bug</span></span><br><span class="line">            plt.bar([sorted_count[i][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)], [sorted_count[i][<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)],color = <span class="string">'r'</span>)</span><br><span class="line"> </span><br><span class="line">            plt.show()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">"Oops!some error happen..."</span>)</span><br><span class="line">        print(<span class="string">"Closing.."</span>)</span><br><span class="line">        exit()</span><br></pre></td></tr></table></figure><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/03/p2.png" alt="p2"></p><hr><p>更新：儘管目前繪圖存在bug，但仍可以透過先指定x軸為數字，再透過xticks()轉為string，達到排序的功能，程式碼如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"> </span><br><span class="line">plt.bar(range(len(count)), count.values(),color = <span class="string">'r'</span>)</span><br><span class="line">plt.xticks(range(len(count)),list(count.keys()))</span><br><span class="line"> </span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>結果如下：<img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/03/figure_1.png" alt="Figure_1"></p><p>github: <a href="https://github.com/john850512/BigData_Analytics" target="_blank" rel="noopener">github</a></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Python]終極密碼</title>
      <link href="/posts/626abc81/"/>
      <url>/posts/626abc81/</url>
      
        <content type="html"><![CDATA[<p>摁，就是終極密碼，沒什麼好講的。</p><p>比較值得提的是要做輸入的防呆，也就是說不能輸入小數、英文…</p><p>對於這個要求使用了try…except…解決了，原理大概是input()會回傳一個String，那如果對這個字串作轉型變成int呢?會有兩種情況：</p><a id="more"></a><ol><li>該字串本來就是可以轉成int的字串，Ex: “123”、”9”…，那就可以很順利的轉型</li><li>該字串本來不是可以轉乘int的字串，Ex: “3.14”、”abc”…，在轉型的時候會產生ValueError，所以就可以透過except去進行例外處理</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"> </span><br><span class="line">ans = random.randint(<span class="number">0</span>,<span class="number">100</span>)</span><br><span class="line">range_max = <span class="number">100</span></span><br><span class="line">range_min = <span class="number">0</span></span><br><span class="line">is_gameOver = <span class="literal">False</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">game_start</span><span class="params">()</span>:</span></span><br><span class="line">    print(ans)</span><br><span class="line">    <span class="keyword">while</span> is_gameOver == <span class="literal">False</span>:</span><br><span class="line">        print(<span class="string">"Guess a number between"</span>,range_min,<span class="string">"and"</span>,range_max,<span class="string">":"</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment">#Since input() will return a string,explicit convet int() will work correct when we input is an integer</span></span><br><span class="line">            <span class="comment">#if we input a float, then int() will cause ValueError</span></span><br><span class="line">            guess = int(input())</span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            print(<span class="string">'Your input doesnt an Integer!'</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        check_fun(guess,ans)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_fun</span><span class="params">(guess,ans)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> range_max,range_min,is_gameOver</span><br><span class="line">    <span class="keyword">if</span> guess == ans:</span><br><span class="line">        is_gameOver = <span class="literal">True</span></span><br><span class="line">        print(<span class="string">"Congulation!!You are guess the correct number!"</span>)</span><br><span class="line">    <span class="keyword">elif</span> guess  ans</span><br><span class="line">        range_max = guess - <span class="number">1</span></span><br><span class="line">        print(<span class="string">"You guess wrong!!Guess a number between"</span>,range_min,<span class="string">"and"</span>,range_max,<span class="string">":"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    game_start()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Weka]連結mysql資料庫</title>
      <link href="/posts/d8815586/"/>
      <url>/posts/d8815586/</url>
      
        <content type="html"><![CDATA[<p>weka是一款很好用的資料分析工具，平常在分析檔案外，也會有遇到需要進入到資料庫進行分析的狀況，以下將介紹如何設定使得weka可以存取mysql資料庫 前置作業：</p><ol><li>下載資料庫驅動:mysql-connector-java-5.1.6.jar</li><li>下載完後新增環境變數：對我的電腦點右鍵-&gt;內容-&gt;進階系統設定-&gt;環境變數-&gt;user的使用者變數-&gt;在”CLASSPATH”內新增mysql-connector-java-5.1.6.jar的路徑(Ex:我的是D:\WEKA\Weka-3-8\lib\mysql-connector-java-5.1.45-bin.jar)，若沒有CLASSPATH則自己新增一個</li><li>修改weka設定檔，到weka的資料夾找到weka.jar，解壓縮後找到weka-&gt;experiment-&gt;DatabaseUtils.props.mysql</li><li>這是一個設定檔，但weka只會尋找”DatabaseUtils.props”這個檔名，所以把後面的.mysql拿掉</li><li>修改下面兩個資料</li></ol><a id="more"></a><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">....</span><br><span class="line"># JDBC driver (comma-separated list)</span><br><span class="line">jdbcDriver&#x3D;com.mysql.jdbc.Driver</span><br><span class="line"> </span><br><span class="line"># database URL</span><br><span class="line">jdbcURL&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;myweka</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>注意jdbcURL=jdbc:mysql://localhost:3306/myweka，3306是資料庫的port，myweka是你要連上的資料庫名稱，這裡先不改也沒關係，到時候進weka IDE可以再改。 修改完成後將DatabaseUtils.props放到weka的目錄下，重啟weka-&gt;explorer-&gt;open db 如果有發現URL有變成剛剛我們輸入的jdbc:mysql://localhost:3306/myweka就代表更改成功了!! 點擊旁邊的藍色小人輸入資料庫帳號密碼，點其插頭的圖片就可以連上資料庫了 <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/02/e693b7e58f96.png" alt="擷取.PNG"></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> weka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Python]透過PIL改變圖片的pixel</title>
      <link href="/posts/629abe3c/"/>
      <url>/posts/629abe3c/</url>
      
        <content type="html"><![CDATA[<p>這次練習的是透過Python讀取一張彩色的圖片，將圖片每個pixel的RGB減半，最後會是圖片亮度變暗的效果。 PIL(Python Imaging Library)，是一款提供給Python的影像處理套件，使用前必須先透過pip安裝相關的套件。 windows如果要在cmd使用pip的話需要設定環境變數：</p><a id="more"></a><ol><li>先安裝pip(Python3.4以上的版本已經幫你裝好了)，安裝方式可以參考這篇網址:<a href="https://jerrynest.io/windows-install-pip/" target="_blank" rel="noopener">在windows下安裝pip</a></li><li>設定環境變數：對我的電腦點右鍵-&gt;內容-&gt;進階系統設定-&gt;環境變數-&gt;使用者變數</li><li>在Path那邊新增pip.exe所在的資料夾(Ex:我的是在D:\Python\Scripts)，找不到Path則自己新增一個</li></ol><p>安裝PIL：</p><ol><li>如果pip不是最新版要先更新，在cmd下輸入<code>python -m pip install --upgrade pip</code>更新pip</li><li>輸入<code>pip install pillow</code></li></ol><hr><p>影像處理小科普：</p><p>圖片就像是一個二維的矩陣，RGB分別代表一個二維的矩陣，也就是說，一張256*256的彩色圖片則會有三層的256*256矩陣。</p><p>我用了兩種方式完成這次的練習： </p><ol><li><p>透過getpixel / putpixel API來直接些改圖片的RGB值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### (1)透過getpixel / putpixel API來直接些改圖片的RGB值</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img = Image.open(<span class="string">"123.jpg"</span>)</span><br><span class="line"><span class="keyword">print</span> (img.format,img.size,img.mode)</span><br><span class="line">img.show()</span><br><span class="line">print(img.size[<span class="number">0</span>],img.size[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(img.size[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(img.size[<span class="number">1</span>]):</span><br><span class="line">        r,g,b = img.getpixel((x,y))</span><br><span class="line">        img.putpixel((x,y),( int(r/<span class="number">2</span>),int(g/<span class="number">2</span>),int(b/<span class="number">2</span>) ))</span><br><span class="line">img.save(<span class="string">"temp.jpg"</span>)</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure></li><li><p>透過<code>load()</code>將圖片載入到記憶體</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### (2)透過load()將圖片載入到記憶體</span></span><br><span class="line"><span class="comment">### load()會將圖片載入到記憶體，並回傳一個Pixel Access Object</span></span><br><span class="line"><span class="comment">### 透過載入圖片到內存，使得操作比起putpixel / getpixel 來的快很多</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"> </span><br><span class="line">img = Image.open(<span class="string">"123.jpg"</span>)</span><br><span class="line">img.show()</span><br><span class="line">pixel = img.load()</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(img.size[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(img.size[<span class="number">1</span>]):</span><br><span class="line">        r,g,b = pixel[x,y]</span><br><span class="line">        <span class="comment">#print(r,g,b)</span></span><br><span class="line">        pixel[x,y] = (int(r/<span class="number">2</span>),int(g/<span class="number">2</span>),int(b/<span class="number">2</span>))</span><br><span class="line">        <span class="comment">#print(pixel[x,y])</span></span><br><span class="line">img.save(<span class="string">"temp2.jpg"</span>)</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure><p>最後呈現的結果會像這樣：</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/02/123.jpg?w=346&amp;h=249" alt=""><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2018/02/temp2.jpg?w=346&amp;h=249" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[研究所推甄]資工所推甄心得3-找尋指導教授</title>
      <link href="/posts/b6f10ec8/"/>
      <url>/posts/b6f10ec8/</url>
      
        <content type="html"><![CDATA[<p>其他文章點這裡：</p><p><a href="https://meetonfriday.com/posts/44bbe391/">[研究所推甄]資工所推甄心得1-備審準備</a></p><p><a href="https://meetonfriday.com/posts/60ab2ab9/">[研究所推甄]資工所推甄心得2-口試過程</a></p><p>這篇來講最後剩下的部分: </p><ol><li>推甄結果 </li><li>推甄前的準備 </li><li>推甄過程的準備 </li><li>口試經驗分享</li><li><strong>找指導教授的過程</strong></li></ol><a id="more"></a><h2 id="找指導教授的過程"><a href="#找指導教授的過程" class="headerlink" title="找指導教授的過程"></a>找指導教授的過程</h2><p>首先有幾個可能會遇到的問題：</p><ul><li>什麼時候找比較妥當？放榜當天就有一堆人開始找了，每位教授可以收的學生也才幾名，如果有想找的老師建議<strong>一但確定錄取了就馬上行動。</strong>這也是推甄最大的優勢，可以最快去找想找的教授，別讓這個優勢白白浪費了。</li><li>如果備取的話要等備到了再來找還是先找好呢?如果備很前面確定會上的那種(可以根據前幾年備取的結果來推)，那也可以開始找了；如果備比較後面，那會建議快備到的時候再找，並且跟老師說明自己目前是還沒備上的備取生。</li></ul><p><strong>找教授是讓教授認識你，你也去了解教授的一個過程，很少會有一次就馬上確定的狀況，所以放榜後就建議趕快行動，因為跟教授約時間談過後也需要給教授時間去考慮，越早讓教授認識你越好。</strong></p><p>找教授前應該做那些功課呢？網路上有很多這方面的文章分享，我覺的都寫的很棒。</p><p>我的話我會覺得找教授前大致上要準備這幾件事情：</p><ol><li>到系上網站找師資列表，有老師們的研究專長，優先找與自己興趣相符的教師</li><li>到該老師實驗室網站了解一下(如果有的話)</li><li>可以透過科技部網站查詢該老師以前做過哪些計劃，以及現在正在接手的計劃項目(這有可能就會變成你未來進去後要做的事情)</li><li>透過Google、PTT或是學長姊打聽一下老師的評價</li></ol><p>大概先列出三到五位最想找的老師，然後開始寄信過去詢問是否能撥空面談，然後繼續把其他較有興趣的老師也做個紀錄，因為等待教授回覆也需要時間，而且有些教授如果收滿了或是不願意就不會回信了，所以這邊要做一下備案如果第一波寄出的都沒有回覆時應該怎麼辦(我第一波寄了五位只回了兩位呀QQ)</p><p>信件內容要表現出禮貌跟誠意，大概可以說你是誰，來自哪裡，目前正取/備取(名次多少)，因為對教授的研究領域有興趣或是想了解，期盼教授可以撥冗…這樣子。並且附上自己的一些資訊讓教授可以了解你。</p><p>因為我在高雄，然後在經費拮据的狀況下我希望能盡量與教授都約在同一天，這樣就不用來回奔波。那時找交大的指導教授的時候我就一次寄出三到五位教授，大約一個禮拜後再寄出五位教授詢問，很幸運的有一位教授願意透過Skype與我面談，而其他越願意與我討論的教授也都剛好可以把時間橋在同一天，所以我只上去一趟就與所有想找的教授見過面了。</p><hr><p>與教授約好時間後，接下來就是準備見面要討論什麼。</p><p>這邊網路上有很多問題可以參考，我自己整理後最後選擇要問教授的一些問題如下：</p><ol><li>實驗室的研究方向?</li><li>教授指導方式?</li><li>教授目前正在著手的計劃?</li><li>如果[某些領域]還不是很熟，是否會建議大四下這半年先做哪方面的學習?</li></ol><p>討論的過程有些教授會主導，所以就只要聆聽就可以了。我也遇過教授想以提問的方式進行，所以事前一定要充分地做好功課才不會出糗。</p><p>然後因為我是備取生，而且我希望跟多位教授聊過後再去思考最後想跟哪位教授，所以我最後都會謝謝教授給我這次討論的機會(禮貌很重要)，並且表明自己是備取生，如果未來備上且希望跟教授一同研究的話會再與教授聯繫。</p><hr><p>與教授聊完後我都會去教授的實驗室詢問學長姊，畢竟…有些問題不好當面問教授或是需要多聽一些人的意見(小聲)，所以下面這些問題就是每位與我談過的教授我都會去實驗室請教的：</p><ol><li>實驗室的氣氛?規定?</li><li>學長姊對老師的評價?</li><li>實驗室是否有薪水?</li><li>兩年準時畢業?</li><li>meeting方式?</li></ol><p>大概這樣，最後都問完後剩下的就是等待備取通知以及決定想跟哪位教授(需要準備指導教授確認表給教授簽完，交給系上才算完成)，到這裡我的推甄過程就正式告一段落了，這段期間很謝謝許多同學、學長姊的幫助、幫忙撰寫推薦信的教授、和我一同討論如何準備推甄的同學、還有幫我審英文讀書計劃的國中好朋友、在背後支持我的家人、願意收我的指導教授……還有謝謝自己三年半來幾乎每天都在自習室待到凌晨的努力，在這裡把這些經歷記錄下來，也希望可以幫到一些未來想推甄的學弟妹們。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[研究所推甄]資工所推甄心得2-口試過程</title>
      <link href="/posts/60ab2ab9/"/>
      <url>/posts/60ab2ab9/</url>
      
        <content type="html"><![CDATA[<p>其他文章點這裡：</p><p><a href="https://meetonfriday.com/posts/44bbe391/">[研究所推甄]資工所推甄心得1-備審準備</a></p><p><a href="https://meetonfriday.com/posts/b6f10ec8/">[研究所推甄]資工所推甄心得3-找尋指導教授</a></p><p>這篇接續上篇沒講完的: </p><ol><li>推甄結果 </li><li>推甄前的準備 </li><li>推甄過程的準備 </li><li><strong>口試經驗分享</strong> </li><li>找指導教授的過程</li></ol><a id="more"></a><h2 id="口試經驗分享"><a href="#口試經驗分享" class="headerlink" title="口試經驗分享"></a>口試經驗分享</h2><h3 id="中山資甲"><a href="#中山資甲" class="headerlink" title="中山資甲"></a>中山資甲</h3><ul><li>初審(50%): 85.0</li><li>基本能力評估及心得報告(50%): 61.75</li><li>總成績: 73.38，正取(正取最後一名: 71.38)</li></ul><p><strong>沒有面試，分成介紹和基本數學/英文測驗兩個部分</strong>，因為沒有面試所以我覺得穿著不用太正式(儘管當天還是不少人穿著正式西裝出現)。</p><p>一開始會有教授(不確定是不是系主任)介紹系上，一些哪一年創立、主要研究領域、優缺點…過程中可以<strong>把聽到的記下來，之後寫心得會用到(對，要寫心得)</strong>；介紹完後會有三個碩士的學長姊會來報告不同的研究主題，之後心得的時候要挑兩篇去寫(概要、心得)。</p><p>全部都聽完會剩下一個小時左右的時間進行心得撰寫和能力測驗，因為是<strong>同時進行所以要分配好時間</strong>。能力測驗有英文跟數學，英文是簡單的英翻中/中翻英(Ex:function-&gt;?)，數學則是比較偏向離散的邏輯推理…因為事前完全沒準備所以這裡有點吃力QQ</p><p>最後我在結束前一分鐘才把全部做完…因為太久沒手寫了寫心得的時候是一個痠到快斷掉但是還是要繼續寫的感覺…</p><h3 id="中央資工"><a href="#中央資工" class="headerlink" title="中央資工"></a>中央資工</h3><ul><li>書面(50%): 85.32</li><li>面試(50%)</li><li>總成績: (通過第一階段)</li></ul><p>這間的時候因為交大數科所已經確認備上，在金援實在不足的情況下(考慮到之後找教授也是一筆不小的負擔)猶豫了很久到底要不要花來回的車錢再去面試，跟家人還有朋友討論了很久後還是打電話過去放棄面試資格了，不過還是很謝謝願意給我這個機會的教授們QQ</p><h3 id="成大資工"><a href="#成大資工" class="headerlink" title="成大資工"></a>成大資工</h3><ul><li>初審(50%): ???</li><li>基本能力評估及心得報告(50%): ???</li><li>總成績: ???，備取8(我不小心把成績單弄丟了所以查不到囧)</li></ul><p>這間是我第一次真正的面試(當時的順序是中山-&gt;成大-&gt;清大，中山沒有面試)，也是過程最慘痛的一間，結果推甄成績是全部裡面算不錯的令我很訝異…<strong>所以說面試的過程並不代表最後的分數會是好或壞。</strong></p><p>成大面試分成ABCD四組，每組有三位教授，我是C組的(後來上網看C組的大家普遍好像都被電得蠻慘的…)，<strong>每位同學有7分鐘的時間(4分鐘介紹，3分鐘問答)，然後要自備三份紙本資料(1~4頁)</strong>給教授們，進去教室後就開始計時。</p><p>首先資料的部分，我一頁是放自己的簡歷、兩頁專題、一頁讀書計畫，然後印彩色這樣。我覺得放簡歷是很正確的選擇，因為之後再自我介紹的時候有瞄到教授們會用比去圈出你簡歷有哪些比較特別的地方。</p><p>自我介紹的時候我就大概說了一下我是誰、有什麼經歷、在校表現如何、然後大概簡述了一下專題…最後說為什麼想來這裡就讀。我覺得事前演練很重要，因為我想講的內容還蠻多的，如何在不會講太快又不結巴的狀況下把這些講完，然後又不會剩下太多時間，這需要多練習幾次，有些人會說靠臨場應對也可以，不過我覺得那是建立在你有充分的準備前提下才有的表現。</p><p>自我介紹完之後教授會針對你提供的紙本內容以及剛剛說過的內容來提問，我在這裡犯了一個天大的錯誤…因為資料中有一頁的內容是我事前準備不足的，但是被教授抓出來問，然後我就沒有回答得很好，所以教授聽不懂的情況下又繼續追問…最後3分鐘的問答時間幾乎都花在我沒準備好的地方…當初出來的時候還以為成大沒希望了QQ</p><p><strong>所以說自己的備審自己最清楚，面試前一定要再重新複習一次呀!!</strong></p><h3 id="清大資工甲"><a href="#清大資工甲" class="headerlink" title="清大資工甲"></a>清大資工甲</h3><ul><li>書審(50%): 86.63</li><li>口試(50%): 88.21</li><li>總成績: 87.42，備取48(最低錄取總分: 85.06)</li></ul><p>這是第二間面試的學校，在經歷成大那慘痛的經驗後，這次我好好調整了要給教授看的資料，<strong>資料內容放的都是比較有信心，也是希望教授能從裡面問的部分</strong>，就結論下來我覺得效果很好，整體面試過程都很順利。</p><p>面試時間<strong>一個人只有六分鐘(天哪比成大還短)，三分鐘自我介紹三分鐘教授提問(一樣要準備紙本)</strong>，所以真的要好好練習自我介紹的部分不要卡住。這邊大概寫一下還有印象被問到的問題，其他的時候比較偏向聊天，就很自然地應對就好(我覺得如何表現出不緊張的態度也是一個很重要的事情)。</p><p>“你會什麼程式語言?”</p><p>“為什麼你有兩個不同的專題作品?”</p><p>“如果近來未來想走哪方面的研究?”</p><p>“我們來個技術的問題…向你專題這樣那樣這樣那樣….還可以怎麼優化它的執行速度?”</p><p>最後一個被問到有點傻眼，但是被發現傻眼是不行的，所以我當下還是試著去掰掰看(我好像講了什麼multi-thread還什麼的…)，就教授的反應看來好像沒有講得很好，不過剛好時間到我就謝謝教授出去了。</p><h3 id="交大數據"><a href="#交大數據" class="headerlink" title="交大數據"></a>交大數據</h3><ul><li>初審(100%): 85.02</li><li>總成績: 85.02，備取11(正取最後一名: 87.20)</li></ul><h3 id="交大資訊聯招"><a href="#交大資訊聯招" class="headerlink" title="交大資訊聯招"></a>交大資訊聯招</h3><ul><li>初審(100%): 81.87</li><li>總成績: 85.02，甲組備取46(正取最後一名: 85.57)</li><li>總成績: 85.02，多媒體組備取23(正取最後一名: 83.74)</li><li>總成績: 85.02，網路組備取14(正取最後一名: 83.04)</li></ul><p>今年甲組居然沒有備上…覺得有點訝異，不過其他都備上也放棄了。</p><p>感覺這間除了在校成績也看中一些特殊經歷，像是是否有參加過什麼比賽得獎這種…感覺啦。</p><h3 id="台大資工"><a href="#台大資工" class="headerlink" title="台大資工"></a>台大資工</h3><ul><li>初審(100%): 76.50</li><li>總成績: 76.50，備取51(正取最後一名: 85.50)</li></ul><p>摁…台大買樂透不意外ˊ_&gt;ˋ</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[研究所推甄]資工所推甄心得1-備審準備</title>
      <link href="/posts/44bbe391/"/>
      <url>/posts/44bbe391/</url>
      
        <content type="html"><![CDATA[<p><strong>嗨嗨～如果這一系列的文章有幫助到你/妳，很歡迎留言給予一些feedback讓我知道或是分享給更多你覺得有需要的人，我會很開心的:D</strong></p><hr><p>其他文章點這裡：</p><p><a href="https://meetonfriday.com/posts/60ab2ab9/">[研究所推甄]資工所推甄心得2-口試過程</a></p><p><a href="https://meetonfriday.com/posts/b6f10ec8/">[研究所推甄]資工所推甄心得3-找尋指導教授</a></p><p>在去年十二月初，交大正式通知我備取遞補上了，經過一番努力和奔波，也順利地找好未來的指導教授了，至此研究所的推甄過程正式告一段落。在這準備的過程中，有很多的困惑與問題需要解決，很感謝網友的一些經驗分享和學長姊一直被我問題轟炸，所以現在也決定發一篇從準備到推甄、找教授的文章回饋社會大眾(?) 也希望能幫助到一些未來有推甄需求的學弟妹(因為系上這方面的資源真的不多…每年推甄的學長姐就那幾位QQ)</p><p>在這系列的文章中會分成幾個部分: </p><ol><li><strong>推甄結果</strong> </li><li><strong>推甄前的準備</strong></li><li><strong>推甄過程的準備</strong> </li><li>口試經驗分享 </li><li>找指導教授的過程</li></ol><a id="more"></a><p>然後提供一些過程中可能會遇到的疑問：</p><ul><li>推甄上了可以考試嗎？可以，推甄只要有去報到名額就會幫你保留到畢業前(繳交畢業證書才算正式完成)，所以報到後可以再去考試的。雖然這樣會多占別人一個名額，但我覺得人生是自己的…有時候得先顧好自己才有餘力去為別人著想xD</li><li>推甄多間都上了可以都去報到嘛？可以，像上面說的只要報到過就會幫你保留資格，可以再決定真的要去哪一間…不過如果已經決定完了就把其他間的資格放棄吧，給別人一個機會。</li></ul><p>(打完發現內容有點多…所以3和4打算下一篇在繼續講…)</p><hr><h2 id="推甄結果"><a href="#推甄結果" class="headerlink" title="推甄結果"></a>推甄結果</h2><p>聽說現在發文都要先附結果才不會被質疑QQ</p><p>小弟高大資工，系排1(1/43)，在校平均(研究所只看到大三下學期)90.41</p><p>經歷:CPE5題、桂冠盃闖關組佳作、ITSA績優團隊、大專生、系上網管、程式助教、DIGI+ Talent實習生(工研院實習)、專題出去比賽有得獎過、參加過一些資訊活動(SITCON、MOPCON)…</p><div class="table-container"><table><thead><tr><th>推甄學校</th><th>結果</th></tr></thead><tbody><tr><td>中山資工</td><td>正取</td></tr><tr><td>中央資工</td><td>通過第一階段</td></tr><tr><td>成大資工</td><td>備取8(有備上，已放棄)</td></tr><tr><td>清大資工</td><td>備取48(有備上，已放棄)</td></tr><tr><td>交大資工(甲組/多媒體/網路)</td><td>備取4X/23/14(除了甲組都有備上，已放棄)</td></tr><tr><td>交大數據科學與工程所</td><td>備取11，已報到</td></tr><tr><td>台大資工</td><td>備5X，沒備上</td></tr></tbody></table></div><p>原本還很猶豫要不要多報幾間，不過每間的報名費都不便宜就放棄了…算一算上面的報名費也有6~7000了，大四上就是個不斷吃土的學期(嘆)</p><h2 id="推甄前的準備"><a href="#推甄前的準備" class="headerlink" title="推甄前的準備"></a>推甄前的準備</h2><p>研究所推甄看的是大一到大三下學期的成績(轉學考過的同學好像不一樣)，資工研究所考試的話則是考六科(OS、計組、離散、現代、資結、演算法、[英文])，所以我在大三前的準備是以系上成績為主，雖然還是有補習，不過我當時的心態是想讓自己能更好的理解課程的內容，考試時可以維持好的成績(像我計組系上老師上一遍，交大線上課程上一遍，補習班又上了一遍…)，然後如果真的不幸沒推上再來準備考試也勉強有點底子。</p><p><strong>推甄首要看校名(4大&gt;4中&gt;普通國立~私立前段&gt;….)，再來看的是在校成績，最後是自身經歷(專題/實習/程式能力/英文…)。</strong>所以顧好三年的在校成績是基本之外，我會建議平時有空還可以多多參與一些資訊活動，這邊講幾個我參加過的，有興趣的可以參考看看: </p><h3 id="資訊活動"><a href="#資訊活動" class="headerlink" title="資訊活動"></a>資訊活動</h3><div class="table-container"><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>1.MOPCON</td><td>唯一在南部的行動科技年會，每年都有不同主題的議程可以聽。</td></tr><tr><td>2.SITCON</td><td>學生計算機年會，由學生為主的社群，我覺得這是個很不錯的交流，可以知道跟你年齡相近的人都在做什麼，順便認識認識其他人。</td></tr><tr><td>3.COSCUP</td><td>開源人年會，也是在北部的，不過我沒參加過就不說了。</td></tr></tbody></table></div><h3 id="實習"><a href="#實習" class="headerlink" title="實習"></a>實習</h3><div class="table-container"><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>1.DIGI+ Talent跨域數位人才加速躍升計畫</td><td>大三以上可以參加，透過海選面試有機會進入一些財團法人單位研習，每個月還有研習津貼，是個很不錯的活動。</td></tr><tr><td>2.中研院暑期實習</td><td>大三以上可以申請的樣子，不過我被刷掉了QQ，有興趣者可以自行google</td></tr></tbody></table></div><h3 id="程式能力"><a href="#程式能力" class="headerlink" title="程式能力"></a>程式能力</h3><div class="table-container"><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>1.競技程式競賽</td><td>一個月一次的ITSA、兩個月一次的CPE、一年一次的NCPC和桂冠賽，都是證明自己程式能力的不錯管道。</td></tr><tr><td>2.程式營隊</td><td>交大寒暑假都會有NCTU PCCA、清大、台大也都有類似的營隊，可以和來自不同學校的同學一起學習切磋(順便可以了解自己學校和其他學校程度上是否有所差異)。</td></tr></tbody></table></div><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><div class="table-container"><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>1.大專生計畫</td><td>大三以上可以參加，與指導教授討論後決定一個研究主題，如果通過的話會有半年的時間可以進行研究，每個月也有研究津貼。不過通過與否還蠻吃在校成績的。</td></tr></tbody></table></div><h2 id="推甄過程的準備"><a href="#推甄過程的準備" class="headerlink" title="推甄過程的準備"></a>推甄過程的準備</h2><p>以高大來說，如果想推四大成績基本上要在前三名比較有機會，中字輩的學校我覺得前十都有機會可以嘗試看看。</p><p>這是當初令我最頭痛的部分，沒有之一，推甄到底要放什麼呢?要準備這個就得扯到各校的推甄簡章，首先請先去看各校簡章，裡面會說他們需要你繳交的備審需要什麼資料。 簡章落落長，所以我當初只看最前面的前幾頁(日期行程表，這樣才知道什麼時候以前要完成什麼、還有報名方式跟金額)，了解怎麼報名後就直接跳到自己要推的研究所去看需要什麼資料。</p><p>不同間學校需要的東西大同小異: </p><ol><li>備審資料 </li><li>在校成績證明 </li><li>研究計畫書 </li><li>推薦信 </li><li>其他有利於申請資料 </li><li>很多很多的錢QQ </li></ol><p><strong>我建議可以先以最完整的一間來做準備，這樣其他間就只需要把備審的內容架構稍作修改就好了(像我就是先以台大做準備，其他學校則按照推定的格式修改即可)。</strong></p><h3 id="備審資料"><a href="#備審資料" class="headerlink" title="備審資料"></a>備審資料</h3><p>我的備審架構分為:簡歷、在校成績、自傳、讀書計畫、專題這幾項。</p><ul><li>一開始我先用簡歷把我的經歷<strong>條列式</strong>的打出來，並且把重點用紅字呈現，希望可以讓教授可以快速地了解我是個什麼樣的人，簡歷我會建議平常就開始整理，有什麼就先寫上去，獎狀也先蒐集好，才不會大四上要開始慢慢回想你的人生。</li><li>在校成績部分將比較優異的科目列出來，由於我的在校成績是每學期都有進步的趨勢，所以我<strong>搭配圖表</strong>將這個優勢呈現出來。</li><li>自傳的部分，家裡有幾個人，有幾個兄弟姊妹就省了。為什麼想念資工?為此大學期間做了什麼努力；又或是因為什麼活動而學到了什麼、萌生了什麼想法，我覺得這是教授會比較想看到的部份。</li><li>讀書計畫，有些學校是叫專題計畫(清大還要求英文的QQ)，這邊我當初也不太懂要寫什麼，上網看有兩派說法:<strong>把你的專題整理一下，或是寫你未來進研究所的規劃(近程、中程、遠程)</strong>，我是用後者的方式進行。</li><li>專題因為我們有把專題整理成一篇中文小論文投稿，所以這裡就直接放上去了，或是如果有其他作品我覺得都可以放，主要是<strong>讓別人知道你做過哪些事情。</strong></li></ul><p>備審部分有些學校會限制頁數(像中山就限制兩頁以內完成，我就只放了簡歷自傳)，不過如果一開始完成了一所學校的版本，這時候只需要根據其他學校規定去做刪減跟排版就好了，如此準備起來就會輕鬆很多。</p><h3 id="在校成績證明"><a href="#在校成績證明" class="headerlink" title="在校成績證明"></a>在校成績證明</h3><p>這個去學校列印成績證明正本掃描就好了。</p><h3 id="研究計劃書"><a href="#研究計劃書" class="headerlink" title="研究計劃書"></a>研究計劃書</h3><p>有些學校會要求這個，這就是我在備審資料裡面說過的，如果有遇到我就會把他從備審抽出來獨立一篇。</p><h3 id="推薦信"><a href="#推薦信" class="headerlink" title="推薦信"></a>推薦信</h3><p>推薦信通常都是要求兩篇，<strong>兩篇通常一篇會是專題的指導教授，另一篇則看你想找誰(系主任?或是教授領域和未來比較想走的方向相近的?或是在該校比較具有影響力的?)</strong> </p><p>這裡會”建議”學弟妹(我們學校的，其它因為不清楚所以不敘述)在大四暑假就想好想找哪幾位老師，寄信詢問他們是否可以幫忙。當初我是想說當面說比較有禮貌，所以有些教授是當面詢問的。 我沒有每間學校都找一樣的老師，像我推甄數科所，我就去詢問教我資料探勘的教授；推甄成大則找在南部比較有名聲的教授…。 </p><p><strong>不同老師對於撰寫推薦信的方式都不一樣，這部份我覺得蠻花時間的，所以建議可以及早準備。</strong></p><p>我這一屆開始都採用線上的方式，所以不需要紙本，到時候繳交推薦信的方式大多都是填入教授的信箱，然後線上系統會寄一封邀請信給教授填寫。這邊要有個共識是<strong>教授們都是很忙的</strong>，所以寄出邀請信後請通知教授，麻煩他們去確認是否有收到信件，並去確認進度(線上系統可以確認教授送出推薦信了沒)，這個建議不要拖到太晚才開始弄，不然最後會搞到自己很緊張。</p><hr><p>最後要做的就是<strong>再三確認各校推甄的日期</strong>，什麼時候該上傳、什麼時候該繳費、什麼時候要寄出紙本資料(儘管大多都是線上，但還有一些資料會要求要妳寄過去，Ex:交大要求成績單)，之後就是等待結果了。我習慣會去PTT GRADUATE這個版查閱，如果有最新資訊通常網友都會在第一時間放在這上面，這樣就不用一直去各校網站看有沒有放榜了。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[ML]基因演算法-找最大值</title>
      <link href="/posts/cdb332e0/"/>
      <url>/posts/cdb332e0/</url>
      
        <content type="html"><![CDATA[<p>第一次接觸基因演算法，以下列了幾個第一次接觸時我自己的疑問:</p><h2 id="什麼是基因演算法-遺傳演算法"><a href="#什麼是基因演算法-遺傳演算法" class="headerlink" title="什麼是基因演算法/遺傳演算法?"></a>什麼是基因演算法/遺傳演算法?</h2><p>模擬生物學上的物進天擇，讓群體透過交配(crossover)、突變(mutation)產生新的後代，不斷繁衍而造成收斂，得到想要的答案，此演算法通常是用於解決最佳化的問題。</p><h2 id="為什麼透過突變跟交配可以確定下次的子代一定會是比較優的解"><a href="#為什麼透過突變跟交配可以確定下次的子代一定會是比較優的解" class="headerlink" title="為什麼透過突變跟交配可以確定下次的子代一定會是比較優的解?"></a>為什麼透過突變跟交配可以確定下次的子代一定會是比較優的解?</h2><p>基因演算法的判斷準則是用一個適應函數為判斷該基因好壞的基準，並且在交配之前其實還有一個步驟:選擇，在這一步通常會讓較好的基因較容易被選中，此過程會逐漸讓基因的適應函數值逐漸進行收斂。<br><a id="more"></a><br>但是僅透過選擇機制無法讓此演算法產生變數(因為不管怎麼選仍然是原本那群基因)，所以透過交配(crossover)讓兩個基因就隨機位置產生新的基因，這樣才能提供演化的變數種子。</p><p>最後基因演算法有可能會得到區域最佳解而非整體最佳解，所以透過突變(mutation)讓基因產生一定的變數，確保基因演算法的計算範圍不會只侷限在同一區，有機會跳到其他區域進行演化。</p><h2 id="參考網址"><a href="#參考網址" class="headerlink" title="參考網址"></a>參考網址</h2><p>我覺得這兩篇文章講解的很好，一篇是如何將資訊轉換成對應的基因序列(內插)，另外一篇則是如何實作一個基因演算法:</p><p>1.<a href="http://edisonx.pixnet.net/blog/post/85835614" target="_blank" rel="noopener">http://edisonx.pixnet.net/blog/post/85835614</a><br>2.<a href="http://edisonx.pixnet.net/blog/post/90563824-%5Bga%5D-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95%E6%A6%82%E5%BF%B5%E7%B4%B0%E8%AA%AA-(i)---%E7%B7%A8%E8%A7%A3%E7%A2%BC" target="_blank" rel="noopener">http://edisonx.pixnet.net/blog/post/90563824-%5Bga%5D-%E5%9F%BA%E5%9B%A0%E6%BC%94%E7%AE%97%E6%B3%95%E6%A6%82%E5%BF%B5%E7%B4%B0%E8%AA%AA-(i)---%E7%B7%A8%E8%A7%A3%E7%A2%BC</a><br>3.維基百科: <a href="https://zh.wikipedia.org/wiki/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95</a></p><hr><p>最後，本次課堂作業是給一個數學函式(看程式碼)，要求基因長度是12bits，求出在區間[0,1]間的最大值，造著參考網址程式碼的想法重新自己打了一遍。</p><p>想法大概是:</p><p>首先要先了解怎麼將資訊轉成基因序列(使用內插法)，這裡大推參考網址的那篇文章，講的很詳細。</p><p>之後要先在母群中放置一定數量的基因。</p><p>再來每一次演化都做一樣的事情:</p><ol><li>選擇一定數量的基因從母群放到交配池中</li><li>在交配池中完成交配和突變</li><li>把結果放回母群中，並更新基因的適應函數</li></ol><p>如何終止? 終止的方式有很多，Timeout、演化次數….這裡使用演化次數來做為終止條件</p><p>程式碼:<a href="https://github.com/john850512/Artificial_Intelligence_Pratice/blob/master/HW2" target="_blank" rel="noopener">github</a></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> algorithm </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[計算機組織]期末專題-MIPS模擬器</title>
      <link href="/posts/d0b7bd91/"/>
      <url>/posts/d0b7bd91/</url>
      
        <content type="html"><![CDATA[<p>計算機組織期末的專題。</p><h2 id="說明"><a href="#說明" class="headerlink" title="說明"></a>說明</h2><h3 id="請設計一個pipelined的cpu模擬器"><a href="#請設計一個pipelined的cpu模擬器" class="headerlink" title="請設計一個pipelined的cpu模擬器"></a>請設計一個pipelined的cpu模擬器</h3><p>(可以任何語言來模擬 ex:C &amp; C++)</p><p>以stall來解決所有的hazard問題，換言之，需要實作hazard的邏輯判斷</p><p>Input 為一名為memory.txt的文字檔，裡面為MIPS的組合語言程式 Output 請輸出此程式執行結果於一名為result.txt的文字檔<br><a id="more"></a></p><h3 id="Instructions"><a href="#Instructions" class="headerlink" title="Instructions"></a>Instructions</h3><p>指令有下列幾種：lw、sw、add、sub、beq(選擇)</p><h3 id="Register-Number"><a href="#Register-Number" class="headerlink" title="Register Number"></a>Register Number</h3><p>32個，初始時$0暫存器的值為0，其他都是1</p><h3 id="Memory-Size"><a href="#Memory-Size" class="headerlink" title="Memory Size"></a>Memory Size</h3><p>32 words，初始時記憶體中的每個word都是1</p><h3 id="利用Stall解決各種Hazard"><a href="#利用Stall解決各種Hazard" class="headerlink" title="利用Stall解決各種Hazard"></a>利用Stall解決各種Hazard</h3><p>當在ID階段發現與前一個指令有相依性時，暫時不再往前執行，等到結果在某個cycle寫入目的地暫存器後，在同一個cycle再完成ID</p><p>Ex: Data Hazard</p><p>在ID階段發現目前指令為beq時，IF不執行抓取指令的動作，等到beq指令完成ID階段後，到達EXE時，在同一cycle才抓取正確位置的指令</p><p>EX: Control Hazard</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2017/03/11.jpg" alt="1"><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2017/03/21.jpg" alt="2"><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2017/03/3.jpg" alt="3"></p><h2 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h2><p>核心重點在於如何模擬pipeline，大家都知道MIPS有5個stage，如果按照順序一定沒戲，因為第一個指令執行完，第二個指令就馬上把第一個指令留下來的資訊覆蓋掉了。</p><p>所以要用while去模擬一個cycle，並從後面開始倒回處理，這樣每次處理的資訊才會剛好是前一個指令留下來的資訊</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2017/03/4.jpg" alt="4"></p><p>接下來就是按照規格跟MIPS架構圖去設計每個stage該有什麼reg變數了。</p><p>程式碼如下：<a href="https://github.com/john850512/MIPS_Simulator/blob/master/Code" target="_blank" rel="noopener">github</a></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> mips </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[ML]類神經網路-學習權重</title>
      <link href="/posts/4a4664e6/"/>
      <url>/posts/4a4664e6/</url>
      
        <content type="html"><![CDATA[<p>人工智慧小作業，假設存在一組線性方程式和一些點，點會在方程式的右半部或左半部是已知的，要如何透過類神經網路的概念來讓電腦自動取得符合所有情況的係數呢？<br><a id="more"></a><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2017/03/1.jpg" alt="1"><br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2017/03/2.jpg" alt="2"></p><p>如果將係數當成圖論上的權重，而座標是節點，一種方法是假設權重一開始是(0,0,0)，接下來依序尋找所有已知點，去判斷此權重是否滿足所有情況，若有誤差則利用此點的資料對權重進行調整，程式碼如下：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">cof</span> //方程式係數</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> x;</span><br><span class="line">    <span class="keyword">int</span> y;</span><br><span class="line">    <span class="keyword">int</span> z;</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//假設有一線性方程式:ax+by+cz-20 = 0，利用學習權重找出a b c</span></span><br><span class="line">    <span class="keyword">int</span> test_data[][<span class="number">4</span>] = <span class="comment">//測試點</span></span><br><span class="line">    &#123;</span><br><span class="line">        &#123;<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">-1</span>&#125;,</span><br><span class="line">        &#123;<span class="number">7</span>,<span class="number">7</span>,<span class="number">7</span>,<span class="number">1</span>&#125;,</span><br><span class="line">        &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">-1</span>&#125;,</span><br><span class="line">        &#123;<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>&#125;,</span><br><span class="line">        &#123;<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>&#125;,</span><br><span class="line">        &#123;<span class="number">4</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>&#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    cof c = &#123;<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;; <span class="comment">//假設權重先等於第一筆資料</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i = <span class="number">0</span>)<span class="comment">//如果點在線的右邊(&gt;0) * 預設結果在右邊(=0)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//權重滿足此點，不用更新</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="comment">//如果點在線的右邊(&gt;0) * 預設結果在左邊(&lt;0) = 結果不符(&lt;0)</span></span><br><span class="line">        &#123;</span><br><span class="line">            c.x += test_data[i][<span class="number">0</span>]* test_data[i][<span class="number">3</span>];</span><br><span class="line">            c.y += test_data[i][<span class="number">1</span>]* test_data[i][<span class="number">3</span>];</span><br><span class="line">            c.z += test_data[i][<span class="number">2</span>]* test_data[i][<span class="number">3</span>];</span><br><span class="line">            <span class="keyword">if</span>(i == <span class="number">5</span>)i = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d %d %d\n"</span>,c.x,c.y,c.z);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    system(<span class="string">"pause"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> deep learning </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[OpenGL]音樂節奏遊戲(2)-遊戲設計</title>
      <link href="/posts/c77dd915/"/>
      <url>/posts/c77dd915/</url>
      
        <content type="html"><![CDATA[<p>這一次要加入主要的遊戲模式程式碼，這裡因為OpenGL不是物件導向，所以必須要自己定義箭頭(Arrow)這個物件，在上一篇我已經把他先定義好了: x,y,list_Num 也就是箭頭的編號(上、下、左、右) </p><p>接下來產生箭頭的方式我使用std的vector去記錄每一個出現的箭頭資訊，並透過glutTimerFunc()產生動畫效果，使得箭頭看起來會不斷地向下移動。我設置了一個全域的二維陣列分別代表每一次該出現上、下、左、右那些箭頭<br><a id="more"></a><br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">樂譜設計:</span></span><br><span class="line"><span class="comment">每列為每個dis執行時的時間軸，每欄分別是上下左右的位置</span></span><br><span class="line"><span class="comment">這邊要注意與timer的配合，太過密集出現的話會圖案重疊</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">int</span> sheet_Music1[<span class="number">500</span>][<span class="number">4</span>] =</span><br><span class="line">&#123;</span><br><span class="line">    <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,</span><br><span class="line">    <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,</span><br><span class="line">    <span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,</span><br><span class="line">    <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,</span><br><span class="line">    <span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,</span><br><span class="line">    <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,</span><br><span class="line">    <span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,</span><br><span class="line">    <span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>接下來是遊戲模式的副程式，把全域的二維陣列一列一列讀，判斷那些箭頭該出現，並將她該出現的x,y,出現哪種方向的箭頭等資訊push_back()到vector中。 同時也更新已存在vector內的箭頭的y值，每次都讓他往下掉，這樣看起來就會有向下移動的效果 </p><p>然後為了避免執行次數太快，用了一個frequence變數去做delay，所以需要在dis()中加入frequence++;的code </p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">play</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> shift = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//cout&lt;&lt; frequence &lt;&lt; endl;</span></span><br><span class="line">    <span class="keyword">if</span> (frequence == <span class="number">10</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">4</span>; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (sheet_Music1[music_Sheet_counter][(i - <span class="number">1</span>)] == <span class="number">1</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                glPushMatrix();</span><br><span class="line">                arrow temp;</span><br><span class="line">                temp.x = <span class="number">75</span> + shift;</span><br><span class="line">                temp.y = <span class="number">550</span> - distance_y;</span><br><span class="line">                temp.list_Num = GAME_Up + (i - <span class="number">1</span>);</span><br><span class="line">                arrow_list.push_back(temp);</span><br><span class="line">                glPopMatrix();</span><br><span class="line">            &#125;</span><br><span class="line">            shift += <span class="number">100</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        frequence = <span class="number">0</span>;</span><br><span class="line">        music_Sheet_counter++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (it = arrow_list.<span class="built_in">begin</span>(); it != arrow_list.<span class="built_in">end</span>() &amp;&amp; !gameover; it++)</span><br><span class="line">    &#123;</span><br><span class="line"> </span><br><span class="line">        glPushMatrix();</span><br><span class="line">        <span class="comment">//cout &lt;&lt; time_counter &lt;&lt; ":  " &lt;x &lt;&lt; " " &lt;y &lt;&lt; " " &lt;list_Num y, 0.0);</span></span><br><span class="line">        it-&gt;y -= distance_y;<span class="comment">//向下移動</span></span><br><span class="line">        glCallList(it-&gt;list_Num);</span><br><span class="line">        glPopMatrix();</span><br><span class="line">        <span class="keyword">if</span> (it-&gt;y list_Num == GAME_Up)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span> (<span class="built_in">abs</span>(it-&gt;y) y) y) list_Num == GAME_Down)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span> (<span class="built_in">abs</span>(it-&gt;y) y) y) list_Num == GAME_Left)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span> (<span class="built_in">abs</span>(it-&gt;y) y) y) list_Num == GAME_Right)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">if</span> (<span class="built_in">abs</span>(it-&gt;y) y) y) &lt;= <span class="number">32</span>)</span><br><span class="line">                    &#123;</span><br><span class="line">                        combo++;</span><br><span class="line">                        <span class="built_in">strcpy</span>(text_Press_Stat[<span class="number">3</span>], <span class="string">"Good"</span>);</span><br><span class="line">                        <span class="built_in">printf</span>(<span class="string">"20\n"</span>);</span><br><span class="line">                        score += <span class="number">20</span> * (<span class="number">1</span> + ((<span class="keyword">float</span>)combo / <span class="number">100.0</span>));</span><br><span class="line">                        it = arrow_list.erase(it);<span class="comment">//eraser會回傳下一個位置</span></span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span></span><br><span class="line">                    &#123;</span><br><span class="line">                        combo = <span class="number">0</span>;</span><br><span class="line">                        <span class="built_in">strcpy</span>(text_Press_Stat[<span class="number">3</span>], <span class="string">"Miss"</span>);</span><br><span class="line">                        <span class="built_in">printf</span>(<span class="string">"MISS\n"</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                glPopMatrix();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再來更新buttom_UI()做按鍵特效，這邊判斷Press_Stat[]和text_Press_Stat[]來讓對應的Arrow與文字變換顏色，然後加點delay即可<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  glutInit(&amp;argc, argv);<span class="comment">//初始化glut</span></span><br><span class="line">  glutInitDisplayMode(GLUT_RGB | GLUT_DEPTH | GLUT_DOUBLE);</span><br><span class="line">  glutInitWindowSize(ww, wh);</span><br><span class="line">  glutInitWindowPosition(<span class="number">450</span>, <span class="number">0</span>);</span><br><span class="line">  glutCreateWindow(<span class="string">"Music Dancing"</span>);</span><br><span class="line"></span><br><span class="line">  init();</span><br><span class="line">  glutDisplayFunc(dis);</span><br><span class="line">  glutMouseFunc(mymouse);</span><br><span class="line">  glutSpecialFunc(mykey);<span class="comment">//上下左右要用特殊的偵測</span></span><br><span class="line">  glutTimerFunc(<span class="number">50</span>, timer, <span class="number">0</span>);</span><br><span class="line">  glutMainLoop();<span class="comment">//glutMainLoop()會反覆執行是回呼函數的function</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>最後更新一下main的內容</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  glutInit(&amp;argc, argv);<span class="comment">//初始化glut</span></span><br><span class="line">  glutInitDisplayMode(GLUT_RGB | GLUT_DEPTH | GLUT_DOUBLE);</span><br><span class="line">  glutInitWindowSize(ww, wh);</span><br><span class="line">  glutInitWindowPosition(<span class="number">450</span>, <span class="number">0</span>);</span><br><span class="line">  glutCreateWindow(<span class="string">"Music Dancing"</span>);</span><br><span class="line"></span><br><span class="line">  init();</span><br><span class="line">  glutDisplayFunc(dis);</span><br><span class="line">  glutMouseFunc(mymouse);</span><br><span class="line">  glutSpecialFunc(mykey);<span class="comment">//上下左右要用特殊的偵測</span></span><br><span class="line">  glutTimerFunc(<span class="number">50</span>, timer, <span class="number">0</span>);</span><br><span class="line">  glutMainLoop();<span class="comment">//glutMainLoop()會反覆執行是回呼函數的function</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>到這邊就有遊戲的大概了，滑鼠點擊偵測的部分就只是小功能，就不再介紹了，我把上一篇的Code加上這篇重新整理了一下順序 完整程式碼在: <a href="https://github.com/john850512/MusicDance/blob/master/%EF%BC%BB%EF%BC%AF%EF%BD%90%EF%BD%85%EF%BD%8E%EF%BC%A7%EF%BC%AC%EF%BC%BD%E9%9F%B3%E6%A8%82%E7%AF%80%E5%A5%8F%E9%81%8A%E6%88%B2\(2\" target="_blank" rel="noopener">Github</a>-%E9%81%8A%E6%88%B2%E8%A8%AD%E8%A8%88) </p><p>至於如何加入音樂?教授有請我做了一篇教學，就直接放上來了xD </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2017/02/14.jpg" alt="14"> </p><p>如果有問題歡迎詢問</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> openGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[OpenGL]音樂節奏遊戲(1)-UI設計</title>
      <link href="/posts/21bd8b50/"/>
      <url>/posts/21bd8b50/</url>
      
        <content type="html"><![CDATA[<p>大三上學期計算機圖學的期末專題，我們這組做的是音樂節奏遊戲，起因突然想到曾經網路上好像有完過類似的所以就想來做這個，不然原本是打算用之前Unity專題或<del>隨便做個動畫交上去交差了事的。</del></p><hr><div align="center">  <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2017/02/11.jpg" width="50%" /> </div><ul><li>遊戲名稱：MusicDance </li><li>開發語言：C/C++ with API(OpenGL) </li><li>開發時程：2天(熬了一整夜…) </li><li>遊戲說明：音樂節奏遊戲，當上下左右的符號掉到對應的框框中，按下對應的按鍵可根據按下的時機獲得不同的分數。<ul><li>遊戲具有Combo系統，Combo數到達一定值以上會有額外分數加乘。   </li></ul></li><li>Demo影片：<a href="https://www.youtube.com/watch?v=Nvfz4QlNFyU&amp;feature=youtu.be" target="_blank" rel="noopener">Youtube</a> </li><li>遊戲畫面：    <a id="more"></a><div align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2017/02/12.jpg" width="45%" /> <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2017/02/13.jpg" width="45%" /> </div></li></ul><hr><p>接下來大致介紹一下是如何設計的： </p><p>首先是UI，先用gluOrtho2D這個函數設置好Clipping Window，接下來就是開始設計UI介面的每個點座標、箭頭的大小、輔助線的位置座標。設計完就開始畫圖瞜！長方形的部份就用glRecti()，文字則用glutBitmapCharacter()來繪製。 </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2017/02/10.jpg" alt="10"> </p><p>遊戲介面程式碼如下: </p><p>這是上半部介面的Code，這邊要注意的是Score和COMBO都是會隨著時間變動的，並且沒有COMBO的時候是不應該顯示COMBO數的 </p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">top_UI</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> shift = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">char</span> text_musicname[] = <span class="string">"MUSIC NAME: What Makes You Beautiful"</span>;</span><br><span class="line">    <span class="keyword">char</span> text_score[] = <span class="string">"SCORE:"</span>;</span><br><span class="line">    <span class="keyword">char</span> text_stop[] = <span class="string">"EXIT"</span>;</span><br><span class="line">    <span class="keyword">char</span> s[<span class="number">30</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">    <span class="keyword">char</span> c[<span class="number">30</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="comment">//頂部的視窗</span></span><br><span class="line">    <span class="comment">//線條寬度:5</span></span><br><span class="line">    glColor3f(<span class="number">1.0f</span>, <span class="number">1.0f</span>, <span class="number">1.0f</span>);</span><br><span class="line">    glRecti(<span class="number">0</span>, <span class="number">580</span>, <span class="number">500</span>, <span class="number">650</span>);</span><br><span class="line">    glColor3f(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>);</span><br><span class="line">    glRecti(<span class="number">5</span>, <span class="number">585</span>, <span class="number">495</span>, <span class="number">645</span>);</span><br><span class="line">    <span class="comment">//頂部功能選單</span></span><br><span class="line">    <span class="comment">//線條寬度:3</span></span><br><span class="line">    glColor3f(<span class="number">1.0f</span>, <span class="number">1.0f</span>, <span class="number">1.0f</span>);<span class="comment">//底色:白色</span></span><br><span class="line">    glRecti(<span class="number">422</span>, <span class="number">550</span>, <span class="number">500</span>, <span class="number">580</span>);</span><br><span class="line">    glColor3f(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>);<span class="comment">//兩個功能選框</span></span><br><span class="line">    glRecti(<span class="number">425</span>, <span class="number">553</span>, <span class="number">497</span>, <span class="number">580</span>);</span><br><span class="line">    <span class="comment">//------------------------------------------------------------------------------------------------</span></span><br><span class="line">    <span class="comment">//|  頂部功能選單文字                                                                            |</span></span><br><span class="line">    <span class="comment">//------------------------------------------------------------------------------------------------</span></span><br><span class="line">    glColor3f(<span class="number">1.0f</span>, <span class="number">1.0f</span>, <span class="number">1.0f</span>);<span class="comment">//白色</span></span><br><span class="line">                                <span class="comment">//MUSIC NAME</span></span><br><span class="line">    shift = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">strlen</span>(text_musicname); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        glRasterPos2i(<span class="number">100</span> + shift, <span class="number">610</span>);</span><br><span class="line">        glutBitmapCharacter(GLUT_BITMAP_9_BY_15, text_musicname[i]);</span><br><span class="line">        shift += glutBitmapWidth(GLUT_BITMAP_9_BY_15, text_musicname[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//SCORE</span></span><br><span class="line">    shift = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">strlen</span>(text_score); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        glRasterPos2i(<span class="number">100</span> + shift, <span class="number">590</span>);</span><br><span class="line">        glutBitmapCharacter(GLUT_BITMAP_9_BY_15, text_score[i]);</span><br><span class="line">        shift += glutBitmapWidth(GLUT_BITMAP_9_BY_15, text_score[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//SCORE VARABLE</span></span><br><span class="line">    itoa(score, s, <span class="number">10</span>);</span><br><span class="line">    shift = <span class="number">0</span>;</span><br><span class="line">    glColor3f(<span class="number">1.0f</span>, <span class="number">0.90f</span>, <span class="number">0.0f</span>);<span class="comment">//黃色</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">strlen</span>(s); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        glRasterPos2i(<span class="number">160</span> + shift, <span class="number">590</span>);<span class="comment">//結束座標(160,590)</span></span><br><span class="line">        glutBitmapCharacter(GLUT_BITMAP_9_BY_15, s[i]);</span><br><span class="line">        shift += glutBitmapWidth(GLUT_BITMAP_9_BY_15, s[i]);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//COMBO VARABLE</span></span><br><span class="line">    itoa(combo, c, <span class="number">10</span>);</span><br><span class="line">    shift = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (combo != <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> (combo &lt;= <span class="number">5</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            glColor3f(<span class="number">1.0f</span>, <span class="number">0.90f</span>, <span class="number">0.0f</span>);<span class="comment">//黃色</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (combo &lt;= <span class="number">10</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            glColor3f(<span class="number">1.0f</span>, <span class="number">0.60f</span>, <span class="number">0.0f</span>);<span class="comment">//橘色</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (combo &lt;= <span class="number">15</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            glColor3f(<span class="number">1.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>);<span class="comment">//紅</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (combo &lt;= <span class="number">20</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            glColor3f(<span class="number">1.0f</span>, <span class="number">0.0f</span>, <span class="number">1.0f</span>);<span class="comment">//紫</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">strlen</span>(c); ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            glRasterPos2i(<span class="number">40</span> + shift, <span class="number">530</span>);<span class="comment">//結束座標(160,590)</span></span><br><span class="line">            glutBitmapCharacter(GLUT_BITMAP_TIMES_ROMAN_24, c[i]);</span><br><span class="line">            shift += glutBitmapWidth(GLUT_BITMAP_TIMES_ROMAN_24, c[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    glColor3f(<span class="number">1.0f</span>, <span class="number">1.0f</span>, <span class="number">1.0f</span>);<span class="comment">//白色</span></span><br><span class="line">    <span class="comment">//EXIT</span></span><br><span class="line">    shift = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">strlen</span>(text_stop); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        glRasterPos2i(<span class="number">445</span> + shift, <span class="number">563</span>);</span><br><span class="line">        glutBitmapCharacter(GLUT_BITMAP_9_BY_15, text_stop[i]);</span><br><span class="line">        shift += glutBitmapWidth(GLUT_BITMAP_9_BY_15, text_stop[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>這是下半部介面的Code<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">buttom_UI</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//底部的視窗</span></span><br><span class="line">  <span class="comment">//線條寬度:5</span></span><br><span class="line">  glColor3f(<span class="number">1.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>);</span><br><span class="line">  glRecti(<span class="number">0</span>, <span class="number">0</span>, <span class="number">500</span>, <span class="number">70</span>);</span><br><span class="line">  glColor3f(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>);</span><br><span class="line">  glRecti(<span class="number">5</span>, <span class="number">5</span>, <span class="number">495</span>, <span class="number">65</span>);</span><br><span class="line">  glColor3f(<span class="number">1.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//輔助線</span></span><br><span class="line">  <span class="keyword">int</span> shift = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">4</span>; ++i)</span><br><span class="line">  &#123;</span><br><span class="line">    glBegin(GL_LINES);</span><br><span class="line">    glColor3f(<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);</span><br><span class="line">    glVertex2i(<span class="number">100</span> + shift, <span class="number">70</span>);</span><br><span class="line">    glVertex2i(<span class="number">100</span> + shift, <span class="number">550</span>);</span><br><span class="line">    glEnd();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//UI:上下左右</span></span><br><span class="line">    glPushMatrix();</span><br><span class="line">    glTranslatef(<span class="number">75</span> + shift, <span class="number">20</span>, <span class="number">0.0</span>);</span><br><span class="line">    arrow_Exterior_color.r = <span class="number">0.0</span>;</span><br><span class="line">    arrow_Exterior_color.g = <span class="number">1.0</span>;</span><br><span class="line">    arrow_Exterior_color.b = <span class="number">0.0</span>;</span><br><span class="line">    glColor3f(arrow_Exterior_color.r, arrow_Exterior_color.g, arrow_Exterior_color.b);</span><br><span class="line">    glCallList(UI_Up + (i – <span class="number">1</span>));</span><br><span class="line">    glPopMatrix();</span><br><span class="line">    shift += <span class="number">100</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//EXIT</span></span><br><span class="line">  shift = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">strlen</span>(text_stop); ++i)</span><br><span class="line">  &#123;</span><br><span class="line">    glRasterPos2i(<span class="number">445</span> + shift, <span class="number">563</span>);</span><br><span class="line">    glutBitmapCharacter(GLUT_BITMAP_9_BY_15, text_stop[i]);</span><br><span class="line">    shift += glutBitmapWidth(GLUT_BITMAP_9_BY_15, text_stop[i]);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>這裡的箭頭是用glCallList()來做的，所以需要先做好上下左右的箭頭<br>Arrow_LIST()這個function裡面除了UI的空心箭頭，還有之後遊戲的實心箭頭，之後會用到<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Arrow_LIST</span><span class="params">()</span><span class="comment">//List Obj</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//————————————————————————————————</span></span><br><span class="line">  <span class="comment">//| UI介面的上下左右: |</span></span><br><span class="line">  <span class="comment">//————————————————————————————————</span></span><br><span class="line">  glNewList(UI_Up, GL_COMPILE);<span class="comment">//上</span></span><br><span class="line">  <span class="comment">//外層</span></span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">15</span>, <span class="number">0</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span>, <span class="number">0</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span>, <span class="number">20</span>);</span><br><span class="line">  glVertex2f(<span class="number">15</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">50</span>, <span class="number">20</span>);</span><br><span class="line">  glVertex2f(<span class="number">25</span>, <span class="number">40</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//內層</span></span><br><span class="line">  glPushMatrix();</span><br><span class="line">  glTranslatef(<span class="number">25</span> – (<span class="number">25</span> * <span class="number">0.9</span>), <span class="number">20</span> – (<span class="number">20</span> * <span class="number">0.9</span>), <span class="number">0.0</span>);</span><br><span class="line">  glColor3f(arrow_interior_color.r, arrow_interior_color.g, arrow_interior_color.b);</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">15</span> * <span class="number">0.9</span>, <span class="number">0</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span> * <span class="number">0.9</span>, <span class="number">0</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span> * <span class="number">0.9</span>, <span class="number">20</span> * <span class="number">0.9</span> + <span class="number">0.5</span>);</span><br><span class="line">  glVertex2f(<span class="number">15</span> * <span class="number">0.9</span>, <span class="number">20</span> * <span class="number">0.9</span> + <span class="number">0.5</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">50</span> * <span class="number">0.9</span>, <span class="number">20</span> * <span class="number">0.9</span> + <span class="number">0.5</span>);</span><br><span class="line">  glVertex2f(<span class="number">25</span> * <span class="number">0.9</span>, <span class="number">40</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span> * <span class="number">0.9</span>, <span class="number">20</span> * <span class="number">0.9</span> + <span class="number">0.5</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glPopMatrix();</span><br><span class="line"></span><br><span class="line">  glEndList();</span><br><span class="line"></span><br><span class="line">  glNewList(UI_Down, GL_COMPILE);<span class="comment">//下</span></span><br><span class="line">  <span class="comment">//外層</span></span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">15</span>, <span class="number">40</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span>, <span class="number">40</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span>, <span class="number">20</span>);</span><br><span class="line">  glVertex2f(<span class="number">15</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">50</span>, <span class="number">20</span>);</span><br><span class="line">  glVertex2f(<span class="number">25</span>, <span class="number">0</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  <span class="comment">//內層</span></span><br><span class="line">  glPushMatrix();</span><br><span class="line">  glTranslatef(<span class="number">25</span> – (<span class="number">25</span> * <span class="number">0.9</span>), <span class="number">20</span> – (<span class="number">20</span> * <span class="number">0.9</span>), <span class="number">0.0</span>);</span><br><span class="line">  glColor3f(arrow_interior_color.r, arrow_interior_color.g, arrow_interior_color.b);</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">15</span> * <span class="number">0.9</span>, <span class="number">40</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span> * <span class="number">0.9</span>, <span class="number">40</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span> * <span class="number">0.9</span>, <span class="number">20</span> * <span class="number">0.9</span> – <span class="number">1</span>);</span><br><span class="line">  glVertex2f(<span class="number">15</span> * <span class="number">0.9</span>, <span class="number">20</span> * <span class="number">0.9</span> – <span class="number">1</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">50</span> * <span class="number">0.9</span>, <span class="number">20</span> * <span class="number">0.9</span> – <span class="number">1</span>);</span><br><span class="line">  glVertex2f(<span class="number">25</span> * <span class="number">0.9</span>, <span class="number">0</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span> * <span class="number">0.9</span>, <span class="number">20</span> * <span class="number">0.9</span> – <span class="number">1</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glPopMatrix();</span><br><span class="line">  glEndList();</span><br><span class="line"></span><br><span class="line">  glNewList(UI_Right, GL_COMPILE);<span class="comment">//右</span></span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">0</span>, <span class="number">10</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span>, <span class="number">30</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">30</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">10</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">40</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">0</span>);</span><br><span class="line">  glVertex2f(<span class="number">40</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  <span class="comment">//內層</span></span><br><span class="line">  glPushMatrix();</span><br><span class="line">  glTranslatef(<span class="number">20</span> – (<span class="number">20</span> * <span class="number">0.9</span>), <span class="number">20</span> – (<span class="number">20</span> * <span class="number">0.9</span>), <span class="number">0.0</span>);</span><br><span class="line">  glColor3f(arrow_interior_color.r, arrow_interior_color.g, arrow_interior_color.b);</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">0</span> * <span class="number">0.9</span>, <span class="number">10</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span> * <span class="number">0.9</span>, <span class="number">30</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span> * <span class="number">0.9</span> + <span class="number">1</span>, <span class="number">30</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span> * <span class="number">0.9</span> + <span class="number">1</span>, <span class="number">10</span> * <span class="number">0.9</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">20</span> * <span class="number">0.9</span> + <span class="number">1</span>, <span class="number">40</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span> * <span class="number">0.9</span> + <span class="number">1</span>, <span class="number">0</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">40</span> * <span class="number">0.9</span>, <span class="number">20</span> * <span class="number">0.9</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glPopMatrix();</span><br><span class="line">  glEndList();</span><br><span class="line"></span><br><span class="line">  glNewList(UI_Left, GL_COMPILE);<span class="comment">//左</span></span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">40</span>, <span class="number">10</span>);</span><br><span class="line">  glVertex2f(<span class="number">40</span>, <span class="number">30</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">30</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">10</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">40</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">0</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//內層</span></span><br><span class="line">  glPushMatrix();</span><br><span class="line">  glTranslatef(<span class="number">20</span> – (<span class="number">20</span> * <span class="number">0.9</span>), <span class="number">20</span> – (<span class="number">20</span> * <span class="number">0.9</span>), <span class="number">0.0</span>);</span><br><span class="line">  glColor3f(arrow_interior_color.r, arrow_interior_color.g, arrow_interior_color.b);</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">40</span> * <span class="number">0.9</span>, <span class="number">10</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">40</span> * <span class="number">0.9</span>, <span class="number">30</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span> * <span class="number">0.9</span> – <span class="number">1</span>, <span class="number">30</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span> * <span class="number">0.9</span> – <span class="number">1</span>, <span class="number">10</span> * <span class="number">0.9</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">20</span> * <span class="number">0.9</span> – <span class="number">1</span>, <span class="number">40</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span> * <span class="number">0.9</span> – <span class="number">1</span>, <span class="number">0</span> * <span class="number">0.9</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span> * <span class="number">0.9</span>, <span class="number">20</span> * <span class="number">0.9</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glPopMatrix();</span><br><span class="line">  glEndList();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//————————————————————————————————</span></span><br><span class="line">  <span class="comment">//| GAME介面的上下左右: |</span></span><br><span class="line">  <span class="comment">//————————————————————————————————</span></span><br><span class="line">  glNewList(GAME_Up, GL_COMPILE);<span class="comment">//上</span></span><br><span class="line">  glColor3f(<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>);</span><br><span class="line">  <span class="comment">//外層</span></span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">15</span>, <span class="number">0</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span>, <span class="number">0</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span>, <span class="number">20</span>);</span><br><span class="line">  glVertex2f(<span class="number">15</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">50</span>, <span class="number">20</span>);</span><br><span class="line">  glVertex2f(<span class="number">25</span>, <span class="number">40</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glEndList();</span><br><span class="line"></span><br><span class="line">  glNewList(GAME_Down, GL_COMPILE);<span class="comment">//下</span></span><br><span class="line">  glColor3f(<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>);</span><br><span class="line">  <span class="comment">//外層</span></span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">15</span>, <span class="number">40</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span>, <span class="number">40</span>);</span><br><span class="line">  glVertex2f(<span class="number">35</span>, <span class="number">20</span>);</span><br><span class="line">  glVertex2f(<span class="number">15</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">50</span>, <span class="number">20</span>);</span><br><span class="line">  glVertex2f(<span class="number">25</span>, <span class="number">0</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glEndList();</span><br><span class="line"></span><br><span class="line">  glNewList(GAME_Right, GL_COMPILE);<span class="comment">//右</span></span><br><span class="line">  glColor3f(<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>);</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">0</span>, <span class="number">10</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span>, <span class="number">30</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">30</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">10</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">40</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">0</span>);</span><br><span class="line">  glVertex2f(<span class="number">40</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glEndList();</span><br><span class="line"></span><br><span class="line">  glNewList(GAME_Left, GL_COMPILE);<span class="comment">//左</span></span><br><span class="line">  glColor3f(<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>);</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">40</span>, <span class="number">10</span>);</span><br><span class="line">  glVertex2f(<span class="number">40</span>, <span class="number">30</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">30</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">10</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glBegin(GL_POLYGON);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">40</span>);</span><br><span class="line">  glVertex2f(<span class="number">20</span>, <span class="number">0</span>);</span><br><span class="line">  glVertex2f(<span class="number">0</span>, <span class="number">20</span>);</span><br><span class="line">  glEnd();</span><br><span class="line">  glEndList();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>UI設置到這裡就大功告成搂！</p><p>完整程式碼在: <a href="https://github.com/john850512/MusicDance/blob/master/%EF%BC%BB%EF%BC%AF%EF%BD%90%EF%BD%85%EF%BD%8E%EF%BC%A7%EF%BC%AC%EF%BC%BD%E9%9F%B3%E6%A8%82%E7%AF%80%E5%A5%8F%E9%81%8A%E6%88%B2\(1\" target="_blank" rel="noopener">Github</a>-UI%E8%A8%AD%E8%A8%88)</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> openGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[20161015]NCPC決賽</title>
      <link href="/posts/f80363b6/"/>
      <url>/posts/f80363b6/</url>
      
        <content type="html"><![CDATA[<p>第二年進到決賽了，雖然以我們學校進決賽沒什麼難度…. </p><p>出發前我還睡過頭，讓兩個學長等了我一下囧   </p><p>時隔一年再次來到中山的地下室(?)，今年的主題風格跟去年不一樣，不過我很喜歡。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/10/14708164_1484112421605004_3432886561191439996_n.jpg" alt="14708164_1484112421605004_3432886561191439996_n"></p><a id="more"></a><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/10/14725718_1484112398271673_2424926390424989006_n.jpg" alt="14725718_1484112398271673_2424926390424989006_n"> </p><p>在簽到時工作人員看到我們的隊名還問說”XXX是哪一位呀”(我們的隊名是XXX大神凱瑞)可是他沒有來TT…   </p><p>(這次心得太晚補了…題目幾乎都忘光了) 一開始題目發下來我們一人拿一題看，剛好我手上的A好像還能解，就花了一些時間把他解掉了。 </p><p>之後就是卡題深淵…大概有一個小時都沒有解出題出來…   </p><p>C題看起來很簡單，是有關二元搜尋樹的，要找一個”Possible examine sequential”…這是什麼鬼，不記得二元搜尋樹有這個特性阿= =? </p><p>後來是從兩個範例測資去推他是”沒有同時有兩個子節點的二元搜尋樹”，抱著死馬當活馬醫的精神才AC了(中間手殘還WA了一次)   </p><p>然後這時才看到I，題目大概是說有幾群家人，有幾個不同數量位置的桌子，同群的不能同桌，問最大可座人數……太晚看到這題了，這也是個大水題阿，想法就是從最小桌和最多人的開始分配即可，於是又花了一點時間迅速AC他。   </p><p>到這邊還剩一個小時左右吧，於是我們決定只解B跟F，看看能不能多解幾題。 </p><p>B是分割的問題，感覺跟數學有某種關係，可是我們一直都找不到規律… </p><p>F是DFS的問題，找尋路上權重最大值，可是一般的DFS會爆，我也不知道要怎麼換寫法或是減少遞迴次數… </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/10/66.jpg" alt="66"> </p><p>於是最後我們還是只有三題~三顆氣球~~，57名，不過比去年的1題進步多了就是。   </p><p>明年還有機會再來玩玩~~</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ncpc </tag>
            
            <tag> competition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Java]關於Arrays.sort()</title>
      <link href="/posts/4d88ba1/"/>
      <url>/posts/4d88ba1/</url>
      
        <content type="html"><![CDATA[<p>最近練習寫Java的時候遇到一個問題:請實作降冪排序。 </p><p>透過查詢文件知道了Java有著Arrays.sort()這個類似於C/C++ sort()的排序function。 </p><p>在C/C++的sort(需include )如果要自定義判斷式可以像下面這樣寫：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a &lt; b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>然後在main裡面這樣寫</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> arr[<span class="number">5</span>] = &#123;<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">2</span>&#125;;</span><br><span class="line">sort(arr,cmp);</span><br></pre></td></tr></table></figure><p>那在Java呢?Arrays.sort()是預設升冪排序的，我們當然也可以自定義一個判斷式，不過Java提供了另一種選擇，也就是我們可以用Collection.reverseOrder這個Java提供的Comparator來快速達成我們的需求。<br><a id="more"></a></p><p>一段降冪排序的Code範例如下: </p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer[] arr = &#123;<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>&#125;;</span><br><span class="line">System.out.println(<span class="string">"before sorting: "</span>);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> temp:arr) &#123;</span><br><span class="line">  System.out.print(temp + <span class="string">" "</span>);</span><br><span class="line">&#125;</span><br><span class="line">Arrays.sort(arr,Collections.reverseOrder());</span><br><span class="line">System.out.println(<span class="string">""</span>);</span><br><span class="line">System.out.println(<span class="string">"after sorting: "</span>);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> temp:arr) &#123;</span><br><span class="line">  System.out.print(temp + <span class="string">" "</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一次進行這個練習的時候是編譯錯誤的，上網Google發現陣列必須是Integer[]而不能是int[] </p><p>於是來探討一下這兩者有什麼差別?上網整理完後大約是這樣 </p><ol><li>Integer[]是物件，int[]是原生型別(不是物件) </li><li>可以想成Integer是將int以及一些資訊包裝過後的類別 </li><li>Integer的初始值是Null，int的初始值是0 </li><li>在JDK1.5提供了簡潔的轉換方法:自動拆箱(autoboxing)、自動封箱(unboxing) </li><li>在某些情況(Arraylist、Hashmap、以及上面這個範例)是無法使用int的，因為傳入的必須是一個物件 </li></ol><p>所以在寫sort的時候必須宣告成Integer的型態!! 現在知道如何做降冪排序了，那如果要自定義Comparator呢?在Java中需要透過implement來override<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cmp</span> <span class="keyword">implements</span> <span class="title">Comparator</span></span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(T temp1, T temp2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> temp1 – temp2;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>這樣自定義好後就可以將Sort()改成這樣子 <code>Arrays.sort(arr, new Cmp());</code></p><p>以上，排序完成!! </p><p>參考網址: </p><ol><li><a href="http://www.cnblogs.com/shenliang123/archive/2011/10/27/2226903.html" target="_blank" rel="noopener">http://www.cnblogs.com/shenliang123/archive/2011/10/27/2226903.html</a> </li><li><a href="https://read01.com/2zNy7L.html" target="_blank" rel="noopener">https://read01.com/2zNy7L.html</a> </li><li><a href="https://www.javaworld.com.tw/jute/post/view?bid=29&amp;id=308357" target="_blank" rel="noopener">https://www.javaworld.com.tw/jute/post/view?bid=29&amp;id=308357</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C槽SSD容量不足解決經驗談</title>
      <link href="/posts/41b51855/"/>
      <url>/posts/41b51855/</url>
      
        <content type="html"><![CDATA[<p>之前有提過C槽容量不足應該做什麼，相信有這困擾的人GOOGLE都會找到跟我一樣的方法：</p><ol><li>清理硬碟</li><li>刪除更新檔、分頁檔</li><li>移除不必要的應用程式</li></ol><p>但是上面的方式終究有限，以我本人來說只要灌了Visual Studio，某些東西會強制安裝在C槽，然後就這樣越積越多就</p><p><strong>爆炸了！！！！！！！！！！！！！</strong><br><a id="more"></a><br>當初只裝了１２８Ｇ的ＳＳＤ真後悔，每次動不動就去清理磁碟，看到的卻是只有幾十ＭＢ可清理，</p><p>分頁檔也更改設定了，應用程式剩一堆微軟的套件也不知道可不可以移，眼看只剩6G的空間….</p><p>於是我就帶著筆電直接去店家詢問我該怎麼救活我的C槽，店家給我了幾個方案，總之大意是：</p><p><strong>把現在的ＳＳＤ資料全部轉到新的２５６Ｇ的ＳＳＤ中（要先備份好Ｃ槽的我的最愛、桌面……）</strong></p><p>不過聽說我的ＳＳＤ是ｍｓａｔａ的，無法相容最新的ＳＳＤ（好像是ｍ２），所以我把我的光碟機那空間挪給２５６Ｇ的ＳＳＤ來使用，開機的資料也都全部移過去。</p><p>問題到此總算解決了，花了一筆小錢，現在電腦上有兩顆ＳＳＤ，固態硬碟用起來也不會有甚麼過熱的問題，還蠻順暢的。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NCPC初賽-不定長度字串轉整數的方法</title>
      <link href="/posts/be041147/"/>
      <url>/posts/be041147/</url>
      
        <content type="html"><![CDATA[<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/10/e693b7e58f96.jpg" alt="擷取.JPG"></p><p>大四的學長今年可能是最後一年了，所以第一次找他們一起組一隊打打看。 </p><p>按照去年的經驗只要有一題基本上初賽就會過，所以也沒有很緊張，連規定的25張code都沒有印就上場了。說來也蠻好笑的，原本系上的考場因為颱風來停電，所以改到圖資的電腦教室去考試，這邊的設備被反倒比系上好……<br><a id="more"></a></p><p>這次總共有五題，程式很猛的學長拿了第一題開始看，另一個學長和我看其他題，剛好看到有一題一副就是”我是水題，快解我”的樣子，所以我就先去看那題了：求一個不定長度的陣列，判斷是不是Max Heap，想想還蠻簡單的，甚至還以為有陷阱。 </p><p>不過不定長度字串的寫法真的不會，印象中每次這種題型我都要用字元一個一個判斷，今年學長教了一個方法，趁這次機會趕快學起來： </p><p>大概是說stringstream是一個c++的stream，透過&gt;&gt;和&lt;&lt;支援字串和不同型態的轉換，和cin相同遇到空白會忽略，EOF為結束 相關資料可以參考：<br><a href="https://dotblogs.com.tw/v6610688/2013/11/08/cplusplus\_stringstream\_int\_and\_string\_convert\_and_clear" target="_blank" rel="noopener">[C++] StringStream－int和sting轉換的另一種方案與清空StringStream</a></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">string</span> s;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span>(getline(<span class="built_in">cin</span>,s))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> n = <span class="number">0</span>;</span><br><span class="line">        <span class="function"><span class="built_in">stringstream</span> <span class="title">ss</span><span class="params">(s)</span></span>;</span><br><span class="line">        <span class="keyword">while</span>(ss &gt;&gt; n )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; n &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>處理完字串就解決完D了! 上傳AC後發現好多人都對了～～學長在ＷＡ兩次後也過Ａ了～ </p><p>剩下的時間我們想把Ｂ解出來，題目大概是給一個長方形的地圖和一些座標，有個機器人能從左上走到右下，走法只能是： </p><ol><li>往右走 </li><li>往下走 </li></ol><p>在放出最少機器人的情況下達成所有座標都被走過，最少機器人的值為多少？   </p><p>原本的想法用Ｇｒｅｅｄｙ每次找最左上方的點，依序往右下找，找到終點就算一條路徑，再用同樣的方法找第二條路徑……範例測資過了卻WA，有點懊惱，不知道錯在哪裡。   </p><p>總之初賽兩題，沒意外應該進決賽了，到時去吃下午茶。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ncpc </tag>
            
            <tag> competitive programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日本大阪京都自由行</title>
      <link href="/posts/e826a870/"/>
      <url>/posts/e826a870/</url>
      
        <content type="html"><![CDATA[<p>這個暑假終於可以去<strong>一直以來都很想去的國家 - 日本！！</strong> </p><p>很開心～～這次是家族旅行，行程是去過很多次的姊姊定的，所以我只負責玩跟帶我爸媽入境（因為跟姐姐做不同的班機），哈。 </p><p>再高雄機場搭飛機的時候，我媽看到一群人在長榮櫃台前排隊，就開始一個一個問「你們也是要去日本嗎?自由行嗎?我們也是耶……」 覺得我媽真的很厲害，擁有一秒跟人變熟的技能，只是害得我跟陌生人有點尷尬就是了，不過在聊天過程中才知道：原來日本的暑假是８月才開始，原本以為現在去熱門景點一定會很多人（因為暑假），因此鬆了一口氣。 </p><p>我也是第一次自己走入境手續，就算看過網路上的教學也還是有點緊張的，全程一直很擔心我會被攔下來用英文問話，然後我英文又很爛結果問一問就被帶走，最後就被拘留了（想太多ｘＤ…最後很順利的入境了） </p><p>想要看怎麼入境的我推薦這個人寫的<a href="http://nicklee.tw/?p=687" target="_blank" rel="noopener">教學</a>，寫的很詳細 </p><p>過程大概就是：出飛機－＞搭車－＞過檢疫站－＞辦理入國手續－＞拿行李－＞成功！ </p><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1636.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1637.jpg?w=1400&h=&crop=1"  /></td>  </tr>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1638.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1639.jpg?w=1400&h=&crop=1" /></td>  </tr></table><a id="more"></a><h1 id="7-7-四-心齋橋筋、道頓堀"><a href="#7-7-四-心齋橋筋、道頓堀" class="headerlink" title="7/7(四) - 心齋橋筋、道頓堀"></a>7/7(四) - 心齋橋筋、道頓堀</h1><p>第一天抵達機場後，我們就乘坐ＪＲ到大阪ｃｈｅｃｋ　ｉｎ，不得不說日本的交通真的好方便，大眾交通幾乎能到達任何地方，可是真的好複雜呀，看著密密麻麻的示意圖頭都昏了。</p><div align="center">  <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1642.jpg" width="70%" /></div><p>放完行李後我們就去了心齋橋筋（聽說筋是什麼路的意思），看到了著名的地標 - 固力果，大家都知道很有名但不知道為啥，後來查了一下原來是固力果公司生產第一款糖果的卡路里正好是跑300公尺所需的熱量，所以廣告就是一個人在跑步的樣子。</p><div class="three-imgs">  <div class="vertical-to-square"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1664.jpg" /></div>  <div class="vertical-to-square"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1647.jpg" /></div></div><p>開始Ｓｈｏｐｐｉｎｇ，第一天剛到日本還有點不習慣，買的東西都要上消費稅，買衣服的時候，因為是第一次自己跟國外店員對話，一開始我用破英文跟他說我是外國人，然後他楞了一下（好尷尬阿～～～）然後就開始用計算機跟我比手畫腳，最後買完終於鬆了一口氣，但是感覺很好玩。</p><p>第一天吃了好多章魚燒（來大阪就是要吃章魚燒呀），我在買的時候一樣是用破英文跟店員交談的，很新鮮的是儘管他們知道我聽不懂英文，仍然會一邊說日文一邊跟你比手畫腳ｘＤ</p><p>午餐原本要吃一蘭拉麵的，可是太陽好熱又要現場排隊，於是我們跑去吃某一家煎餃，也是很好吃。值得一提的是，這裡店家的招牌都好有趣，街道望眼看去整條都是立體的食物看版，真的很好拍。</p><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1648.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1649.jpg?w=1400&h=&crop=1"  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1652.jpg?w=1400&h=&crop=1"  /></td>  </tr>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1653.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1654.jpg?w=1400&h=&crop=1" /></td>  </tr></table><p>晚上逛道頓堀，夜晚的商家搭配著各式各樣奇特的招牌也給人一種非常熱鬧的感覺～ </p><p>大阪燒是今日的晚餐，第一次吃大阪燒覺得真的很好吃呢！ </p><p>然後今天一天下來我發現日本沒、有、手、搖、杯，這實在是太震驚了呀！！！他們都是喝水嗎？！！！也太健康了吧～～</p><p>受不了的我於是跑去麥當勞買了一杯綠色的飲料（？）後來才知道我喝的是蘇打…..。 </p><div align="center">  <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1669.jpg?w=700&h=&crop=1" width="20%" /></div> <table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1670.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1671.jpg?w=1400&h=&crop=1"  /></td>  </tr>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1682.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1686.jpg?w=1400&h=&crop=1" /></td>  </tr></table><p>一個小插曲是晚上時媽媽在飯店的房間發現了一隻小強（其實不小…還蠻大的）然後他就堅持要客訴，於是我們就去櫃檯跟他們反映這件事…然後我一時想不起來小強的英文，於是…. </p><p>我：＂excuse me……＂ </p><p>櫃台：＂Yes?＂ </p><p>我：＂Ahhhh…My mom find a “big bug”in her bathroom…” </p><p>櫃台：”……cockroach?” </p><p>我：”!!!!!!ahhhh….yes…” </p><p>當下真的是超想一頭撞在牆上的，重點是他居然還聽懂了！！！！</p><h1 id="7-8-五-大阪環球"><a href="#7-8-五-大阪環球" class="headerlink" title="7/8(五) - 大阪環球"></a>7/8(五) - 大阪環球</h1><p>今天只有一個行程，就是！！！！大阪環球！！！！不論男女老少都喜愛的環球～～～～ </p><div align="center">  <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1703.jpg" width="70%" /></div><p>來環球前先買好票就不用再入口傻傻的排隊了，<strong>先買票是很重要的，包含後面遊樂設施的快速通關卷</strong> </p><p>買了就能通行無阻，一路從排隊的最後排爽爽的走到最前排，咻～ </p><p>可是不便宜……嗚….. </p><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1705.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1704.jpg?w=1400&h=&crop=1"  /></td>  </tr></table><p>不囉嗦，直接上圖片！</p><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1741.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1736.jpg?w=1400&h=&crop=1"  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1726.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1723.jpg?w=1400&h=&crop=1"  /></td>  </tr>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1715.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1713.jpg?w=1400&h=&crop=1"  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1711.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1708.jpg?w=1400&h=&crop=1"  /></td>  </tr></table><p>還有哈利波特主題區！！！ </p><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1763.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1764.jpg?w=1400&h=&crop=1"  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1771.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1776.jpg?w=1400&h=&crop=1"  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1779.jpg?w=1400&h=&crop=1"  /></td>  </tr>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1780.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1781.jpg?w=1400&h=&crop=1"  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1786.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1789.jpg?w=1400&h=&crop=1"  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1809.jpg?w=1400&h=&crop=1"  /></td>  </tr></table><table align="center" border=0>  <tr>    <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1801.jpg?w=700&h=&crop=1" width="50%" /></td>     <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1805.jpg?w=700&h=&crop=1" width="50%" /></td>  </tr></table><p>做了很多很刺激的娛樂設施，真的很舒壓ｘＤ感覺一學期下來壓力真的累積了不少。 </p><p>有個很好奇的點是：白天的時候不時會看到穿制服的學生也出現在排隊陣容中，這時候不是應該在上課嗎（？）還是日本都是早上就放學了（？？？？） </p><p>國外的遊樂園真的是個很歡樂的地方，走在路上看到每個工作人員都是很享受再跟人一起玩樂的感覺，搭配各種音樂、劇場表演，被這種氣氛感染一天下來真的很愉快！！ </p><p>今天沒什麼好說的，就是讚～</p><h1 id="7-9-六-大阪城、大阪今昔生活館、泡溫泉、瀧見小路、梅田藍天大廈"><a href="#7-9-六-大阪城、大阪今昔生活館、泡溫泉、瀧見小路、梅田藍天大廈" class="headerlink" title="7/9(六) - 大阪城、大阪今昔生活館、泡溫泉、瀧見小路、梅田藍天大廈"></a>7/9(六) - 大阪城、大阪今昔生活館、泡溫泉、瀧見小路、梅田藍天大廈</h1><p>今天走文藝風，去看日本三大名城之一，還沒入城光看護城河就覺得很有魄力，走近一看更是不同凡響。</p><div align="center">  <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1867.jpg" width="70%" />    </div><p>大家都拿著自拍棒，所以這時候就不用害羞，也拿起自拍棒就對了！反正別人也不認識我。 </p><p>當下我很認真的一樓一樓參觀，仔細的看了關於大阪城的翩翩歷史，好好地當了一回小文青，不過這裡還是放幾張照片代表就好。 </p><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1871.jpg?w=700&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1880.jpg?w=700&h=&crop=1"  /></td>  </tr></table><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1891.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1879.jpg?w=1400&h=&crop=1"  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1873.jpg?w=1400&h=&crop=1"  /></td>  </tr></table><p>Ｐ.S.　裡面可以讓你玩ＣｏｓＰｌａｙ唷，都來了當然要體驗一下以前大將的服裝啦，不過頭盔戴起來好重呀～   </p><p>之後去今昔生活館，在還沒去前做過功課還以為是一條街道，後來發現原來是在一棟建築物裡面有點傻眼到，哈。可惜的是當天的和服都出租完畢了，沒能穿和服進去古時候日本的街道拍照。 </p><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1898.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1900.jpg?w=1400&h=&crop=1"  /></td>  </tr></table><p>路過看到的日本電視台ＮＨＫ，這造型好奇特呀～</p><div align="center">  <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1894.jpg" width="70%" />    </div><p>下午去泡溫泉，第一次泡還蠻不習慣的…..各種意義上…..就不說了ｘＤ   </p><p>晚上去瀧見小路吃晚餐，之後去梅田藍天大廈看夜景，夜景真的很漂亮，不過這裡要小心亮度，不小心就會被旁邊閃瞎（戴墨鏡） 網路上有看到一種說法是：日本都有限制建築物的高度，所以景色看起來都會很漂亮，因為不會被許多高樓大廈擋住，不知道是不是真的，不過看到的住宅真的都不高就是了。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1932.jpg" alt="IMAG1932"> </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1926.jpg" alt="IMAG1926"></p><h1 id="7-10-日-黑門市場、京都Check-in"><a href="#7-10-日-黑門市場、京都Check-in" class="headerlink" title="7/10(日) - 黑門市場、京都Ｃｈｅｃｋ　ｉｎ"></a>7/10(日) - 黑門市場、京都Ｃｈｅｃｋ　ｉｎ</h1><p>黑門市場～大大的燈籠超級顯眼的～到底為什麼連一個市場都可以弄的這麼有創意～～～ </p><p>在這裡可以吃到很新鮮的海鮮，以及買伴手禮～～ </p><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1939.jpg?w=700&h=&crop=1" width="50%" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1937.jpg?w=700&h=&crop=1" width="50%"  /></td>  </tr>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1935.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1942.jpg?w=1400&h=&crop=1" /></td>  </tr></table><p>這是我們在大阪的最後一站</p><p>之後就出發文化之都 - 京都啦！大阪掰～～</p><h1 id="7-11-一-清水寺、返家"><a href="#7-11-一-清水寺、返家" class="headerlink" title="7/11(一) - 清水寺、返家"></a>7/11(一) - 清水寺、返家</h1><p>在日本的最後一站～清水寺～～<strong>京都最古老的寺院</strong></p><p>京都有很多神社，其中最想去的是伏見稻荷神社，不過留給下次了。</p><p>熱門景點理所當然的觀光客也多，走在路上聽的不是日文而是中文居多ｘＤ</p><p>好好的參拜過後買了幾個御守回去，送人自用兩相宜。</p><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1992.jpg?w=700&h=&crop=1" width="50%" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag2008.jpg?w=700&h=&crop=1" width="50%"  /></td>  </tr>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag1990.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag2033.jpg?w=1400&h=&crop=1" /></td>  </tr></table><p>再往裡面走一點，就會遇到地主神社了。事先沒有做過功課所以完全不知道裏頭還有一個神社。 </p><p>老實說這邊比清水寺好拍多了　怎麼拍都很有感覺ｘＤＤ </p><p>整個就是我的菜呀～ </p><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag2042.jpg?w=700&h=&crop=1" width="50%" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag2054.jpg?w=700&h=&crop=1" width="50%"  /></td>  </tr>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag2044.jpg?w=1400&h=&crop=1" /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag2049.jpg?w=1400&h=&crop=1" /></td>  </tr></table><p>在京都只逛了清水寺我們就要搭ＪＲ回關西機場返家了，但是但是但是但是！！！！！ </p><p>我們抓三個小時左右從京都回去，原本預計到了還會有半個小時左右的空閒，但當我們正要上ＪＲ的時候，一位會說中文的服務生來說：因為出了人身事故，所以此班列車目前停駛，不知道哪時候會出發，如果趕時間可以搭公車…..算了一下公車到關西機場的時間距離出境手續結束辦理只剩５分鐘的時間，５分鐘！！！！！ </p><p>一下車就飛奔到櫃檯呀，最後好險是趕上了，超級驚險阿～呼～</p><h1 id="後記"><a href="#後記" class="headerlink" title="後記"></a>後記</h1><p>第一次去日本，又是自助旅行，真的覺得做功課的姊姊們很辛苦，不然日本的交通那麼複雜，我們一定會迷路呀～～ </p><p>這五天真的走了很多路（姐姐的Ｉｐｈｏｎｅ顯示每天至少１萬步），不同於搭遊覽車，所有景點都是步行＋大眾交通的，每天回到飯店就是累到快癱掉了（在這大推休足時間…睡前貼一包真的超舒服呀）不過也因此更接近日本的日常。 </p><p>這五天觀察到了日本與台灣很多不一樣的地方，像是沒有手搖杯（這很重要）、日本店家都很早關、很有秩序(真的體會到了)….. </p><p>跟日本用英文彼此交談的感覺真的很好玩，雖然很對不起他們我的英文這麼爛….，不過每當有日本人可以用英文跟我全程溝通無障礙的時候（通常會是景點的服務人員）我就會超級興奮，居然有人聽的懂耶～～ </p><p>題外話，我們在某一天晚上向一位日本人問路的時候，那時真心覺得對不起她…因為他感覺就真的很想幫我們，可是又無法用英文表達出來，只能靠比手畫腳的方式，然後我們只能「歐～～歐歐～～」的回應她ｘＤ </p><p>之後還要再自助旅行來日本，等著我吧！我下次會好好練好英文再來的！！ </p><p>附上最後的戰利品，其實自己沒有買什麼東西…..倒是幫別人代購了很多 </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/08/imag2066.jpg" alt="IMAG2066.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> travel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Assembly]Set TextColor</title>
      <link href="/posts/eaba0a86/"/>
      <url>/posts/eaba0a86/</url>
      
        <content type="html"><![CDATA[<h2 id="題目描述"><a href="#題目描述" class="headerlink" title="題目描述"></a>題目描述</h2><p>Write a program that displays a single character in all possible combinations of foreground and background colors(16*16 = 256).The Colors are numbered from 0 to 15, so you can use a nested loop to generate all possible combination.   </p><h2 id="題意"><a href="#題意" class="headerlink" title="題意"></a>題意</h2><p>用迴圈列印出所有顏色的排列組合。 </p><h2 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h2><p>先介紹一下SetTextColor這個Procedure: </p><p>他會參考al這個暫存器，我們知道al是一個byte = 8bit。 </p><p>所以al又分成 前4bit(背景色) + 後4bit(前景色)，假色今天要用藍底白字就可以這樣寫:<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mov al , white + ( blue*16 )</span><br></pre></td></tr></table></figure><br>或<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mov al , blue </span><br><span class="line">shl al ,  4 </span><br><span class="line">add al , white</span><br></pre></td></tr></table></figure></p><p>或直接用數字代表顏色。     </p><h2 id="程式碼"><a href="#程式碼" class="headerlink" title="程式碼"></a>程式碼</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INCLUDE Irvine32.inc</span><br><span class="line">.data</span><br><span class="line">myMessage BYTE &#39;A&#39;</span><br><span class="line">newLine BYTE 0dh,0ah,0 </span><br><span class="line">.code</span><br><span class="line">main PROC</span><br><span class="line">    call Clrscr</span><br><span class="line">    push ebx    ;先儲存一開始暫存器的值</span><br><span class="line">    push ecx</span><br><span class="line"> </span><br><span class="line">    mov eax,0</span><br><span class="line">    push eax    ;先將0存起來，L1跑完一輪拿出來用</span><br><span class="line">    mov ecx,16</span><br><span class="line">L2: push ecx    ;儲存外層loop的值</span><br><span class="line">    mov ebx,0</span><br><span class="line">    mov ecx,16  ;設定內層loop的值</span><br><span class="line">L1: push eax    ;內層暫存eax的值，因為WriteChar會用到al</span><br><span class="line">    shl eax,4</span><br><span class="line">    add eax,ebx</span><br><span class="line">    call SetTextColor</span><br><span class="line">    mov  al,myMessage</span><br><span class="line">    call WriteChar</span><br><span class="line">    inc ebx</span><br><span class="line">    pop eax</span><br><span class="line">    Loop L1</span><br><span class="line"> </span><br><span class="line">    pop ecx     ;將原先外層loop的值取出</span><br><span class="line">    pop eax     ;將原先eax的值拿出來加1</span><br><span class="line">    inc eax</span><br><span class="line">    push eax    ;在儲存回去</span><br><span class="line">    mov  edx,OFFSET newLine     ;換行</span><br><span class="line">    call WriteString</span><br><span class="line">    Loop L2</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    pop ecx</span><br><span class="line">    pop ebx</span><br><span class="line">    mov eax,WHITE + (Black SHL 4)       ;設回原本設定</span><br><span class="line">    call SetTextColor</span><br><span class="line">    exit</span><br><span class="line">main ENDP</span><br><span class="line">END main</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> assembly </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Assembly]GCD</title>
      <link href="/posts/1cdf8b01/"/>
      <url>/posts/1cdf8b01/</url>
      
        <content type="html"><![CDATA[<p>GCD，<strong>最大公因數</strong>（Greatest Common Divisor，簡寫為<strong>G.C.D.</strong>；或Highest Common Factor，簡寫為<strong>H.C.F.</strong>），指某幾個整數共有因數中最大的一個。   </p><p>介紹: <a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E5%85%AC%E5%9B%A0%E6%95%B8" target="_blank" rel="noopener">維基百科</a>   </p><p>想法: 從C/C++程式碼轉換成組合語言。<br><a id="more"></a></p><h2 id="迴圈版本"><a href="#迴圈版本" class="headerlink" title="迴圈版本"></a>迴圈版本</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INCLUDE Irvine32.inc</span><br><span class="line">.data</span><br><span class="line">myMessage BYTE &quot;Please enter two integer number.&quot;,0dh,0ah,0</span><br><span class="line">intVal1 SDWORD ?</span><br><span class="line">intVal2 SDWORD ?</span><br><span class="line">.code</span><br><span class="line">main PROC</span><br><span class="line">    call Clrscr</span><br><span class="line">    mov  edx,OFFSET myMessage</span><br><span class="line">    call WriteString</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    call ReadInt</span><br><span class="line">    mov intVal1,eax</span><br><span class="line">    cdq                 ;abs</span><br><span class="line">    xor eax , edx       ;abs</span><br><span class="line">    sub eax , edx       ;abs</span><br><span class="line">    mov ebx , eax</span><br><span class="line"> </span><br><span class="line">    call ReadInt</span><br><span class="line">    mov intVal1,eax</span><br><span class="line">    cdq                 ;abs</span><br><span class="line">    xor eax , edx       ;abs</span><br><span class="line">    sub eax , edx       ;abs</span><br><span class="line">    mov ecx , eax</span><br><span class="line"> </span><br><span class="line">    call GCD</span><br><span class="line">    call WriteInt</span><br><span class="line">    exit</span><br><span class="line">main ENDP</span><br><span class="line">;---------------------------------------------------------</span><br><span class="line">GCD PROC </span><br><span class="line">;   Yo this func can help U to find the GCD from two Integer</span><br><span class="line">;   Receives: intVal1 in ebx , intVal2 in ecx</span><br><span class="line">;   Returns: GCD in eax</span><br><span class="line">;---------------------------------------------------------</span><br><span class="line">    mov eax , ebx</span><br><span class="line">L1: cdq</span><br><span class="line">    idiv ecx</span><br><span class="line">    mov eax , ecx </span><br><span class="line">    mov ecx , edx </span><br><span class="line">    cmp ecx , 0</span><br><span class="line">    jg L1</span><br><span class="line">    ret</span><br><span class="line">GCD ENDP</span><br><span class="line">END main</span><br></pre></td></tr></table></figure><h2 id="遞迴版本"><a href="#遞迴版本" class="headerlink" title="遞迴版本"></a>遞迴版本</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INCLUDE Irvine32.inc</span><br><span class="line">.data</span><br><span class="line">myMessage BYTE &quot;Please enter two integer number.&quot;,0dh,0ah,0</span><br><span class="line">intVal1 SDWORD ?</span><br><span class="line">intVal2 SDWORD ?</span><br><span class="line">.code</span><br><span class="line">main PROC</span><br><span class="line">    call Clrscr</span><br><span class="line">    mov ecx , 5</span><br><span class="line">L1:</span><br><span class="line">    mov  edx,OFFSET myMessage</span><br><span class="line">    call WriteString</span><br><span class="line">     </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    call ReadInt</span><br><span class="line">    mov intVal1,eax</span><br><span class="line">    cdq                 ;abs</span><br><span class="line">    xor eax , edx       ;abs</span><br><span class="line">    sub eax , edx       ;abs</span><br><span class="line">    push eax</span><br><span class="line"> </span><br><span class="line">    call ReadInt</span><br><span class="line">    mov intVal1,eax</span><br><span class="line">    cdq                 ;abs</span><br><span class="line">    xor eax , edx       ;abs</span><br><span class="line">    sub eax , edx       ;abs</span><br><span class="line">    push eax</span><br><span class="line"> </span><br><span class="line">    call GCD</span><br><span class="line">    call WriteInt</span><br><span class="line">    call CrLf</span><br><span class="line">    Loop L1</span><br><span class="line"> </span><br><span class="line">    exit</span><br><span class="line">main ENDP</span><br><span class="line">;---------------------------------------------------------</span><br><span class="line">GCD PROC </span><br><span class="line">;   Yo this func can help U to find the GCD from two Integer Implentnt by Recursion</span><br><span class="line">;</span><br><span class="line">;   int gcd(int m, int n) &#123; </span><br><span class="line">;        if(n &#x3D;&#x3D; 0) </span><br><span class="line">;             return m; </span><br><span class="line">;        else</span><br><span class="line">;             return gcd(n, m % n); </span><br><span class="line">;   &#125;</span><br><span class="line">;</span><br><span class="line">;   Receives: intVal1 in ebx , intVal2 in ecx</span><br><span class="line">;   Returns: GCD in eax</span><br><span class="line">;---------------------------------------------------------</span><br><span class="line">    push ebp</span><br><span class="line">    mov ebp , esp</span><br><span class="line"> </span><br><span class="line">    mov eax , [ebp+8]       ;n</span><br><span class="line">    cmp eax,0               ; if n &#x3D;&#x3D; 0</span><br><span class="line">    ja L1</span><br><span class="line">    mov eax , [ebp+12]      ;m</span><br><span class="line">    je Quit</span><br><span class="line"> </span><br><span class="line">L1:</span><br><span class="line">    mov eax , [ebp+12]</span><br><span class="line">    cdq</span><br><span class="line">    mov ebx , [ebp+8]</span><br><span class="line">    idiv ebx                </span><br><span class="line">    push [ebp+8]            ; n</span><br><span class="line">    push edx                ; m % n</span><br><span class="line">    call GCD</span><br><span class="line"> </span><br><span class="line">Return:</span><br><span class="line"> </span><br><span class="line">Quit:</span><br><span class="line">    pop ebp</span><br><span class="line">    ret 8</span><br><span class="line">GCD ENDP</span><br><span class="line">END main</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> assembly </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[C]Find All Shortest Path</title>
      <link href="/posts/6c292315/"/>
      <url>/posts/6c292315/</url>
      
        <content type="html"><![CDATA[<h2 id="題目"><a href="#題目" class="headerlink" title="題目"></a>題目</h2><p>輸入為一名為 DS4.txt 的檔案，其中有一筆 n*n(n&lt;=26)個數字(彼此以空格隔 開)組成的以對角線對稱的方陣，代表一無向圖(undirected graph)。每個數字 Nij 代表從第 i (字母)走到第 j (字母)的路徑權重(0 代表沒有對應路徑)，而 其從左到右、從上到下依序代表英文的 A、B、C、D、E…。 </p><p>此圖為一有向圖(directed graph)，每個數字 Nij 代表從第 i (字母)走到第 j (字母)的路徑權重(0 代表沒有對應路徑)，最後會 輸入起點與終點(以空行隔開)，輸入檔名 DS4.txt。 </p><h2 id="輸出說明"><a href="#輸出說明" class="headerlink" title="輸出說明"></a>輸出說明</h2><p>輸出起點到終點之 Shortest path(不只一個)，第一行輸出找到的個數，第二 行輸出所需的 cost 最後輸出找到的 Shortest path，每筆中間隔一換行，若 沒找到 Shortest path 則輸出「沒有找到」。(輸出檔名：Ans4.txt)<br><a id="more"></a></p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>0 1 2 0 0 0 0 3 4 0 0 0 0 0 0 0 A D </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>1 4 A-B-D </p><hr><p>想法: <a href="http://www.csie.ntnu.edu.tw/~u91029/Path.html#4" target="_blank" rel="noopener">Dijkstra</a>的應用 </p><ul><li>參考網址:<a href="http://bbs.csdn.net/topics/390446273" target="_blank" rel="noopener">http://bbs.csdn.net/topics/390446273</a>   </li></ul><p>要注意的地方:   </p><ol><li>紀錄路徑的Disjoint Set要用二微陣列來儲存(多個父親的情況) </li><li>在更新路徑的判斷式中(d[a]+w[a][b] &lt; d[b])，修改成(d[a]+w[a][b] &lt;= d[b]) 等於時將此點也放入DisjointSet中，如果小於時則清空此點所有Parent再重新放入新的Parent </li><li>輸出路徑時的寫法   </li></ol><p>兩種實作方式: </p><ol><li>(implement by Vector Array) </li></ol><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span>** <span class="built_in">map</span>;</span><br><span class="line"><span class="keyword">int</span> node_count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> start_point = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> end_point = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> path_num = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> min_path_weight = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">bool</span> PathExist = <span class="literal">true</span>;</span><br><span class="line"><span class="built_in">vector</span> parent[<span class="number">100</span>];</span><br><span class="line"><span class="keyword">int</span>* d;</span><br><span class="line"><span class="keyword">bool</span>* visit;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">build_map</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//輸入地圖</span></span><br><span class="line">    <span class="built_in">queue</span> temp;</span><br><span class="line">    <span class="keyword">int</span> input = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;input) == <span class="number">1</span>)<span class="comment">//use queue to store the data</span></span><br><span class="line">    &#123;</span><br><span class="line">        temp.push(input);</span><br><span class="line">        node_count++;</span><br><span class="line">    &#125;</span><br><span class="line">    node_count = (<span class="keyword">int</span>)<span class="built_in">sqrt</span>((<span class="keyword">float</span>)node_count);<span class="comment">//get the count</span></span><br><span class="line"> </span><br><span class="line">    <span class="built_in">map</span> = <span class="keyword">new</span> <span class="keyword">int</span>* [node_count];<span class="comment">//build the map</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; node_count ; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">map</span>[i] = <span class="keyword">new</span> <span class="keyword">int</span> [node_count];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span> ; j &lt; node_count ; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(temp.front() == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">map</span>[i][j] = <span class="number">0x3fffffff</span>;<span class="comment">//沒有連通設無限大</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">map</span>[i][j] = temp.front();</span><br><span class="line">            &#125;</span><br><span class="line">            temp.pop();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//輸入起點終點</span></span><br><span class="line">    <span class="keyword">char</span> start_char;</span><br><span class="line">    <span class="keyword">char</span> end_char;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%c"</span>,&amp;start_char);</span><br><span class="line">    start_point = start_char - <span class="string">'A'</span>; <span class="comment">//change char to integer</span></span><br><span class="line">    getchar();<span class="comment">//ignore '\n' char</span></span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%c"</span>,&amp;end_char);</span><br><span class="line">    end_point = end_char - <span class="string">'A'</span>; <span class="comment">//change char to integer</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">//for test</span></span><br><span class="line">    <span class="comment">//printf("%d %d\n",start_point,end_point);</span></span><br><span class="line">    <span class="comment">/*for(int i = 0 ; i &lt; node_count ; ++i)</span></span><br><span class="line"><span class="comment">    &#123;</span></span><br><span class="line"><span class="comment">        for(int j = 0 ; j &lt; node_count ; ++j)</span></span><br><span class="line"><span class="comment">        &#123;</span></span><br><span class="line"><span class="comment">            printf("%d ",map[i][j]);</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        printf("\n");</span></span><br><span class="line"><span class="comment">    &#125;*/</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Dijkstra</span><span class="params">(<span class="keyword">int</span> s)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    d = <span class="keyword">new</span> <span class="keyword">int</span>[node_count];</span><br><span class="line">    visit = <span class="keyword">new</span> <span class="keyword">bool</span> [node_count];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; node_count ; ++i) <span class="comment">//初始化</span></span><br><span class="line">    &#123;</span><br><span class="line">        d[i] = <span class="number">1e9</span>;</span><br><span class="line">        parent[i].push_back(i);</span><br><span class="line">        visit[i] = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    d[s] = <span class="number">0</span>;</span><br><span class="line">    parent[s][<span class="number">0</span>] = s;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; node_count ; ++k)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> a = <span class="number">-1</span>, b = <span class="number">-1</span> , <span class="built_in">min</span> = <span class="number">1e9</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; node_count ; ++i)<span class="comment">//沒有走訪過的點中尋找一個最小值，第一次必定從起點開始</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(!visit[i] &amp;&amp; d[i] &lt; <span class="built_in">min</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                a = i;</span><br><span class="line">                <span class="built_in">min</span> = d[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(a == <span class="number">-1</span>)<span class="keyword">break</span>;<span class="comment">//起點連結的所有點都找完</span></span><br><span class="line"> </span><br><span class="line">        visit[a] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> b = <span class="number">0</span> ; b &lt; node_count ; ++b)<span class="comment">//更新a點到所有沒走訪過點的路徑</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(!visit[b] &amp;&amp; d[a] + <span class="built_in">map</span>[a][b] &lt;= d[b])</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(d[a] + <span class="built_in">map</span>[a][b] &lt; d[b])</span><br><span class="line">                &#123;</span><br><span class="line">                    parent[b].<span class="built_in">clear</span>();</span><br><span class="line">                    d[b] = d[a] + <span class="built_in">map</span>[a][b];</span><br><span class="line">                &#125;</span><br><span class="line">                parent[b].push_back(a);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">CheckIfPathExist</span><span class="params">(<span class="keyword">int</span> e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//printf("%d\n",d[e]);</span></span><br><span class="line">    <span class="keyword">if</span>(d[e] == <span class="number">1e9</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"沒有找到\n"</span>);</span><br><span class="line">        PathExist = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">CalcPathWeight</span><span class="params">(<span class="keyword">int</span> s, <span class="keyword">int</span> e)</span> </span>&#123;<span class="comment">//隨便找一條到的了的路徑計算</span></span><br><span class="line">    <span class="keyword">if</span> (e == s) &#123;</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    min_path_weight += <span class="built_in">map</span>[ (parent[e][<span class="number">0</span>]) ][e];</span><br><span class="line">    CalcPathWeight( s, parent[e][<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">CalcPathNum</span><span class="params">(<span class="keyword">int</span> s, <span class="keyword">int</span> e)</span> </span>&#123;<span class="comment">//走訪所有路徑計算</span></span><br><span class="line">    <span class="keyword">if</span> (e == s) &#123;</span><br><span class="line">        path_num++;</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; parent[e].<span class="built_in">size</span>(); ++i ) &#123;</span><br><span class="line">        CalcPathNum( s, parent[e][i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">searchPath</span><span class="params">(<span class="keyword">int</span> s, <span class="keyword">int</span> e, <span class="keyword">int</span> sta[], <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    *查找从源点v到终点u的路径，并输出</span></span><br><span class="line"><span class="comment">    *原理:一般的DisjointSet查找做變化，當某個點有兩個以上的Parent時，第二個Parent開始要將前一個的路徑補完整並且換行。</span></span><br><span class="line"><span class="comment">    *sta陣列用來記錄前面的路徑資訊，用來補足路徑資訊時會用到，len代表最小路徑樹的level值</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">if</span> (e == s) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%c"</span>,s+<span class="string">'A'</span>);</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    sta[len] = e;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i  <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = len - <span class="number">1</span>  ; j &gt;= <span class="number">0</span> ; --j) &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"-&gt;%c"</span>,sta[j]+<span class="string">'A'</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; (<span class="string">"%c"</span>,e+<span class="string">'A'</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    freopen(<span class="string">"DS4.txt"</span>,<span class="string">"r"</span>,<span class="built_in">stdin</span>);</span><br><span class="line">    freopen(<span class="string">"Ans4.txt"</span>,<span class="string">"w"</span>,<span class="built_in">stdout</span>);</span><br><span class="line">    build_map();</span><br><span class="line">    Dijkstra(start_point);</span><br><span class="line">    CheckIfPathExist(end_point);</span><br><span class="line">    <span class="keyword">if</span>(PathExist)</span><br><span class="line">    &#123;</span><br><span class="line">        CalcPathNum( start_point, end_point);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,path_num);</span><br><span class="line">        CalcPathWeight( start_point, end_point);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,min_path_weight);</span><br><span class="line">        <span class="keyword">int</span>* sta = <span class="keyword">new</span> <span class="keyword">int</span> [node_count];<span class="comment">//紀錄路徑資訊</span></span><br><span class="line">        searchPath( start_point, end_point, sta, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>(implement by 2-dimensional Array)</li></ol><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">int</span> **matrix;</span><br><span class="line"><span class="keyword">int</span> *min_d;<span class="comment">//紀錄起點到各點的最短路徑</span></span><br><span class="line"><span class="keyword">int</span> **parent;<span class="comment">//記錄在最短路徑時的父點是誰</span></span><br><span class="line"><span class="keyword">int</span> *visit;</span><br><span class="line"><span class="keyword">int</span> node = <span class="number">0</span>;<span class="comment">//node數量</span></span><br><span class="line"><span class="keyword">int</span> pathnum = <span class="number">0</span>;<span class="comment">//最小路徑總共有幾條</span></span><br><span class="line"><span class="keyword">int</span> minpathvalue = <span class="number">0</span>;<span class="comment">//最小路徑的權值</span></span><br><span class="line"><span class="keyword">int</span> start_point = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> end_point = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> pathexist = <span class="number">1</span>; <span class="comment">//判斷路徑是否存在</span></span><br><span class="line"><span class="keyword">int</span> *length;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">build_matrix</span><span class="params">()</span><span class="comment">//算出DS4的資料個數並把值放進Queue中</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> temp;<span class="comment">//資料個數</span></span><br><span class="line">    <span class="keyword">int</span> num=<span class="number">0</span>;<span class="comment">//資料值</span></span><br><span class="line">    <span class="built_in">queue</span> Q;</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;temp)!=<span class="number">1</span>)<span class="keyword">break</span>;</span><br><span class="line">        num++;</span><br><span class="line">        Q.push(temp);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//把起點跟終點讀進去</span></span><br><span class="line">    <span class="keyword">char</span> start,<span class="built_in">end</span>;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%c"</span>,&amp;start);</span><br><span class="line">    start_point = start<span class="number">-65</span>;</span><br><span class="line">    getchar();<span class="comment">//把換行給消掉</span></span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%c"</span>,&amp;<span class="built_in">end</span>);</span><br><span class="line">    end_point = <span class="built_in">end</span><span class="number">-65</span>;</span><br><span class="line"> </span><br><span class="line">    node = <span class="keyword">int</span>(<span class="built_in">sqrt</span>(<span class="keyword">float</span>(num)));</span><br><span class="line">    matrix = <span class="keyword">new</span> <span class="keyword">int</span> *[node];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;node;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        matrix[i] = <span class="keyword">new</span> <span class="keyword">int</span> [node];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;node;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            matrix[i][j] = Q.front(); </span><br><span class="line">            Q.pop();</span><br><span class="line">            <span class="keyword">if</span>(matrix[i][j] == <span class="number">0</span>) matrix[i][j] = <span class="number">0x3fffffff</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*for(int i=0;i&lt;node;i++)</span></span><br><span class="line"><span class="comment">    &#123;</span></span><br><span class="line"><span class="comment">        for(int j=0;j&lt;node;j++)</span></span><br><span class="line"><span class="comment">        &#123;</span></span><br><span class="line"><span class="comment">            cout&lt;&lt;matrix[i][j];</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        cout&lt;&lt;endl;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    cout&lt;&lt;start&lt;&lt;end;*/</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dijkstra</span><span class="params">(<span class="keyword">int</span> startpoint)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    min_d = <span class="keyword">new</span> <span class="keyword">int</span> [node];</span><br><span class="line">    parent = <span class="keyword">new</span> <span class="keyword">int</span> *[node];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;node;i++)&#123;</span><br><span class="line">        parent[i]= <span class="keyword">new</span> <span class="keyword">int</span> [node];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;node;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                parent[i][j]=<span class="number">-1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    visit = <span class="keyword">new</span> <span class="keyword">int</span> [node];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;node; i++) visit[i] = <span class="number">0</span>;   <span class="comment">// 初始化，每個點都還沒拜訪過</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;node; i++) min_d[i] = <span class="number">1e9</span>;</span><br><span class="line">  </span><br><span class="line">    min_d[start_point] = <span class="number">0</span>; <span class="comment">//最小距離初始化(起點)</span></span><br><span class="line">     </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;node;i++)</span><br><span class="line">    &#123;</span><br><span class="line">                parent[i][<span class="number">0</span>]= i; <span class="comment">//一開始父點的值都會是自己</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//parent[start-65] = start-65;//父點初始化(起點)</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;node; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> a = <span class="number">-1</span>, b = <span class="number">-1</span>, <span class="built_in">min</span> = <span class="number">1e9</span>; <span class="comment">//a跟b代表從某點到某點 一開始都沒跑所以設-1 距離設無限大</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;node; i++)</span><br><span class="line">            <span class="keyword">if</span> ( visit[i]!=<span class="number">1</span> &amp;&amp; min_d[i] &lt; <span class="built_in">min</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                a = i;  <span class="comment">// 記錄這一條邊</span></span><br><span class="line">                <span class="built_in">min</span> = min_d[i];</span><br><span class="line">            &#125;</span><br><span class="line">  </span><br><span class="line">        <span class="keyword">if</span> (a == <span class="number">-1</span>) <span class="keyword">break</span>;     <span class="comment">// 起點有連通的最短路徑都已找完</span></span><br><span class="line">        visit[a] = <span class="number">1</span>;</span><br><span class="line">  </span><br><span class="line">        <span class="comment">// 以新找到的最短路徑做更新</span></span><br><span class="line">        <span class="keyword">for</span> (b=<span class="number">0</span>; b&lt;node; b++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (!visit[b] &amp;&amp; min_d[a] + matrix[a][b] &lt;= min_d[b])</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(min_d[a]+matrix[a][b] &lt; min_d[b])</span><br><span class="line">                &#123;</span><br><span class="line">                    parent[b][<span class="number">0</span>] = a;</span><br><span class="line">                    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;node<span class="number">-1</span>;i++)</span><br><span class="line">                    &#123;</span><br><span class="line">                        parent[b][i]=<span class="number">-1</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;node;j++)</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="keyword">if</span>(parent[b][j]==<span class="number">-1</span>)parent[b][j]=a;<span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                min_d[b] = min_d[a]+matrix[a][b] ;</span><br><span class="line">            &#125;</span><br><span class="line">         &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">If_exist</span><span class="params">(<span class="keyword">int</span> endpoint)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">/*這個副程式用來判斷使否有路徑，若沒有直接結束程式</span></span><br><span class="line"><span class="comment">    若有責開始判斷題目要求*/</span></span><br><span class="line">    <span class="keyword">if</span>(min_d[endpoint] == <span class="number">1e9</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">"沒有找到"</span>&lt;= <span class="number">0</span> ; row_num--)</span><br><span class="line">        <span class="comment">//&#123;</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;length[row];i++)</span><br><span class="line">            &#123;</span><br><span class="line">                calculate_path(startpoint,parent[endpoint][i],parent[endpoint][i]);</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="comment">//&#125;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">other_path</span><span class="params">(<span class="keyword">int</span> startpoint,<span class="keyword">int</span> endpoint)</span> <span class="comment">//計算其他路徑的</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(endpoint == startpoint)</span><br><span class="line">    &#123;<span class="keyword">return</span>;&#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    minpathvalue = minpathvalue+matrix[(parent[endpoint][<span class="number">0</span>])][endpoint];</span><br><span class="line">    other_path(startpoint,parent[endpoint][<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">printPath</span><span class="params">(<span class="keyword">char</span> startpoint, <span class="keyword">char</span> endpoint, <span class="keyword">int</span> before[], <span class="keyword">int</span> len,<span class="keyword">int</span> row)</span> <span class="comment">//印出路徑</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    *查找?源?v到??u的路?，并?出</span></span><br><span class="line"><span class="comment">    *原理:一般的DisjointSet查找做變化，當某個點有兩個以上的Parent時，第二個Parent開始要將前一個的路徑補完整並且換行。</span></span><br><span class="line"><span class="comment">    *before陣列用來記錄前面的路徑資訊，用來補足路徑資訊時會用到，len代表最小路徑樹的level值</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">if</span> (endpoint == startpoint) &#123;</span><br><span class="line">       <span class="comment">//cout&lt;&lt;startpoint+65; </span></span><br><span class="line">        <span class="comment">/*本來用cout無法輸出英文字母 需要把他轉成char型態的輸出</span></span><br><span class="line"><span class="comment">        但是不知道怎麼轉 就想說值街用回printf %C這樣最方便*/</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%c"</span>,startpoint+<span class="string">'A'</span>);</span><br><span class="line">        <span class="keyword">return</span> ;</span><br><span class="line">    &#125;</span><br><span class="line">    before[len] = endpoint;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i  <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = len - <span class="number">1</span>  ; j &gt;= <span class="number">0</span> ; --j) &#123;</span><br><span class="line">                <span class="comment">//cout&lt;"&lt;%c",before[j]+'A');</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        printPath( startpoint, parent[endpoint][i], before, len + <span class="number">1</span>, parent[endpoint][i]);</span><br><span class="line">        <span class="comment">//cout&lt;"&lt;%c",endpoint+'A');</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    freopen(<span class="string">"DS4.txt"</span>,<span class="string">"r"</span>,<span class="built_in">stdin</span>);</span><br><span class="line">    freopen(<span class="string">"Ans4.txt"</span>,<span class="string">"w"</span>,<span class="built_in">stdout</span>);</span><br><span class="line">    build_matrix();</span><br><span class="line">    dijkstra(start_point);</span><br><span class="line">    <span class="keyword">if</span>(If_exist)</span><br><span class="line">    &#123;</span><br><span class="line">        length = <span class="keyword">new</span> <span class="keyword">int</span> [node];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;node;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            length[i] = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j&lt;node;j++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(parent[i][j]==<span class="number">-1</span>)<span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    length[i] = length[i]+<span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        calculate_path(start_point,end_point,end_point);</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;pathnum&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        other_path(start_point,end_point);</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;minpathvalue&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">int</span> *before = <span class="keyword">new</span> <span class="keyword">int</span> [node];<span class="comment">//紀錄路徑資訊</span></span><br><span class="line">        printPath(start_point,end_point,before,<span class="number">0</span>,end_point);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2016波克城市數位遊戲設計-參賽心得</title>
      <link href="/posts/231cc914/"/>
      <url>/posts/231cc914/</url>
      
        <content type="html"><![CDATA[<p>寫這篇文章其實已經比賽結束好一陣子了，因為我一直在等照片，後來等不到怕忘記就先寫了，順便把手上的照片放上來。 </p><p>大學以來參加的第一個較大規模專題競賽竟然不是自己系上的。會有這個比賽是因為高大資管與波克城市的合作計畫，由波克城市提供豐厚的獎品(前三名總共:4台Mac+4台筆電+4台PS4)以及講師來分享經驗，每組花一個學期製作一個單機休閒App，雖然最後都沒得到獎，但對我來說仍然是一個很寶貴的經驗，於是想將這些記錄下來。 </p><div align="center">  <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/13344805_1361583450524569_7605267586142400751_n.jpg" width="30%" /></div><p>(我們美術設計的帥氣海報)<br><a id="more"></a></p><div align="center">  <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/13435471_1361583433857904_2662107738779589322_n.jpg" width="70%" /></div>(每組遊戲的海報) <div align="center">  <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/13407273_1361583403857907_3457890409076938274_n.jpg" width="70%" /></div><p>(豐盛的獎品…我也好想要一台) </p><p>雖然沒有得獎，還是不免來介紹一下我們的遊戲吧xD最後再來說心得:</p><hr><h2 id="遊戲名稱-SaveMore"><a href="#遊戲名稱-SaveMore" class="headerlink" title="遊戲名稱: SaveMore"></a>遊戲名稱: SaveMore</h2><ul><li>遊戲類型: 3D類型的類似RPG遊戲 </li><li>開發工具: Unity - C# </li><li>github: <a href="https://github.com/john850512/SaveMore" target="_blank" rel="noopener">https://github.com/john850512/SaveMore</a> </li><li>Demo: <a href="https://www.youtube.com/watch?v=ob6LvOYn8Yc&amp;feature=youtu.be" target="_blank" rel="noopener">https://www.youtube.com/watch?v=ob6LvOYn8Yc&amp;feature=youtu.be</a> </li></ul><h2 id="背景故事"><a href="#背景故事" class="headerlink" title="背景故事"></a>背景故事</h2><p>在未來的世界，病毒因為某次的突變產生出人類身體無法抵抗的品種，只有病毒自己本身才能摧毀自己。於是未來的科學家發明了能自由操控的進化血球，進化血球擁有特殊的能力可以在病毒入侵的部位創造一個特殊空間，在這空間中進化血球可以將病毒吞噬並吐出撞擊其他的病毒，使之消滅而不會影響人物身體的原本機能。  </p><h2 id="遊戲架構"><a href="#遊戲架構" class="headerlink" title="遊戲架構"></a>遊戲架構</h2><p>一開始我們以人物的身體作為背景，凸顯出我們的防衛主題，在遊戲開始時身體的某些部位會逐漸出現病狀(紅色警訊)，玩家可以點擊進入遊戲關卡。 </p><p>進入遊戲關卡後，玩家需要操控虛擬搖桿來控制進化血球的移動及攻擊，發射武器將病毒擊暈並吞噬，之後吐出病毒已消滅其他病毒，消滅全部病毒即可成功防守，當前關卡也會消失。 人物年齡以及免疫力會隨著關卡的破關次數逐漸增加，反之若玩家進行關卡失敗則免疫力降低，當免疫力過低時玩家則會死亡，整個遊戲到此結束。 為了增加遊戲的多元性，不同部位的關卡同時發病時，會引發特殊關卡出現，特殊關卡有著更強大的魔王，魔王有更多元的攻擊模式、移動模式以及生命力等待玩家挑戰。</p><h2 id="遊戲畫面"><a href="#遊戲畫面" class="headerlink" title="遊戲畫面"></a>遊戲畫面</h2><h3 id="進入畫面"><a href="#進入畫面" class="headerlink" title="進入畫面"></a>進入畫面</h3><div align="center">  <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/11.png" width="50%" /></div><h3 id="角色選擇"><a href="#角色選擇" class="headerlink" title="角色選擇"></a>角色選擇</h3><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/13.png?w=1400&h=&crop=1"  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/14.png?w=1400&h=&crop=1"   /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/15.png?w=1400&h=&crop=1"   /></td>  </tr></table><h3 id="成就系統"><a href="#成就系統" class="headerlink" title="成就系統"></a>成就系統</h3><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/19.png?w=1400&h="  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/18.png?w=1400&h="   /></td>  </tr>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/17.png?w=1400&h="  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/16.png?w=1400&h="   /></td>  </tr></table><h3 id="遊玩畫面"><a href="#遊玩畫面" class="headerlink" title="遊玩畫面"></a>遊玩畫面</h3><table align="center" border=0>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/23.png?w=1400&h="  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/25.png?w=1400&h="   /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/20.png?w=1400&h="   /></td>  </tr>  <tr>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/24.png?w=1400&h="  /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/29.png?w=1400&h="   /></td>      <td align="center"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/06/28.png?w=1400&h="   /></td>  </tr></table><hr><p>這一個學期6個月的製作過程中，從一開始什麼都不會開始摸Unity、C#，到後來有一定的了解，學習的過程真的蠻艱辛的，還要兼顧其他的課業……。 </p><p>客觀看一下這次前三名的作品，都會發現一個共通性: <strong>遊戲完成度很高</strong>，不論是美術的角色、遊戲設計、UI風格都跟遊戲很完整的搭配，在遊戲可玩度上也是很完整，幾乎可以上PlayStore也不奇怪的作品。 </p><p>反省一下我們的作品，其實我覺得在構想上並不差，但是遊戲風格沒有一致姓，不論是遊戲風格、UI都沒有很完整的搭配起來(風格上其實差異蠻大的)。</p><p>而在程式部分也頗差，沒有使遊戲能很順暢地遊玩(當時評審幾乎沒有一個順利破一關過，只有我們已經很了解的人才有辦法破關)，而且設計上還有不少大BUG(儘管當下沒使他們發現)……。 </p><p>而且時間分配上也很糟糕，明明我覺得我們是很早開工的，但是實際上到最後一天的清晨才把大部分的雛形給做完，在中間拖了很多時間，常常開會時上週的東西到這週還是沒有交出來，最後就是在爆程式的肝(我在報告前一天完全沒有睡…..)，以後專題一定要更加注意時間。 </p><p>接下來細說一下我的部分吧xD我是負責程式的部分，這次做這件作品真的學到很多，首先是深刻的體驗到了效能的重要性，遊戲是很重視使用者體驗的，一點點的Lag都會讓使用者不快，尤其我們的遊戲又有怪物AI，要怎麼讓多個AI在地圖上又不至於會卡住就很重要了(後來用了亂數分散怪物AI的追蹤時間)。再來還有很多同步問題，在存取同一個物件資源的時候，如何保護他使得其他程式無法同時控制此資源(當時為了魔法鎮的放大縮小搞了好久阿)…..等，很多東西都是邊學邊做，不斷的Google、查API文檔，看論壇、請教學長才得以完成。 </p><p>上大學以來沒做過什麼可以拿出來玩的作品，雖然此次沒有得獎，但真的收穫良多。     </p><p>最後附上評審在評分遊戲時問的相關問題以及面試(他還有提供實習的機會)時的相關問題: </p><p>評分時: </p><ol><li>你覺得製作這款遊戲上最困難的地方是那些? <ul><li>遊戲效能的優化、怪物的AI </li></ul></li><li>怪物的AI是如何去寫的? <ul><li>用A* Algorithm </li></ul></li><li>魔王的AI是如何去撰寫的? <ul><li>先想好行動模式，分別撰寫 </li></ul></li><li>虛擬搖桿是自己寫出來的嗎? <ul><li>參考網路上的OpenSource </li></ul></li><li>這款遊戲的美術風格? <ul><li>(忘記我們美術當時說什麼了)   </li></ul></li></ol><p>面試時: </p><ol><li>你覺得在製作這款遊戲時和企劃、美術間遇到的最大困難是什麼? <ul><li>溝通問題，每人的想法都不一樣 </li></ul></li><li>你對寫程式有熱情嗎?你接受在公司待12小時以上嗎? <ul><li>廢話這裡當然要說有 </li></ul></li><li>如果你是程式，你會同時想學習企劃嗎? <ul><li>會，因為這款遊戲製作中我也有參與到企劃，還蠻有興趣的 </li></ul></li><li>如果進我們公司，你最想在這裡學到什麼? <ul><li>和不同部門間的溝通、業界的生態 </li></ul></li><li>說出一款你最喜歡玩的手機遊戲並介紹他 <ul><li>落櫻散華抄</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> competition </tag>
            
            <tag> unity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Unity]FBX模型的動畫在手機上失效了</title>
      <link href="/posts/73b67c08/"/>
      <url>/posts/73b67c08/</url>
      
        <content type="html"><![CDATA[<p>最近遇到一件很弔詭的事情，在開發遊戲的過程中一切順利，但在我Build一套測試版本在手機上執行時，他就這麼發生了:   </p><p>“我3D模型的動畫在電腦上可以正常顯示，但手機上卻不見我的動畫???” </p><p>這沒道理啊?????我電腦上就看的到阿?????   </p><p>接著我花了整整兩天去Debug，以下是辛酸血淚，結論在最後:<br><a id="more"></a></p><ol><li>我的動畫是有用Script做SetActive的功能，也就是我會讓他在某個時機消失、某個時機出現，一開始我推測是因為這樣造成無法正常顯示，於是我直接刪去所有會影響到該Object的Script <ul><li>結果: 不是Script的問題 </li></ul></li><li>在手機上我其他的動畫是可以正常顯示的(Cube做成的動畫)，於是合理懷疑是因為Fbx模型跟動畫之間有某種錯誤，開始往這方向查。</li></ol><p>最後排除了所有人為可能造成的Bug後，我開始思考有可能是Unity版本支持的問題，手機上無法支援Animator。 Animator以及Animation兩種Component的差異，轉載自<a href="http://www.ceeger.com/forum/read.php?tid=14483" target="_blank" rel="noopener">以下網站</a>，在此節錄一名網友的回答: </p><blockquote><p>animation是Unity4.0之前使用的动画系统，Animator是之后加入的新版动画系统，后者兼容前者，而前者已经不提供更新支持，所以后者是趋势。 </p><p><strong>不过animation依然可以使用，选择legacy模式就可以</strong>     </p></blockquote><p>所以，接下來就要把Fbx模型改成Legacy模式即可解決。 </p><ol><li>點選FBX模型的物件-&gt;Rig 將Animation Type改為”Legency”，Generation”Store in Root(New)” </li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/05/1.png" alt="1"> </p><ol><li>點選右上角選單-&gt;進入Debug模式</li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/05/2.png" alt="2"> </p><ol><li>點選該模型的Animation動畫-&gt;將在Debug模式下的Legency打勾</li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/05/3.png" alt="3"> </p><ol><li>如果FBX物件上是用Animator，將它移除，用舊版本的Animation即可，把動畫放進去就好了</li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/05/4.png" alt="4">   </p><p>到此，動畫就可以正常在手機上執行摟!!</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c# </tag>
            
            <tag> unity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[C/C++]Huffman Tree</title>
      <link href="/posts/e220c953/"/>
      <url>/posts/e220c953/</url>
      
        <content type="html"><![CDATA[<p>Huffman Tree，中文霍夫曼樹，常用來做資料壓縮的一種技巧，使得出現機率高的字母使用較短的編碼，反之出現機率低的則使用較長的編碼，這便使編碼之後的字串的平均長度、期望值降低，從而達到無失真壓縮資料的目的。   </p><p>相關的介紹請看<a href="https://zh.wikipedia.org/wiki/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81#.E8.B3.87.E6.96.99.E5.A3.93.E7.B8.AE" target="_blank" rel="noopener">維基百科</a>   </p><p>以下使用動態Link List實作Create Huffman Tree 以及設定樹葉節點(Leaf node)的編碼 </p><p>(網路上另一版本的寫法請參閱:<a href="http://www.sharejs.com/codes/cpp/5464" target="_blank" rel="noopener">http://www.sharejs.com/codes/cpp/5464</a>)<br><a id="more"></a><br>實作上的一些小細節: </p><ol><li>Struct是可以使用建構子的，Struct與Class的差異請參閱:<a href="http://genwoxuec.blog.51cto.com/1852764/503334" target="_blank" rel="noopener">http://genwoxuec.blog.51cto.com/1852764/503334</a> </li><li>在動態新增node時，要注意link list是以point去做操作的，而一連串的資料又必須先存起來，所以要動態新增二維的空間，可以想成一開始先新增(資料個數)列，”但是之後每一列必須再新增一個節點空間(每列的第一個)”。 </li><li>實作作法是: 每次做前先排序過。 每次都找最小的兩個權值(current_position、current_position+1)，新增一個點去儲存相加的權值，並使得這兩個最小點分別為左子、右子點，再存回陣列current_position+1的位置中。 </li></ol><p>由於已經設定好左右子點了，所以把新點存回陣列把原本的資料覆蓋掉也沒關係，下一次從current_position+1的地方繼續找兩個最小值，直到(個數-1)為止。 </p><ol><li>編碼的創立則是利用樹的走訪，先走完左邊再走右邊，同時更新子節點的編碼內容，走到樹葉則輸出結果。   </li></ol><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> <span class="meta-keyword">warning</span>(disable:4996)</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">node</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    node()</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;weight = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;msg = <span class="string">'?'</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;encode = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;left = <span class="keyword">this</span>-&gt;right = <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    node(<span class="keyword">int</span> weight,node* left,node* right)<span class="comment">//insert new node</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;weight = weight;</span><br><span class="line">        <span class="keyword">this</span>-&gt;msg = <span class="string">'?'</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;encode = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;left = left;</span><br><span class="line">        <span class="keyword">this</span>-&gt;right = right;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">char</span> msg;</span><br><span class="line">    <span class="built_in">string</span> encode;</span><br><span class="line">    <span class="keyword">int</span> weight;</span><br><span class="line">    node *left;</span><br><span class="line">    node *right;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(node* n1, node* n2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> n1-&gt;weight weight;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Create_HuffmanTree</span><span class="params">(<span class="keyword">int</span> <span class="built_in">size</span>,node** data,node*&amp; root)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> current_position = <span class="number">0</span>;</span><br><span class="line">    node* min1;</span><br><span class="line">    node* min2;</span><br><span class="line">    <span class="keyword">while</span> (current_position != <span class="built_in">size</span> <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        min1 = data[current_position];</span><br><span class="line">        min2 = data[current_position+<span class="number">1</span>];</span><br><span class="line">        node* newnode = <span class="keyword">new</span> node(min1-&gt;weight+min2-&gt;weight,min1,min2);</span><br><span class="line">        root = newnode;</span><br><span class="line">        current_position++;</span><br><span class="line">        data[current_position] = newnode;</span><br><span class="line">        sort(data+current_position,data+<span class="built_in">size</span>,cmp);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Huffman_Encoding</span><span class="params">(node* ptr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(ptr-&gt;left == <span class="literal">NULL</span> &amp;&amp; ptr-&gt;right == <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%c %d %s\n"</span>,ptr-&gt;msg,ptr-&gt;weight,ptr-&gt;encode.c_str());</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(ptr-&gt;left != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        ptr-&gt;left-&gt;encode = ptr-&gt;encode +<span class="string">"0"</span>;</span><br><span class="line">        Huffman_Encoding(ptr-&gt;left);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(ptr-&gt;right != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        ptr-&gt;right-&gt;encode = ptr-&gt;encode + <span class="string">"1"</span>;</span><br><span class="line">        Huffman_Encoding(ptr-&gt;right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Inorder_traversal</span><span class="params">(node* ptr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(ptr != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        Inorder_traversal(ptr-&gt;left);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%c %d\n"</span>,ptr-&gt;msg,ptr-&gt;weight);</span><br><span class="line">        Inorder_traversal(ptr-&gt;right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    freopen(<span class="string">"in.txt"</span>,<span class="string">"r"</span>,<span class="built_in">stdin</span>);</span><br><span class="line">    <span class="keyword">int</span> <span class="built_in">size</span> = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">char</span> msg;</span><br><span class="line">    <span class="keyword">int</span> frequency;</span><br><span class="line">    node* root = <span class="keyword">new</span> node;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;<span class="built_in">size</span>);</span><br><span class="line">    node** data = <span class="keyword">new</span> node* [<span class="built_in">size</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i msg,&amp;data[i]-&gt;weight);</span><br><span class="line">    &#125;</span><br><span class="line">    sort(data+<span class="number">0</span>,data+<span class="built_in">size</span>,cmp);</span><br><span class="line">    <span class="comment">/*for(int i = 0 ; i msg,data[i]-&gt;weight);</span></span><br><span class="line"><span class="comment">    &#125;*/</span></span><br><span class="line">    Create_HuffmanTree(<span class="built_in">size</span>,data,root);</span><br><span class="line">    Inorder_traversal(root);</span><br><span class="line">    Huffman_Encoding(root);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Unity]How to Instantiate A Image And SetParent</title>
      <link href="/posts/a323f86e/"/>
      <url>/posts/a323f86e/</url>
      
        <content type="html"><![CDATA[<p>標題是這樣打啦，因為我不知道中文要怎麼翻比較恰當，主題大概是: 我要如何用Script使得在遊戲中可以產生一個Image，並設定他的Parent。   </p><p>這是我昨天Google到半夜3點的結論，總結一下: </p><ol><li>要產生一個UI物件時，要先將該物件作為Prefeb(預置物) </li><li>Image在某個版本(確切幾版後不清楚)後需要再Canvas下才會顯示出來。 </li><li>所以在製作時，必須要先產生該圖片，並設定他的父親(OOP繼承的概念)，但是這裡有個問題: <ul><li>Unity的Prefeb是無法SetParent的(原因大概是因為這樣就會改變預製物原先的狀態)   <a id="more"></a></li></ul></li></ol><p>好，接下來一點一點來解決 </p><ol><li>首先，先把要生成的Image做成Prefab，然後我在Hierarchy先放了一個Canvas作為等下Image的父親 </li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/05/e59c96e789871.png" alt="圖片1.png"> </p><p>接下來，就是Instantiate  Image，這裡要注意的是，Image是Unity的UI物件，所以不是用GameObject。 </p><p>程式碼最前面要先加上<br><figure class="highlight cs"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> UnityEngine.UI;</span><br><span class="line"></span><br><span class="line">Image NewArraw = (Image)Instantiate(arraw,GameObject.Find(<span class="string">"Player"</span>).transform.position,</span><br><span class="line"> </span><br><span class="line">GameObject.Find(<span class="string">"Player"</span>).transform.rotation) <span class="keyword">as</span> Image;</span><br><span class="line"> </span><br><span class="line"><span class="comment">//產生一個arraw的Image在Player的位置</span></span><br></pre></td></tr></table></figure></p><ol><li>再來設定他的父親，這裡會用到兩個API:SetParent、Find。 </li></ol><figure class="highlight cs"><table><tr><td class="code"><pre><span class="line">NewArraw.transform.SetParent(GameObject.Find(<span class="string">"ArrawCanvas"</span>).transform, <span class="literal">false</span>);</span><br></pre></td></tr></table></figure><p>這樣就成功摟！！</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/05/e59c96e789872.png" alt=""></p><ol><li>最後來講一下我學到的經驗: 之所以會卡那麼久主要是因為我在Instantiate的時候我一開始用了兩種方法</li></ol><figure class="highlight cs"><table><tr><td class="code"><pre><span class="line">GameObject.Find();</span><br><span class="line"></span><br><span class="line">Resources.Load();</span><br></pre></td></tr></table></figure><p>這兩種都可以回傳一個GameObject的參考，所以可以當成Instantiate的參數使用。 兩者的差別是在於GameObject.Find()必須要在目前Hierarchy下有的物件才找的到，Resources.Load()需要創一個Resources的資料夾，並把Prefeb放進去。   </p><p>由於我不想要讓物件一開始就存在遊戲中，我使用了後者的方法，但是這種方法就會造成無法SetParent，理由同我上述所說的，完畢。</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c# </tag>
            
            <tag> unity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[C/C++]約瑟夫問題變形-求最小m值</title>
      <link href="/posts/62430f51/"/>
      <url>/posts/62430f51/</url>
      
        <content type="html"><![CDATA[<p>題目說明:</p><p>(參考網址: <a href="http://openhome.cc/Gossip/AlgorithmGossip/JosephusProblem.htm" target="_blank" rel="noopener">http://openhome.cc/Gossip/AlgorithmGossip/JosephusProblem.htm</a>)</p><blockquote><p>據說著名猶太歷史/數學家約瑟夫（Josephus）有過以下的故事：在羅馬人佔領喬塔帕特後，40個猶太士兵與約瑟夫躲到一個洞中，眼見脫逃無望，一群人決定集體自殺，然而私下約瑟夫與某個傢伙並不贊成，於是約瑟夫建議自殺方式，41個人排成圓圈，由第1個人 開始報數，每報數到3的人就必須自殺，然後由下一個重新報數，直到所有人都自殺身亡為止。約瑟夫與不想自殺的那個人分別排在第16個與第31個位置，於是逃過了這場死亡遊戲。</p></blockquote><p>一般來說的問題是求出最後一個活著的編號，在文章內有提到這有一個遞迴定義式:</p><script type="math/tex; mode=display">g(1, k) = 0 g(n, k) = (g(n - 1, k) + k) % n -</script><p>接下來要說一個相關的變形題:<br><a id="more"></a></p><p>給予k個好人與壞人(0&lt;k&lt;13)排成圓圈，編號一開始先排k個好人，再來是k個壞人，求出最小的m使得在壞人都死光前沒有好人被殺死。</p><p>如果按造題意，用最直白的寫法會是:</p><ol><li><p>建好circular link list</p></li><li><p>m = 1開始算起，從編號1依序刪除第m個點，如果刪掉壞人則繼續，直到所有壞人被刪光為止</p></li><li><p>刪到好人則m = m+1重新計算</p></li></ol><p>以下是一些提升效率的優化</p><ol><li><p>m必為k+1開始(好人一定不會被刪到)</p></li><li><p>設置bool變數去儲存是否被刪除，取代link list的delete node</p></li><li><p>用餘數運算子去計算要走的步數:例如要走7步，有三個點，7%3 = 1，實際上只須走一步即可。</p></li></ol><p>加上第三點後效率會大大提升，我估計時間複雜度會從原本的O(n*m)變成O(k*m)(不知是否有錯…)</p><p>實測沒加第三點時跑k = 13要花20分鐘以上，優化後k = 13花不到1秒多。</p><p>以下是優化後的程式碼:<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">document.<span class="built_in">write</span>('</span><br><span class="line">DATA HOSTED WITH ♥ BY PASTEBIN.COM - DOWNLOAD RAW - SEE ORIGINAL</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;cstdlib&gt;</span><br><span class="line">#include &lt;cstdio&gt;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line">class people</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    people()</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;is_badman = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;is_delect = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;num = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">this</span>-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    people(<span class="keyword">int</span> k)</span><br><span class="line">    &#123;</span><br><span class="line">        head = <span class="keyword">new</span> people();<span class="comment">//用個開頭節點設置link list</span></span><br><span class="line">        ptr = head;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span> ; i &lt;= k ; ++i)<span class="comment">//設置好人</span></span><br><span class="line">        &#123;</span><br><span class="line">            people *n = <span class="keyword">new</span> people();</span><br><span class="line">            n-&gt;num = i;</span><br><span class="line">            n-&gt;is_badman = <span class="literal">false</span>;</span><br><span class="line">            ptr-&gt;next = n;</span><br><span class="line">            ptr = ptr-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = k+<span class="number">1</span> ; i &lt;= <span class="number">2</span>*k ; ++i)<span class="comment">//設置壞人</span></span><br><span class="line">        &#123;</span><br><span class="line">            people *n = <span class="keyword">new</span> people();</span><br><span class="line">            n-&gt;num = i;</span><br><span class="line">            n-&gt;is_badman = <span class="literal">true</span>;</span><br><span class="line">            ptr-&gt;next = n;</span><br><span class="line">            ptr = ptr-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        ptr-&gt;next = head-&gt;next;<span class="comment">//把link list串成環狀，head重新指向有值得第一個</span></span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> solve(<span class="keyword">int</span> k)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> n = k+<span class="number">1</span>;<span class="comment">//答案必是最小k+1開始找</span></span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">run</span> = n;<span class="comment">//走的步數</span></span><br><span class="line">        <span class="keyword">int</span> alive = <span class="number">2</span> * k;<span class="comment">//目前的node數</span></span><br><span class="line">        <span class="keyword">int</span> temp = <span class="number">0</span>;<span class="comment">//已刪掉的壞人數</span></span><br><span class="line">        ptr = head;</span><br><span class="line">        <span class="keyword">while</span>(temp != k)<span class="comment">//如果所有壞人被刪掉就離開</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">run</span> = (n<span class="number">-1</span>) % alive;<span class="comment">//把走n-1次(不包含自己)，優化成(n-1)%alive</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; <span class="built_in">run</span> ; )</span><br><span class="line">            &#123;</span><br><span class="line">                ptr = ptr-&gt;next;</span><br><span class="line">                <span class="keyword">if</span>(ptr-&gt;is_delect)<span class="keyword">continue</span>;<span class="comment">//刪過的點就不算</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    ++i;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(!(ptr-&gt;is_badman))<span class="comment">//該點是好人則不是解，直接跳下個n</span></span><br><span class="line">            &#123;</span><br><span class="line">                n++;</span><br><span class="line">                <span class="comment">//初始化設定</span></span><br><span class="line">                alive = <span class="number">2</span> * k;</span><br><span class="line">                temp = <span class="number">0</span>;</span><br><span class="line">                ptr = head;</span><br><span class="line">                <span class="keyword">while</span>(ptr-&gt;next != head)<span class="comment">//把刪除過的點還原</span></span><br><span class="line">                &#123;</span><br><span class="line">                    ptr-&gt;is_delect = <span class="literal">false</span>;</span><br><span class="line">                    ptr = ptr-&gt;next;</span><br><span class="line">                &#125;</span><br><span class="line">                ptr-&gt;is_delect = <span class="literal">false</span>;<span class="comment">//最後一個沒被初始化到，另外寫</span></span><br><span class="line">                ptr = head;<span class="comment">//重設ptr指向開頭</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span><span class="comment">//壞人</span></span><br><span class="line">            &#123;</span><br><span class="line">                ptr-&gt;is_delect = <span class="literal">true</span>;<span class="comment">//刪掉</span></span><br><span class="line">                <span class="keyword">while</span>(ptr-&gt;next-&gt;is_delect)<span class="comment">//跳下一個沒被刪過的點</span></span><br><span class="line">                &#123;</span><br><span class="line">                    ptr = ptr-&gt;next;</span><br><span class="line">                &#125;</span><br><span class="line">                ptr = ptr-&gt;next;</span><br><span class="line"> </span><br><span class="line">                alive--;<span class="comment">//現存node數-1</span></span><br><span class="line">                temp++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//member</span></span><br><span class="line">    <span class="keyword">int</span> num;</span><br><span class="line">    <span class="keyword">bool</span> is_badman;</span><br><span class="line">    <span class="keyword">bool</span> is_delect;</span><br><span class="line">    people *next;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    people *head;</span><br><span class="line">    people *ptr;</span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">int</span> main()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; k;</span><br><span class="line">    people p(k);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; p.solve(k) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">');</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Unity]解決單次偵測方向鍵造成腳色瞬間移動的問題</title>
      <link href="/posts/8c2edcc9/"/>
      <url>/posts/8c2edcc9/</url>
      
        <content type="html"><![CDATA[<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/04/1.jpg" alt="1"></p><p>最近在做Unity腳色移動控制時遇上了一個問題: 一般讓角色移動的方式有兩類:</p><ol><li>給物體加個Force/Velocity </li><li>偵測一次方向鍵移動一次 </li></ol><p>第一種情況不會有延遲的問題，因為只需要偵測方向鍵即可改變移動方向跟移動向量 </p><p>問題在於第二種，因為Unity的Update與FixedUpdate是以每一幀為單位，但這樣會遇到一個問題， 假設我在玩遊戲時我想使玩家連續移動，於是我按了5下右鍵，這時有可能在第一幀移動了一個單位， 但剩下的4格單位卻在下一幀一瞬間移動完，造成有瞬間移動大量距離的情形。 於是我用Timer去計時，修改成: 每一秒只偵測一次按鍵，然後動作完就把所有的設定清除掉。 </p><a id="more"></a><p>ResetInputAxes: 在一帧中重置所有的输入，重置输入指令之后所有的方向轴都被设置为0并且所有的 按键都被设置为0。 </p><p>(網址來源:<a href="http://www.ceeger.com/Script/Input/Input.ResetInputAxes.html" target="_blank" rel="noopener">http://www.ceeger.com/Script/Input/Input.ResetInputAxes.html</a>) </p><p>這樣就可以解決問題了，概念有點像C/C++的fflush清空前一次輸入的Buffer。 </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/04/2.jpg" alt="2"> </p><figure class="highlight cs"><table><tr><td class="code"><pre><span class="line">time = time + Time.deltaTime;</span><br><span class="line"><span class="keyword">if</span> (time &gt; <span class="number">1f</span>)<span class="comment">//每一秒執行一次</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (Input.GetKeyDown(KeyCode.UpArrow))</span><br><span class="line">    &#123;&#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (Input.GetKeyDown(KeyCode.RightArrow))</span><br><span class="line">    &#123;&#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (Input.GetKeyDown(KeyCode.DownArrow))</span><br><span class="line">    &#123;&#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (Input.GetKeyDown(KeyCode.LeftArrow))</span><br><span class="line">    &#123;&#125;</span><br><span class="line">    Input.ResetInputAxes();<span class="comment">//重製所有輸入</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c# </tag>
            
            <tag> unity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>升到Win10後C槽容量快不夠了</title>
      <link href="/posts/6a560d71/"/>
      <url>/posts/6a560d71/</url>
      
        <content type="html"><![CDATA[<p>升級到Win10後，系統將會自動進行更新的動作，常常不知不覺磁碟機容量就爆了，尤其是再有加裝SSD的情形下，看著C槽的容量越來越少常常會感到擔憂。   </p><p>以下是幾種清理一些不必要的資料方法:   </p><a id="more"></a><ol><li>控制台-&gt;新增及移除程式，把不必要的程式移除 </li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/02/e693b7e58f96.png" alt="擷取.PNG"> </p><ol><li>C槽-&gt;內容-&gt;磁碟清理 </li></ol><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/02/e693b7e58f961.png" alt="擷取"> </p><p>跑完之後Win10的自動更新檔在這兒清除-&gt;清理系統檔 </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/02/e693b7e58f962.png" alt="擷取"> </p><ol><li>瀏覽器的下載，平時在使用瀏覽器時，多少會從網路上下載一些資源，這些資源都會在C槽的【下載】資料夾內，所以建議定期清理裡面過大的資料。 </li></ol><p>補充一下: 在C槽跟目錄下顯示隱藏的資料夾時可以看到兩個資料夾 </p><ul><li>$Windows.~WS:Win10的更新檔都會在這裡，用上述步驟即可清除 </li><li>$SysReset:升級成Win10時會保存一個月的紀錄檔，可以用來還原成原本系統(一個月後會自動刪除)     </li></ul><p>定時清除以上步驟，多少可以增加C槽的容量，最後平時記得資料都存放在其他槽，桌面也不要放太大的資料(用捷徑)。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SITCON 2016</title>
      <link href="/posts/51a6f5ce/"/>
      <url>/posts/51a6f5ce/</url>
      
        <content type="html"><![CDATA[<p>SITCON2016，學生計算機年會，係一學生自發組成的研討會，秉持著以學生為主軸的核心價值，藉以凝聚與傳遞學生的力量。希望藉由提供一個經驗交流與資訊技術實務分享的平台，使已有成就者能夠傳承己身經驗，嶄露頭角者在激盪中獲得靈感與啟發，並同時讓初入茅廬者得以對資訊科學有更深入的了解；進而達到「學以致用、教學相長」的目標。。   第一次參加SITCON，地點在台北中研院，為此特別北上一趟，剛好又碰到寒流冷個要死…..。 很感恩的，學生票不用收費，但聽說當時票在開搶後5分鐘內就全部搶完了，想想我還真是幸運。 </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/02/534136_1277656382250610_9209176967644072851_n.jpg" alt="534136_1277656382250610_9209176967644072851_n"><br>(進場就會給你一個手提袋跟名牌，好希望名牌給個套子喔..比較有質感xD) 今年有一個天使計劃，可以讓第一次參加的人能和參加過的人有交流的機會，原本我和一位我的夥伴約好了，結果被他放鴿子了…。<br><a id="more"></a></p><p>先說總結，整體下來感覺還不錯，分別來說說我喜歡的和覺得不太妥的地方: </p><ul><li>喜歡的則是感覺整體下來真的很用心，每個議程都有他們的特色，聽到了許多值得學習的東西；在外頭的攤位也令我蠻感興趣的，幾乎每一個都有跟他們詢問過，也拿了一堆資料回來。 </li><li>覺得不太好的則是議程時間安排上似乎有點趕，不知道去年是不是也這樣，在short talk中是沒有休息時間的，如果要趕到其他場去聽別的演講就會比較Delay到。 </li></ul><p>而有些場次真的很熱門…整個空間都塞爆了 </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/02/12795426_1277658162250432_4097133826010381815_n.jpg" alt="12795426_1277658162250432_4097133826010381815_n"><br>(前面地上那一排和最後面都塞滿了…) </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/02/12790896_1277657098917205_4037859932928146850_n.jpg" alt="12790896_1277657098917205_4037859932928146850_n"><br>(論壇時的情況，聽洪士灝教授及其他教授、老師們探討國高中資訊教育的問題，人真的很多…) </p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/02/10400744_1277656602250588_6199758067602520524_n.jpg" alt="10400744_1277656602250588_6199758067602520524_n"><br>(走廊上的白板，開放給大家戰各種議題如編譯器、作業系統、環境…)   </p><p>然後來說一下這次年會的心得: </p><p>這次參加其實主要是衝著ADR來的xD，不過除此之外也聽到了不少我覺得有興趣的主題: </p><ul><li>陳任軒的【新創公司如何獲得資金】是個不錯的經驗談，在學生時期能從什麼方面去獲得資金，事後在他們攤位也與他們的行政交流了一下，蠻佩服他們團隊的。 </li><li>Ray的Short Talk【網釣就在你身旁~校園資安報告】其實他是所有聽過議程裡面我覺得最有趣的，認為他的講談技巧真的很出色，用輕鬆幽默的方式提及網安的重要性。 </li><li>ADR的【防毒擋不住?勒索病毒猖獗與實作】，真的是太精彩了，整個40分鐘沒有冷場，數個DEMO搭配理論講解，說明各種病毒與防毒軟體的演進以及最近的勒索病毒如何實作，還是一句話，太精采了，整場座無虛席呢。   </li></ul><p>其他的議程其實也不錯，但就講出印象比較深刻的主題。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> conference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA10310]Dog and Gopher</title>
      <link href="/posts/83bf4483/"/>
      <url>/posts/83bf4483/</url>
      
        <content type="html"><![CDATA[<blockquote><p>A large eld has a gopher and a dog. The dog wants to eat the gopher, while the gopher wants to run to safety through one of several gopher holes dug in the surface of the eld. Neither the dog nor the gopher is a math major; however, neither is entirely stupid. The gopher decides on a particular gopher hole and heads for that hole in a straight line at a xed speed. The dog, which is very good at reading body language, anticipates which hole the gopher has chosen, and heads at double the speed of the gopher to the hole, where it intends to gobble up the gopher. If the dog reaches the hole rst, the gopher gets gobbled; otherwise, the gopher escapes. You have to select a hole for the gopher through which it can escape, if such a hole exists.<br><a id="more"></a></p></blockquote><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>The input le contains several sets of input. The rst line of each set contains one integer and four oating point numbers. The integer n denotes how many holes are in the set and the four oating point numbers denote the (x; y) coordinates of the gopher followed by the (x; y) coordinates of the dog. Subsequent n lines of input each contain two oating point numbers: the (x; y) coordinates of a gopher hole. All distances are in meters; to the nearest mm. Input is terminated by end of le. There is a blank line between two consecutive sets. </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>Your should output a single line for each set of input. For each set, if the gopher can escape the output line should read `The gopher can escape through the hole at (x; y).’ identifying the appropriate hole to the nearest mm. Otherwise the output line should read`The gopher cannot escape.’ If the gopher may escape through more than one hole, report the one that appears rst in the input. There are not more than 1000 gopher holes in a set of input and all coordinates are between -10000 and +10000.   </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 1.000 1.000 2.000 2.000</span><br><span class="line">1.500 1.500</span><br><span class="line">2 2.000 2.000 1.000 1.000</span><br><span class="line">1.500 1.500</span><br><span class="line">2.500 2.500</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The gopher cannot escape.</span><br><span class="line">The gopher can escape through the hole at (2.500,2.500).</span><br></pre></td></tr></table></figure><p>題意: 計算距離回答Gopher是否被Dog吃到，且D的速度是G的兩倍。 想法:距離問題，注意精確度以及距離剛好是兩倍的情形也算…。   </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;   </span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">int</span> n; </span><br><span class="line">  <span class="keyword">double</span> x1,x2,y1,y2; </span><br><span class="line">  <span class="keyword">double</span> hx,hy; </span><br><span class="line">  <span class="keyword">bool</span> <span class="built_in">end</span> = <span class="literal">false</span>; </span><br><span class="line">  <span class="keyword">while</span>(~<span class="built_in">scanf</span>(<span class="string">"%d %lf %lf %lf %lf"</span>,&amp;n,&amp;x1,&amp;y1,&amp;x2,&amp;y2)) &#123; </span><br><span class="line">    <span class="built_in">end</span> = <span class="literal">false</span>; </span><br><span class="line">    <span class="comment">//printf("%d %f %f %f %f\n",n,x1,y1,x2,y2); </span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; n ; ++i) &#123; </span><br><span class="line">      <span class="built_in">scanf</span>(<span class="string">"%lf %lf"</span>,&amp;hx,&amp;hy); </span><br><span class="line">      <span class="keyword">if</span>(!<span class="built_in">end</span>) &#123; </span><br><span class="line">        <span class="keyword">if</span>( <span class="number">4.0</span> * ((hx-x1)*(hx-x1)+(hy-y1)*(hy-y1)) &lt;= ((hx-x2)*(hx-x2)+(hy-y2)*(hy-y2)) ) &#123; </span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">"The gopher can escape through the hole at (%.3lf,%.3lf).\n"</span>,hx,hy); </span><br><span class="line">          <span class="built_in">end</span> = <span class="literal">true</span>; </span><br><span class="line">        &#125;</span><br><span class="line">      &#125; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">if</span>(!<span class="built_in">end</span>)<span class="built_in">printf</span>(<span class="string">"The gopher cannot escape.\n"</span>); </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA10300]Ecological Premium</title>
      <link href="/posts/ac27d39f/"/>
      <url>/posts/ac27d39f/</url>
      
        <content type="html"><![CDATA[<blockquote><p>German farmers are given a premium depending on the conditions at their farmyard. Imagine the following simplied regulation: you know the size of each farmer’s farmyard in square meters and the number of animals living at it. We won’t make a difference between different animals, although this is far from reality. Moreover you have information about the degree the farmer uses environment-friendly equipment and practices, expressed in a single integer greater than zero. The amount of money a farmer receives can be calculated from these parameters as follows. First you need the space a single animal occupies at an average. This value (in square meters) is then multiplied by the parameter that stands for the farmer’s environment-friendliness, resulting in the premium a farmer is paid per animal he owns. To compute the nal premium of a farmer just multiply this premium per animal with the number of animals the farmer owns.<br><a id="more"></a></p></blockquote><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>The rst line of input contains a single positive integer n (&lt; 20), the number of test cases. Each test case starts with a line containing a single integer f (0 &lt; f &lt; 20), the number of farmers in the test case. This line is followed by one line per farmer containing three positive integers each: the size of the farmyard in square meters, the number of animals he owns and the integer value that expresses the farmers environment-friendliness. Input is terminated by end of le. No integer in the input is greater than 100000 or less than 0. </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each test case output one line containing a single integer that holds the summed burden for Ger- many’s budget, which will always be a whole number. Do not output any blank lines. </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">3</span><br><span class="line">5</span><br><span class="line">1 1 1</span><br><span class="line">2 2 2</span><br><span class="line">3 3 3</span><br><span class="line">2 3 4</span><br><span class="line">8 9 2</span><br><span class="line">3</span><br><span class="line">9 1 8</span><br><span class="line">6 12 1</span><br><span class="line">8 1 1</span><br><span class="line">3</span><br><span class="line">10 30 40</span><br><span class="line">9 8 5</span><br><span class="line">100 1000 70</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">38</span><br><span class="line">86</span><br><span class="line">7445</span><br></pre></td></tr></table></figure><p>題意: 計算某個值，公式已給你了，仔細研究會發現只是第一項*第三項的總和，水題。 </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;  </span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">int</span> n; </span><br><span class="line">  <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n); </span><br><span class="line">  <span class="keyword">while</span>(n--) &#123; </span><br><span class="line">    <span class="keyword">int</span> m,<span class="built_in">size</span>,num,cof,ans = <span class="number">0</span>; </span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;m); </span><br><span class="line">    <span class="keyword">while</span>(m--) &#123; </span><br><span class="line">      <span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>,&amp;<span class="built_in">size</span>,&amp;num,&amp;cof); </span><br><span class="line">      ans += <span class="built_in">size</span> * cof; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,ans); </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA10282]Babelfish - 為什麼Map的Key不能用Char?</title>
      <link href="/posts/791c4cfd/"/>
      <url>/posts/791c4cfd/</url>
      
        <content type="html"><![CDATA[<blockquote><p>You have just moved from Waterloo to a big city. The people here speak an incomprehensible dialect of a foreign language. Fortunately, you have a dictionary to help you understand them. </p></blockquote><a id="more"></a><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>Input consists of up to 100,000 dictionary entries, followed by a blank line, followed by a message of up to 100,000 words. Each dictionary entry is a line containing an English word, followed by a space and a foreign language word. No foreign word appears more than once in the dictionary. The message is a sequence of words in the foreign language, one word on each line. Each word in the input is a sequence of at most 10 lowercase letters. </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>Output is the message translated to English, one word per line. Foreign words not in the dictionary should be translated as ‘eh’. </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dog ogday cat atcay pig igpay froot ootfray loops oopslay</span><br><span class="line">atcay ittenkay oopslay</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat eh loops</span><br></pre></td></tr></table></figure><p>題意: 查字典，翻出原本的英文 </p><p>想法: 用MAP很簡單就可以達成，但這題我卡了很久，原因在於我用了char<em> 而不是string，所以資料根本不會被insert到map中，如果一定要用的話就要用const char</em>，原因可以看一下這些地方: </p><ul><li><a href="http://stackoverflow.com/questions/4157687/using-char-as-a-key-in-stdmap" target="_blank" rel="noopener">http://stackoverflow.com/questions/4157687/using-char-as-a-key-in-stdmap</a> </li><li><a href="http://stackoverflow.com/questions/12136071/c-mapstdstring-vs-mapchar-performance-i-know-again" target="_blank" rel="noopener">http://stackoverflow.com/questions/12136071/c-mapstdstring-vs-mapchar-performance-i-know-again</a> </li></ul><p>後來把char* 改成string就AC了。 </p><p>歐對了，還有一個小地方是，find是找map對應的key值，所以存的時候順序要對。</p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="built_in">map</span> d; </span><br><span class="line">  <span class="keyword">char</span> s[<span class="number">25</span>],a[<span class="number">11</span>],b[<span class="number">11</span>]; </span><br><span class="line">  <span class="keyword">while</span>(gets(s) &amp;&amp; s[<span class="number">0</span>] != <span class="string">'\0'</span>) &#123; </span><br><span class="line">    <span class="built_in">sscanf</span>(s,<span class="string">"%s %s"</span>,a,b); </span><br><span class="line">    d.insert(pair(b,a)); </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">while</span>(gets(s)) &#123; </span><br><span class="line">    <span class="built_in">map</span>::iterator it; it = d.<span class="built_in">find</span>(s); </span><br><span class="line">    <span class="keyword">if</span>(it == d.<span class="built_in">end</span>()) <span class="built_in">printf</span>(<span class="string">"eh\n"</span>); </span><br><span class="line">    <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"%s\n"</span>,it-&gt;second.c_str()); </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA10281] Average Speed</title>
      <link href="/posts/49c85007/"/>
      <url>/posts/49c85007/</url>
      
        <content type="html"><![CDATA[<blockquote><p>You have bought a car in order to drive from Waterloo to a big city. The odometer on their car is broken, so you cannot measure distance. But the speedometer and cruise control both work, so the car can maintain a constant speed which can be adjusted from time to time in response to speed limits, traffic jams, and border queues. You have a stopwatch and note the elapsed time every time the speed changes. From time to time you wonder, “how far have I come?”. </p><p>To solve this problem you must write a program to run on your laptop computer in the passenger seat.<br><a id="more"></a></p></blockquote><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>Standard input contains several lines of input: Each speed change is indicated by a line specifying the elapsed time since the beginning of the trip (hh:mm:ss), followed by the new speed in km/h. Each query is indicated by a line containing the elapsed time. At the outset of the trip the car is stationary. Elapsed times are given in non-decreasing order and there is at most one speed change at any given time. </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each query in standard input, you should print a line giving the time and the distance travelled, in the format below. </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">00:00:01 100</span><br><span class="line">00:15:01</span><br><span class="line">00:30:01</span><br><span class="line">01:00:01 50</span><br><span class="line">03:00:01</span><br><span class="line">03:00:05 140</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">00:15:01 25.00 km</span><br><span class="line">00:30:01 50.00 km</span><br><span class="line">03:00:01 200.00 km</span><br></pre></td></tr></table></figure><p>題意: 給你某個時間的速度，沒有給則代表沒改變，要你求總共走的距離。 </p><p>想法: 簡單的求距離，容易卡在精確度的問題以及I/O上，因為不定長度所以用字串存取再去分割 ，上網查了之後發現sscanf的回傳值代表正確的讀取數，而讀取失敗的則不會有影響(Ex:sscanf去分割4個變數，但在只有輸入三個變數的情形下並不會出錯，第4個變數值不會更動，而回傳值是3(假設型態皆正確))，於是利用sscanf來寫即可。     </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;  </span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">char</span> str[<span class="number">200</span>]; </span><br><span class="line">  <span class="keyword">int</span> h,m,s,t,t1 = <span class="number">0</span>,c; </span><br><span class="line">  <span class="keyword">double</span> ans = <span class="number">0</span>,v = <span class="number">0</span>,v1 = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">while</span>(gets(str)) &#123; </span><br><span class="line">    c = <span class="built_in">sscanf</span>(str,<span class="string">"%d:%d:%d %lf"</span>,&amp;h,&amp;m,&amp;s,&amp;v1); </span><br><span class="line">    t = h * <span class="number">3600</span> + m * <span class="number">60</span> + s; </span><br><span class="line">    <span class="comment">//printf("*%f\n",div * v1/3600.0); </span></span><br><span class="line">    ans += (t - t1) * v/<span class="number">3600.0</span>; t1 = t; </span><br><span class="line">    <span class="keyword">if</span>(c == <span class="number">3</span>) <span class="built_in">printf</span>(<span class="string">"%.2d:%.2d:%.2d %.2lf km\n"</span>,h,m,s,ans); </span><br><span class="line">    <span class="keyword">else</span> v = v1; </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA10279]Mine Sweeper</title>
      <link href="/posts/73bef056/"/>
      <url>/posts/73bef056/</url>
      
        <content type="html"><![CDATA[<blockquote><p>The game Minesweeper is played on an n by n grid. In this grid are hidden m mines, each at a distinct grid location. The player repeatedly touches grid positions. If a position with a mine is touched, the mine explodes and the player loses. </p><p>If a position not containing a mine is touched, an integer between 0 and 8 appears denoting the number of adjacent or diagonally adjacent grid positions that contain a mine. A sequence of moves in a partially played game is illustrated below. </p><p>Here, n is 8, m is 10, blank squares represent the integer 0, raised squares represent unplayed positions, and the gures resembling asterisks represent mines. The leftmost image represents the partially played game. From the rst image to the second, the player has played two moves, each time choosing a safe grid position. From the second image to the third, the player is not so lucky; he chooses a position with a mine and therefore loses. The player wins if he continues to make safe moves until only m unplayed positions remain; these must necessarily contain the mines. </p><p>Your job is to read the information for a partially played game and to print the corresponding board.<br><a id="more"></a></p></blockquote><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>The rst line of input contains how many game you have to solved followed by blank line. The rst line of each description contains a single postitive integer n 10. The next n lines represent the positions of the mines. Each line represents the contents of a row using n characters: a period indicates an unmined position while an asterisk indicates a mined position. The next n lines are each n characters long: touched positions are denoted by an `x, and untouched positions by a period. The sample input corresponds to the middle gure above. There is a blank line between each consecutive game description. </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each test case, your output should represent the board, with each position lled in appropriately. Positions that have been touched and do not contain a mine should contain an integer between 0 and 8. If a mine has been touched, all positions with a mine should contain an asterisk. All other positions should contain a period. Print a blank line between each consecutive 2 consecutive test cases. </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1</span><br><span class="line">8</span><br><span class="line">…..*</span><br><span class="line">……*.</span><br><span class="line">….…</span><br><span class="line">……..</span><br><span class="line">……..</span><br><span class="line">…....</span><br><span class="line">….*.</span><br><span class="line">…..*..</span><br><span class="line">xxx…..</span><br><span class="line">xxxx….</span><br><span class="line">xxxx….</span><br><span class="line">xxxxx…</span><br><span class="line">xxxxx…</span><br><span class="line">xxxxx…</span><br><span class="line">xxx…..</span><br><span class="line">xxxxx…</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">001…..</span><br><span class="line">0013….</span><br><span class="line">0001….</span><br><span class="line">00011…</span><br><span class="line">00001…</span><br><span class="line">00123…</span><br><span class="line">001…..</span><br><span class="line">00123…</span><br></pre></td></tr></table></figure><p>題意: 模擬踩地雷遊戲，第一張地圖給你有地雷的地方，要你輸出第二張地圖中x的位置是什麼東西。 </p><p>想法: 基本上I/O做好就沒問題，把地雷四周都多+1即可，要注意在字元以及整數的I/O轉換間換行會被吃進去造成輸入錯誤 </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt; </span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">int</span> n = <span class="number">0</span>,m = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;m) == <span class="number">1</span>) &#123; </span><br><span class="line">    <span class="keyword">bool</span> f = <span class="literal">true</span>; </span><br><span class="line">    <span class="keyword">while</span>(m--) &#123; </span><br><span class="line">      <span class="keyword">if</span>(!f) <span class="built_in">printf</span>(<span class="string">"\n"</span>); </span><br><span class="line">      f = <span class="literal">false</span>; <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n); </span><br><span class="line">      <span class="keyword">char</span> <span class="built_in">map</span>[<span class="number">10</span>+<span class="number">1</span>][<span class="number">10</span>+<span class="number">1</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line">      <span class="keyword">int</span> ans[<span class="number">10</span>+<span class="number">1</span>][<span class="number">10</span>+<span class="number">1</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span> ; i &lt;= n ; ++i) &#123; </span><br><span class="line">        <span class="keyword">while</span>(getchar()!=<span class="string">'\n'</span>)&#123;&#125; </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span> ; j &lt;= n ; ++j) &#123; </span><br><span class="line">          <span class="built_in">scanf</span>(<span class="string">"%c"</span>,&amp;<span class="built_in">map</span>[i][j]); </span><br><span class="line">          <span class="keyword">if</span>(<span class="built_in">map</span>[i][j] == <span class="string">'*'</span>) &#123; </span><br><span class="line">            ans[i<span class="number">-1</span>][j]++; </span><br><span class="line">            ans[i<span class="number">-1</span>][j<span class="number">-1</span>]++; </span><br><span class="line">            ans[i<span class="number">-1</span>][j+<span class="number">1</span>]++; </span><br><span class="line">            ans[i+<span class="number">1</span>][j]++; </span><br><span class="line">            ans[i+<span class="number">1</span>][j+<span class="number">1</span>]++; </span><br><span class="line">            ans[i+<span class="number">1</span>][j<span class="number">-1</span>]++; </span><br><span class="line">            ans[i][j<span class="number">-1</span>]++; </span><br><span class="line">            ans[i][j+<span class="number">1</span>]++; </span><br><span class="line">            ans[i][j] = <span class="number">-1</span>; </span><br><span class="line">          &#125; </span><br><span class="line">        &#125; </span><br><span class="line">      &#125; </span><br><span class="line">      <span class="keyword">bool</span> fir = <span class="literal">true</span>; </span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span> ; i &lt;= n ; ++i) &#123; </span><br><span class="line">        <span class="keyword">if</span>(!fir)<span class="built_in">printf</span>(<span class="string">"\n"</span>); </span><br><span class="line">        fir = <span class="literal">false</span>; </span><br><span class="line">        <span class="keyword">while</span>(getchar()!=<span class="string">'\n'</span>)&#123;&#125; </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span> ; j &lt;= n ; ++j) &#123; </span><br><span class="line">          <span class="built_in">scanf</span>(<span class="string">"%c"</span>,&amp;<span class="built_in">map</span>[i][j]); </span><br><span class="line">          <span class="keyword">if</span>(<span class="built_in">map</span>[i][j] == <span class="string">'x'</span>) &#123; </span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%d"</span>,ans[i][j]); </span><br><span class="line">            <span class="keyword">if</span>(ans[i][j] == <span class="number">-1</span>) <span class="built_in">printf</span>(<span class="string">"*"</span>); </span><br><span class="line">          &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%c"</span>,<span class="built_in">map</span>[i][j]); </span><br><span class="line">          &#125; </span><br><span class="line">        &#125; </span><br><span class="line">      &#125; </span><br><span class="line">    &#125; </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2016中正大學資管系面試</title>
      <link href="/posts/bda31788/"/>
      <url>/posts/bda31788/</url>
      
        <content type="html"><![CDATA[<p>PS:2014年，從我部落格轉過來的 </p><p>因為學測國文採計而非常吃虧的我 好險面試過程還不壞…   </p><p>摁…不說廢話了 面試場所長這個樣子 是3對3的形式 總共20分鐘 </p><div class="table-container"><table><thead><tr><th style="text-align:center">一直微笑的教授1號</th><th style="text-align:center">看不到臉的教授2號</th><th style="text-align:center">沒什麼特徵的教授3號</th></tr></thead><tbody><tr><td style="text-align:center">我</td><td style="text-align:center">面試1號</td><td style="text-align:center">面試2號</td></tr></tbody></table></div><p>中間大約隔了一大張桌子的距離 為什麼會說看不到臉的教授…因為他被電腦檔住了</p><p>而1號2號…雖然這樣說有點無禮 但在整個過程中他們真的帶給了我蠻大的歡樂..<br><a id="more"></a><br>一開始叫大家輪流自我介紹   此時1號則是非常緊張的忘詞 然後還很可愛的跟教授說~對不起我太緊張了(低頭認錯)   </p><p>2號則是像小朋友一樣說話時不停的轉動他的椅子   </p><p>話說在我自我介紹時 聽到我會寫程式時教授3號的表情是這個樣子-&gt;˙-˙-&gt;˙.˙-&gt;˙o˙-&gt;˙O˙-&gt;˙0˙   </p><p>之後3個教授輪流發問   </p><h2 id="教授1"><a href="#教授1" class="headerlink" title="教授1"></a>教授1</h2><p>Q: 你們的英文能力通過道什麼程度?</p><ul><li>英文中級初試過 但沒時間考複試<br>Q: 如果你想交換學生(或不想 並說理由) 你會選哪裡的哪所大學?</li><li>(乾…這是要我怎麼掰) 我想先繼續就讀研究所 暫時沒這方面的想法 </li></ul><p>1號此時眼睛一亮:我和前面這位同學的想法一樣(!!)   </p><h2 id="教授2"><a href="#教授2" class="headerlink" title="教授2"></a>教授2</h2><p>Q: 你們三位都就讀私立學校 為什麼要選私立?不會學費很貴媽?會不會後悔?原本可以上哪所公立?</p><ul><li>政府有補助 在學校也學了很多東西…這類的官方回答   </li></ul><p>話說此時1號的回答: 我原本可以就讀雄中(!) 但我會就讀__是因為..哦…摁…因為其實家裡和兩所學校的距離都差不多(咦?那位啥不讀雄中?)..   </p><h2 id="教授3"><a href="#教授3" class="headerlink" title="教授3"></a>教授3</h2><p>Q: 請說一下未來10年你們的計畫 想做什麼都可以</p><ul><li>在讀完研究所後 可能在職場上先從業務助理這方面的基層做起 累積實務經驗以及知識   </li></ul><p>大致上是這樣 問題沒很多 主要是因為1.2號回答時都有點鬼打牆…所以花了不少時間等他們回答- -  </p><p>雖然有點對不起他們 但總之有著兩位陪伴真的令我感到安心不少- -|||     </p><p>對了 過程中不知道是哪段談話聽到2號如是說:   “…在醫療管理這方面….我知道中興(!!!!!)大學….不不不…對不起我太緊張了(低頭認錯) 是中正大學….”   </p><p>這…     </p><p>多多累積好運氣 希望能正取。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2016高雄大學資工系面試</title>
      <link href="/posts/4e832f8e/"/>
      <url>/posts/4e832f8e/</url>
      
        <content type="html"><![CDATA[<p>因為高雄大學資工面試的資料真的太少了(我只找到兩筆..) 因此決定來造福一下學弟妹… </p><p>PS:2014年的內容，從我的部落格轉過來的</p><p>面試前學長會先叫六個進去一間很空曠又很陰暗的房間   </p><p>裏頭有六張椅子 + 一個功率非常高的冷氣不斷的吹你…   之後一批三個進去另外一個房間 房見內有3個辦公事 裡面住著3個BOSS…</p><p>不是…是教授 每一間你有7分鐘的攻略時間 時間到了門外會有學長將你強制登出(敲門- -)<br><a id="more"></a></p><p>房間內大概長這個樣子-&gt;        </p><div class="table-container"><table><thead><tr><th style="text-align:center">教授</th></tr></thead><tbody><tr><td style="text-align:center">桌子</td></tr><tr><td style="text-align:center">你</td></tr></tbody></table></div><p>P.s面試當天早上在PTT上看到一篇高大資工2009年的面試文說有英文自我介紹…</p><p>當下嚇得半死 還好這次沒有 我個人是帶了3份備審資料去…不過他們桌上電腦都有資料 其實是不用帶的   </p><p>歐對了先說，他們事前絕對沒看過你備審，現場大約花不到20秒就把你的自傳給跳過去了… 所以一定要將賣點給標清楚 大約是當你用滾輪再速看PDF檔的時候也能看得到的那種程度 因為我有放作品集…這部分他們有看比較久。   </p><h2 id="第一間"><a href="#第一間" class="headerlink" title="第一間"></a>第一間</h2><p>教授人還不錯，先叫你自我介紹完後，開始針對妳被審問問題。<br>Q:　為什麼想念這個系?<br>-　因為這是我的興趣…(教授:摁不錯 這種年紀就自己學習這種東西很少見&gt;&lt;) </p><p>Q:　你的學測成績?英文、數學幾級分?<br>-　64、英13數14 </p><p>Q：　你還申請了哪幾間?<br>-　各種資工 第一階段通過只有兩間 </p><p>Q：　假如你兩間都上了 你會選擇哪一間?<br>-　高雄資工(廢話，見人說人話 見鬼說鬼話)因為… </p><p>Q：　你對於我們學校的課程規劃了解到什麼程度?<br>-　大一、二是基礎理論 之後才會有專業領域之類的<br>  -　記得去看課程規劃~ </p><p>Q：　那你未來比較有興趣的是哪方面的領域?<br>-　嵌入式系統(跳入陷阱) </p><p>!!!! </p><p>Q：　好…那我們來練習一下 請你念一下這段英文並說一下她的大意 (桌上有一張紙 應該是問你想走哪方面之後 選出那方面的內容來給妳念) (所以我念的那段是嵌入式系統的)<br>-　…(英文爛到爆的我感覺這方面有點悲劇- -) 然後它的大意大概是說….(教授:不錯阿 你的統整能力還蠻好的&gt;&lt;)   </p><h2 id="第二間"><a href="#第二間" class="headerlink" title="第二間"></a>第二間</h2><p>Q: 自我介紹</p><ul><li>…</li></ul><p>其實之後的大方向都和第一間差不多 都朝著你的被審來問問題 只不過看備審的時間取決於你的自我介紹時間就是了…所以要掌控好  </p><p>Q: 妳知道我們學校主要的課程規劃媽?</p><ul><li>計算機概論 系統 智慧型…(教授:你有去看過我們的網站齁 我:是:) ) </li></ul><p>Q: 那你知道我們系目前有幾位教授媽?</p><ul><li>11位專任教師 6位與電機系合聘 </li></ul><p>!!!! </p><p>教授: 不錯 妳是我面試到目前唯一一個能完整回答這個問題的人 妳真的很認真在準備 </p><p>我:謝謝&gt;|||&lt;(暗爽)   </p><h2 id="第三間"><a href="#第三間" class="headerlink" title="第三間 "></a>第三間 </h2><p>教授的態度有點冷漠… 然後看東西也很慢 所以你在等她發問的時候會有一段尷尬的小沉默 也沒問道什麼問題就結束了 </p><p>雖然不至於很兇啦 但.. </p><p>Q: 自我介紹</p><ul><li>… </li></ul><p>Q: 你有參加過GIS研習營…那你學到了什麼?</p><ul><li>… </li></ul><p>Q: 那你覺得她造成的影響有哪些?</p><ul><li>…掰得有點爛 感覺她不怎麼喜歡 </li></ul><p>Q:那你知道我們資工系主要發展方向?</p><ul><li>網路通訊 資料庫…我照前兩間的講…但還沒講完就被它糾正了QAQ </li></ul><p>Q: 那這方面你最有興趣的?</p><ul><li>嵌入式系統 </li></ul><p>Q: 妳念道明高中 那他是私立學校學費會很貴媽?</p><ul><li>你是不會自己讀讀看歐一個學期大約3萬多 但有補助的話和公立差不多 </li></ul><p>中間沉默的時間真的有點多 沒問道什麼就結束了。   </p><p>好啦，就這樣。掰。 多多累積白色物質，希望能獲得正取。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA10407]Simple division</title>
      <link href="/posts/1259e14f/"/>
      <url>/posts/1259e14f/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Integer division between a dividend n and a divisor d yields a quotient q and a remainder r. q is the integer which maximizes q ∗d such that q∗d ≤ n and r = n−q∗d. For any set of integers there is an integer d such that each of the given integers when divided by d leaves the same remainder.<br><a id="more"></a></p></blockquote><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>Each line of input contains a sequence of nonzero integer numbers separated by a space. The last number on each line is 0 and this number does not belong to the sequence. There will be at least 2 and no more than 1000 numbers in a sequence; not all numbers occuring in a sequence are equal. The last line of input contains a single 0 and this line should not be processed. </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each line of input, output the largest integer which when divided into each of the input integers leaves the same remainder. </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><p>701 1059 1417 2312 0 14 23 17 32 122 0 14 -22 17 -31 -124 0 0 </p><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><p>179 3 3 </p><p>題意: 找出一個最大除數使得所有數字都能有相同的餘數。由餘式定理可以推出如果每個數的餘數都要相同時，他們之間一定會有公因數，將兩兩數之差做過一輪GCD即可。 </p><p>我的以往GCD寫法讓我錯了不少次… </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b)</span> </span>&#123; </span><br><span class="line">  <span class="keyword">int</span> temp = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">while</span>(a % b) &#123; </span><br><span class="line">    temp = b; </span><br><span class="line">    b = a % b; </span><br><span class="line">    a = temp; </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> b; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>這樣寫在b是非零的狀況下是可行的，但一但b等於零則會造成錯誤(跟0取餘數)。 </p><p>以下兩種寫法則可以避免掉這種狀況: </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123; </span><br><span class="line">  <span class="keyword">return</span> b == <span class="number">0</span> ? a : gcd( b, a % b ); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b)</span> </span>&#123; </span><br><span class="line">  <span class="keyword">int</span> temp = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">while</span>(b != <span class="number">0</span>) &#123; </span><br><span class="line">    temp = b; </span><br><span class="line">    b = a % b; </span><br><span class="line">    a = temp; </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> a; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt; </span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a,<span class="keyword">int</span> b)</span> </span>&#123; </span><br><span class="line">  <span class="keyword">int</span> temp = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">while</span>(b != <span class="number">0</span>) &#123; </span><br><span class="line">    temp = b; </span><br><span class="line">    b = a % b; </span><br><span class="line">    a = temp; </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> a; </span><br><span class="line">&#125; </span><br><span class="line"><span class="keyword">int</span> arr1[<span class="number">1005</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line"><span class="keyword">int</span> div1[<span class="number">1005</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;arr1[<span class="number">0</span>])) &#123; </span><br><span class="line">    <span class="keyword">if</span>(arr1[<span class="number">0</span>] == <span class="number">0</span>) <span class="keyword">break</span>; </span><br><span class="line">    <span class="keyword">int</span> n = <span class="number">1</span>; </span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;arr1[n])) &#123; </span><br><span class="line">      <span class="keyword">if</span>(arr1[n] == <span class="number">0</span>) <span class="keyword">break</span>; </span><br><span class="line">      div1[n<span class="number">-1</span>] = <span class="built_in">abs</span>(arr1[n]-arr1[n<span class="number">-1</span>]); n++; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">int</span> temp = div1[<span class="number">0</span>]; </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span> ; i &lt; n<span class="number">-1</span> ; ++i) &#123; </span><br><span class="line">      temp = gcd(temp,div1[i]); </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,temp); </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA10382]Watering</title>
      <link href="/posts/6aed5321/"/>
      <url>/posts/6aed5321/</url>
      
        <content type="html"><![CDATA[<blockquote><p>n sprinklers are installed in a horizontal strip of grass l meters long and w meters wide. Each sprinkler is installed at the horizontal center line of the strip. For each sprinkler we are given its position as the distance from the left end of the center line and its radius of operation. What is the minimum number of sprinklers to turn on in order to water the entire strip of grass?<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/01/e693b7e58f962.png" alt="擷取"><br><a id="more"></a></p></blockquote><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>Input consists of a number of cases. The ﬁrst line for each case contains integer numbers n, l and w with n ≤ 10000. The next n lines contain two integers giving the position of a sprinkler and its radius of operation. (The picture above illustrates the ﬁrst case from the sample input.) </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each test case output the minimum number of sprinklers needed to water the entire strip of grass. If it is impossible to water the entire strip output ‘-1’. </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><p>8 20 2 5 3 4 1 1 2 7 2 10 2 13 3 16 2 19 4 3 10 1 3 5 9 3 6 1 3 10 1 5 3 1 1 9 1 </p><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><p>6 2 -1 </p><p>題意: 用最小數目的圓形把長方形面積都覆蓋住，貪心問題。 </p><p>想法: 一開始想以圓形的最左和最右當端點，但是後來發現會有無法完全覆蓋(有隙縫)的情形，參考網路上的想法後改成轉換成長方形左右兩端當成端點，並且要注意圓形的半徑如果無法將長方形的寬覆蓋住則不列入考慮。 </p><p>這題看似簡單……做起來折騰了好久阿。</p><p>CPE上的測資真的弱到爆，CPE過了拿去UVA居然沒過，為此DEBUG好久……後來結論是我在for判斷的時候是以圓圈的數量作為條件，所以如果是最後一個點才完成覆蓋所有面積，這樣的情形我就不會判斷到，所以要移動一下判斷的位置     </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//使用Java的考生請注意，最外層的類別(class)需命名為 main 。 </span></span><br><span class="line"><span class="comment">//如果遇到超乎想像的狀況，請更改編譯器試看看!! 各編譯器特性不同!! </span></span><br><span class="line"><span class="comment">//預設測資、隨機測資、固定測資是用來幫助除錯用的。批改時，只看暗中測資是否通過!! </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt; </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(pair&lt;<span class="keyword">double</span>,<span class="keyword">double</span>&gt; p1,pair&lt;<span class="keyword">double</span>,<span class="keyword">double</span>&gt; p2)</span> </span>&#123; </span><br><span class="line">  <span class="keyword">return</span> p1.second &gt; p2.second; </span><br><span class="line">&#125; </span><br><span class="line">pair&lt;double,double&gt; change(pair&lt;double,double&gt; p,int w) &#123; </span><br><span class="line">  <span class="keyword">double</span> r = <span class="built_in">sqrt</span>(p.second*p.second - (<span class="keyword">double</span>)w*w/<span class="number">4</span>); </span><br><span class="line">  p.second = p.first + r; </span><br><span class="line">  p.first = p.first - r; </span><br><span class="line">  <span class="keyword">return</span> p; </span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">int</span> n,l,w;</span><br><span class="line">  <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>,&amp;n,&amp;l,&amp;w) == <span class="number">3</span>) &#123; </span><br><span class="line">    <span class="keyword">double</span> pos,rad; </span><br><span class="line">    <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">double</span>,<span class="keyword">double</span>&gt; &gt;s; </span><br><span class="line">    <span class="keyword">while</span>(n--) &#123; </span><br><span class="line">      <span class="built_in">scanf</span>(<span class="string">"%lf %lf"</span>,&amp;pos,&amp;rad); </span><br><span class="line">      <span class="keyword">if</span>(<span class="number">2</span>*rad &gt; w) </span><br><span class="line">        s.push_back( change(make_pair(pos,rad),w) ); </span><br><span class="line">    &#125; </span><br><span class="line">    sort(s.<span class="built_in">begin</span>(),s.<span class="built_in">end</span>(),cmp); </span><br><span class="line">    <span class="keyword">double</span> start = <span class="number">0</span>; </span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>; </span><br><span class="line">    <span class="keyword">bool</span> flag = <span class="literal">false</span>; </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; s.<span class="built_in">size</span>() ; ++i) &#123; </span><br><span class="line">      <span class="keyword">double</span> temp = <span class="number">0</span>; </span><br><span class="line">      <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">double</span>,<span class="keyword">double</span>&gt; &gt;::iterator it; </span><br><span class="line">      <span class="keyword">for</span>(it = s.<span class="built_in">begin</span>() ; it != s.<span class="built_in">end</span>() ; it++) &#123; </span><br><span class="line">        <span class="keyword">if</span>( it-&gt;first &lt;= start &amp;&amp; it-&gt;second &gt; temp) &#123; </span><br><span class="line">          <span class="comment">//printf("* %lf %lf\n",it-&gt;first,it-&gt;second); </span></span><br><span class="line">          temp = it-&gt;second; </span><br><span class="line">          <span class="comment">//printf("new start : %lf\n",temp); </span></span><br><span class="line">        &#125; </span><br><span class="line">      &#125; </span><br><span class="line">      start = temp; </span><br><span class="line">      count++; </span><br><span class="line">      <span class="keyword">if</span>(start &gt;= l) &#123; </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,count); </span><br><span class="line">        flag = <span class="literal">true</span>; </span><br><span class="line">        <span class="keyword">break</span>; </span><br><span class="line">      &#125; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">if</span>(!flag) <span class="built_in">printf</span>(<span class="string">"-1\n"</span>); </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(大學宿營)爆炸的半年 - 籌備過程</title>
      <link href="/posts/19a2f45f/"/>
      <url>/posts/19a2f45f/</url>
      
        <content type="html"><![CDATA[<p>上次說到分完組別，各自進行該做的事情…… 這時就要靠總召跟副召了，籌備期間其實很長，所以也很容易偷懶，如果不盯好進度到時就會開天窗。 </p><p>所以像是各組再開會的時候盡量都到場，了解更多資訊也才好做準備。 之後的總召就要跟同學以及學長姐協調好比較重要事情的日期，像是驗收、場勘。驗收時間允許的話當然越多次越好，但也要有一定的完成度在驗收，不然什麼都沒有就請學長姊看很不禮貌，通常會有一驗、二驗、總驗，在二驗跟總驗的時候像晚會的火舞(火球、火棍、香舞)都要上火試試看，因為上火的感覺跟平時練習是差蠻多的，所以一定要提早習慣。 </p><p>活動組的遊戲企畫書出來後就是分配關主給同學，在分配的原則上都是幹部盡量不當，尤其是總召三天都要負責確認流程的流暢性，所以盡量不要出現會找不到人的情況。 之後就是驗關，驗關主要是確認關主能夠明確知道自己這關在幹嘛、能夠流暢地介紹自己的關卡，而且還有一點就是整體時間，驗關的重點在於全部遊戲都真正跑過一輪後花的時間有多久，這個時間如果太少就要想如何補，太多則要思考是否刪減，才能符合流程表上的時間。</p><p>晚會的部分也是個大項目，活動組則要想要如何去穿插表演才能夠流暢又不會冷場，當然這也要驗過跑過一次才知道實際的情況，不跑的話絕對會跟預期的落差差很大。 </p><p>最後在宿營前場勘時各活動負責人記得要再確認好個關卡的點位，確保各關主知道自己的位置在哪裡。   </p><p>總務的話上次就說過，最好在暑假前就跑完贊助，之後沒事則可以幫忙其他組，還有器材的部分，像晚上的地燈、麥克風…一些設備能先借的都要先借起來。   </p><p>美宣道具做好後，確實清點道具數量無誤裝箱即可~~沒有什麼大問題。   </p><p>其實還有很多小細節，我也只是把我想到的打出來。宿營籌備期很長很辛苦，所以只要不放推有很長的時間可以慢慢思考缺了什麼。有任何問題都可以問學長姊或是上網參考別人的經驗….. 儘管過程很辛苦~最後辦完事很愉快的~</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>(大學宿營)爆炸的半年 - 分工與分組</title>
      <link href="/posts/500778d8/"/>
      <url>/posts/500778d8/</url>
      
        <content type="html"><![CDATA[<p>寒假閒閒無聊賴，就來分享一下我們宿營籌備的甘苦談。 </p><p>先來講一下我們的宿營，我們是規模6~70人的三天兩夜宿營(包含工作人員)，原本預計只花五個月(九月底)就可以辦完的，最後前前後後總共花了7個月(十一月底)才辦完。 </p><p>我是宿營的副召，總召、副召要做什麼呢?又有什麼該注意的事情呢?</p><p>我在下面都會講到 在籌備的過程中遇到了不少麻煩跟困難，不過最後還是圓滿地辦完啦~所以來分享一下個人覺得辦宿營的幾個注意事項給大家參考: </p><h2 id="分工與分組"><a href="#分工與分組" class="headerlink" title="分工與分組:"></a>分工與分組:</h2><p>學長姊們大約在五月左右就讓我們選完總召跟副召了，然後總召讓我們順利在暑假前順利分完組別(決策、公關、活動、總務、美宣)<br><a id="more"></a></p><h3 id="決策"><a href="#決策" class="headerlink" title="決策"></a>決策</h3><p>名義上就是各組組長都會在裡面…不過我覺得這個組其實是可以不用的，因為整個活動下來並沒有什麼太大幫助。 </p><p>要說什麼是總召要先完成的，就是【計劃書】跟【場地】以及【日期】的決定了，計劃書要先生出來好讓公關去拉贊，這部分參考上一屆的作細修即可。 </p><p>場勘完決定好場地後讓活動組開始籌備，決定場地時要先去跟他們簽約，然後這方面可能包含突發狀況的訂金退還跟一些細節，白紙黑字的確認好到時比較不會有問題。</p><p>地點最好在暑假的一半前就決定好。 </p><p>日期則要考慮到會不會衝到學校的課程以及期中考，以及準備好雨備方案。我們宿營之所以延期是因為當時的台南登革熱疫情嚴重，考慮到安全問題不得已只好延期並轉至高雄澄清湖 然後總召副召都要跟上一屆的學長姐做好交接工作，保持好聯繫，因為一定會有自己在弄時沒有想到的事情，所以必須要不斷的去跟組員、學長姊做好討論，才不會宿營當天卻發現有什麼東西開天窗了。 </p><p>暑假的部分可以跟同學討論，最好能提前回來(有些系甚至是不回家了)進行準備，如果日期訂在開學後還很久那就算了，如果訂在開學後一個月左右的暑假還不提前準備根本是找死，算一算平常上課時間、期中考、所有遊戲驗關、學長姐的一驗、二驗、總驗、二次場勘……一定會很趕。 </p><p>之後的活動總召跟副召都要定期盯進度，就我副召當時的作法是:我幾乎每個組的工作都有在裡面，不過事後想想我其實不用這麼累的…，重點是要定期確認進度的狀況好做調整，不然到時候宿營快到了就會有種爆炸的感覺。 </p><h3 id="公關"><a href="#公關" class="headerlink" title="公關"></a>公關</h3><p>任何活動都需要經費，所以公關就必須想辦法去做贊助幫忙減輕一些活動支出的負擔，這部分的話越早開始越好，理論上一選出來總召計畫書出來就可以開始拉贊了，拉贊有沿街或是找廠商等不同方式，我們是沿街拉贊，鎖定學校周圍的商家開始沿路走。</p><p>因為大學不同系的宿營時間上都差不多在同一時段，所以大家的拉贊時間也會是差不多的，先開始做有很大的機率成功，因為商家被拉贊久了也會降低贊助的意願。 </p><h3 id="活動"><a href="#活動" class="headerlink" title="活動"></a>活動</h3><p>活動組則是整個宿營的核心，不論是遊戲的細節、晚會的流程、宿營三天下來的時間安排……所有的細節都需要考慮到。一般都會先跟學長姐拿他們那一屆的企劃書回來參考，看是要稍微改內容(反正下一屆也沒玩過)或是整個大改都是可以的。</p><p>每個項目都一定要有一個負責人，再給負責人幾個人一起工作。 活動組的開會時間要是最頻繁的，通常給個固定時間告知要開會比較方便盯進度。各項活動企劃書出來後才能讓美宣可以做道具跟驗關。 </p><p>當時有個問題就是:先場勘在完成企劃書或是先弄出企劃書在去場勘?地點一開始定在虎頭埤時因為我們完全不熟所以先去場勘過了，再來制定內容;後來地點改到澄清湖時我們因為去年自己就是半在澄清湖，比較熟悉後就是先完成遊戲內容之後再去現場做安排。 然後場勘強烈建議活動的負責人至少要有一個去才知道要怎麼安排。 </p><h3 id="總務"><a href="#總務" class="headerlink" title="總務"></a>總務</h3><p>就…收錢阿???所有花到的錢都請同學們先留著發票代墊，宿營結束再統一請款，然後統計學弟妹去的人數，根據人數跟拉贊金額去估說每人要繳多少，我們大二的除了學弟妹繳的錢以外每人又多出了不少，所以拉贊還是很重要的，能拉多少就盡量拉、努力拉… </p><h3 id="美宣"><a href="#美宣" class="headerlink" title="美宣"></a>美宣</h3><p>美宣應該是除了活動最辛苦的，包辦了所有宿營需要的道具跟器材準備。在製作道具的過程中最好是請不是美宣的也一起來幫忙，不然太累人了。學弟妹的名牌以及小冊子、關主的名牌，遊戲的道具……等，最後都準備完後以一個活動一箱全部封在一起，才不會搞混。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>[UVA10487]Closest Sums</title>
      <link href="/posts/c350afbc/"/>
      <url>/posts/c350afbc/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Given is a set of integers and then a sequence of queries. A query gives you a number and asks to ﬁnd a sum of two distinct numbers from the set, which is closest to the query number.<br><a id="more"></a></p></blockquote><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>Input contains multiple cases. Each case starts with an integer n (1 &lt; n ≤ 1000), which indicates, how many numbers are in the set of integer. Next n lines contain n numbers. Of course there is only one number in a single line. </p><p>The next line contains a positive integer m giving the number of queries, 0 &lt; m &lt; 25. The next m lines contain an integer of the query, one per line. </p><p>Input is terminated by a case whose n = 0. Surely, this case needs no processing. </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>Output should be organized as in the sample below. For each query output one line giving the query value and the closest sum in the format as in the sample. Inputs will be such that no ties will occur. </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><p>5 3 12 17 33 34 3 1 51 30 3 1 2 3 3 1 2 3 3 1 2 3 3 4 5 6 0 </p><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Case 1: Closest sum to 1 is 15. Closest sum to 51 is 51. Closest sum to 30 is 29. Case 2: Closest sum to 1 is 3. Closest sum to 2 is 3. Closest sum to 3 is 3. Case 3: Closest sum to 4 is 4. Closest sum to 5 is 5. Closest sum to 6 is 5.</span><br></pre></td></tr></table></figure><p>題意: 給你一些數字兩兩相加組成一個集合，再給你指定的數字，要你找出集合中離指定數字最接近的數。 想法:走訪所有元素相加做排序判斷即可。 </p><p>第一次解這題的時候就直接暴力解了，後來用了Binary Search重寫了一次。 </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//使用Java的考生請注意，最外層的類別(class)需命名為 main 。 </span></span><br><span class="line"><span class="comment">//如果遇到超乎想像的狀況，請更改編譯器試看看!! 各編譯器特性不同!! </span></span><br><span class="line"><span class="comment">//預設測資、隨機測資、固定測資是用來幫助除錯用的。批改時，只看暗中測資是否通過!! </span></span><br><span class="line">#‎include‬ &lt;iostream&gt;; </span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;; </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> all_ans[<span class="number">1000005</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">binary_search</span><span class="params">(<span class="keyword">int</span> (&amp;arr)[<span class="number">1000005</span>] , <span class="keyword">int</span> count , <span class="keyword">int</span> <span class="built_in">find</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> upper = count - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> lower = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(lower &lt;= upper)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> mid = (upper + lower) / <span class="number">2</span>;</span><br><span class="line">        <span class="comment">//printf("* %d %d\n",arr[mid],find);</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">find</span> &gt; arr[mid]) lower = mid + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(<span class="built_in">find</span> &lt; arr[mid]) upper = mid - <span class="number">1</span>; </span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> arr[mid]; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="comment">//printf("-&gt;%d %d\n",lower,upper);</span></span><br><span class="line">    <span class="keyword">if</span>(upper &lt; <span class="number">0</span>) <span class="keyword">return</span> arr[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(lower &gt; count - <span class="number">1</span>) <span class="keyword">return</span> arr[ count - <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="built_in">abs</span>(arr[lower]-<span class="built_in">find</span>) &gt; <span class="built_in">abs</span>(arr[upper]-<span class="built_in">find</span>) ? arr[upper] : arr[lower]; </span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n,m;</span><br><span class="line">    <span class="keyword">int</span> num = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> arr[<span class="number">1005</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;amp;n) == <span class="number">1</span> &amp;amp;&amp;amp; n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Case %d:\n"</span>,num++);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; n ; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;arr[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; n ; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = i+<span class="number">1</span> ; j &lt; n ; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                all_ans[count++] = arr[i] + arr[j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        sort(all_ans+<span class="number">0</span>,all_ans+count);</span><br><span class="line">        <span class="comment">/*for(int i = 0 ; i &lt; count ; ++i)</span></span><br><span class="line"><span class="comment">&#123;</span></span><br><span class="line"><span class="comment">printf("%d ",all_ans[i]);</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">printf("\n");*/</span></span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;m);</span><br><span class="line">        <span class="keyword">while</span>(m--)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> query;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;query);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"Closest sum to %d is %d.\n"</span>,query,binary_search(all_ans,count,query));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CPE考試心得</title>
      <link href="/posts/3fd7abf6/"/>
      <url>/posts/3fd7abf6/</url>
      
        <content type="html"><![CDATA[<p>(最後更新時間: 20160322)</p><h2 id="CPE考試心得-3-22更新"><a href="#CPE考試心得-3-22更新" class="headerlink" title="[CPE考試心得] - 3/22更新"></a>[CPE考試心得] - 3/22更新</h2><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/01/e693b7e58f964.png" alt="擷取"> </p><p>我在大一的下學期開始考CPE，在此之前我並沒有任何經驗(參加CPE的時間點比我接觸競技程式更早)所以我也不知道該往什麼方向準備，就這樣懵懵懂懂的上了，然後當然就被電慘了xD   </p><p>現在雖然也是很不起眼，但多少有點心得:  CPE的題目是按造難易度去分的，簡單的都會在前面，但也沒有一定，有時候有些簡單題也會出現在後半段的題目，所以還是可以大致瀏覽後再開始動工。一般來說有一題一定是<a href="https://cpe.cse.nsysu.edu.tw/environment.php#starList" target="_blank" rel="noopener">一顆星選題</a>，也就是說，把題目都做過一次後理論上應該是可以對一題以上的。 </p><a id="more"></a><p>按照題目敘述打就可以解決的問題大約是前兩題，第三題開始就是要稍微思考一些寫法了(資料結構、演算法……)所以就開始有困難度了。   </p><p>考試的時候，由於題目是全英文，如果又遇到很長的題目常常會卡很多時間在讀題，所以有時候先看題目的範例輸入跟輸出來判斷這是在做什麼，能節省蠻多的時間。   </p><p>然後CPE有一些怪怪(?)的功能，像電子辭典、C++函示庫……有時候還蠻方便的。   </p><p>剩下的說明我之前在巴哈上有看到一位寫的<a href="http://home.gamer.com.tw/creationDetail.php?sn=2192261" target="_blank" rel="noopener">心得文章</a>，覺得很詳細 考試前該準備、注意的都有了，可以參考看看</p><h3 id="第一次考試-3-24"><a href="#第一次考試-3-24" class="headerlink" title="第一次考試(3/24)"></a>第一次考試(3/24)</h3><p>雖然說有題會從一星選題中出，但考的時候還是沒全部寫完 抱著「沒這麼雖吧」的心態去考了，結果還真的出我沒做過的題目…  </p><p>考試前的練習 看到第一題Hello World還愣了一下 然後花了30秒把它寫出來還蠻有信心的，誰知之後的時間角度跟DNA-Sorting就卡住了。 </p><p>考試開始後雖然沒看過，但是第一題還是在不知道「degree是什麼意思」(題目好像是9’s….)的情況下努力做出來了，並且預設測資都過了，但暗中測資就是過不了哀哀。 </p><p>第二題忘記是什麼，不過也是測資都過了暗中測資卻過不了，當下真的很沮喪阿，第一次的3小時就這樣無功而返。 </p><h3 id="第二次考試-5-26"><a href="#第二次考試-5-26" class="headerlink" title="第二次考試(5/26)"></a>第二次考試(5/26)</h3><p>第一次考完後，抱著一定要再把一星題做完的心態度過了這三個月，但最終還是沒做完(不~~)。</p><p>不過出乎意料的，這次的題目難度似乎沒有上次那麼難，第一題是很簡單的loop。 </p><p>第二題的時候手太急沒按到看題就直接解了，一度還看不懂題目再說甚麼，後來才知道是單純的排序找答案，解完花了不少時間。 </p><p>第三題是陣列比對，只要換個角度去想，用對角線來找大小就能解出來，覺得題意了解是很重要的。 </p><p>寫完三題才花了8X分鐘整個很興奮，看了第四題，要用Linked List去模擬佇列，但我沒有實際實作過，最後就在部分測資過的了部分過不了的情況下放棄了。 </p><p>最後看了看成績，44名，耶~通過畢業門檻了! </p><h3 id="第三次考試-10-6"><a href="#第三次考試-10-6" class="headerlink" title="第三次考試(10/6)"></a>第三次考試(10/6)</h3><p>這次考試原本是頗有信心的，畢竟暑假也稍微特訓過了，結果……。 </p><p>題外話，這是考試的時候，系統一堆問題，搞得無法準時開始，有點糟糕。 </p><p>不得不說這次的第一題難度真的比二、三題難: 找一個字串的最大子字串，這題以前做過，是被歸類為兩顆星難度的題目，第二題的字串反轉還簡單多了。第三題則是數學問題，求最大效益的加班費分配方式，我當下一直想到Greedy……</p><p>後來看到解法只需要兩個Sort時超不甘心的，不然就三題了。 第四題就真的沒辦法了，雖然看得懂題目但完全沒想法。 </p><p>之前暑訓交到的中正同學這次四題，真的覺得他好猛……。</p><h3 id="第四次考試-12-22"><a href="#第四次考試-12-22" class="headerlink" title="第四次考試(12/22)"></a>第四次考試(12/22)</h3><p>這次考試…先說總結，幹，超幹的。 </p><p>學校的考試環境真的爛到一個爆炸，上次被雷過連這次也被雷，正式開始的第一題是很簡單的題目，把所有位數加起來直到位數等於一就是答案了，然後我居然卡了40分鐘!!一TLE，不斷的Debug到最後只剩一行gets()也無法過，最後很不耐煩地把所有可能都試了，弄成cin才能過(回家後自己重寫了一遍，確定我當下的寫法是可行的)。 </p><p>第二題枚舉所有的可能再作排序即可，當時覺得數量太多可能會爆炸，好險沒有。 </p><p>第三題是要模擬七段顯示器，這題當時真的卡到我了，想不出辦法只好用最笨的方法，不斷的去switch所有情形…但是前面的時間卡太多了，判斷沒時間做完就放棄了。 </p><p>第六題則是先透過線性篩法+DP先把資料存完，然後再比對即可，但這題當時只剩15分鐘，可能太緊張所以有哪部分寫錯了並沒有過。 </p><p>最後只過兩題，真的很不甘心……哀哀下次可能要換學校考了。 </p><h3 id="第五次考試-3-22"><a href="#第五次考試-3-22" class="headerlink" title="第五次考試(3/22)"></a>第五次考試(3/22)</h3><p>這次仍舊選了在自己學校考，抱著沒有那麼雷吧?的心態，但是仍然還是被小雷了一下(測試環境期間整間教室的系統是死當的，所以根本沒有寫道練習題也無法測試環境，還好環境是可以正常使用的)，然後第二題的時候系統又當了一次，又花了些許時間重寫第二題不過後面就沒雷我這點十分慶幸。 </p><p>這次總共過了五題，很開心自己的題數有所進步外，也覺得這次的難易度有點偏易: </p><p>第一題根本水題，要算馬力歐總共往上跳了幾次往下跳了幾次，設兩個變數去存值即可。 </p><p>第二題字串轉換，按照鍵盤上的格式往左移一個輸出該字元，一開始先把表建好去做Search即可。 </p><p>第三題要找Ascii出現的頻率，建個表去存每個字出現的次數以及頻率最大值，然後由後往前搜並依序印出與最大值相同的值即可。 </p><p>第四題質數問題，已經好幾次考試都有出質數問題了，看了一下N的範圍感覺一般做法也不會爆，不過我還是覺得線性塞法學起來後真的很方便，寫法也很容易上手。</p><p>這題稍微再邏輯上卡了一下，想說要如何依據奇數和偶數的長度去找中心點……當下先用DP做了一次長度計算，再去找prime[長度/2] 就會是中心點，把往前跟後面各c個輸出即可。 </p><p>第五題大數運算(大數擺第五題??)就大數…每次看到這種題目都會好想去學java，根本秒殺。不會用java的我在這題上debug了不少時間(太久沒寫大數了..)，所以後面沒什麼時間做其他題了。   </p><p>總結起來這次的難度真的降低不少，不過答對這麼多題還是很開心的，繼續努力演算法或許之後可以有更好的成績。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> competitive programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA11389]The Bus Driver Problem</title>
      <link href="/posts/3b0c2151/"/>
      <url>/posts/3b0c2151/</url>
      
        <content type="html"><![CDATA[<blockquote><p>In a city there are n bus drivers. Also there are n morning bus routes and n afternoon bus routes with various lengths. Each driver is assigned one morning route and one evening route. For any driver, if his total route length for a day exceeds d, he has to be paid overtime for every hour after the ﬁrst d hours at a ﬂat r taka / hour. </p><p>Your task is to assign one morning route and one evening route to each bus driver so that the total overtime amount that the authority has to pay is minimized.<br><a id="more"></a></p></blockquote><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>The ﬁrst line of each test case has three integers n, d and r, as described above. In the second line, there are n space separated integers which are the lengths of the morning routes given in meters. Similarly the third line has n space separated integers denoting the evening route lengths. The lengths are positive integers less than or equal to 10000. The end of input is denoted by a case with three 0’s. </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each test case, print the minimum possible overtime amount that the authority must pay. Constraints • 1 ≤ n ≤ 100 • 1 ≤ d ≤ 10000 • 1 ≤ r ≤ 5 </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><p>2 20 5 10 15 10 15 2 20 5 10 10 10 10 0 0 0 </p><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><h2 id="50-0"><a href="#50-0" class="headerlink" title="50 0"></a>50 0</h2><p>做個排序比大小即可，排序完就是最佳解。 </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">#‎include‬ &lt;iostream&gt;; </span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;; </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">int</span> n = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">int</span> d = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">int</span> r = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d %d %d"</span>, &amp;n, &amp;d, &amp;r)&amp;&amp;(n&amp;&amp;d&amp;&amp;r)) &#123; </span><br><span class="line">    <span class="keyword">int</span> morn[<span class="number">101</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line">    <span class="keyword">int</span> even[<span class="number">101</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; n ; ++i) <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;morn[i]); </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; n ; ++i) <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;even[i]); </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    printf("%d %d %d\n",n,d,r); </span></span><br><span class="line"><span class="comment">    for(int i = 0 ; i &lt; n ; ++i) printf("%d ",morn[i]); printf("\n"); for(int i = 0 ; i &lt; n ; ++i) printf("%d ",even[i]); printf("\n");</span></span><br><span class="line"><span class="comment">    */</span> </span><br><span class="line">    sort(morn+<span class="number">0</span>,morn+n); </span><br><span class="line">    sort(even+<span class="number">0</span>,even+n); </span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>; </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i <span class="number">0</span>) &#123; </span><br><span class="line">      sum += ( (morn[i] + even[(n-i)<span class="number">-1</span>])-d ) * r; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="comment">//printf("%d %d %d\n",i,(morn[i] + even[(n-i)-1])-d,sum); </span></span><br><span class="line">  &#125; </span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,sum); </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA10533]Digit</title>
      <link href="/posts/a4456a87/"/>
      <url>/posts/a4456a87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>A prime number is a positive number, which is divisible by exactly two diﬀerent integers. </p><p>A digit prime is a prime number whose sum of digits is also prime. For example the prime number 41 is a digit prime because 4 + 1 = 5 and 5 is a prime number. 17 is not a digit prime because 1 + 7 = 8, and 8 is not a prime number. </p><p>In this problem your job is to ﬁnd out the number of digit primes within a certain range less than 1000000.<br><a id="more"></a></p></blockquote><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>First line of the input ﬁle contains a single integer N (0 &lt; N ≤ 500000) that indicates the total number of inputs. Each of the next N lines contains two integers t1 and t2 (0 &lt; t1 ≤ t2 &lt; 1000000). </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each line of input except the ﬁrst line produce one line of output containing a single integer that indicates the number of digit primes between t1 and t2 (inclusive). </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><p>3 10 20 10 100 100 10000 </p><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><p>1 10 576 </p><p>Note: You should at least use scanf() and printf() to take input and produce output for this problem. cin and cout is too slow for this problem to get it within time limit. </p><p>透過線性篩法劍質數表，再透過DP存即可。   </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//使用Java的考生請注意，最外層的類別(class)需命名為 main 。 </span></span><br><span class="line"><span class="comment">//如果遇到超乎想像的狀況，請更改編譯器試看看!! 各編譯器特性不同!! </span></span><br><span class="line"><span class="comment">//預設測資、隨機測資、固定測資是用來幫助除錯用的。批改時，只看暗中測資是否通過!! </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;; </span></span></span><br><span class="line"><span class="keyword">bool</span> prime[<span class="number">1000005</span>]; </span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1000005</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sieve</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; <span class="number">1000005</span> ; ++i) &#123; </span><br><span class="line">    prime[i] = <span class="literal">true</span>; </span><br><span class="line">  &#125; </span><br><span class="line">  prime[<span class="number">0</span>] = <span class="literal">false</span>; </span><br><span class="line">  prime[<span class="number">1</span>] = <span class="literal">false</span>; </span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span> ; i &lt;= <span class="number">1000</span> ; ++i) &#123; </span><br><span class="line">    <span class="keyword">if</span>(!prime[i]) <span class="keyword">continue</span>; </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">2</span> ; i * j &lt;= <span class="number">1000005</span> ; ++j) &#123; </span><br><span class="line">      prime[i * j ] = <span class="literal">false</span>; </span><br><span class="line">    &#125; </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  for(int i = 0 ; i &lt; 10000 ; ++i) &#123; </span></span><br><span class="line"><span class="comment">    if(prime[i]) printf("%d ",i); </span></span><br><span class="line"><span class="comment">  &#125;*/</span> </span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">check</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span> ; i &lt; <span class="number">1000005</span> ; ++i) &#123; </span><br><span class="line">    dp[i] = dp[i<span class="number">-1</span>]; </span><br><span class="line">    <span class="keyword">if</span>(!prime[i])<span class="keyword">continue</span>; </span><br><span class="line">    <span class="comment">//printf("* %d\\n",i); </span></span><br><span class="line">    <span class="keyword">char</span> temp[<span class="number">7</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>; </span><br><span class="line">    <span class="built_in">sprintf</span>(temp,<span class="string">"%d"</span>,i); </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; <span class="built_in">strlen</span>(temp) ; ++i) &#123; </span><br><span class="line">      sum += temp[i] - <span class="string">'0'</span>; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="comment">//printf("sum %d\\n",sum); </span></span><br><span class="line">    <span class="keyword">if</span>(prime[sum]) &#123; </span><br><span class="line">      dp[i]++; </span><br><span class="line">      <span class="comment">//printf("dp %d i %d\\n",dp[i],i); </span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  sieve(); </span><br><span class="line">  check(); </span><br><span class="line">  <span class="keyword">int</span> n = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">int</span> a1,a2; </span><br><span class="line">  <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n); </span><br><span class="line">  <span class="keyword">while</span>(n--) &#123; </span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>,&amp;a1,&amp;a2); </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\\n"</span>,dp[a2]- dp[a1<span class="number">-1</span>]); </span><br><span class="line">  &#125; </span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA706]LC-Display</title>
      <link href="/posts/8625bafd/"/>
      <url>/posts/8625bafd/</url>
      
        <content type="html"><![CDATA[<p>A friend of you has just bought a new computer. Until now, the most powerful computer he ever used has been a pocket calculator. Now, looking at his new computer, he is a bit disappointed, because he liked the LC-display of his calculator so much. So you decide to write a program that displays numbers in an LC-display-like style on his computer.</p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>The input file contains several lines, one for each number to be displayed. Each line contains two integers <em>s</em>, <em>n</em> ( <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://uva.onlinejudge.org/external/7/706img1.gif" alt="$1 \le s \le 10, 0 \le n \le 99\,999\,999$">), where <em>n</em> is the number to be displayed and <em>s</em> is the size in which it shall be displayed. The input file will be terminated by a line containing two zeros. This line should not be processed. </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>Output the numbers given in the input file in an LC-display-style using <em>s</em> <code>&lt;tt&gt;-&lt;/tt&gt;&#39;&#39; signs for the horizontal segments and &lt;i&gt;s&lt;/i&gt;</code>|’’ signs for the vertical ones. Each digit occupies exactly <em>s</em>+2 columns and 2<em>s</em>+3 rows. (Be sure to fill all the white space occupied by the digits with blanks, also for the last digit.) There has to be exactly one column of blanks between two digits. Output a blank line after each number. (You will find a sample of each digit in the sample output.)</p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2 12345</span><br><span class="line">3 67890</span><br><span class="line">0 0</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">      --   --        -- </span><br><span class="line">   |    |    | |  | |   </span><br><span class="line">   |    |    | |  | |   </span><br><span class="line">      --   --   --   -- </span><br><span class="line">   | |       |    |    |</span><br><span class="line">   | |       |    |    |</span><br><span class="line">      --   --        -- </span><br><span class="line"></span><br><span class="line"> ---   ---   ---   ---   --- </span><br><span class="line">|         | |   | |   | |   |</span><br><span class="line">|         | |   | |   | |   |</span><br><span class="line">|         | |   | |   | |   |</span><br><span class="line"> ---         ---   ---       </span><br><span class="line">|   |     | |   |     | |   |</span><br><span class="line">|   |     | |   |     | |   |</span><br><span class="line">|   |     | |   |     | |   |</span><br><span class="line"> ---         ---   ---   ---</span><br></pre></td></tr></table></figure><hr><p>解法:想不出來，分段討論、暴力解題。</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">char</span> <span class="built_in">map</span>[<span class="number">23</span>+<span class="number">1</span>][<span class="number">12</span>+<span class="number">1</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">int</span> n = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">char</span> in[<span class="number">9</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line">  <span class="keyword">int</span> colfir = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">bool</span> rowfir = <span class="literal">true</span>; </span><br><span class="line">  <span class="keyword">bool</span> testfir = <span class="literal">true</span>; </span><br><span class="line">  <span class="keyword">while</span>(~<span class="built_in">scanf</span>(<span class="string">"%d %s"</span>,&amp;n,in) &amp;&amp; n &amp;&amp; in[<span class="number">0</span>]) &#123; </span><br><span class="line">    <span class="keyword">int</span> len = <span class="built_in">strlen</span>(in); </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; <span class="number">2</span> * n + <span class="number">3</span> ; ++i) &#123; </span><br><span class="line">      <span class="keyword">if</span>(i == <span class="number">0</span>) &#123; </span><br><span class="line">        colfir = len - <span class="number">1</span>; </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span> ,k = <span class="number">0</span>;j &lt; len * (<span class="number">2</span> + n) ; j += (<span class="number">2</span> + n),++k) &#123;</span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">          <span class="keyword">switch</span>(in[k]) &#123; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'0'</span>: </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'1'</span>: </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'2'</span>: </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'3'</span>: </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'4'</span>: </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'5'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'6'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'7'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'8'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'9'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">          &#125; </span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">        &#125; </span><br><span class="line">      &#125; </span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(i == <span class="number">2</span> * n + <span class="number">2</span>) &#123; </span><br><span class="line">        colfir = len - <span class="number">1</span> ; </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span> ,k = <span class="number">0</span>;j &lt; len * (<span class="number">2</span> + n) ; j += (<span class="number">2</span> + n),++k) &#123;</span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">          <span class="keyword">switch</span>(in[k]) &#123; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'0'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'1'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'2'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'3'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'4'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'5'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'6'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'7'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'8'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'9'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">          &#125; </span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">        &#125; </span><br><span class="line">      &#125; </span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(i == n + <span class="number">1</span>) &#123; </span><br><span class="line">        colfir = len - <span class="number">1</span>; </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span> ,k = <span class="number">0</span>;j &lt; len * (<span class="number">2</span> + n) ; j += (<span class="number">2</span> + n),++k) &#123;</span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">          <span class="keyword">switch</span>(in[k]) &#123; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'0'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'1'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'2'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'3'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'4'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'5'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'6'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'7'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'8'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'9'</span>:</span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">"-"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">          &#125; </span><br><span class="line">          <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">        &#125; </span><br><span class="line">      &#125; </span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(i == <span class="number">0</span>) &#123; </span><br><span class="line">        colfir = len <span class="number">-1</span>; </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span> ,k = <span class="number">0</span>;j &lt; len * (<span class="number">2</span> + n) ; j += (<span class="number">2</span> + n),++k) &#123; </span><br><span class="line">          <span class="keyword">switch</span>(in[k]) &#123; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'0'</span>: <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'1'</span>: <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'2'</span>: <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'3'</span>: <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'4'</span>: <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'5'</span>: <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'6'</span>: <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'7'</span>: <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'8'</span>: <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'9'</span>: <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">          &#125; </span><br><span class="line">        &#125; </span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">        colfir = len - <span class="number">1</span>; </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span> ,k = <span class="number">0</span>;j &lt; len * (<span class="number">2</span> + n) ; j += (<span class="number">2</span> + n),++k) &#123; </span><br><span class="line">          <span class="keyword">switch</span>(in[k]) &#123; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'0'</span>: </span><br><span class="line">              <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'1'</span>: </span><br><span class="line">              <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'2'</span>: </span><br><span class="line">              <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'3'</span>: </span><br><span class="line">              <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'4'</span>: </span><br><span class="line">              <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'5'</span>: </span><br><span class="line">              <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'6'</span>: </span><br><span class="line">              <span class="built_in">printf</span>(<span class="string">"|"</span>); <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) </span><br><span class="line">              <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'7'</span>: </span><br><span class="line">              <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'8'</span>: </span><br><span class="line">              <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">            <span class="keyword">case</span> <span class="string">'9'</span>: </span><br><span class="line">              <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; n ; ++k) <span class="built_in">printf</span>(<span class="string">" "</span>); <span class="built_in">printf</span>(<span class="string">"|"</span>); </span><br><span class="line">              <span class="keyword">if</span>(colfir--) <span class="built_in">printf</span>(<span class="string">" "</span>); </span><br><span class="line">              <span class="keyword">break</span>; </span><br><span class="line">          &#125; </span><br><span class="line">        &#125; </span><br><span class="line">      &#125; </span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"\n"</span>); </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n"</span>); </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA11332]Summing Digits</title>
      <link href="/posts/1534d28/"/>
      <url>/posts/1534d28/</url>
      
        <content type="html"><![CDATA[<p>For a positive integer n, let f(n) denote the sum of the digits of n when represented in base 10. </p><p>It is easy to see that the sequence of numbers n,f(n),f(f(n)),f(f(f(n))),… eventually becomes a single digit number that repeats forever. Let this single digit be denoted g(n). </p><p>For example, consider n = 1234567892. </p><p>Then: f(n) = 1+2+3+4+5+6+7+8+9+2 = 47 f(f(n)) = 4 + 7 = 11 f(f(f(n))) = 1 + 1 = 2 Therefore, g(1234567892) = 2. </p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>Each line of input contains a single positive integer n at most 2,000,000,000. </p><p>Input is terminated by n = 0 which should not be processed. </p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each such integer, you are to output a single line containing g(n). </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><p>2 11 47 1234567892 0 </p><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><p>2 2 2 2 </p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">char</span> s[<span class="number">11</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line">  <span class="keyword">int</span> len = <span class="number">0</span>; </span><br><span class="line">  <span class="keyword">while</span>(gets(s) &amp;&amp; s[<span class="number">0</span>] != <span class="string">'0'</span>) &#123; </span><br><span class="line">    <span class="comment">//puts(s); </span></span><br><span class="line">    len = <span class="built_in">strlen</span>(s); </span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>) &#123; </span><br><span class="line">      <span class="keyword">if</span>(len == <span class="number">1</span>) &#123; </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%s\n"</span>,s); </span><br><span class="line">        <span class="keyword">break</span>; </span><br><span class="line">      &#125; </span><br><span class="line">      <span class="keyword">int</span> temp = <span class="number">0</span>; </span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; len ; ++i) &#123; </span><br><span class="line">        temp += s[i] - <span class="string">'0'</span>; </span><br><span class="line">      &#125; </span><br><span class="line">      <span class="comment">//printf("temp %d\n",temp); </span></span><br><span class="line">      <span class="built_in">sprintf</span>(s,<span class="string">"%d"</span>,temp); </span><br><span class="line">      <span class="comment">//printf("* %s\n",s); </span></span><br><span class="line">      len = <span class="built_in">strlen</span>(s); </span><br><span class="line">      <span class="comment">//printf("* len %d\n",len); </span></span><br><span class="line">    &#125; </span><br><span class="line">    <span class="built_in">memset</span>(s,<span class="number">0</span>,<span class="keyword">sizeof</span>(s)); </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[PHP]網購網站-簡易留言板製作</title>
      <link href="/posts/1445c2b6/"/>
      <url>/posts/1445c2b6/</url>
      
        <content type="html"><![CDATA[<p>用PHP製作出簡易網路留言板</p><p>功能: </p><ol><li>顯示該會員的個人留言紀錄 </li><li>紀錄又分成買家跟賣家 </li><li>可以做出買家公告(對所有會員)</li></ol><p>做完大概長這個樣子:</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2016/01/e693b7e58f96.png" alt="擷取.PNG"></p><p>原理: 透過PHP連結到MySQL對資料庫做存取的動作，買家和賣家的背景圖案則是用PowerPoint做出很陽春的圖示樣板。<a id="more"></a></p><ol><li><p>先在MySQL建一個board的資料表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> <span class="string">`board`</span> (</span><br><span class="line"> <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">100</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"> <span class="string">`username`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">COLLATE</span> utf8_unicode_ci <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line"> <span class="string">`IsUserMsg`</span> <span class="built_in">tinyint</span>(<span class="number">1</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line"> <span class="string">`subject`</span> <span class="built_in">varchar</span>(<span class="number">100</span>) <span class="keyword">COLLATE</span> utf8_unicode_ci <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line"> <span class="string">`content`</span> <span class="built_in">text</span> <span class="keyword">COLLATE</span> utf8_unicode_ci,</span><br><span class="line"> <span class="string">`time`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span></span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">10</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COLLATE</span>=utf8_unicode_ci;</span><br></pre></td></tr></table></figure><p>對應欄位資料</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">id -&gt;留言編號</span><br><span class="line">username -&gt;會員帳號</span><br><span class="line">IsUserMsg -&gt;用來判斷會員或官方</span><br><span class="line">subject -&gt;主題</span><br><span class="line">content -&gt;內容</span><br><span class="line">time -&gt;留言時間</span><br></pre></td></tr></table></figure></li><li><p>輸出留言訊息</p><figure class="highlight php"><table><tr><td class="code"><pre><span class="line">$results = mysql_query(<span class="string">"SELECT * FROM `board` WHERE `username` = '"</span>.$_SESSION[<span class="string">"username"</span>]. <span class="string">"' OR `username` ='admin' ORDER BY `time` DESC"</span>); </span><br><span class="line"><span class="meta">&lt;?php</span> </span><br><span class="line"><span class="keyword">while</span>($row_RecBoard=mysql_fetch_assoc($results))&#123; </span><br><span class="line">  <span class="keyword">if</span>($row_RecBoard[<span class="string">'IsUserMsg'</span>])&#123; <span class="meta">?&gt;</span> </span><br><span class="line">  &lt;div id = <span class="string">"member_msg"</span>&gt; </span><br><span class="line">    &lt;p style=<span class="string">"color:black;font-family:'微軟正黑體';text-align:left;font-size:24px;"</span>&gt;<span class="meta">&lt;?php</span> <span class="keyword">echo</span> $row_RecBoard[<span class="string">'subject'</span>]; <span class="meta">?&gt;</span>&lt;/p&gt; </span><br><span class="line">    <span class="meta">&lt;?php</span> <span class="keyword">echo</span> $row_RecBoard[<span class="string">'content'</span>];<span class="meta">?&gt;</span> &lt;p style=<span class="string">"text-align:right"</span>&gt;<span class="meta">&lt;?php</span> <span class="keyword">echo</span> $row_RecBoard[<span class="string">'time'</span>].<span class="string">""</span>;<span class="meta">?&gt;</span>&lt;/p&gt; </span><br><span class="line">  &lt;/div&gt; </span><br><span class="line"><span class="meta">&lt;?php</span> &#125; <span class="keyword">else</span>&#123; <span class="meta">?&gt;</span> </span><br><span class="line">  &lt;div id = <span class="string">"admin_msg"</span>&gt; </span><br><span class="line">    &lt;p style=<span class="string">"color:black;font-family:'微軟正黑體';text-align:left;font-size:24px;"</span>&gt;<span class="meta">&lt;?php</span> <span class="keyword">echo</span> $row_RecBoard[<span class="string">'subject'</span>]; <span class="meta">?&gt;</span>&lt;/p&gt; </span><br><span class="line">    <span class="meta">&lt;?php</span> <span class="keyword">echo</span> $row_RecBoard[<span class="string">'content'</span>];<span class="meta">?&gt;</span> &lt;p style=<span class="string">"text-align:right"</span>&gt;<span class="meta">&lt;?php</span> <span class="keyword">echo</span> $row_RecBoard[<span class="string">'time'</span>].<span class="string">""</span>;<span class="meta">?&gt;</span>&lt;/p&gt; </span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">  &lt;hr /&gt;</span><br><span class="line"><span class="meta">&lt;?php</span>  &#125; <span class="comment">//while <span class="meta">?&gt;</span></span></span><br></pre></td></tr></table></figure></li></ol><p>mysql語法中username = $_SESSION[“username”] ，是因為在已經登入的情形下才能存取我的留言版，所以我直接用session去比對username即可。 按照時間排序去讀取資料表的內容後，只將自己帳號的和admin(公告)的內容取出來，然後透過IsUserMsg去判斷是買家的留言或是賣家的留言，並搭配不同的背景圖片就可以完成瞜!</p><ol><li>新增留言訊息</li></ol><figure class="highlight php"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(<span class="keyword">isset</span>($_POST[<span class="string">"action"</span>])&amp;&amp;($_POST[<span class="string">"action"</span>]==<span class="string">"add"</span>))&#123;</span><br><span class="line">    $sql_query = <span class="string">"INSERT INTO `board` (`id` ,`username` ,`IsUserMsg` ,`subject` ,`content` ,`time`) VALUES ("</span>;</span><br><span class="line">    $sql_query .= <span class="string">"NULL,"</span>;</span><br><span class="line">    $sql_query .= <span class="string">"'"</span> .$_SESSION[<span class="string">"username"</span>]. <span class="string">"', 1 ,"</span>;</span><br><span class="line">    $sql_query .= <span class="string">"'"</span> .$_POST[<span class="string">"subject"</span>]. <span class="string">"',"</span>;</span><br><span class="line">    $sql_query .= <span class="string">"'"</span> .$_POST[<span class="string">"boardcontent"</span>]. <span class="string">"' ,NOW() );"</span>;</span><br><span class="line">    <span class="keyword">if</span>(!mysql_query($sql_query))<span class="keyword">die</span>(<span class="string">"留言失敗！"</span>);</span><br><span class="line">    <span class="comment">//重新導向回到主畫面</span></span><br><span class="line">    header(<span class="string">"Location: board.php"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然後新增一個表格</p><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">action</span> = <span class="string">""</span> <span class="attr">method</span> = <span class="string">"POST"</span> <span class="attr">name</span> = <span class="string">"add"</span> <span class="attr">id</span> = <span class="string">"add"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">table</span> <span class="attr">border</span>=<span class="string">"5"</span> <span class="attr">width</span>=<span class="string">"70%"</span> <span class="attr">cellspacing</span>=<span class="string">"1"</span> <span class="attr">align</span>=<span class="string">"center"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span> <span class="attr">bgcolor</span>=<span class="string">"#B8B8FF"</span>&gt;</span><span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"subject"</span>&gt;</span>主題<span class="tag">&lt;/<span class="name">label</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span> <span class="attr">bgcolor</span>=<span class="string">"#F5F5F5"</span>&gt;</span><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"subject"</span> <span class="attr">id</span>=<span class="string">"subject"</span> <span class="attr">size</span>=<span class="string">"49"</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span> <span class="attr">bgcolor</span>=<span class="string">"#B8B8FF"</span>&gt;</span><span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"boardcontent"</span>&gt;</span>內容<span class="tag">&lt;/<span class="name">label</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span> <span class="attr">bgcolor</span>=<span class="string">"#F5F5F5"</span>&gt;</span><span class="tag">&lt;<span class="name">textarea</span> <span class="attr">name</span>=<span class="string">"boardcontent"</span> <span class="attr">id</span>=<span class="string">"boardcontent"</span> <span class="attr">cols</span>=<span class="string">"50"</span> <span class="attr">rows</span>=<span class="string">"10"</span>&gt;</span><span class="tag">&lt;/<span class="name">textarea</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">td</span> <span class="attr">bgcolor</span>=<span class="string">"#F5F5F5"</span> <span class="attr">colspan</span> = <span class="string">"2"</span> <span class="attr">align</span> = <span class="string">"center"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span> = <span class="string">"hidden"</span> <span class="attr">name</span> = <span class="string">"action"</span> <span class="attr">value</span> = <span class="string">"add"</span> &gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"submit"</span> <span class="attr">value</span>=<span class="string">"留言"</span> <span class="attr">align</span>=<span class="string">"center"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"reset"</span> <span class="attr">value</span>=<span class="string">"重新設定"</span> <span class="attr">align</span>=<span class="string">"center"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在PHP中要如何與HTML的傳送的表單資料去做連結呢?方法就是透過<br><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span> = <span class="string">"hidden"</span> <span class="attr">name</span> = <span class="string">"action"</span> <span class="attr">value</span> = <span class="string">"add"</span> &gt;</span></span><br></pre></td></tr></table></figure><br>設置一個hidden(隱藏)屬性的欄位，當我按下submit按鈕後，如我form的action沒有填上網址就會回傳自己這個網址，此時這個欄位的資料也會一併被送出，所以就可以靠檢查這個欄位來與php做連結搂!<br>一樣的，因為我是在已經登入完成的情形下去做新增，所以<br><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$sql_query .= "'" .$_SESSION["username"]. "', 1 ,";</span><br></pre></td></tr></table></figure><br>已經有了username的資料了，不用另外做輸入，時間的部分透過NOW()直接取得目前時間。</p><p>留言板的部分就到這裡就大功告成了，你問我圖案怎麼做?開啟PPT然後圖案拉一拉、顏色換一換摟~</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> php </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA272]TEX Quotes</title>
      <link href="/posts/59464f58/"/>
      <url>/posts/59464f58/</url>
      
        <content type="html"><![CDATA[<blockquote><p>TeX is a typesetting language developed by Donald Knuth. It takes source text together with a few typesetting instructions and produces, one hopes, a beautiful document. </p><p>Beautiful documents use `` and “ to delimit quotations, rather than the mundane “ which is what is provided by most keyboards.</p><p>Keyboards typically do not have an oriented double-quote, but they do have a left-single-quote ` and a right-single-quote ‘. Check your keyboard now to locate the left-single-quote key ` (sometimes called the ``backquote key”) and the right-single-quote key ‘ (sometimes called the ``apostrophe” or just ``quote”). </p><p>Be careful not to confuse the left-single-quote ` with the ``backslash” key. TeX lets the user type two left-single-quotes `` to create a left-double-quote `` and two right-single-quotes ‘’ to create a right-double-quote ‘’. Most typists, however, are accustomed to delimiting their quotations with the un-oriented double-quote “.</p><p>If the source contained</p><p>“To be or not to be,” quoth the bard, “that is the question.”</p><p>then the typeset document produced by TeX would not contain the desired form:</p><p>``To be or not to be,” quoth the bard, ``that is the question.”</p><p>In order to produce the desired form, the source file must contain the sequence:</p><p>``To be or not to be,’’ quoth the bard, ``that is the question.’’</p><p>You are to write a program which converts text containing double-quote (“) characters into text that is identical except that double-quotes have been replaced by the two-character sequences required by TeX for delimiting quotations with oriented double-quotes. The double-quote (“) characters should be replaced appropriately by either `` if the “ opens a quotation and by ‘’ if the “ closes a quotation. Notice that the question of nested quotations does not arise: The first “ must be replaced by ``, the next by ‘’, the next by ``, the next by ‘’, the next by ``, the next by ‘’, and so on.</p><p><h2><span style="color:#0070e8;"><a name="SECTION0001001000000000000000"></a>Input and Output</span></h2><br>Input will consist of several lines of text containing an even number of double-quote (“) characters. Input is ended with an end-of-file character. The text must be output exactly as it was input except that:</p><ul><li>he first “ in each pair is replaced by two ` characters: `` and</li><li>the second “ in each pair is replaced by two ‘ characters: ‘’.</li></ul><p><h2><span style="color:#0070e8;"><a name="SECTION0001002000000000000000"></a>Sample Input</span></h2></p><pre>"To be or not to be," quoth the Bard, "thatis the question".The programming contestant replied: "I must disagree.To `C' or not to `C', that is The Question!"</pre><h2><span style="color:#0070e8;"><a name="SECTION0001003000000000000000"></a>Sample Output</span></h2><pre>``To be or not to be,'' quoth the Bard, ``thatis the question''.The programming contestant replied: ``I must disagree.To `C' or not to `C', that is The Question!''</pre></blockquote><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span>  <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">//freopen("in.txt","r",stdin);</span></span><br><span class="line">  <span class="keyword">char</span> c ;</span><br><span class="line">  <span class="keyword">int</span> n = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span>(c = getchar() ) &#123;</span><br><span class="line">    <span class="keyword">if</span>(c == EOF) <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">if</span>(c == <span class="string">'"'</span> &amp;&amp; n % <span class="number">2</span> == <span class="number">1</span>) &#123;</span><br><span class="line">      n++;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"''"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(c == <span class="string">'"'</span> &amp;&amp; n % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">      n++;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"``"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"%c"</span>,c);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA476]Points in Figures: Rectangles</title>
      <link href="/posts/627bdd0d/"/>
      <url>/posts/627bdd0d/</url>
      
        <content type="html"><![CDATA[<p>Given a list of rectangles and a list of points in the <em>x</em>-<em>y</em> plane, determine for each point which figures (if any) contain the point.</p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>There will be <em>n</em>( <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://uva.onlinejudge.org/external/4/476img1.gif" alt="tex2html_wrap_inline220"> ) rectangles descriptions, one per line. The first character will designate the type of figure (``r’’ for rectangle). This character will be followed by four real values designating the <em>x</em>-<em>y</em> coordinates of the upper left and lower right corners. The end of the list will be signalled by a line containing an asterisk in column one. The remaining lines will contain the <em>x</em>-<em>y</em> coordinates, one per line, of the points to be tested. The end of this list will be indicated by a point with coordinates 9999.9 9999.9; these values should not be included in the output. Points coinciding with a figure border are not considered inside.</p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each point to be tested, write a message of the form:</p><p>Point i is contained in figure j</p><p>for each figure that contains that point. If the point is not contained in any figure, write a message of the form:</p><p>Point i is not contained in any figure</p><p>Points and figures should be numbered in the order in which they appear in the input.</p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">r 8.5 17.0 25.5 -8.5</span><br><span class="line">r 0.0 10.3 5.5 0.0</span><br><span class="line">r 2.5 12.5 12.5 2.5</span><br><span class="line">*</span><br><span class="line">2.0 2.0</span><br><span class="line">4.7 5.3</span><br><span class="line">6.9 11.2</span><br><span class="line">20.0 20.0</span><br><span class="line">17.6 3.2</span><br><span class="line">-5.2 -7.8</span><br><span class="line">9999.9 9999.9</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Point 1 is contained in figure 2</span><br><span class="line">Point 2 is contained in figure 2</span><br><span class="line">Point 2 is contained in figure 3</span><br><span class="line">Point 3 is contained in figure 3</span><br><span class="line">Point 4 is not contained in any figure</span><br><span class="line">Point 5 is contained in figure 1</span><br><span class="line">Point 6 is not contained in any figure</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">#‎include‬ &lt;iostream&gt;; </span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;; </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">cood</span> &#123;</span> </span><br><span class="line">  <span class="keyword">float</span> lx; </span><br><span class="line">  <span class="keyword">float</span> ly; </span><br><span class="line">  <span class="keyword">float</span> rx; </span><br><span class="line">  <span class="keyword">float</span> ry; </span><br><span class="line">&#125;; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  freopen(<span class="string">"in.txt"</span>,<span class="string">"r"</span>,<span class="built_in">stdin</span>); </span><br><span class="line">  <span class="keyword">char</span> c = <span class="number">0</span>; </span><br><span class="line">  cood p; </span><br><span class="line">  <span class="built_in">vector</span>&lt;cood&gt;; r; </span><br><span class="line">  <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%c"</span>,&amp;c) == <span class="number">1</span> &amp;&amp; c != <span class="string">'*'</span>) &#123; </span><br><span class="line">    <span class="keyword">float</span> templx,temply,temprx,tempry; </span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%f %f %f %f"</span>, &amp;p.lx, &amp;p.ly, &amp;p.rx, &amp;p.ry); </span><br><span class="line">    r.push_back(p); </span><br><span class="line">    while(getchar()!='\\n')&#123;&#125; </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">float</span> input1,input2; </span><br><span class="line">  <span class="keyword">int</span> num = <span class="number">1</span>; </span><br><span class="line">  <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%f %f"</span>,&amp;input1,&amp;input2) == <span class="number">2</span> ) &#123; </span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">fabs</span>(input1<span class="number">-9999.9</span>)&lt;<span class="number">1e-3</span> &amp;&amp; <span class="built_in">fabs</span>(input2<span class="number">-9999.9</span>)&lt;<span class="number">1e-3</span>) </span><br><span class="line">      <span class="keyword">break</span>; </span><br><span class="line">    <span class="keyword">bool</span> flag = <span class="literal">false</span>; </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; r.<span class="built_in">size</span>() ; ++i) &#123; </span><br><span class="line">      <span class="keyword">if</span>( (r[i].lx &lt; input1 &amp;&amp; input1 &lt; r[i].rx) &amp;&amp; (r[i].ly &gt;; input2 &amp;&amp; input2 &gt;; r[i].ry) ) &#123; </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Point %d is contained in figure %d\\n"</span>,num,i+<span class="number">1</span>); flag = <span class="literal">true</span>; </span><br><span class="line">      &#125; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">if</span>(!flag) <span class="built_in">printf</span>(<span class="string">"Point %d is not contained in any figure\\n"</span>,num); </span><br><span class="line">    num++; </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[PHP]incomplete class解決方法</title>
      <link href="/posts/dffa06cb/"/>
      <url>/posts/dffa06cb/</url>
      
        <content type="html"><![CDATA[<p>近日再寫PHP的時候，再參考一些SESSION使用資料的程式碼時會出現__PHP_Incomplete_Class的問題: </p><p>我寫了一個Class，然後用SESSION去儲存這個Class，但當我下次取他的時候就會造成這個錯誤。 </p><p>這個錯誤跟寫法與設定有關係，上網GOOGLE後發現這個造成這個ERROR的原因是因為PHP是直譯式的，所以如果我在讀取SESSION前沒有先去定義Class時程式就會出錯，因為他不知道這是個物件。   </p><p>網路上的參考解決辦法是<a id="more"></a></p><ol><li>將Class的定義(引入)置於SESSION_START()以前 </li><li>PHP Autoloader(參考網址:<a href="http://php-autoloader.malkusch.de/en/features/session/" target="_blank" rel="noopener">http://php-autoloader.malkusch.de/en/features/session/</a>)   </li></ol><p>但我用第一個方法後還是無法正確執行，後來檢查了許久發現我的php.ini設定中session.auto_start=1，是自動啟動的，這樣的話順序調了還是一樣是錯的，所以建議將session.auto_start改成0，然後程式碼內都加上session.start()如此一來就可以避免掉這個問題了。</p>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> php </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA490]Rotating Sentences</title>
      <link href="/posts/f41ac22d/"/>
      <url>/posts/f41ac22d/</url>
      
        <content type="html"><![CDATA[<blockquote><p>In ``Rotating Sentences,’’ you are asked to rotate a series of input sentences 90 degrees clockwise. So instead of displaying the input sentences from left to right and top to bottom, your program will display them from top to bottom and right to left. </p><h4 id="Input-and-Output"><a href="#Input-and-Output" class="headerlink" title="Input and Output"></a>Input and Output</h4><p>As input to your program, you will be given a maximum of 100 sentences, each not exceeding 100 characters long. Legal characters include: newline, space, any punctuation characters, digits, and lower case or upper case English letters. (NOTE: Tabs are not legal characters.) The output of the program should have the last sentence printed out vertically in the leftmost column; the first sentence of the input would subsequently end up at the rightmost column.</p><h4 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Rene Decartes once said,</span><br><span class="line">&quot;I think, therefore I am.&quot;</span><br></pre></td></tr></table></figure><h4 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;R</span><br><span class="line">Ie</span><br><span class="line"> n</span><br><span class="line">te</span><br><span class="line">h </span><br><span class="line">iD</span><br><span class="line">ne</span><br><span class="line">kc</span><br><span class="line">,a</span><br><span class="line"> r</span><br><span class="line">tt</span><br><span class="line">he</span><br><span class="line">es</span><br><span class="line">r</span><br><span class="line">eo</span><br><span class="line">fn</span><br><span class="line">oc</span><br><span class="line">re</span><br><span class="line">e</span><br><span class="line"> s</span><br><span class="line">Ia</span><br><span class="line"> i</span><br><span class="line">ad</span><br><span class="line">m,</span><br><span class="line">.</span><br><span class="line">&quot;</span><br></pre></td></tr></table></figure></blockquote><p>想法:</p><p>蠻簡單的題目，改變一下輸出順序即可。要注意的是如果前一行的長度比後面的句子短，在倒轉後會造成缺少空白的情形，所以要隨時記錄最大長度的句子，並把所有比他小的句子都補齊空白。</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">char</span> s[<span class="number">101</span>][<span class="number">101</span>];</span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">memset</span>(s,<span class="number">0</span>,<span class="keyword">sizeof</span>(s));</span><br><span class="line">  <span class="keyword">while</span> (gets(s[i])) &#123;</span><br><span class="line">    len = len &lt; <span class="built_in">strlen</span>(s[i]) ? <span class="built_in">strlen</span>(s[i]) : len;</span><br><span class="line">    i++;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span> ; j &lt; i ; ++j) &#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="built_in">strlen</span>(s[j]) ; k &lt; len ; ++k) &#123;</span><br><span class="line">      s[j][k] = <span class="string">' '</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span> ; k &lt; len ; ++k) &#123; </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = i<span class="number">-1</span> ; j &gt;= <span class="number">0</span> ; --j) &#123;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"%c"</span>,s[j][k]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Win10解決開始功能表無法開啟的問題</title>
      <link href="/posts/ed4d436b/"/>
      <url>/posts/ed4d436b/</url>
      
        <content type="html"><![CDATA[<p>就在今天開啟電腦發現我的開始功能表下面那一列都無法開啟，只能在點右鍵會有反應…</p><p>這沒道理阿???我昨天關機前明明就好好的，想了想有可能是win10強制更新的造成的BUG，上網GOOGLE(本來連上網都沒辦法…因為還要去設定連線，好險電腦有記錄我手機的WIFI)了許多方法都沒有效果…</p><p>後來把更新改回上一次版本，然後重新開機就恢復了!!以下是我的方法:<a id="more"></a></p><ol><li><p>按WIN + I 開啟WIN10的系統設定<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2015/12/1.png" alt="1"></p></li><li><p>點選更新與安全性 -&gt;復原<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2015/12/2.png" alt="2"></p></li><li><p>與上面圖片不一樣的是，此時中間應該會有個”組建”的選項，選擇組建的復原選項，然後等他跑完自動重新開機即可恢復正常。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NCPC複賽</title>
      <link href="/posts/76d461ec/"/>
      <url>/posts/76d461ec/</url>
      
        <content type="html"><![CDATA[<p>初賽完兩個禮拜就是複賽了，兩個禮拜其實也沒很長，所以這中間也沒什麼特別的準備就上了。比賽地點在中山大學，和隊友們從學校搭捷運去西子灣轉公車，一下車就看到以前暑訓的神人室友，所以就一起在中山裡面找路了(因為沒有人知道路……)。     <a href="https://john850512.files.wordpress.com/2015/11/12079338_1194183207264595_3732027757611351140_n.jpg" target="_blank" rel="noopener"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2015/11/12079338_1194183207264595_3732027757611351140_n.jpg?w=300" alt="12079338_1194183207264595_3732027757611351140_n"></a><br><a id="more"></a><br>不過不算難找啦，因為很明顯xD，看看這場面，第一次參加這種大排場的活動難免小興奮一下。進去後還真羨慕中山，裡面的設備都好高級喔。<br><a href="https://john850512.files.wordpress.com/2015/11/12111999_1194183233931259_1984670590503969065_n.jpg" target="_blank" rel="noopener"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2015/11/12111999_1194183233931259_1984670590503969065_n.jpg?w=168" alt="12111999_1194183233931259_1984670590503969065_n"></a> </p><p>簽到後拿到了參賽證，名字的由來是因為當時在想名字的時候: </p><blockquote><p>我: 欸隊名要想什麼啊? </p><p>A: 都可以阿 </p><p>我: 好難想喔… A對阿…取名字比寫程式還要難… </p><p>我: … </p><p>A: … 然後隊名就決定了!!   </p></blockquote><p>早上都是環境測試，一看嚇到我，C++居然要用netbeans寫，趕快先去搞懂怎麼用環境……，因此測試題目好像都沒什麼裡，不過有一題是初賽的題目。然後他可以帶自己的紙本進場，可是我初賽的時候準備的STL資料都不見了!!!結果我們什麼都沒準備。 </p><p>中午吃飯是在一間會議室，不過位置好像沒很充足，似乎有些晚來的沒位置坐。 下午就開始考試摟，一樣是我先Code隊友讀題，第一題是解方程式，用高斯消去解，但是一個小錯誤WA了兩次(沒有考慮方程式的位置可能會使對角線變成0)，第二題感覺好長好難就跳過了，後來我們專注於第四題跟最後一題上(感覺這三題是最簡單的……事後證明真的是這三題最容易)，不過最終還是只接出第一題就是了。       <a href="https://john850512.files.wordpress.com/2015/11/12141625_1194183057264610_1781659511601623050_n.jpg" target="_blank" rel="noopener"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2015/11/12141625_1194183057264610_1781659511601623050_n.jpg?w=300" alt="12141625_1194183057264610_1781659511601623050_n"></a>   <a href="https://john850512.files.wordpress.com/2015/11/12074544_1194183070597942_4596394916110677763_n.jpg" target="_blank" rel="noopener"><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2015/11/12074544_1194183070597942_4596394916110677763_n.jpg?w=300" alt="12074544_1194183070597942_4596394916110677763_n"></a> </p><p>參賽證明，雖然第一次參加只是要拿個經驗，但我覺得表現尚可，希望明年能有更好的成績。</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ncpc </tag>
            
            <tag> competition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NCPC初賽</title>
      <link href="/posts/f1df07cc/"/>
      <url>/posts/f1df07cc/</url>
      
        <content type="html"><![CDATA[<p>參加Ncpc的起源要從大一的暑訓開始，在那裏遇到的人幾乎都是專門參加程式競賽的，而回來後學校教授也推薦我們組一隊去比比看，於是就和兩位同年級的開始著手準備了。 說是準備，其實也是把去年的初賽題目拿來寫一遍而已，經過一個禮拜的折磨好不容易寫完了五題，但那都是寫寫停停、上網找找Liberary而完成的就是了。順帶一提，在暑訓期間的高手、神人們在參賽名單上全部都出現了。</p><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2015/11/12109223_916108731805553_6288868111926439114_n.png?w=300" alt="12109223_916108731805553_6288868111926439114_n"><br><a id="more"></a><br>測試題目時好不容易5題…小小開心了下</p><p>初賽地點在各自的學校，一想到要用學校的電腦就….哀，選了一台可以順利編譯的，但滑鼠頗雷就是了。 初賽是3個小時，總題數忘記了，我按照和暑訓的朋友討論和上網找別人比賽的經驗做參考，一開始第一題先讓我Code，兩位隊友則先看其他題目。這次的第一題是算簡單的題目，但我居然卡在input那邊，用C寫不知道筆數的資料輸入，最後透過字串及很麻煩的轉換終於完成了，但居然CE了兩次，因為用到保留字去命名Array了，天啊!!為了這件事我還先把這題放著讓隊友去解別題，後來解完的時候時間大幅增加了不少。</p><p>隊友那題也是跟我卡一樣的Input，但因為那題目過長，我沒有仔細讀完，所以我也沒辦法幫他Code input的部分，最後隊友那題因為連input都無法順利完成索性放棄做別題，一樣是我Code隊友在旁邊幫忙做Testing，但礙於時間壓力並沒有完成有點可惜。</p><p>最後只AC了一題，看了一下學校5隊有4隊都AC1題，我們是時間最久的。但還是有進決賽就是了，決賽當天我們學校只有兩隊去，因為其他隊伍要去吃直屬聚xD 檢討了一下這次的比賽，部分原因或許是我們隊都還不熟悉這種比賽機制，我不知道該如何在3小時比賽時間內去分配每個人的工作及時間?大家對競賽的基礎都不一樣，所以這方面真的要好好想想</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ncpc </tag>
            
            <tag> competitive programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MOPCON 2015</title>
      <link href="/posts/64dd6af2/"/>
      <url>/posts/64dd6af2/</url>
      
        <content type="html"><![CDATA[<p>MOPCON(行動科技年會)，號稱南台灣最大的開源年會。 第一次參加這類的活動，當時看到這個活動是在FB的一個開源社群裡看到的，看了一下內容後二話不說就訂票了。在此之前我完全沒有接觸過任何Open-Source的相關接觸，或者應該說，我知道這個東西，或許我也正在使用它，但我沒有仔細的去深入研究過。 </p><p>現場當天我稍微遲到了5分鐘(遲到的壞習慣還是改不掉..)，一進場整個就愣住了<a id="more"></a>，現場人數又令我再度體驗到我參加了一個不得了的活動的事實，能和這麼多興趣相同的人聚在一起真的是一件很熱血的事情，雖然說除了幾位大神級的人物外其實沒幾個認識哈哈(看到Jserv還是很激動的，畢竟FB上久仰大名了，哈)。 </p><p>對了，當天還巧遇了兩位學姊以及電機系的朋友。 <img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://john850512.files.wordpress.com/2015/11/12189126_923234171093009_7763905761736784415_n.jpg?w=300" alt="12189126_923234171093009_7763905761736784415_n"> </p><p>接下來就是一連串的演講，每個時段有三個不同的演講主題，要自己抉擇要聽哪一場，看了一下介紹後我都是選擇比較沒有技術層面的主題，因為我覺得太深入的可能聽不懂……。 </p><p>歐對了，第一場演講我就發現許多人都會帶著自己的筆電，當時我還好奇為什麼大家都要帶電腦……，演講開始後我發現原來有個HackPad可以讓大家一起紀錄演講內容的東西，這真的超好用!!而且大家都記錄的很詳盡，如果事後想在回顧也不用怕漏掉什麼。 </p><p>一整天下來收穫很多，覺得對自己最有幫助的是xdite演講的【Intro to Growth Hacking for developers】、和 Akane Lee的【使用者要的不是功能！】吧，Akane Lee以使用者的角度去對產品做分析，告訴我們設計者眼中好的產品跟使用者眼中好的產品差別在哪裡；xdite的主題是Growth Hacking，原本還不知道這是什麼，但聽完真是令人佩服的五體投地，不論是演講技巧跟內容都非常精彩，聽完都想馬上來練習一下了。 </p><p>有點小可惜的是聽了LINE Platform的主題而沒聽到影像辨識的主題， LINE Platform主要在講說他們的新功能以及應用，但這些都比較偏向公司客群，所以感覺目前對我還沒什麼幫助。 下午還有下午茶時間，拿點心的人排的好長好長一串(忘記拍照了)，不過茶點個人覺得還蠻好吃的，雖然戴牙套不能咬硬的所以吃得有點煎熬就是了。 </p><p>因為隔天有事情無法完整聽完兩天的內容(隔天下午還有一個我超想聽的主題-MUKI的)，但是還是很開心參加了這次的活動，如果有空明年我也一定會再來參加的!!</p>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> conference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[UVA967]Circular</title>
      <link href="/posts/c21dcde9/"/>
      <url>/posts/c21dcde9/</url>
      
        <content type="html"><![CDATA[<h2 id="題目敘述"><a href="#題目敘述" class="headerlink" title="題目敘述"></a>題目敘述</h2><blockquote><p>A circular prime is a prime number that remains prime as each leftmost digit (most significant digit), in turn, is moved to the right hand side. For instance, the number 19937 is a circular prime, since all numbers in the sequence 19937, 99371, 93719, 37199 and 71993 are prime numbers.<a id="more"></a> </p><p>Your objective is to write a program that, given a range, computes the number of circular primes in that range. Input The input consists of a sequence of pairs of integers i and j, with one pair of integers per input line. All integers will be less than 1,000,000 and greater or equal to 100. You can assume that in any pair i is lesser or equal than j. </p><p>You should process all pairs of integers, and for each such pair, count the number of circular primes between i and j, including i and j. Input is terminated by a line just with the number ‘-1’. Output For each pair of input integers, defining a range, the output should be: ‘No Circular Primes.’ (if there are no circular primes in the range), ‘1 Circular Prime.’ (if only one circular prime exists in the range), or ‘n Circular Primes.’ (if there are n circular primes in the range, and n is greater than one). </p></blockquote><h2 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h2><p>先建好Circular表，然後再去做比對。因為質數的性質(一個合數n一定可以拆成一個小於根號n的 * 一個大於根號n的)，所以1000000只要找1000內是否有它的因數即可。 而且只需要判斷是否是質數的倍數即可(線性篩法)，做完再用字串去做DP。</p><p>作法參考: <a href="http://mypaper.pchome.com.tw/zerojudge/post/1322891238" target="_blank" rel="noopener">A - CIRCULAR</a></p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> max 1000000 </span></span><br><span class="line"><span class="keyword">int</span> dp[<span class="number">1000001</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line"><span class="keyword">int</span> prime[<span class="number">1001</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line"><span class="keyword">bool</span> mark[<span class="number">1001</span>] = &#123;<span class="number">0</span>&#125;; </span><br><span class="line"><span class="keyword">int</span> num = <span class="number">0</span>; </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sieve</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  mark[<span class="number">0</span>] = mark[<span class="number">1</span>] = <span class="number">1</span>; </span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span> ; i &lt; <span class="number">1000</span> ; ++i) &#123; </span><br><span class="line">    <span class="keyword">if</span>(!mark[i]) &#123; </span><br><span class="line">      prime[num++] = i; </span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">2</span> ; i * j &lt; <span class="number">1000</span> ; ++j) &#123; </span><br><span class="line">        mark[i * j] = <span class="number">1</span>; </span><br><span class="line">        &#125; </span><br><span class="line">      &#125; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="comment">/* </span></span><br><span class="line"><span class="comment">    for(int i = 0 ; i &lt; num ; ++i) &#123; </span></span><br><span class="line"><span class="comment">      printf("%d\n",primei); </span></span><br><span class="line"><span class="comment">    &#125; </span></span><br><span class="line"><span class="comment">    */</span> </span><br><span class="line">  &#125; </span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">judge_prime</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123; </span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; prime[i] &lt; <span class="built_in">sqrt</span>(n) &amp;&amp; i &lt; num ; ++i ) &#123; </span><br><span class="line">    <span class="keyword">if</span>(!(n % prime[i])) <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span>; </span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bulid_dp</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">100</span> ; i &lt;= <span class="number">1000000</span> ; ++i) &#123; </span><br><span class="line">    <span class="keyword">char</span> str[<span class="number">8</span>]; </span><br><span class="line">    dp[i] += dp[i<span class="number">-1</span>]; </span><br><span class="line">    <span class="built_in">sprintf</span>(str,<span class="string">"%d"</span>,i); </span><br><span class="line">    <span class="keyword">int</span> len = <span class="built_in">strlen</span>(str); </span><br><span class="line">    <span class="keyword">int</span> n,temp; </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span> ; j &lt;= len ; ++j) &#123; </span><br><span class="line">      <span class="built_in">sscanf</span>(str,<span class="string">"%d"</span>,&amp;n); </span><br><span class="line">      <span class="keyword">if</span>(judge_prime(n) == <span class="number">0</span>) <span class="keyword">break</span>; </span><br><span class="line">      temp = str[<span class="number">0</span>]; </span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">1</span> ; k &lt; len ; ++k) &#123; </span><br><span class="line">        str[k<span class="number">-1</span>] = str[k]; </span><br><span class="line">      &#125; </span><br><span class="line">      strle[n<span class="number">-1</span>] = temp; </span><br><span class="line">      <span class="keyword">if</span>(j == len) &#123; </span><br><span class="line">        dp[i++]; <span class="keyword">break</span>; </span><br><span class="line">      &#125; </span><br><span class="line">    &#125; </span><br><span class="line">  &#125; </span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  sieve(); </span><br><span class="line">  bulid_dp(); </span><br><span class="line">  <span class="keyword">int</span> a,b; </span><br><span class="line">  <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;a) &amp;&amp; a != <span class="number">-1</span>) &#123; </span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;b); </span><br><span class="line">    <span class="keyword">int</span> ans = dp[b]- dp[a]; </span><br><span class="line">    <span class="keyword">if</span>(ans == <span class="number">0</span> ) <span class="built_in">printf</span>(<span class="string">"No Circular Primes.\\n"</span>); </span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(ans == <span class="number">1</span>) <span class="built_in">printf</span>(<span class="string">"1 Circular Prime.\\n"</span>); </span><br><span class="line">    <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"%d Circular Primes.\\n"</span>,ans); </span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c </tag>
            
            <tag> c++ </tag>
            
            <tag> uva </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
