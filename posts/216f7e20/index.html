<!DOCTYPE html>
<html lang="zh-TW">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="keyword"  content="programming, study, daily, entertainment">

    <meta property="og:type" content="website" />
    <meta property="og:site_name" content="星期五。見面">
    <meta property="og:locale" content="zh-TW" />
    
    <meta property="og:title" content="[ML/DL]從計算機編碼的角度看Entropy, Cross Entropy和KL-Divergence" />
    
    
    <meta property="og:description" content="Entropy概念最早被用於熱力學，在1948年由Shannon將此概念引入[information theory]中，被用來**計算根據訊息的機率分布對訊息編碼所需要的平均編碼長度**，也因此Entropy也被稱之Shannon Entropy。在早期計算機設備昂貴的年代，如果需要傳遞一段由26個英文字母組成的英文文本，按照[ASCII code的編碼](https://zh.wikipedia.org/wiki/ASCII)每一個字元共需要8個bit進行編碼，但實際上並不是每個字母出現的頻率(或機率)都是相等的，因此，**如果可以讓越常出現的字元使用較少的bit數，而較不常出現的字元使用較多的bit數，就有機會整體傳送的bits數較少。**" />
    <meta name="description" content="Entropy概念最早被用於熱力學，在1948年由Shannon將此概念引入[information theory]中，被用來**計算根據訊息的機率分布對訊息編碼所需要的平均編碼長度**，也因此Entropy也被稱之Shannon Entropy。在早期計算機設備昂貴的年代，如果需要傳遞一段由26個英文字母組成的英文文本，按照[ASCII code的編碼](https://zh.wikipedia.org/wiki/ASCII)每一個字元共需要8個bit進行編碼，但實際上並不是每個字母出現的頻率(或機率)都是相等的，因此，**如果可以讓越常出現的字元使用較少的bit數，而較不常出現的字元使用較多的bit數，就有機會整體傳送的bits數較少。**">
    
    
    <meta property="og:image" content="/img/avatar.jpg" />
    

    <link rel="shortcut icon" href="/img/avatar.jpg">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          [ML/DL]從計算機編碼的角度看Entropy, Cross Entropy和KL-Divergence - MeetonFriday
        
    </title>
    <!-- canonical -->
    
    
      <link rel="canonical" href="https://meetonfriday.com/posts/216f7e20/">
    

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
<link rel="stylesheet" href="/css/dusign-dark.css">

    
<link rel="stylesheet" href="/css/dusign-common-dark.css">

    <!-- 
<link rel="stylesheet" href="/css/font-awesome.css">
 -->
    
<link rel="stylesheet" href="/css/toc.css">


    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <!-- <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css">
    
    <!-- Google Analytics -->
    
      <!-- <script>
          // dynamic User by Hux
          var _gaId = 'UA-163346001-1';
          var _gaDomain = 'auto';
      
          // Originial
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      
          ga('create', _gaId, _gaDomain);
          ga('send', 'pageview');
      </script> -->
      
      <!-- Global site tag (gtag.js) - Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-163346001-1"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
      
        gtag('config', 'UA-163346001-1');
      </script>
      
    <!-- google ad sense-->
    <script data-ad-client="ca-pub-9561340457908416" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="星期五。見面" type="application/atom+xml">
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.5)), url('/img/header_img/header-bg.jpg')
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#deep learning" title="deep learning">deep learning</a>
                            
                              <a class="tag" href="/tags/#machine learning" title="machine learning">machine learning</a>
                            
                        </div>
                        <h1>[ML/DL]從計算機編碼的角度看Entropy, Cross Entropy和KL-Divergence</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by John on
                            2019-09-26
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">2.6k</span> and
                                Reading Time <span class="post-count">10</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
        
            <div class="waveWrapper">
                <div class="wave wave_before" style="background-image: url('/img/wave-dark.png')"></div>
                <div class="wave wave_after" style="background-image: url('/img/wave-dark.png')"></div>
            </div>
        
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">星期五。見面</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/reading/">Reading</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>

    
    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Container -->

    
        
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">
        
    

                <h2 id="Shannon-Entropy"><a href="#Shannon-Entropy" class="headerlink" title="Shannon Entropy"></a>Shannon Entropy</h2><h3 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h3><p>Entropy概念最早被用於熱力學，在1948年由Shannon將此概念引入<a href="https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E8%AE%BA" target="_blank" rel="noopener">information theory</a>中，被用來<strong>計算根據訊息的機率分布對訊息編碼所需要的平均編碼長度</strong>，也因此Entropy也被稱之Shannon Entropy。</p>
<a id="more"></a>
<h3 id="情境思考"><a href="#情境思考" class="headerlink" title="情境思考"></a>情境思考</h3><p>在早期計算機設備昂貴的年代，如果需要傳遞一段由26個英文字母組成的英文文本，按照<a href="https://zh.wikipedia.org/wiki/ASCII" target="_blank" rel="noopener">ASCII code的編碼</a>每一個字元共需要8個bit進行編碼，但實際上並不是每個字母出現的頻率(或機率)都是相等的，因此，<strong>如果可以讓越常出現的字元使用較少的bit數，而較不常出現的字元使用較多的bit數，就有機會整體傳送的bits數較少。</strong></p>
<ul>
<li>你說節省傳送的bit數量有什麼好處? cost is money阿孩子</li>
<li>根據鴿籠原理，任何無損壓縮技術不可能縮短任何訊息，如果有一些訊息變短，則至少有一條訊息變長。</li>
<li>在實際使用中，由於我們通常只關注於壓縮特定的某一類訊息，一些編碼技術恰好使用了這特性。<ul>
<li>對於壓縮編碼的技術，可以參考<a href="https://zh.wikipedia.org/wiki/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81" target="_blank" rel="noopener">霍夫曼編碼</a></li>
</ul>
</li>
</ul>
<p>對於上述的敘述，實際上，英文文本的平均編碼長度大約在4.7bits，也就是說，給予一段長度為10的英文文本:</p>
<ul>
<li>不使用任何編碼壓縮技術，直接使用ASCII傳送的話需要 $8\times10 = 80$ bits</li>
<li>使用Entropy的編碼技術來傳送的話平均只要$4.7\times10 = 47$ bits</li>
</ul>
<p>所以要怎麼知道對於某些訊息，應該要用多少bit來編碼呢? Entropy可以幫助你</p>
<h3 id="Entropy-formula"><a href="#Entropy-formula" class="headerlink" title="Entropy formula"></a>Entropy formula</h3><p>先完整看一次Entropy的公式</p>
<p>$Entropy = -\sum_ip_ilog_b(p_i)$</p>
<ul>
<li>注意這裡log的底沒有限定是什麼，不同的b對應到不同單位，不過為了從information theory的角度出發，下面會用b=2來講解，此時的單位為<strong>bit</strong></li>
</ul>
<h3 id="理解公式"><a href="#理解公式" class="headerlink" title="理解公式"></a>理解公式</h3><p>這鬼東西跟編碼長度到底有什麼鬼關係?</p>
<p>簡化一下剛剛提到的情境來思考這個問題，現在假設需要傳遞一個由$S={S_1, S_2, S_3, S_4}$組成的字串，使用ASCII的話每個字元需要8 bits來進行傳遞，但實際上只有四種可能，也就是說我們只需要</p>
<p>$log_2(4)$</p>
<p>2個bit就可以來進行四個答案的編碼了: {00, 01, 10, 11}</p>
<p>而我們又可以將上述的式子改寫一下</p>
<p>$log_2(4) = log_2(\frac{1}{\frac{1}{4}}) = -log_2(\frac{1}{4})$</p>
<p>這裡的1/4可以看成是一個隨機變數的機率，並且1/4其實對應到了上面這個例子沒有提到的一個假設: <strong>4個字元出現的機率是相等的</strong>，基於機率相等的假設下，$S_1, S_2, S_3, S_4$都可以透過上述的公式算出需要2個bit的編碼。</p>
<p>不過並不是每個case都這麼是機率相等的，<strong>為了可以套用在機率不同的情況下，需要引入期望值的概念</strong>，也就是說式子變成了</p>
<p>$-\sum_ip_ilog_2(p_i)$</p>
<ul>
<li>實際上，將 $p_1 = p_2 = p_3 = p_4 = 1/4$ 帶入Entropy的公式也會得到2</li>
<li>在information theory中，$-log_2(\frac{1}{p})$也稱之為資訊本體(self-information)，也就是說，<strong>越不常出現的訊息往往帶著越大的資訊含量</strong>。</li>
</ul>
<p>所以到這裡我們可以說，<strong>Entropy在算的是對於每個編碼，所需要長度的期望值</strong>。</p>
<hr>
<p>接下來繼續考慮上述問題，但是這次$S={S_1, S_2, S_3, S_4}$的機率並不相等了，假設:</p>
<ul>
<li>$P(S_1)=1/4$</li>
<li>$P(S_2)=1/8$</li>
<li>$P(S_3)=1/2$</li>
<li>$P(S_4)=1/8$</li>
</ul>
<p>在這種情況下，各別需要的編碼數為:</p>
<ul>
<li>$S_1$需要2 bits編碼</li>
<li>$S_2$需要3 bits編碼</li>
<li>$S_3$需要1 bits編碼</li>
<li>$S_4$需要3 bits編碼</li>
</ul>
<p>我們可以隨便編，例如給$S_1$:10、$S_2$:011、$S_3$:0、$S_4$:111之類的，只要沒有衝突到就好了，然後算一下編碼長度的期望值</p>
<p>$-\sum_ip_ilog_b(p_i) = 1/4\times2+1/8\times3+1/2\times1+1/8\times3 = 1.75$</p>
<p>所以平均編碼長度總共是1.75個bit。</p>
<p>你說小數是什麼鬼?阿不都說了是平均==，如果還覺得哪裡怪怪的話，那接下來來個實際情境想一下。</p>
<hr>
<p>接續上述問題，假設由$S={S_1, S_2, S_3, S_4}$組成的字串長度為200個字元，並且字元出現的次數恰好完全符合上述的機率分布，也就是說</p>
<ul>
<li>$S_1$出現了50次</li>
<li>$S_2$出現了25次</li>
<li>$S_3$出現了100次</li>
<li>$S_4$出現了25次</li>
</ul>
<p>所以傳遞這個字串總共需要$50\times2+25\times3+100\times1+50\times2=375$個bit，平均每個字元只用了375/200 = 1.75個bit。</p>
<h3 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h3><p>簡介了從Information Theory的Encoding角度去看待Entropy這件事，並透過簡單的例子回顧公式的意義。</p>
<p>重申一次，所以Entropy在Information Theory中的用途是<strong>計算根據訊息的機率分布對訊息編碼所需要的平均編碼長度</strong>。</p>
<h3 id="延伸討論-Entropy必不能為負"><a href="#延伸討論-Entropy必不能為負" class="headerlink" title="延伸討論: Entropy必不能為負?"></a>延伸討論: Entropy必不能為負?</h3><ol>
<li>從數學的角度講，因為$0=0$</li>
<li>從資訊意義的角度，取自<a href="https://www.zhihu.com/question/263824964" target="_blank" rel="noopener">为什么信息量不能为负值？</a></li>
</ol>
<blockquote>
<p>因为，信息值为负的已经不能叫做信息，而被叫做干扰。<br>香农的理论是很简朴而典型的，A想要把某个信息a传递给B，但传播信息的信道存在种种异常，比如，A站在山顶喊话“我喜欢你”，结果因为风太大，B只听到“我喜欢”，虽然信息量丢失，但仍有一部分信息传达了，那就是“我（A）”和“喜欢（L）”。</p>
<p>第二次，A还对B说，“我喜欢你”，但因为风太大，B听成了，“我喜欢隔壁老王”。那么B所得到的信息是什么呢？“我(A)”和“喜欢(L)”这两个信息的置信度提高了，而“你(B)”的置信度降低了。</p>
<p>但是当A说，“我喜欢你”，B可能会因为风太大而听成“老李讨厌隔壁老王”么？可能性很低。</p>
</blockquote>
<hr>
<h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h2><p>知道Entropy在information theory的意義後，後面就很好介紹了。</p>
<p>先說結論，Cross Entropy可以這樣理解: <strong>使用了估計出來的編碼後所得到的平均編碼長度</strong>。</p>
<h3 id="Cross-Entropy-formula"><a href="#Cross-Entropy-formula" class="headerlink" title="Cross Entropy formula"></a>Cross Entropy formula</h3><p>先完整看一次Entropy的公式$Cross Entropy = -\sum_ip_ilog_b(q_i)$</p>
<ul>
<li>注意儘管公式很像Shannon Entropy，但不同的地方在於log裡面用的是q(x)</li>
</ul>
<h3 id="理解公式-1"><a href="#理解公式-1" class="headerlink" title="理解公式"></a>理解公式</h3><p>對於上述公式我們可以這麼理解:<br>對於某個需要被傳遞的資料，$p$是真正的機率分布，但這個機率分布我們事實上不知道，所以我們透過了神奇的Machine Learning / Deep Learning等黑技術去學習到了一個我們以為的機率分布$q$。</p>
<p>所以接下來我們使用$-\sum_ilog_b(q_i)$來進行編碼，但事實上資料真正的機率分布是$p$，所以我們得到的平均編碼長度就變成了$-\sum_ip_ilog_b(q_i)$。</p>
<p>也就是說，當$q$估計的越準(越接近q)，平均編碼長度才會是最短的(接近Shannon Entropy)。</p>
<h3 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h3><ul>
<li>實際上從ML/DL的角度來看，Cross Entropy Loss的概念就是，今天有一個資料真正的機率分布以及透過model學習出來的分布，我們希望這兩個機率分布越接近越好。</li>
</ul>
<h3 id="延伸討論-Cross-Entropy遇到-log0"><a href="#延伸討論-Cross-Entropy遇到-log0" class="headerlink" title="延伸討論: Cross Entropy遇到$log0$?"></a>延伸討論: Cross Entropy遇到$log0$?</h3><ul>
<li>這在training中常常會遇到，implement上通常會採用$log(x+ \epsilon)$來handle這個狀況。</li>
</ul>
<h2 id="KL-Divergence"><a href="#KL-Divergence" class="headerlink" title="KL-Divergence"></a>KL-Divergence</h2><p>接下來來介紹KL-Divergence，一樣先說結論: KL-Divergence實際上是<strong>當估出來的編碼方式和理想上的編碼有差時，而導致平均編碼差度的誤差值</strong>。</p>
<ul>
<li>實際上，這很常發生，因為我們不知道理想上的編碼方式應該是如何(不知道機率分布)，所以我們使用model估出來的編碼方式可能會產生誤差。</li>
</ul>
<h3 id="KL-Divergence-formula"><a href="#KL-Divergence-formula" class="headerlink" title="KL-Divergence formula"></a>KL-Divergence formula</h3><p>公式如下:</p>
<p>$KL-Divergence = \sum_ip_ilog_b(\frac{p_i}{q_i})$</p>
<h3 id="理解公式-2"><a href="#理解公式-2" class="headerlink" title="理解公式"></a>理解公式</h3><p>再看一遍Cross Entropy的公式和它的定義: 使用了估計出來的編碼後所得到的平均編碼長度</p>
<p>$Cross Entropy = -\sum_ip_ilog_b(q_i)$</p>
<p>那這個估計出來的編碼長度到底理想上的編碼長度差了多少?我們可以來算算看</p>
<script type="math/tex; mode=display">Cross Entropy-Shannon Entropy =-\sum_ip_ilog_b(q_i) -(-\sum_ip_ilog_b(p_i)) \\=\sum_ip_ilog_b(p_i)-\sum_ip_ilog_b(q_i) \\=\sum_ip_ilog_b(\frac{p_i}{q_i})</script><p>驚!這不就是KL-Divergency的公式嗎，再仔細看…<br>又驚!這不就是KL-Divergency的定義嗎</p>
<h3 id="結論-1"><a href="#結論-1" class="headerlink" title="結論"></a>結論</h3><ul>
<li>實際上KL-Divergence算的就是兩個機率分布的距離。</li>
</ul>
<h3 id="延伸討論-為何DL常使用Cross-Entropy而不是KL"><a href="#延伸討論-為何DL常使用Cross-Entropy而不是KL" class="headerlink" title="延伸討論: 為何DL常使用Cross Entropy而不是KL?"></a>延伸討論: 為何DL常使用Cross Entropy而不是KL?</h3><p>簡單來說，在ML的task裡面，KL和CE是等價的，只是算KL的話還必須多算一個Entropy，所以大家都只算CE。</p>
<p>詳細的解釋可以看下面這篇，取自<a href="https://ai.stackexchange.com/questions/3065/why-has-cross-entropy-become-the-classification-standard-loss-function-and-not-k" target="_blank" rel="noopener">Why has cross entropy become the classification standard loss function and not Kullbeck Leibler divergence?</a></p>
<blockquote>
<p>When it comes to classification problem in machine learning, the cross entropy and KL divergence are equal. As already stated in the question, the general formula is this:$H(p,q)=H(p)+DKL(p||q)$<br>Where p a “true” distribution and q is an estimated distribution, H(p,q) is the cross-entropy, H(p) is the entropy and D is the Kullback-Leibler divergence.<br>Note that in machine learning, p is a one-hot representation of the ground-truth class, i.e., $p=[0,…,1,…,0]$<br>which is basically a delta-function distribution. But the entropy of the delta function is zero, hence KL divergence simply equals the cross-entropy.<br>In fact, even if H(p) wasn’t 0 (e.g., soft labels), it is fixed and has no contribution to the gradient. In terms of optimization, it’s safe to simply remove it and optimize the Kullback-Leibler divergence.</p>
</blockquote>
<h3 id="延伸討論-為何DL常使用CE而不是LSE"><a href="#延伸討論-為何DL常使用CE而不是LSE" class="headerlink" title="延伸討論: 為何DL常使用CE而不是LSE?"></a>延伸討論: 為何DL常使用CE而不是LSE?</h3><p>在gradient更新上速度的不同，有機會下次再說。</p>
<h2 id="總結-1"><a href="#總結-1" class="headerlink" title="總結"></a>總結</h2><p>從計算機編碼的角度來介紹Shannon Entropy的意義，並延伸介紹了Cross Entropy以及KL-Divergence，最後在總結三者之間的關係</p>
<p>$Cross Entropy = Shannon Entropy + KL-Divergence$</p>
<p>我們常常在做的Min Cross Entropy實際上就是是在Min KL-Divergence，當KL-Divergence = 0時，這代表這兩個機率分布是沒有差異的。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://zh.wikipedia.org/wiki/ASCII" target="_blank" rel="noopener">ASCII</a></li>
<li><a href="https://en.wiktionary.org/wiki/Shannon_entropy" target="_blank" rel="noopener">Shannon entropy(weki)</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81" target="_blank" rel="noopener">霍夫曼編碼</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA" target="_blank" rel="noopener">熵(wiki)</a>)</li>
<li><a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E4%BF%A1%E6%81%AF" target="_blank" rel="noopener">self-information</a></li>
<li><a href="https://www.cnblogs.com/frombeijingwithlove/p/5931750.html" target="_blank" rel="noopener">熵和编码长度</a></li>
<li><a href="https://www.zhihu.com/question/263824964" target="_blank" rel="noopener">为什么信息量不能为负值？</a></li>
<li><a href="https://www.cnblogs.com/ljy2013/p/6432269.html" target="_blank" rel="noopener">交叉熵(Cross Entropy)</a></li>
<li><a href="https://www.zhihu.com/question/41252833" target="_blank" rel="noopener">如何通俗的解释交叉熵与相对熵?</a></li>
<li><a href="https://ai.stackexchange.com/questions/3065/why-has-cross-entropy-become-the-classification-standard-loss-function-and-not-k" target="_blank" rel="noopener">Why has cross entropy become the classification standard loss function and not Kullbeck Leibler divergence?</a></li>
<li><a href="https://www.zhihu.com/question/336677048/answer/760789081" target="_blank" rel="noopener">分类问题中损失函数为什么交叉熵用的多，而不是KL?</a></li>
</ul>

                <hr>
                <!-- Pager -->
                <ul class="pager">
    
    <li class="previous">
        <a href="/posts/882f9d11/" data-toggle="tooltip" data-placement="top" title="[ML]Sklearn的scoring parameters">&larr; Newer Post</a>
    </li>
    
    
    <li class="next">
        <a href="/posts/6400f047/" data-toggle="tooltip" data-placement="top" title="[LeetCode]各種Max subarray問題-2">Older Post &rarr;</a>
    </li>
    
</ul>

                <!-- tip start -->
                

                
                <!-- tip end -->

                <!-- Sharing -->
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Shannon-Entropy"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Shannon Entropy</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#簡介"><span class="toc-nav-number">1.1.</span> <span class="toc-nav-text">簡介</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#情境思考"><span class="toc-nav-number">1.2.</span> <span class="toc-nav-text">情境思考</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Entropy-formula"><span class="toc-nav-number">1.3.</span> <span class="toc-nav-text">Entropy formula</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#理解公式"><span class="toc-nav-number">1.4.</span> <span class="toc-nav-text">理解公式</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#結論"><span class="toc-nav-number">1.5.</span> <span class="toc-nav-text">結論</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#延伸討論-Entropy必不能為負"><span class="toc-nav-number">1.6.</span> <span class="toc-nav-text">延伸討論: Entropy必不能為負?</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Cross-Entropy"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">Cross Entropy</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Cross-Entropy-formula"><span class="toc-nav-number">2.1.</span> <span class="toc-nav-text">Cross Entropy formula</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#理解公式-1"><span class="toc-nav-number">2.2.</span> <span class="toc-nav-text">理解公式</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#總結"><span class="toc-nav-number">2.3.</span> <span class="toc-nav-text">總結</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#延伸討論-Cross-Entropy遇到-log0"><span class="toc-nav-number">2.4.</span> <span class="toc-nav-text">延伸討論: Cross Entropy遇到$log0$?</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#KL-Divergence"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">KL-Divergence</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#KL-Divergence-formula"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text">KL-Divergence formula</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#理解公式-2"><span class="toc-nav-number">3.2.</span> <span class="toc-nav-text">理解公式</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#結論-1"><span class="toc-nav-number">3.3.</span> <span class="toc-nav-text">結論</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#延伸討論-為何DL常使用Cross-Entropy而不是KL"><span class="toc-nav-number">3.4.</span> <span class="toc-nav-text">延伸討論: 為何DL常使用Cross Entropy而不是KL?</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#延伸討論-為何DL常使用CE而不是LSE"><span class="toc-nav-number">3.5.</span> <span class="toc-nav-text">延伸討論: 為何DL常使用CE而不是LSE?</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#總結-1"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">總結</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Reference"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">Reference</span></a></li></ol>
          
          </div>
        </aside>
      
    

        </div>
    </div>
</article>

	
    <!-- disqus embedded js code start (one page only need to embed once) -->	
    <script type="text/javascript">	
        /* * * CONFIGURATION VARIABLES * * */	
        var disqus_shortname = "meetonfriday";	
        var disqus_identifier = "https://meetonfriday.com/posts/216f7e20/";	
        var disqus_url = "https://meetonfriday.com/posts/216f7e20/";	
        (function() {	
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;	
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';	
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);	
        })();	
    </script>	
    <!-- disqus embedded js code start end -->	
    

    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/john850512">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank"  href="https://www.linkedin.com/in/john85051232">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/meetonfridayyy">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                    <li>
                        <a target="_blank"  href="https://mail.google.com/mail/?view=cm&fs=1&to=john85051232@gmail.com">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; John 2025 
                    <br>
                    Powered by 
                    <a href="https://github.com/john850512/hexo-theme-jiji" target="_blank" rel="noopener">
                        <i>hexo-theme-jiji</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=john850512&repo=hexo-theme-jiji&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://meetonfriday.com/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>

<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<!--<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>-->
<script async src="//cdn.jsdelivr.net/npm/busuanzi@2.3.0"></script>




	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        
        <!-- background effects end -->
    
    
    <!-- background animation-->>
    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var i=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){i&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,e,a=0;a<r.length;a++)t=r[a],0<=(e=t.getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>

</html>
