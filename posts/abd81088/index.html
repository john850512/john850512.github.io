<!DOCTYPE html>
<html lang="zh-TW">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="keyword"  content="programming, study, daily, entertainment">

    <meta property="og:type" content="website" />
    <meta property="og:site_name" content="星期五。見面">
    <meta property="og:locale" content="zh-TW" />
    
    <meta property="og:title" content="[論文速速讀]Efficient Estimation of Word Representation in Vector Space" />
    
    
    <meta property="og:description" content="這篇就是鼎鼎大名的word2vec，在2013年由google提出。Word2vec是一種word embedding技術，為什麼需要word embedding? 因為這樣可以將NLP task的word轉成一個numeric vector，然後加以進行分析和運算。word2vec的paper，提出了Skip-gram跟CBOW，相較於傳統的方法，能夠大幅提高訓練速度。" />
    <meta name="description" content="這篇就是鼎鼎大名的word2vec，在2013年由google提出。Word2vec是一種word embedding技術，為什麼需要word embedding? 因為這樣可以將NLP task的word轉成一個numeric vector，然後加以進行分析和運算。word2vec的paper，提出了Skip-gram跟CBOW，相較於傳統的方法，能夠大幅提高訓練速度。">
    
    
    <meta property="og:image" content="https://res.cloudinary.com/meet-on-friday/image/upload/v1593446352/blog_posts/%E6%93%B7%E5%8F%96_m6rwzi.png" />
    

    <link rel="shortcut icon" href="/img/avatar.jpg">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          [論文速速讀]Efficient Estimation of Word Representation in Vector Space - MeetonFriday
        
    </title>
    <!-- canonical -->
    
    
      <link rel="canonical" href="https://meetonfriday.com/posts/abd81088/">
    

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
<link rel="stylesheet" href="/css/dusign-dark.css">

    
<link rel="stylesheet" href="/css/dusign-common-dark.css">

    <!-- 
<link rel="stylesheet" href="/css/font-awesome.css">
 -->
    
<link rel="stylesheet" href="/css/toc.css">


    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <!-- <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css">
    
    <!-- Google Analytics -->
    
      <!-- <script>
          // dynamic User by Hux
          var _gaId = 'UA-163346001-1';
          var _gaDomain = 'auto';
      
          // Originial
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      
          ga('create', _gaId, _gaDomain);
          ga('send', 'pageview');
      </script> -->
      
      <!-- Global site tag (gtag.js) - Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-163346001-1"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
      
        gtag('config', 'UA-163346001-1');
      </script>
      
    <!-- google ad sense-->
    <script data-ad-client="ca-pub-9561340457908416" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="星期五。見面" type="application/atom+xml">
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.5)), url('/img/header_img/header-bg.jpg')
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#deep learning" title="deep learning">deep learning</a>
                            
                              <a class="tag" href="/tags/#paper" title="paper">paper</a>
                            
                        </div>
                        <h1>[論文速速讀]Efficient Estimation of Word Representation in Vector Space</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by John on
                            2020-06-29
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">2.3k</span> and
                                Reading Time <span class="post-count">10</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
        
            <div class="waveWrapper">
                <div class="wave wave_before" style="background-image: url('/img/wave-dark.png')"></div>
                <div class="wave wave_after" style="background-image: url('/img/wave-dark.png')"></div>
            </div>
        
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">星期五。見面</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/reading/">Reading</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>

    
    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Container -->

    
        
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">
        
    

                <p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p>
<p>這篇就是<strong>鼎鼎大名的word2vec，在2013年由google提出</strong>。</p>
<p>Word2vec是一種word embedding技術，為什麼需要word embedding? 因為這樣可以<strong>將NLP task的word轉成一個numeric vector，然後加以進行分析和運算</strong>。</p>
<p>paper: <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Efficient Estimation of Word Representation in Vector Space
</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote>
<p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p>
</blockquote>
<p>word2vec的paper，提出了Skip-gram跟CBOW，相較於傳統的方法，能夠大幅提高訓練速度。</p>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote>
<p>Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary. This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data.</p>
</blockquote>
<p>傳統的一些nlp task把word視作不可分割的單元(atomic units)，詞和詞之間不具相似度，因為他們都只是vocab table的index。這種方法簡單且robust，n-gram就是其中的一個應用。</p>
<blockquote>
<p>However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less. Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques.</p>
</blockquote>
<p>但是這種方法也有他的極限，當資料詞庫過少的時候表現就會不好。</p>
<p>這篇paper的目標:</p>
<ul>
<li>可從大量資料(數十億level)中學習word vectors的技術<ul>
<li>據目前所知，還沒有方法提出可以在幾百萬的資料上學習50-100的詞向量</li>
</ul>
</li>
<li>希望學習到相似的詞距離相近，而且詞有不同的相似度(multiple degrees of similarity)<ul>
<li>名詞的多個詞尾可能在相近的空間中被找到</li>
</ul>
</li>
<li>新的模型可以做到<strong>向量的線性操作</strong> <ul>
<li>vector(”King”) - vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen</li>
</ul>
</li>
</ul>
<p>接下來介紹NNLM(Neural Network Language Model): 一個線性投影層 + 一個非線性隱藏層，用來學習詞向量表示和統計語言模型</p>
<blockquote>
<p>… Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are first learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM.</p>
</blockquote>
<p>提到說NNLM先透過single hidden layer來訓練word vectors，然後在使用word vectors來train NNLM，所以即使NNLM沒有train完也可以train好word vectors。</p>
<p>word2vec就是基於這個想法的擴展應用。</p>
<h2 id="Model-Architectures"><a href="#Model-Architectures" class="headerlink" title="Model Architectures"></a>Model Architectures</h2><p>為了比較模型好壞，先定義接下來訓練深度模型的複雜度皆為:<script type="math/tex">O = E\times T\times Q</script></p>
<ul>
<li>E: 迭代次數</li>
<li>T: 訓練集的詞個數</li>
<li>Q: 模型參數</li>
<li>Common choice is E = 3 − 50 and T up to one billion</li>
</ul>
<p>使用mini-batch + Adagrad + SGD來訓練</p>
<h3 id="Feedforward-Neural-Net-Language-Model-NNLM"><a href="#Feedforward-Neural-Net-Language-Model-NNLM" class="headerlink" title="Feedforward Neural Net Language Model (NNLM)"></a>Feedforward Neural Net Language Model (NNLM)</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1593446401/blog_posts/ClFEz8v_wgqndr.jpg" alt=""></p>
<p>(上圖是原始paper(A Neural Probabilistic Language Model)的圖，annotation可能會跟下面word2vec的對不起來，下面使用原本word2vec的annotation)</p>
<p>有4層: input, projection, hidden, output</p>
<ul>
<li>input layer: <strong>前</strong>N words使用one-hot encoding成V維的向量，V是(vocab size)<ul>
<li>注意這裡是用前N words，而不是用所有words來訓練，這是和word2vec中CBOW投影層的差異!!</li>
</ul>
</li>
<li>projection layer: input(NxV)會使用同一個projection matrix(VxD)投影到projection layer P(NxD)<ul>
<li>D是投影後的維度</li>
<li>共用一個projection matrix，所以這裡的cost還算低</li>
</ul>
</li>
<li>hidden layer: 隱藏層來計算整個word的機率，有H個neuron</li>
<li>output layer有V個neuron</li>
</ul>
<p>所以整體的模型參數量是<script type="math/tex">Q=N×D+N×D×H+H×V</script></p>
<ul>
<li>其中output layer的HxV最重要<ul>
<li>有一些優化的方法，例如hierarchical softmax，使用binary tree representations of the vocabulary(Huffman tree)，可以降到$\log_2(V)$</li>
</ul>
</li>
<li>所以其實主要的計算量在hidden layer</li>
</ul>
<h3 id="Recurrent-Neural-Net-Language-Model-RNNLM"><a href="#Recurrent-Neural-Net-Language-Model-RNNLM" class="headerlink" title="Recurrent Neural Net Language Model (RNNLM)"></a>Recurrent Neural Net Language Model (RNNLM)</h3><p>原始paper: Linguistic Regularities in Continuous Space Word Representations</p>
<p>只有input, hidden, output層，訓練複雜度是<script type="math/tex">Q=D×H+H×V</script></p>
<ul>
<li>D和隱藏層H有相同的維度<ul>
<li>使用hierarchical softmax + huffman tree，H×V可以降低為H×$\log_2V$，所以大部分的複雜度來自D×H</li>
</ul>
</li>
</ul>
<h3 id="Parallel-Training-of-Neural-Networks"><a href="#Parallel-Training-of-Neural-Networks" class="headerlink" title="Parallel Training of Neural Networks"></a>Parallel Training of Neural Networks</h3><p>使用DistBelief框架，該框架可以使用平行計算</p>
<h2 id="New-Log-linear-Models"><a href="#New-Log-linear-Models" class="headerlink" title="New Log-linear Models"></a>New Log-linear Models</h2><p>提出了兩個新模型，之前的研究表示<strong>大部分的複雜度是由於模型的非線性層導致的</strong>，所以這裡試圖減少非線性層的參數量</p>
<h3 id="Continuous-Bag-of-Words-Model"><a href="#Continuous-Bag-of-Words-Model" class="headerlink" title="Continuous Bag-of-Words Model"></a>Continuous Bag-of-Words Model</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1593446401/blog_posts/eAzmaER_yg2aym.png" alt=""></p>
<p>和NNLM相似，但刪除了非線性層，並且投影層是所有的words共用(NNLM是前N words共用)</p>
<ul>
<li>所有的单词都投影到同一个位置（所有向量取平均值）。这样不考虑单词的位置顺序信息，叫做词袋模型<ul>
<li>詞的順序對於不影響投影</li>
</ul>
</li>
<li>会用到将来的词，例如如果窗口 windows 为 2，这样训练中心词的词向量时，会选取中心词附近的 4 个上下文词（前面 2 个后面 2 个）</li>
<li>输出层是一个 log-linear 分类器</li>
</ul>
<p>整體的模型參數量為:<script type="math/tex">Q=N×D+D×log(V)</script></p>
<ul>
<li>log(V)是用到了hierarchical softmax + huffman tree</li>
</ul>
<h3 id="Continuous-Skip-gram-Model"><a href="#Continuous-Skip-gram-Model" class="headerlink" title="Continuous Skip-gram Model"></a>Continuous Skip-gram Model</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1593446401/blog_posts/3rwiJtE_t8r2c1.png" alt=""></p>
<p>跟CBOW相似，不過是根據中心的詞去預測上下文</p>
<ul>
<li>通過實驗發現，windows越大效果越好(但cost也越大)</li>
<li>距離較遠的word通常關聯性較小，所以透過抽取較少的樣本(降低機率)來降低對距離較遠word的權重</li>
</ul>
<p>整體複雜度:<script type="math/tex">Q=C×(D+D×log(V))</script></p>
<ul>
<li>C是window size的2倍，也就是要預測的word個數<ul>
<li>也就是說預測每個word所需的參數量是D+D×log(V)</li>
<li>看到logV就知道用到了hierarchical softmax</li>
</ul>
</li>
</ul>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><blockquote>
<p>We follow previous observation that there can be many different types of similarities between words, for example, word big is similar to bigger in the same sense that small is similar to smaller. Example of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further denote two pairs of words with the same relationship as a question, as we can ask: ”What is the word that is similar to small in the same sense as biggest is similar to big?”</p>
</blockquote>
<p>單詞間具有相似性，並且可透過向量加減法來得到語意上的正確結果</p>
<h2 id="延伸討論-Word2vec-implement-detail"><a href="#延伸討論-Word2vec-implement-detail" class="headerlink" title="延伸討論: Word2vec implement detail"></a>延伸討論: Word2vec implement detail</h2><p>下面講的可能不在原始paper內，不過是實作上可能會被提及的技術。根據一些網站資源學習了之後加以整理:</p>
<h3 id="Hidden-layer沒有activation-function"><a href="#Hidden-layer沒有activation-function" class="headerlink" title="Hidden layer沒有activation function"></a>Hidden layer沒有activation function</h3><p>在input-hidden層，沒有非線性變換，而是簡單地把所有vector加總並取平均，以減少計算複雜度</p>
<h3 id="Hierarchical-Softmax-v-s-Negative-sampling"><a href="#Hierarchical-Softmax-v-s-Negative-sampling" class="headerlink" title="Hierarchical Softmax v.s. Negative sampling"></a>Hierarchical Softmax v.s. Negative sampling</h3><p>實際上，input layer有CBOW和Skip-gram兩種版本，output也有Hierarchical Softmax和Negative sampling兩種版本</p>
<h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p>一般正常input-hidden-output的model如果套用在embedding training，因為output層的softmax計算量很大(要去算所有詞的softmax機率，再去找機率最大值)<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1593446400/blog_posts/6m9UYxV_kjxacf.png" alt=""></p>
<ul>
<li>透過huffman tree + Hierarchical Softmax，tree的根節點是每一個word，透過一步步走到leaf來求得softmax的值<ul>
<li>從root到leaf只需要log(V)步</li>
</ul>
</li>
<li>如此可以大幅的加快求得softmax的速度</li>
<li>缺點: 但是如果我们的训练样本里的中心词w是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了<ul>
<li>於是有了Negative Sampling </li>
</ul>
</li>
</ul>
<h4 id="Negative-sampling"><a href="#Negative-sampling" class="headerlink" title="Negative sampling"></a>Negative sampling</h4><p>定義window內的為正樣本，window外的為負樣本，如此就可以不用把全部的word拿進來一起train</p>
<ul>
<li>只需要window內的所有正樣本 + sampling一定數量的負樣本就足夠訓練模型</li>
</ul>
<p>透過Unigram distribution來模擬負樣本(不在window內的word)被選中的機率</p>
<ul>
<li>設計這個分佈時希望詞被抽到的機率要跟這個詞出現的頻率有關，出現在文本中的頻率越高越高越有可能被抽到</li>
</ul>
<p>公式為:<script type="math/tex">P(w_i) = \frac{ {f(w_i)}^{3/4}  }{\sum_{j=0}^{n}\left( {f(w_j)}^{3/4} \right ) }</script><br>$f(w_i)$代表$w_i$出現次數(頻率)，3/4是實驗try出來的數據</p>
<ul>
<li>例如：有一個詞編號是 100，它出現在整個文本中 1000 次，所以 100 在 unigram table 就會出現 $1000^{0.75} = 177$ 次</li>
</ul>
<p>至於要選幾個詞當 negative sample，paper 中建議如下</p>
<blockquote>
<p>Our experiments indicate that values of k in the range 5–20 are useful for small training datasets, while for large datasets the k can be as small as 2–5.</p>
</blockquote>
<h3 id="Subsampling-of-frequent-words"><a href="#Subsampling-of-frequent-words" class="headerlink" title="Subsampling of frequent words"></a>Subsampling of frequent words</h3><p>英文中 “the”, “a”, “in”，中文中的「的」、「是」等等這種詞，其實在句子中並沒有辦法提供太多資訊但又常常出現，對訓練沒有太大幫助，所以就用一個機率來決定這個詞是否要被丟掉，公式如下<script type="math/tex">P(f_i) = (\sqrt{\frac{f(w_i)}{0.001}} + 1) \cdot \frac{0.001}{f(w_i)}</script></p>
<h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>word2vec真滴神，然後skip-gram似乎普遍比CBOW好用</p>
<p>現在有很多方法可以直接使用word2vec，如gensim，不過了解了背後的由來相信能夠對於word2vec更有感覺，以及更清楚應該如何調整相關參數。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://ctjoy.github.io/jekyll/update/2017/10/23/word2vec-tutorial.html" target="_blank" rel="noopener">Word2vec Tutorial</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/7243513.html" target="_blank" rel="noopener">word2vec原理(二) 基于Hierarchical Softmax的模型</a></li>
</ul>

                <hr>
                <!-- Pager -->
                <ul class="pager">
    
    <li class="previous">
        <a href="/posts/aa55d3f9/" data-toggle="tooltip" data-placement="top" title="[論文速速讀]系列文章介紹">&larr; Newer Post</a>
    </li>
    
    
    <li class="next">
        <a href="/posts/aa73b0a2/" data-toggle="tooltip" data-placement="top" title="[論文速速讀]PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization">Older Post &rarr;</a>
    </li>
    
</ul>

                <!-- tip start -->
                

                
                <!-- tip end -->

                <!-- Sharing -->
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Abstract"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Abstract</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Introduction"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">Introduction</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Model-Architectures"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">Model Architectures</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Feedforward-Neural-Net-Language-Model-NNLM"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text">Feedforward Neural Net Language Model (NNLM)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Recurrent-Neural-Net-Language-Model-RNNLM"><span class="toc-nav-number">3.2.</span> <span class="toc-nav-text">Recurrent Neural Net Language Model (RNNLM)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Parallel-Training-of-Neural-Networks"><span class="toc-nav-number">3.3.</span> <span class="toc-nav-text">Parallel Training of Neural Networks</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#New-Log-linear-Models"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">New Log-linear Models</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Continuous-Bag-of-Words-Model"><span class="toc-nav-number">4.1.</span> <span class="toc-nav-text">Continuous Bag-of-Words Model</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Continuous-Skip-gram-Model"><span class="toc-nav-number">4.2.</span> <span class="toc-nav-text">Continuous Skip-gram Model</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Results"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">Results</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#延伸討論-Word2vec-implement-detail"><span class="toc-nav-number">6.</span> <span class="toc-nav-text">延伸討論: Word2vec implement detail</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Hidden-layer沒有activation-function"><span class="toc-nav-number">6.1.</span> <span class="toc-nav-text">Hidden layer沒有activation function</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Hierarchical-Softmax-v-s-Negative-sampling"><span class="toc-nav-number">6.2.</span> <span class="toc-nav-text">Hierarchical Softmax v.s. Negative sampling</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Hierarchical-Softmax"><span class="toc-nav-number">6.2.1.</span> <span class="toc-nav-text">Hierarchical Softmax</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Negative-sampling"><span class="toc-nav-number">6.2.2.</span> <span class="toc-nav-text">Negative sampling</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Subsampling-of-frequent-words"><span class="toc-nav-number">6.3.</span> <span class="toc-nav-text">Subsampling of frequent words</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#結論"><span class="toc-nav-number">7.</span> <span class="toc-nav-text">結論</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#References"><span class="toc-nav-number">8.</span> <span class="toc-nav-text">References</span></a></li></ol>
          
          </div>
        </aside>
      
    

        </div>
    </div>
</article>

	
    <!-- disqus embedded js code start (one page only need to embed once) -->	
    <script type="text/javascript">	
        /* * * CONFIGURATION VARIABLES * * */	
        var disqus_shortname = "meetonfriday";	
        var disqus_identifier = "https://meetonfriday.com/posts/abd81088/";	
        var disqus_url = "https://meetonfriday.com/posts/abd81088/";	
        (function() {	
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;	
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';	
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);	
        })();	
    </script>	
    <!-- disqus embedded js code start end -->	
    

    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/john850512">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank"  href="https://www.linkedin.com/in/john85051232">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/meetonfridayyy">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                    <li>
                        <a target="_blank"  href="https://mail.google.com/mail/?view=cm&fs=1&to=john85051232@gmail.com">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; John 2025 
                    <br>
                    Powered by 
                    <a href="https://github.com/john850512/hexo-theme-jiji" target="_blank" rel="noopener">
                        <i>hexo-theme-jiji</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=john850512&repo=hexo-theme-jiji&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://meetonfriday.com/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>

<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<!--<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>-->
<script async src="//cdn.jsdelivr.net/npm/busuanzi@2.3.0"></script>




	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        
        <!-- background effects end -->
    
    
    <!-- background animation-->>
    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var i=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){i&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,e,a=0;a<r.length;a++)t=r[a],0<=(e=t.getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>

</html>
