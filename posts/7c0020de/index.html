<!DOCTYPE html>
<html lang="zh-TW">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="keyword"  content="programming, study, daily, entertainment">

    <meta property="og:type" content="website" />
    <meta property="og:site_name" content="星期五。見面">
    <meta property="og:locale" content="zh-TW" />
    
    <meta property="og:title" content="[論文速速讀]Deep Residual Learning for Image Recognition" />
    
    
    <meta property="og:description" content="ResNet，ILSVRC 2015年的冠軍，透過有名的Residual block降低了梯度在深層時會gradient vanish的問題，成功的達到了歷代CNN model都不能到的深度(152層)。這篇paper發表於CVPR 2016，搜尋這篇paper，會發現被cite的次數高達了57446次！" />
    <meta name="description" content="ResNet，ILSVRC 2015年的冠軍，透過有名的Residual block降低了梯度在深層時會gradient vanish的問題，成功的達到了歷代CNN model都不能到的深度(152層)。這篇paper發表於CVPR 2016，搜尋這篇paper，會發現被cite的次數高達了57446次！">
    
    
    <meta property="og:image" content="https://res.cloudinary.com/meet-on-friday/image/upload/v1602237599/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%885.58.15_csqgbj.png" />
    

    <link rel="shortcut icon" href="/img/avatar.jpg">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          [論文速速讀]Deep Residual Learning for Image Recognition - MeetonFriday
        
    </title>
    <!-- canonical -->
    
    
      <link rel="canonical" href="https://meetonfriday.com/posts/7c0020de/">
    

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
<link rel="stylesheet" href="/css/dusign-dark.css">

    
<link rel="stylesheet" href="/css/dusign-common-dark.css">

    <!-- 
<link rel="stylesheet" href="/css/font-awesome.css">
 -->
    
<link rel="stylesheet" href="/css/toc.css">


    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <!-- <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css">
    
    <!-- Google Analytics -->
    
      <!-- <script>
          // dynamic User by Hux
          var _gaId = 'UA-163346001-1';
          var _gaDomain = 'auto';
      
          // Originial
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      
          ga('create', _gaId, _gaDomain);
          ga('send', 'pageview');
      </script> -->
      
      <!-- Global site tag (gtag.js) - Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-163346001-1"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
      
        gtag('config', 'UA-163346001-1');
      </script>
      
    <!-- google ad sense-->
    <script data-ad-client="ca-pub-9561340457908416" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="星期五。見面" type="application/atom+xml">
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.5)), url('/img/header_img/header-bg.jpg')
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#deep learning" title="deep learning">deep learning</a>
                            
                              <a class="tag" href="/tags/#paper" title="paper">paper</a>
                            
                        </div>
                        <h1>[論文速速讀]Deep Residual Learning for Image Recognition</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by John on
                            2020-10-09
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">3.8k</span> and
                                Reading Time <span class="post-count">14</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
        
            <div class="waveWrapper">
                <div class="wave wave_before" style="background-image: url('/img/wave-dark.png')"></div>
                <div class="wave wave_after" style="background-image: url('/img/wave-dark.png')"></div>
            </div>
        
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">星期五。見面</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/reading/">Reading</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>

    
    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Container -->

    
        
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">
        
    

                <p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>paper: <a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1512.03385.pdf</a></p>
<p>ResNet，ILSVRC 2015年的冠軍，透過有名的Residual block降低了梯度在深層時會gradient vanish的問題，成功的達到了歷代CNN model都不能到的深度(152層)。</p>
<p>這篇paper發表於CVPR 2016，搜尋這篇paper，會發現被cite的次數高達了<strong>57446</strong>次！</p>
<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/c_scale,w_450/v1602165108/blog_posts/phplBGAjV_n8umpx.jpg" alt=""></p>
<a id="more"></a>
<p>(和之前文章相同，本篇論文導讀主要在架構和classification上，所以沒有提到ResNet在detection的部分，有興趣的讀者可以再自己去看原文。)</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote>
<p>We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.</p>
</blockquote>
<p>提出了一個殘差訓練框架，使得能夠訓練更深的模型。同時他們證明了residual network可以更好地優化和透過達到更深的深度來提高準確度。</p>
<p>最後他們成功達到VGG層數的8倍，也就是152層，並使用了ensemble model的情況下達到了該年的ILVRC第一名。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>近代的研究指出模型的深度是設計時一個非常重要的考量，許多學者們透過較多層數的模型取得了一些較好的研究成果。</p>
<blockquote>
<p>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers?<br>An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [1, 9], which hamper convergence from the beginning</p>
</blockquote>
<p>隨著模型深度變成一個很重要的issue，大家開始思考：</p>
<p><strong>是不是只要簡單的加深層數，就能學習到一個好的網路？</strong></p>
<p>這個問題會牽涉到模型在深層的情況下容易造成gradient vanishing / exploding的問題，在近期的一些研究中，發現可以透過normalization有效的降低這種問題，使得模型仍然能夠有效運作。</p>
<p>當深層模型能夠有效收斂的時候，大家發現了另一個問題：<strong>退化(degradation)，當深度增加的時候，準確度會到達飽和並且快速地下降</strong></p>
<blockquote>
<p>Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error</p>
</blockquote>
<p>意外的是，這種現象並不是overfitting引起的，而是在深層的時候，模型參數的誤差造成的影響會越來越大，如同蝴蝶效應般所導致。</p>
<ul>
<li>就自己的理解來解釋一下這一段，如果是overfitting現象，我們合理的猜測我們應該可以得到一個training loss越來越低但validate loss越來越高的loss曲線</li>
<li>但就下面的實驗我們並沒有發現這件事，loss在train/valid都持續在下降，但明顯就是比20層的高了一個error gap</li>
</ul>
<p>下圖是簡單的用3x3 Conv來進行疊加模型，然後實驗了在20層和56層時的結果，可以看到56層的error都比20層高</p>
<ul>
<li>那個error突然下降的地方並不是這張圖想呈現的重點，不過其實我蠻好奇這個部分的，我猜想可能是因為在訓練一定的epoch後走到了一個山谷導致error瞬間降低很多的緣故</li>
</ul>
<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1601212075/blog_posts/%E6%88%AA%E5%9C%96_2020-09-27_%E4%B8%8B%E5%8D%889.07.22_awlf4w.png" alt=""></p>
<p>這個現象很奇怪ㄋㄟ，為什麼深層的模型error curve硬是多了一個gap？</p>
<p>讓我們從一個簡單的case來思考：</p>
<ol>
<li>首先，先考慮一個較淺層的神經網路架構做為對照組</li>
<li>和一個較深層的神經網路架構</li>
</ol>
<p>該case存在一個solution可以來達成建構這個較深層的架構: <strong>也就是新添加一個identity mapping來增加層數。由於深層的模型具有更大的solution space，理論上他不應該有更高的training error，至少也應該有個跟淺層一樣的training error(也就是identity mapping case)</strong></p>
<ul>
<li>identity mapping到底是什麼概念? 可以想成我新增了一層，但新增一層後output卻和沒新增是相同的(該層單純將資訊完整地往下一層傳遞)，我們可以稱該層是一個identity mapping</li>
</ul>
<p>這個solution搭配實驗顯示了其實在深層模型訓練時<strong>最佳化模型是有一定的困難的</strong>，並且目前的solutions都不太容易解決這個現象。</p>
<p>所以他們就提出Residual block來想辦法解決這個問題。</p>
<h3 id="Residual-learning-framework"><a href="#Residual-learning-framework" class="headerlink" title="Residual learning framework"></a>Residual learning framework</h3><p>Residual block的架構如下：</p>
<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602230221/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%883.56.37_ag3moh.png" alt=""></p>
<p>假設我們的input x經過兩層layer後我們期望他學到的特徵是$H(x)$ (desired underlying mapping)</p>
<p>實際上x經過兩層後不一定能完全學到$H(x)$，我們把理想跟實際上學到的差距稱之Residual</p>
<script type="math/tex; mode=display">Residual = H(x) - x</script><p>我們把Residual用$F(x)$ (residual mapping)來稱呼</p>
<script type="math/tex; mode=display">F(x) = H(x) - x</script><script type="math/tex; mode=display">H(x) = F(x) + x</script><p>所以現在模型的目標變成學習residual mapping了。</p>
<p>…</p>
<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602230683/blog_posts/Li_si_leh_kong_sann_siau__kjp2pp.jpg" alt=""></p>
<p>如果你看到這邊是這個表情的話，下面讓我試著來用白話一點的方式來敘述:</p>
<p>原本我們希望我們的網路層可以學習到我們期望的特徵$H(x)$，不過上面也說了其實學習上是有困難的，尤其在深層網路的時候</p>
<ul>
<li>就像爸爸總是希望你考100分，所以請了一個家教希望能把你交到100分的程度，<strong>那對老師來說他的目標是”把你教到100分”</strong></li>
</ul>
<p>所以我們不如轉換思維，把目標分解，希望添加的網路層只需要學習自身$x$到$H(x)$的差距就好，學習$H(x)-x$，也就是$F(x)$的這個部分，這邊作者假設相較於學習$H(x)$，學習$F(x)$更加容易優化</p>
<ul>
<li>如果你本身就可以考70分了，那新來的家教就可以只需要<strong>把目標放在”把你成績提高30分”就好了</strong></li>
</ul>
<blockquote>
<p>To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.</p>
</blockquote>
<p>值得一提的是，最極端的狀況下，$x$如果已經完美的fit $H(x)$ (也就是說這層什麼都不學就是最佳解時)，那麼$F(x)$當作0即可，因為他根本不用學習任何東西。把$F(x)$當作0比透過一堆非線性層來直接fit $x \rightarrow H(x)$容易多了。</p>
<ul>
<li>也就是如果你本來就可以考100分了，那老師就很開心，直接薪水小偷當起來，他根本不用教你什麼</li>
</ul>
<p>上面介紹完了residual learning framework的部分，在實作上也非常簡單，透過<strong>“shortcut connection”</strong>就可以達成</p>
<ul>
<li>在deep learning框架中簡單的透過<code>torch.add()</code> or <code>tf.add()</code>來完成</li>
<li>shortcut connection幾乎<strong>不會帶來額外的cost</strong>，因為沒有任何新增的參數，頂多也就是那少少的一個法運算</li>
<li>實作上要<strong>注意shape不對等的case</strong>(因為有些層的stride=2會縮小shape)，此時就會搭配萬能的1x1 Conv來解決，下面會提到</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Residual-Representations"><a href="#Residual-Representations" class="headerlink" title="Residual Representations"></a>Residual Representations</h3><p>介紹了一些之前的相關研究，我覺得我上面Residual講得夠清楚了，所以這裡跳過！</p>
<blockquote>
<p>These methods suggest that a good reformulation or preconditioning can simplify the optimization.</p>
</blockquote>
<p>相關related works指出，好的重構或預處理可以簡化最佳化的困難度。</p>
<h3 id="Shortcut-Connection"><a href="#Shortcut-Connection" class="headerlink" title="Shortcut Connection"></a>Shortcut Connection</h3><p>shortcut其實也在不少的研究上被提出過，<strong>用來解決gradient vanishing / exploding的問題。</strong></p>
<ul>
<li>如果還記得<a href="https://meetonfriday.com/posts/263e065d/">GoogleNet</a>的朋友應該可以很直覺地想到他們就是在auxiliary classifiers的設計中就是把中間層的output直接接到classifier上</li>
<li>還有另一個related work是highway networks，一個gated-shortcut機制，有興趣的再去看吧</li>
</ul>
<p>再強調一次，在本文中他們認為深層網路學不好的主因不是gradient vanishing / exploding沒(問題可以透過parameter normalization / initialization來達到一定程度的克服)，但不可否認的是<strong>shutcut的設計本身也可以一定程度的解決該現象</strong>，來看一下梯度的計算：</p>
<p>$H(x) = F(x) + x$，$H(x)$對$x$的梯度變成了</p>
<p>$\frac{\delta{H}}{\delta{x}}$上再額外加上$\frac{\delta{x}}{\delta{x}}=1$的部分</p>
<h2 id="Deep-Residual-Learning"><a href="#Deep-Residual-Learning" class="headerlink" title="Deep Residual Learning"></a>Deep Residual Learning</h2><p>Introduction解釋過的部分這裡就會跳過，主要來介紹網路架構的部分。</p>
<h3 id="Network-Architectures"><a href="#Network-Architectures" class="headerlink" title="Network Architectures"></a>Network Architectures</h3><p>先放架構，網路那麼長截圖很難欸…想看的人請自己放大後把你的頭往右旋轉90度，不然就給我乖乖去看原文</p>
<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/a_90/v1602235479/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%885.24.21_rhhpgw.png" alt=""></p>
<p>為了和ResNet比較，需要有個對照組，也就是Plain Network</p>
<ul>
<li>基於VGG的架構設計，大部分都是3x3 Conv</li>
<li>相同input shape的layer具有相同數量的filter</li>
<li>如果shape減半，則filter size加倍，以保持相同的時間複雜度</li>
<li>最後接個GAP和softmax<ul>
<li>不知道GAP的去看NIN那篇論文吧…什麼你還沒看過NIN這篇論文?! 快去看<a href="https://meetonfriday.com/posts/a151bfa2/">[論文速速讀]Network In Network</a></li>
</ul>
</li>
</ul>
<p>而對於ResNet</p>
<ul>
<li>就是在Plain Network上加上shortcut connection</li>
<li>值得一提的是，<strong>架構中的黑色虛線代表input shape改變了(stride=2)</strong>，此時$x$和$H(x)$會因為input shape(或者說dimension)不同無法直接相加，他們考慮了兩個做法：<ul>
<li>(A) 硬加：dimension不足的地方我補0給你，給我硬加喔</li>
<li>(B) <strong>萬能的1x1 Conv</strong>: 什麼你還沒看過NIN這篇論文?! 快去看<a href="https://meetonfriday.com/posts/a151bfa2/">[論文速速讀]Network In Network</a><ul>
<li>1x1 Conv實作細節可以參考我寫的<a href="https://meetonfriday.com/posts/fb19d450/">[Pytorch]逐步解釋ResNet34程式碼</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>前處理和參數設置的部分，跳過。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="ImageNet-Classification"><a href="#ImageNet-Classification" class="headerlink" title="ImageNet Classification"></a>ImageNet Classification</h3><p>做ImageNet分類時的網路架構圖，這個是用表格呈現的，不用再看到瞎掉</p>
<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602236091/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%885.33.44_amkih1.png" alt=""></p>
<p>這是Top1 error rate<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602236154/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%885.35.24_leo0gt.png" alt=""></p>
<p>這是error curve<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602236327/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%885.38.19_ropcow.png" alt=""></p>
<p>左邊的圖示對照組，右邊是ResNet，細線代表training error而粗線代表valid error，好了我們來看圖說故事：</p>
<ul>
<li>在左邊的圖中，深層網路的error比淺層還高，發現了<strong>模型退化</strong>的問題<ul>
<li>和前面討論過的一樣：在具有較大solution space的深層網路不應該學的比淺層差，在optimization過程中遇到了困難</li>
</ul>
</li>
<li>在右邊的圖中，深層模型的error比淺層低了！代表網路有好好最佳化喔～棒棒噠</li>
</ul>
<p>他們認為這個現象跟vanishing gradients無關，因為他們有加入BN，並且做了一些驗證。實驗數據其實也表明，儘管是plain network，仍然有一定程度的準確率，所以模型仍然是work的。關於造成的原因會在以後繼續研究。</p>
<p>ImageNet講完了，對於其他dataset的實驗就不在提起(我好懶喔…放過我吧QQ)，有興趣的可以參考原文。下面將會講一些和dataset無關的實驗分析。</p>
<h3 id="Deeper-Bottleneck-Architectures"><a href="#Deeper-Bottleneck-Architectures" class="headerlink" title="Deeper Bottleneck Architectures"></a>Deeper Bottleneck Architectures</h3><h4 id="Identity-vs-Projection-Shortcuts"><a href="#Identity-vs-Projection-Shortcuts" class="headerlink" title="Identity vs. Projection Shortcuts"></a>Identity vs. Projection Shortcuts</h4><p>對於shortcut的實作方式他們也比較了三種做法</p>
<ol>
<li>zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameterfree</li>
<li>projection shortcuts are used for increasing dimensions, and other shortcuts are identity</li>
<li>all shortcuts are projections</li>
</ol>
<p>1的方法是我們最熟悉的，也就是沒有新增任何參數的版本，但在實驗中方案3的效果比較好。</p>
<p>儘管在實驗中發現3的效果最好(可能是因為projections額外的參數帶來的好處)，但他們<strong>在後續的實驗還是採用了1的方案，因為沒有額外參數的設計對於接下來要把模型變得更深的實驗非常重要。</strong></p>
<h4 id="Deeper-Bottleneck-Architectures-1"><a href="#Deeper-Bottleneck-Architectures-1" class="headerlink" title="Deeper Bottleneck Architectures"></a>Deeper Bottleneck Architectures</h4><p>接下來他們考慮把模型弄的跟長頸鹿一樣長，可是礙於資源有限，慾望無窮(公民課本有教)，所以他們把ResNet的building block結構改了一下，變成了<strong>bottlenet building block</strong></p>
<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602237599/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%885.58.15_csqgbj.png" alt=""></p>
<p>簡單來說，就是先透過1x1 Conv降維，降低3x3 Conv計算上的負擔，然後再用1x1 Conv升維變回原本的shape。</p>
<ul>
<li>在這種設計下，如果是使用Projection Shortcuts，你會發現shortcut mapping連接的都是高維度的dimension，會有大量的參數需要被計算</li>
</ul>
<p>透過bottlenet building block，ResNet-50、ResNet-101就誕生了，然後還是要來嗆一下VGG: </p>
<p><strong>儘管我們都這麼深了(?)，複雜度仍然比你的VGG16 / VGG19低喔</strong></p>
<p>最後放個實驗結果：<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602238018/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%886.06.46_yr5j8r.png" alt=""></p>
<h3 id="Analysis-of-Layer-Responses"><a href="#Analysis-of-Layer-Responses" class="headerlink" title="Analysis of Layer Responses"></a>Analysis of Layer Responses</h3><p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1602238534/blog_posts/%E6%88%AA%E5%9C%96_2020-10-09_%E4%B8%8B%E5%8D%886.15.22_y3scos.png" alt=""></p>
<p>這張圖顯示了各種模型的response deviations </p>
<ul>
<li>The responses are the outputs of each 3×3 layer, after BN and before other nonlinearity (ReLU/addition)</li>
</ul>
<blockquote>
<p>These results support our basic motivation (Sec.3.1) that the residual functions might be generally closer to zero than the non-residual functions</p>
</blockquote>
<p>可以發現到ResNet系列的response相對較小，應證了之前的假設: residual function比non-residual functions更能學到identify mapping的關係(也就是residual趨近0的效果)</p>
<p>此外，越深層的ResNet有越小的response，代表層數越深的時候，透過resdidual block的機制，越來越多層會將residual mapping的部分降低(也就是越來越多層開始選擇做少少事就好，當個薪水小偷)來控制模型的最佳化機制。</p>
<h3 id="Exploring-Over-1000-layers"><a href="#Exploring-Over-1000-layers" class="headerlink" title="Exploring Over 1000 layers"></a>Exploring Over 1000 layers</h3><p><strong>“好想知道能不能再深入喔&gt;&lt;”</strong></p>
<p>有錢人的幸福就是這麼樸實無華，他們最後設計了一個1202層的ResNet。</p>
<p>結果發現效果比152層的差，他們認為這是overfitting造成的(這麼小的dataset幹嘛用這麼大的模型)</p>
<p>是說…你真的搞出1202層我們也沒辦法用啦..</p>
<h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>哇喔終於打完了，其實這些論文的概念都大概知道，可是要把一篇論文讀完所花的時間還是很久QQ</p>
<p>最後，來做個總結吧！</p>
<p>在ResNet這篇文章中探討了深層神經網路模型會遇到的問題</p>
<ul>
<li>不是梯度爆炸/消失所造成的，儘管確實會影響，但在Batch Normalization等技術出來後該issue被有效的改善</li>
<li>也不是overfitting，因為模型確實有一定程度的效果</li>
<li>他們認為這個現象起因於模型退化(degradation)，在深層模型中造成最佳化的複雜度大大增加</li>
</ul>
<p><strong>透過identity mapping來分解原本要優化的目標</strong>，基於這個想法提出了ResNet，ResNet可以帶來什麼好處？</p>
<ul>
<li><strong>沒有額外參數</strong>: identity mapping只是一個簡單的shortcut connection，沒有任何需要訓練的參數，降低了模型的複雜度並可以訓練更多層</li>
<li><strong>自適應深度</strong>: 根據前面的敘述你會發現，根據$x$的狀況，模型可以動態調整每一層$F(x)$要學習的目標<ul>
<li>如果網路模型夠淺: 那可以讓每一層的模型專注在學習特徵表達，也就是$F(x)$要學到一些representation</li>
<li>如果網路模型很深: 那可以讓某些層當薪水小偷，也就是$F(x)\rightarrow0$，做個identity mapping的功用將$x$往下傳就好。反正深層模型的solution space那麼大，讓其他人做事就好</li>
</ul>
</li>
<li><strong>減輕gradient vanishing / exploding的現象</strong>: 儘管論文說這不是造成深層模型訓練困難的主因，但shortcut機制確實能夠減緩梯度爆炸/消失的問題</li>
</ul>
<p>在模型的實踐上</p>
<ul>
<li>ResNet-18, ResNet-34採用的是一般的building block</li>
<li>ResNet-50, ResNet-101, ResNet-152採用的是bottleneck build block，為了降低模型的複雜度</li>
<li>關於ResNet-34的Pytorch實作分析，可以參考<a href="https://meetonfriday.com/posts/fb19d450/">[Pytorch]逐步解釋ResNet34程式碼</a></li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/54072011?utm_source=com.tencent.tim&amp;utm_medium=social&amp;utm_oi=41268663025664" target="_blank" rel="noopener">【AI Talking】CVPR2016 最佳论文， ResNet 现场演讲
</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/56961832" target="_blank" rel="noopener">ResNet论文笔记及代码剖析</a></li>
<li><a href="https://medium.com/@hupinwei/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-resnet%E4%B9%8B%E6%AE%98%E5%B7%AE%E5%AD%B8%E7%BF%92-f3ac36701b2f" target="_blank" rel="noopener">(深度學習)ResNet之殘差學習</a></li>
</ul>

                <hr>
                <!-- Pager -->
                <ul class="pager">
    
    <li class="previous">
        <a href="/posts/957fed6c/" data-toggle="tooltip" data-placement="top" title="2020台灣設計展 in Hsinchu 參訪心得">&larr; Newer Post</a>
    </li>
    
    
    <li class="next">
        <a href="/posts/39485259/" data-toggle="tooltip" data-placement="top" title="[Linux Kernel慢慢學]探討Designated Initializers">Older Post &rarr;</a>
    </li>
    
</ul>

                <!-- tip start -->
                

                
                <!-- tip end -->

                <!-- Sharing -->
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#前言"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">前言</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Abstract"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">Abstract</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Introduction"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">Introduction</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Residual-learning-framework"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text">Residual learning framework</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Related-Work"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">Related Work</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Residual-Representations"><span class="toc-nav-number">4.1.</span> <span class="toc-nav-text">Residual Representations</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Shortcut-Connection"><span class="toc-nav-number">4.2.</span> <span class="toc-nav-text">Shortcut Connection</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Deep-Residual-Learning"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">Deep Residual Learning</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Network-Architectures"><span class="toc-nav-number">5.1.</span> <span class="toc-nav-text">Network Architectures</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Implementation"><span class="toc-nav-number">5.2.</span> <span class="toc-nav-text">Implementation</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Experiments"><span class="toc-nav-number">6.</span> <span class="toc-nav-text">Experiments</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#ImageNet-Classification"><span class="toc-nav-number">6.1.</span> <span class="toc-nav-text">ImageNet Classification</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Deeper-Bottleneck-Architectures"><span class="toc-nav-number">6.2.</span> <span class="toc-nav-text">Deeper Bottleneck Architectures</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Identity-vs-Projection-Shortcuts"><span class="toc-nav-number">6.2.1.</span> <span class="toc-nav-text">Identity vs. Projection Shortcuts</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Deeper-Bottleneck-Architectures-1"><span class="toc-nav-number">6.2.2.</span> <span class="toc-nav-text">Deeper Bottleneck Architectures</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Analysis-of-Layer-Responses"><span class="toc-nav-number">6.3.</span> <span class="toc-nav-text">Analysis of Layer Responses</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Exploring-Over-1000-layers"><span class="toc-nav-number">6.4.</span> <span class="toc-nav-text">Exploring Over 1000 layers</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#結論"><span class="toc-nav-number">7.</span> <span class="toc-nav-text">結論</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#References"><span class="toc-nav-number">8.</span> <span class="toc-nav-text">References</span></a></li></ol>
          
          </div>
        </aside>
      
    

        </div>
    </div>
</article>

	
    <!-- disqus embedded js code start (one page only need to embed once) -->	
    <script type="text/javascript">	
        /* * * CONFIGURATION VARIABLES * * */	
        var disqus_shortname = "meetonfriday";	
        var disqus_identifier = "https://meetonfriday.com/posts/7c0020de/";	
        var disqus_url = "https://meetonfriday.com/posts/7c0020de/";	
        (function() {	
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;	
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';	
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);	
        })();	
    </script>	
    <!-- disqus embedded js code start end -->	
    

    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/john850512">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank"  href="https://www.linkedin.com/in/john85051232">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/meetonfridayyy">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                    <li>
                        <a target="_blank"  href="https://mail.google.com/mail/?view=cm&fs=1&to=john85051232@gmail.com">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; John 2025 
                    <br>
                    Powered by 
                    <a href="https://github.com/john850512/hexo-theme-jiji" target="_blank" rel="noopener">
                        <i>hexo-theme-jiji</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=john850512&repo=hexo-theme-jiji&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://meetonfriday.com/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>

<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<!--<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>-->
<script async src="//cdn.jsdelivr.net/npm/busuanzi@2.3.0"></script>




	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        
        <!-- background effects end -->
    
    
    <!-- background animation-->>
    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var i=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){i&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,e,a=0;a<r.length;a++)t=r[a],0<=(e=t.getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>

</html>
