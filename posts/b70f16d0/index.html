<!DOCTYPE html>
<html lang="zh-TW">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="keyword"  content="programming, study, daily, entertainment">

    <meta property="og:type" content="website" />
    <meta property="og:site_name" content="星期五。見面">
    <meta property="og:locale" content="zh-TW" />
    
    <meta property="og:title" content="[論文速速讀]Attention is not Explanation" />
    
    
    <meta property="og:description" content="NAACL2019 2019的一篇文章，旨在透過一系列的實驗提出一種新的看法:**attention機制並不是解釋性，我們不能用attention來說作為XAI的技術**。這其實是個蠻特別的觀點，Attention自從2016年崛起後，一堆研究都是基於他而擴展的，現在這篇論文出來卻是打臉了attention的可解釋性。" />
    <meta name="description" content="NAACL2019 2019的一篇文章，旨在透過一系列的實驗提出一種新的看法:**attention機制並不是解釋性，我們不能用attention來說作為XAI的技術**。這其實是個蠻特別的觀點，Attention自從2016年崛起後，一堆研究都是基於他而擴展的，現在這篇論文出來卻是打臉了attention的可解釋性。">
    
    
    <meta property="og:image" content="https://res.cloudinary.com/meet-on-friday/image/upload/v1591634093/blog_posts/01_gwf9vq.png" />
    

    <link rel="shortcut icon" href="/img/avatar.jpg">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          [論文速速讀]Attention is not Explanation - MeetonFriday
        
    </title>
    <!-- canonical -->
    
    
      <link rel="canonical" href="https://meetonfriday.com/posts/b70f16d0/">
    

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
<link rel="stylesheet" href="/css/dusign-dark.css">

    
<link rel="stylesheet" href="/css/dusign-common-dark.css">

    <!-- 
<link rel="stylesheet" href="/css/font-awesome.css">
 -->
    
<link rel="stylesheet" href="/css/toc.css">


    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <!-- <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">-->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css">
    
    <!-- Google Analytics -->
    
      <!-- <script>
          // dynamic User by Hux
          var _gaId = 'UA-163346001-1';
          var _gaDomain = 'auto';
      
          // Originial
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      
          ga('create', _gaId, _gaDomain);
          ga('send', 'pageview');
      </script> -->
      
      <!-- Global site tag (gtag.js) - Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-163346001-1"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
      
        gtag('config', 'UA-163346001-1');
      </script>
      
    <!-- google ad sense-->
    <script data-ad-client="ca-pub-9561340457908416" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="星期五。見面" type="application/atom+xml">
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.5)), url('/img/header_img/header-bg.jpg')
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#deep learning" title="deep learning">deep learning</a>
                            
                              <a class="tag" href="/tags/#paper" title="paper">paper</a>
                            
                        </div>
                        <h1>[論文速速讀]Attention is not Explanation</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by John on
                            2020-06-09
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">2.8k</span> and
                                Reading Time <span class="post-count">11</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
        
            <div class="waveWrapper">
                <div class="wave wave_before" style="background-image: url('/img/wave-dark.png')"></div>
                <div class="wave wave_after" style="background-image: url('/img/wave-dark.png')"></div>
            </div>
        
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">星期五。見面</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/reading/">Reading</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>

    
    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Container -->

    
        
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">
        
    

                <p>〖想觀看更多中文論文導讀，至<a href="https://meetonfriday.com/posts/aa55d3f9/">[論文速速讀]系列文章介紹</a>可以看到目前已發布的所有文章！〗</p>
<p>NAACL2019 2019的一篇文章，旨在透過一系列的實驗提出一種新的看法:<strong>attention機制並不是解釋性，我們不能用attention來說作為XAI的技術</strong>。這其實是個蠻特別的觀點，Attention自從2016年崛起後，一堆研究都是基於他而擴展的，現在這篇論文出來卻是打臉了attention的可解釋性。</p>
<p>…所以以後不能再用attention來做XAI(Explainable AI)了嗎? QQ饅頭喔</p>
<p>也別難過得太早，同一年又有另一篇論文跳出來了，EMNLP 2019的”Attention is not not Explanation”，看這個標題就知道擺明就是要跟這篇對著幹，所以先看完這篇再去看如何被反駁好像也是蠻有趣的? </p>
<p>題外話，我是站Attention is not not Explanation這邊，希望那篇可以狠狠打臉這篇xD</p>
<a id="more"></a>
<p>論文網址: <a href="https://arxiv.org/pdf/1902.10186.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1902.10186.pdf</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote>
<p>In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs.<br>In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations” for predictions</p>
</blockquote>
<p>attention雖然已經被證實可以提升效果，但是很多人也同時把他吹捧成具有<strong>透明性(transparency)</strong>，可以幫助解釋深度學習這個黑盒子。</p>
<p>這篇的作者認為，attention跟model之間的關係其實尚不明朗，不准你這麼說!</p>
<p>所以他們設計了一系列的NLP實驗來度量attention對於模型預測時提供的解釋性分數，然後他們發現:</p>
<ol>
<li>注意力的權重和使用gradient-based來評估模型好壞的方法其實沒有甚麼相關性</li>
<li>不同的注意力分布可以產生相同的預測效果(如果注意力是指模型的重要性的話，那為何不同的重要性分布可以有一樣的效果呢?)</li>
</ol>
<p>所以，他們認為不能也不應該把attention當作提供解釋性的方法(不過本質上他們並沒有否定attention唷)。</p>
<p>Github code的部分在: <a href="https://github.com/successar/AttentionExplanation" target="_blank" rel="noopener">https://github.com/successar/AttentionExplanation</a></p>
<h2 id="Introduction-and-Motivation"><a href="#Introduction-and-Motivation" class="headerlink" title="Introduction and Motivation"></a>Introduction and Motivation</h2><blockquote>
<p>Li et al. (2016) summarized this commonly held view in NLP: “Attention provides an important way to explain the workings of neural models”. Indeed, claims that attention provides interpretability are common in the literature</p>
</blockquote>
<p>2016年後，普遍認為attention是可以來幫助解釋模型運作的一種注意力機制。這項共識來自於一個假設:</p>
<p><strong>注意力權重高的feature比較大的程度影響了模型的output，所以我們可以說他是這個task的重要特徵</strong></p>
<p>不過這項假設實際上並沒有正式的被評估過。所以就有了這篇。</p>
<blockquote>
<p>Assuming attention provides a faithful explanation for model predictions, we might expect the following properties to hold.</p>
<ol>
<li>Attention weights should correlate with feature importance<br>measures (e.g., gradient-based measures); </li>
<li>Alternative (or counterfactual) attention weight configurations ought to yield corresponding changes in prediction (and if they do not then are equally plausible as explanations). </li>
</ol>
</blockquote>
<p>假設注意力機制可以為模型提供合理的解釋性，那他們期望下列的幾個性質也是同時存在的:</p>
<ol>
<li><strong>attention weight應該跟feature important measures(例如gradient-based的方法)具有相關性</strong></li>
<li><strong>不同的attention weight分布應該會造成不同的prediction，因為注意的地方不同了。反之，如果不會造成不同的prediction，則可以用來當作他是解釋性的一種依據</strong></li>
</ol>
<p>然後他們說他們在很多種NLP task上做實驗，可是卻沒發現上述這兩種現象:&lt;</p>
<p>接下來他們用了一個例子來描述2的現象:<br>使用了一個標準具有attention的BiLSTM網路來做情感分析，模型原始的attention distribution $\alpha$下發現asking, waste是造成負面情緒($y=0.01$)的重要token。</p>
<ol>
<li>他們發現這樣子的attention weight和gradient-based measures of feature importance的相關性非常的小</li>
<li>他們另外造了一個分布$\tilde{\alpha}$(此時重要的token變成了myself, was)卻能得到一樣的prediction。如果更換其他分布對於預測的準確度也不會降低太多(只有0.006的誤差)</li>
</ol>
<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591634093/blog_posts/01_gwf9vq.png" alt=""></p>
<blockquote>
<p>We thus caution against using attention weights to highlight input tokens “responsible for” model outputs and onstructing just-so stories on this basis.</p>
</blockquote>
<p>基於上述的發現，他們告誡大家不要再把attention當作解釋性來用惹。</p>
<h3 id="Research-questions-and-contributions"><a href="#Research-questions-and-contributions" class="headerlink" title="Research questions and contributions"></a>Research questions and contributions</h3><blockquote>
<p>We investigate whether this holds across tasks by exploring the following empirical questions.</p>
<ol>
<li>To what extent do induced attention weights correlate with measures of feature importance – specifically, those resulting from gradients and leave-one-out (LOO) methods?</li>
<li>Would alternative attention weights (and hence distinct heatmaps/“explanations”) necessarily yield different predictions?</li>
</ol>
</blockquote>
<p>作者通過以上的問題設置來進行多個NLP的實驗</p>
<ol>
<li>attention weight和使用gradient-based/LOO得出的feature importance之間的相關性?</li>
<li>不同的attention weight是否會導致不同的prediction?</li>
</ol>
<h2 id="Datasets-and-Tasks"><a href="#Datasets-and-Tasks" class="headerlink" title="Datasets and Tasks"></a>Datasets and Tasks</h2><p>在下面三個領域、多個資料集上來驗證上面的這兩個問題</p>
<ol>
<li>binary text classification</li>
<li>Question Answering (QA)</li>
<li>Natural Language Inference (NLI)</li>
</ol>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>實驗的詳細結果的detail可以在這個網站上看到: <a href="https://successar.github.io/AttentionExplanation/docs/" target="_blank" rel="noopener">Attention is not Explanation — Results</a></p>
<h3 id="Correlation-Between-Attention-and-Feature-Importance-Measures"><a href="#Correlation-Between-Attention-and-Feature-Importance-Measures" class="headerlink" title="Correlation Between Attention and Feature Importance Measures"></a>Correlation Between Attention and Feature Importance Measures</h3><p>比較了1. gradient-based 2. leave one out(LOO)的相關性，algo如下圖所示</p>
<ul>
<li>gradient-based的就是根據backward去回推gradient</li>
<li>LOO就是看移除某個token $t$時，prediction的TVD與attention weight之間的差異</li>
</ul>
<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591638767/blog_posts/01_yigug7.png" alt=""></p>
<blockquote>
<p>Note that we disconnect the computation graph at the attention module so that the gradient does not flow through this layer: This means the gradients tell us how the prediction changes as a function of inputs, keeping the attention distribution fixed.</p>
</blockquote>
<p>值得注意的地方是，<strong>他們把attention layer的computer graph斷開了，也就是說在做gradient backward的時候gradient並不會經過這一層</strong></p>
<ul>
<li>這裡指的是說，forward照做，可是backward的時候attention layer的前一層gradient會直接傳到attention layer的後一層，attention layer這一層並不會去參與到backward的計算</li>
<li>作者說這樣做是想要考慮同樣具有attention機制下，gradient-based的評估方法跟attention weight的相關性</li>
</ul>
<p>詳細的結果如下表所示:</p>
<p><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591640094/blog_posts/01_nivwxo.png" alt=""></p>
<p>可以看到有三個欄位，分別是:</p>
<ol>
<li>BiLSTM下gradient-based方法跟attention weight的相關性<ul>
<li>BiRNN + attention</li>
</ul>
</li>
<li>Average方法的gradient-based方法跟attention weight的相關性<ul>
<li>將token經過linear projection layer(以及ReLU)的encoding + attention</li>
</ul>
</li>
<li>BiLSTM下的LOO方法跟attention weight的相關性</li>
</ol>
<p>可以看到，BiLSTM下的相關性大多都低於0.5，除了一些dataset(Diabetes和一些QA corpora)相關性相對比較大。作者認為因為這些dataset提供了足夠大的樣本數，這可以讓gradient-based和attention的相關性好一點(不過還是相對差於其他的方法)</p>
<p>再來，數據發現基於簡單的linear projection embedding的model和gradient-based的相關性甚至還比較大。他們假設問題出在BiRNN會看過所有的input然後才產生hidden state，所以這個hidden state包含了整體的information。</p>
<p>接下來他們又去分析了gradient-based和LOO的相關性(這裡沒放圖，可以對照論文的Fig.3-Fig.5)，發現gradient-based和LOO的相關性確實比attention來的高</p>
<p>細節可以再去看原文，不過這一章節主要就是反應了作者的第一個問題，也就是，attention和gradient-based的相關性其實並不高。並且用單純的方法來做embedding+attention反倒比用什麼BiRNN得到的相關性還高。這些數據讓人重新思考注意力到底具不具有解釋性。</p>
<h3 id="Counterfactual-Attention-Weights"><a href="#Counterfactual-Attention-Weights" class="headerlink" title="Counterfactual Attention Weights"></a>Counterfactual Attention Weights</h3><p>接下來他們想要看的是，不同的注意力分佈是否能夠產生出相同的預測？</p>
<p>因為如果按照”注意力機制是解釋性”的想法，作者認為不同的分佈會對於模型的輸出產生不同的結果。</p>
<blockquote>
<p>We experiment with two means of constructing such distributions. </p>
<ol>
<li>First, we simply scramble the original attention weights $\hat{\alpha}$, re-assigning each value to an arbitrary, randomly sampled index (input feature). </li>
<li>Second, we generate an adversarial attention distribution: this is a set of attention weights that is maximally distinct from $\hat{\alpha}$ but that nonetheless yields an equivalent prediction (i.e., prediction within some of $\hat{y}$)</li>
</ol>
</blockquote>
<p>第一個測驗方法是單純的打亂注意力的排列順序，演算法如下:<br><img src="https://meetonfriday.com/img/lazy-loading-animation.gif" data-original="https://res.cloudinary.com/meet-on-friday/image/upload/v1591688571/blog_posts/%E8%9E%A2%E5%B9%95%E5%BF%AB%E7%85%A7_2020-06-09_%E4%B8%8B%E5%8D%883.41.19_emk8jt.png" alt=""></p>
<p>第二種為產生一組<strong>adversarial attention distribution $\alpha$</strong>(在能夠產生相同prediction下與原本分布差異最大的分佈)，產生的方法主要是解一個最佳化問題</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\underset{\alpha^{(1)}, \ldots, \alpha^{(k)}}{\operatorname{maximize}} & f\left(\left\{\alpha^{(i)}\right\}_{i=1}^{k}\right) \\
\text { subject to } & \forall i \operatorname{TVD}\left[\hat{y}\left(\mathbf{x}, \alpha^{(i)}\right), \hat{y}(\mathbf{x}, \hat{\alpha})\right] \leq \epsilon
\end{array}</script><p>其中， <script type="math/tex">f\left(\left\{\alpha^{(i)}\right\}_{i=1}^{k}\right)</script> 為:</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{k} \operatorname{JSD}\left[\alpha^{(i)}, \hat{\alpha}\right]+\frac{1}{k(k-1)} \sum_{i=1} \operatorname{JSD}\left[\alpha^{(i)}, \alpha^{(j)}\right]</script><p>兩個子實驗的Detail我就不講了，自己去看，不過大致上就是作者發現可以透過不同的attention distribution來產生出相同的prediction</p>
<h2 id="Discussion-and-Conclusions"><a href="#Discussion-and-Conclusions" class="headerlink" title="Discussion and Conclusions"></a>Discussion and Conclusions</h2><p>作者透過一系列的實驗來驗證提出的兩個問題:</p>
<ol>
<li>attention跟gradient-based相關性其實並不高</li>
<li>不同的attention distribution可以有相同的prediction</li>
</ol>
<p>並用了這兩個來結論說，他們認為attention儘管可以幫助模型提升效能，但他背後的機制目前尚未明朗，我們不能把它當作Explainable的技術來看</p>
<p>再來，這篇文章有一些<strong>important limitations</strong></p>
<ol>
<li>使用gradient-based的方法來比較相關性，這其實是把gradient-based當作ground truth來看了，但其實他們也不能完全代表真正的可解釋性</li>
<li>要如何評估attention和gradient-based的相關性要多少才叫好？其實誰也不知道</li>
<li>評估標準使用了Kendall $\tau$ measure，這個方法再有不相關的特徵時會因噪聲而帶給了不夠精準的評估</li>
<li>實際上這篇文章只在少部分的attention varients上進行實驗(在文章中只用了BiLSTM)</li>
<li>儘管不同的attention distribution可以產生相同的prediction，但他們不否認<strong>可能同時存在多種解釋性</strong>，也就是說模型今天可能透過不同的注意力組合來得到相同的推論。</li>
<li>最後，目前只在分類任務上做實驗，他們把其他任務當作future work了(ㆆᴗㆆ)</li>
</ol>
<h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>雖然論文落落長，不過主要就是瘋狂做實驗來驗證兩個問題:</p>
<ol>
<li><strong>attention weight是否跟feature important measures(例如gradient-based的方法)具有相關性，如果有的話則可以說明具有解釋性</strong></li>
<li><strong>不同的attention weight分布應該會造成不同的prediction，因為注意的地方不同了；反之，如果不會造成不同的prediction，則可以用來當作他是解釋性的一種依據</strong></li>
</ol>
<p>如果這兩個性質不存在，作者認為就不能將attention作為解釋性的一種技術依據。</p>
<p>不過其實後來在”Attention is not not Explanation”中，有對這篇的論點還有做實驗的方法提出了一些觀點，認為這篇的論點還不足以說attention不具解釋性(注意我的用詞，<strong>並不是說他們證明了attention具有解釋性而是說認為attention不能說不具有解釋性喔</strong>…我沒有在玩繞口令)。</p>
<p>兩篇都看完的話，你到底站哪一邊呢~</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://www.zhihu.com/question/314463239" target="_blank" rel="noopener">如何评价NAACL2019 paper：Attention is not Explanation?</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2019-09-07-4" target="_blank" rel="noopener">注意力机制不能提高模型可解释性？不，你这篇论文搞错了</a></li>
</ul>

                <hr>
                <!-- Pager -->
                <ul class="pager">
    
    <li class="previous">
        <a href="/posts/c41c815d/" data-toggle="tooltip" data-placement="top" title="[DL]深入了解gradient descent">&larr; Newer Post</a>
    </li>
    
    
    <li class="next">
        <a href="/posts/5a78c86/" data-toggle="tooltip" data-placement="top" title="新竹租屋經驗談">Older Post &rarr;</a>
    </li>
    
</ul>

                <!-- tip start -->
                

                
                <!-- tip end -->

                <!-- Sharing -->
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Abstract"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Abstract</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Introduction-and-Motivation"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">Introduction and Motivation</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Research-questions-and-contributions"><span class="toc-nav-number">2.1.</span> <span class="toc-nav-text">Research questions and contributions</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Datasets-and-Tasks"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">Datasets and Tasks</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Experiments"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">Experiments</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Correlation-Between-Attention-and-Feature-Importance-Measures"><span class="toc-nav-number">4.1.</span> <span class="toc-nav-text">Correlation Between Attention and Feature Importance Measures</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#Counterfactual-Attention-Weights"><span class="toc-nav-number">4.2.</span> <span class="toc-nav-text">Counterfactual Attention Weights</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Discussion-and-Conclusions"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">Discussion and Conclusions</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#結論"><span class="toc-nav-number">6.</span> <span class="toc-nav-text">結論</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#References"><span class="toc-nav-number">7.</span> <span class="toc-nav-text">References</span></a></li></ol>
          
          </div>
        </aside>
      
    

        </div>
    </div>
</article>

	
    <!-- disqus embedded js code start (one page only need to embed once) -->	
    <script type="text/javascript">	
        /* * * CONFIGURATION VARIABLES * * */	
        var disqus_shortname = "meetonfriday";	
        var disqus_identifier = "https://meetonfriday.com/posts/b70f16d0/";	
        var disqus_url = "https://meetonfriday.com/posts/b70f16d0/";	
        (function() {	
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;	
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';	
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);	
        })();	
    </script>	
    <!-- disqus embedded js code start end -->	
    

    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/john850512">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank"  href="https://www.linkedin.com/in/john85051232">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/meetonfridayyy">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                    <li>
                        <a target="_blank"  href="https://mail.google.com/mail/?view=cm&fs=1&to=john85051232@gmail.com">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                
                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; John 2025 
                    <br>
                    Powered by 
                    <a href="https://github.com/john850512/hexo-theme-jiji" target="_blank" rel="noopener">
                        <i>hexo-theme-jiji</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=john850512&repo=hexo-theme-jiji&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://meetonfriday.com/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>

<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<!--<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>-->
<script async src="//cdn.jsdelivr.net/npm/busuanzi@2.3.0"></script>




	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        
        <!-- background effects end -->
    
    
    <!-- background animation-->>
    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var i=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){i&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,e,a=0;a<r.length;a++)t=r[a],0<=(e=t.getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>

</html>
